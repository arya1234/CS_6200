{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval On Summarized Text And Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import original and summarized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "read_file_path = 'output_led_large_text_sum.txt'\n",
    "imported_data = {}\n",
    "# Import from JSON file\n",
    "with open(read_file_path, 'r') as json_file:\n",
    "    imported_data = json.load(json_file)\n",
    "\n",
    "read_file_path = 'data.json'\n",
    "imported_data2 = []\n",
    "with open(read_file_path, 'r') as json_file:\n",
    "    imported_data2 = json.load(json_file)\n",
    "\n",
    "read_file_path = 'pegasus.txt'\n",
    "pegasus_imported_data = []\n",
    "with open(read_file_path, 'r') as json_file:\n",
    "    pegasus_imported_data = json.load(json_file)\n",
    "\n",
    "read_file_path = 'distil_Bart.txt'\n",
    "distilbart_imported_data = []\n",
    "with open(read_file_path, 'r') as json_file:\n",
    "    distilbart_imported_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus of document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_led_large = [doc for id, doc in imported_data.items()]\n",
    "corpus_pegasus = [doc for id, doc in pegasus_imported_data.items()]\n",
    "corpus_distilbart = [doc for id, doc in distilbart_imported_data.items()]\n",
    "\n",
    "original_corpus = [doc for doc in imported_data2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'query': 'Quantum computing algorithms overview', 'narrative': 'Relevant documents must contain information about various algorithms used in quantum computing. These algorithms can run on a quantum computer. Documents on that do not mention any of the quantum algorithms like Deutsch–Jozsa algorithm, Bernstein-Vazirani algorithm, Simon’s algorithm, Quantum phase estimation algorithm, Shor’s algorithm, Grover’s algorithm, etc. should be considered irrelevant.', 'documents': [{'title': 'Quantum algorithm', 'url': 'https://en.wikipedia.org/wiki/Quantum_algorithm', 'relevance_score': 4}, {'title': 'Algorithm', 'url': 'https://en.wikipedia.org/wiki/Quantum_computing', 'relevance_score': 0}, {'title': 'Shor’s algorithm', 'url': 'https://en.wikipedia.org/wiki/Grover%27s_algorithm', 'relevance_score': 3}, {'title': 'Grover’s algorithm', 'url': 'https://en.wikipedia.org/wiki/Shor%27s_algorithm', 'relevance_score': 3}, {'title': 'Deutsch–Jozsa algorithm', 'url': 'https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm', 'relevance_score': 3}, {'title': 'Quantum computing', 'url': 'https://en.wikipedia.org/wiki/Quantum_computing', 'relevance_score': 2}, {'title': 'Quantum mechanics', 'url': 'https://en.wikipedia.org/wiki/Quantum_mechanics', 'relevance_score': 0}]}\n"
     ]
    }
   ],
   "source": [
    "read_file_path = 'sample-annotations.json'\n",
    "annotated_data = []\n",
    "# Import from JSON file\n",
    "with open(read_file_path, 'r') as json_file:\n",
    "    annotated_data = json.load(json_file)\n",
    "\n",
    "queries = annotated_data['queries'];\n",
    "print(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/meet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  # Add this import statement\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    clean_text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # Tokenize and remove non-alphanumeric characters, convert to lowercase\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    clean_text = ' '.join([word for word in tokens if word.isalnum()]).lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    clean_text = ' '.join([word for word in clean_text.split() if word not in stop_words])\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BM25 scores for all queries\n",
    "def get_query_results(model, queries):\n",
    "    query_results = []\n",
    "    for query in queries:\n",
    "        tokenized_query = query['query'].lower().split() # tokenize query\n",
    "        doc_scores = model.get_scores(tokenized_query) # get BM25 scores\n",
    "        query_results.append({\n",
    "            'query_id': query['id'],\n",
    "            'query': query['query'],\n",
    "            'scores': [{ \n",
    "                'id': i + 1,\n",
    "                'score': doc_scores[i],\n",
    "                'title': original_corpus[i]['title'],\n",
    "            } for i in range(len(doc_scores))] # add document id, score, and title\n",
    "        })\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docid, title apping for documents\n",
    "doc_mappings = [(doc['id'], doc['title']) for doc in original_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average precision for a query\n",
    "def get_average_precision(relevant_docs, retrieved_docs):\n",
    "    # Calculate precision for each retrieved document\n",
    "    precisions = []\n",
    "    total_relevant_retrieved = 0\n",
    "    # Calculate precision for each relevant retrieved document\n",
    "    for i in range(len(retrieved_docs)):\n",
    "        if retrieved_docs[i] in relevant_docs:\n",
    "            total_relevant_retrieved += 1\n",
    "            precisions.append(total_relevant_retrieved / (i + 1))\n",
    "    # Calculate average precision\n",
    "    return sum(precisions) / len(precisions) if len(precisions) > 0 else 0;\n",
    "\n",
    "def get_average_mrr(relevant_docs, retrieved_docs):\n",
    "    # Calculare mean reciprocal rank\n",
    "    for i in range(len(retrieved_docs)):\n",
    "        if retrieved_docs[i] in relevant_docs:\n",
    "            return 1 / (i + 1)\n",
    "    return 0;\n",
    "\n",
    "# get average precision for all queries\n",
    "def get_mean_average_precision(queries, results, doc_mappings, eval_func=get_average_precision, k=10):\n",
    "    eval_scores = []\n",
    "    # Loop through each query\n",
    "    for query_data in queries:\n",
    "        query = query_data['query']\n",
    "        annotated_documents = query_data['documents']\n",
    "\n",
    "        # Extract true labels and predicted scores\n",
    "        annotated_documents = [{\n",
    "            'title': doc['title'],\n",
    "            'relevance_score': doc['relevance_score']\n",
    "        } for doc in annotated_documents]\n",
    "        annotated_documents = sorted(annotated_documents, key=lambda k: k['relevance_score'], reverse=True)\n",
    "\n",
    "        # Rank documents based on your retrieval model\n",
    "        current_result = [result for result in results if result['query'] == query][0] # Get scores for query\n",
    "        ranked_documents = current_result['scores'] # Get ranked documents with scores\n",
    "        ranked_documents = sorted(ranked_documents, key=lambda k: k['score'], reverse=True) # Sort documents by score (highest to lowest)\n",
    "\n",
    "        # Get true labels\n",
    "        relevant_docs = []\n",
    "        for doc in annotated_documents:\n",
    "            if doc['relevance_score'] > 0: # If document is relevant\n",
    "                relevant_docs.append(list(filter(lambda x: x[1] == doc['title'], doc_mappings))[0][0]) # Add document id to relevant documents\n",
    "\n",
    "        predicted_scores = [doc['id'] for doc in ranked_documents][:k]\n",
    "\n",
    "        eval_scores.append(eval_func(relevant_docs, predicted_scores))\n",
    "\n",
    "    # Calculate metrics\n",
    "    map_score = sum(eval_scores) / len(eval_scores)\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_dcg_at_k(queries, results, doc_mappings, k = 10):\n",
    "    eval_scores = []\n",
    "    # Loop through each query\n",
    "    for query_data in queries:\n",
    "        query = query_data['query']\n",
    "        annotated_documents = query_data['documents']\n",
    "\n",
    "        # Extract true labels and predicted scores\n",
    "        annotated_documents = [{\n",
    "            'title': doc['title'],\n",
    "            'relevance_score': doc['relevance_score']\n",
    "        } for doc in annotated_documents]\n",
    "        annotated_documents = sorted(annotated_documents, key=lambda k: k['relevance_score'], reverse=True)\n",
    "\n",
    "        # Rank documents based on your retrieval model\n",
    "        current_result = [result for result in results if result['query'] == query][0] # Get scores for query\n",
    "        ranked_documents = current_result['scores'] # Get ranked documents with scores\n",
    "        ranked_documents = sorted(ranked_documents, key=lambda k: k['score'], reverse=True) # Sort documents by score (highest to lowest)\n",
    "        top_k_ranked_documents = ranked_documents[:k]\n",
    "\n",
    "        actual_dcg_scores = []\n",
    "        prev_sum = 0\n",
    "        for i, doc in enumerate(top_k_ranked_documents, 1):\n",
    "            annotated_score = list(filter(lambda x: x['title'] == doc['title'], annotated_documents))[0]['relevance_score'] if doc['title'] in [doc['title'] for doc in annotated_documents] else 0\n",
    "            if i == 1:\n",
    "                actual_dcg_scores.append(annotated_score)\n",
    "                prev_sum = annotated_score\n",
    "            else:\n",
    "                prev_sum += annotated_score / np.log2(i);\n",
    "                actual_dcg_scores.append(prev_sum)\n",
    "        \n",
    "        ideal_dcg_scores = []\n",
    "        prev_sum = 0\n",
    "        for i, doc in enumerate(annotated_documents, 1):\n",
    "            if i == 1:\n",
    "                ideal_dcg_scores.append(doc['relevance_score'])\n",
    "                prev_sum = doc['relevance_score']\n",
    "            else:\n",
    "                prev_sum += doc['relevance_score'] / np.log2(i);\n",
    "                ideal_dcg_scores.append(prev_sum)\n",
    "\n",
    "        eval_scores.append(actual_dcg_scores[-1] / ideal_dcg_scores[-1])\n",
    "\n",
    "    # Calculate metrics\n",
    "    dcg_score = sum(eval_scores) / len(eval_scores)\n",
    "    return dcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP, MRR, and NDCG of BM25 model on corpus of documents summarized with Led Large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Led Large), Retrieval model: BM25): 0.8996\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Led Large), Retrieval model: BM25: 0.9600\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Led Large), Retrieval model: BM25: 0.8105\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAP and MRR score for summarization with Led Large model on BM25\n",
    "clean_corpus_led_large = [clean_text(doc) for doc in corpus_led_large] # clean corpus\n",
    "tokenized_corpus_led_large = [doc.split() for doc in clean_corpus_led_large] # tokenize corpus\n",
    "bm25_led_large = BM25Okapi(tokenized_corpus_led_large) # initialize BM25 model\n",
    "results_led_large = get_query_results(bm25_led_large, queries) # get BM25 scores for all queries\n",
    "\n",
    "map_score_led_large = get_mean_average_precision(queries, results_led_large, doc_mappings, get_average_precision) # get MAP score\n",
    "mrr_score_led_large = get_mean_average_precision(queries, results_led_large, doc_mappings, get_average_mrr) # get MRR score \n",
    "dcg_score_led_large = get_mean_dcg_at_k(queries, results_led_large, doc_mappings, 10) # get DCG score\n",
    "\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Led Large), Retrieval model: BM25): {map_score_led_large:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Led Large), Retrieval model: BM25: {mrr_score_led_large:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Led Large), Retrieval model: BM25: {dcg_score_led_large:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP, MRR, and NDCG of BM25 model on corpus of documents summarized with Pegasus model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Pegasus), Retrieval model: BM25: 0.8049\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Pegasus), Retrieval model: BM25: 0.8667\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Pegasus), Retrieval model: BM25: 0.7068\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAP and MRR score for summarization with Pegasus on BM25\n",
    "clean_corpus_pegasus = [clean_text(doc) for doc in corpus_pegasus] # clean corpus\n",
    "tokenized_corpus_pegasus = [doc.split() for doc in clean_corpus_pegasus] # tokenize corpus\n",
    "bm25_pegasus = BM25Okapi(tokenized_corpus_pegasus) # initialize BM25 model\n",
    "results_pegasus = get_query_results(bm25_pegasus, queries) # get BM25 scores for all queries\n",
    "\n",
    "map_score_pegasus = get_mean_average_precision(queries, results_pegasus, doc_mappings, get_average_precision) # get MAP score\n",
    "mrr_score_pegasus = get_mean_average_precision(queries, results_pegasus, doc_mappings, get_average_mrr) # get MRR score\n",
    "dcg_score_pegasus = get_mean_dcg_at_k(queries, results_pegasus, doc_mappings, 10) # get DCG score\n",
    "\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Pegasus), Retrieval model: BM25: {map_score_pegasus:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Pegasus), Retrieval model: BM25: {mrr_score_pegasus:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Pegasus), Retrieval model: BM25: {dcg_score_pegasus:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP, MRR, and NDCG of BM25 model on corpus of documents summarized with Distil BART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Distil Bart), Retrieval model: BM25: 0.8946\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Distil Bart), Retrieval model: BM25: 0.9800\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Distil Bart), Retrieval model: BM25: 0.8469\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAP and MRR score for summarization with Distil Bart on BM25\n",
    "clean_corpus_distilbart = [clean_text(doc) for doc in corpus_distilbart] # clean corpus\n",
    "tokenized_corpus_distilbart = [doc.split() for doc in clean_corpus_distilbart] # tokenize corpus\n",
    "bm25_distilbart = BM25Okapi(tokenized_corpus_distilbart) # initialize BM25 model\n",
    "results_distilbart = get_query_results(bm25_distilbart, queries) # get BM25 scores for all queries\n",
    "\n",
    "map_score_distilbart = get_mean_average_precision(queries, results_distilbart, doc_mappings, get_average_precision) # get MAP score\n",
    "mrr_score_distilbart = get_mean_average_precision(queries, results_distilbart, doc_mappings, get_average_mrr) # get MRR score\n",
    "dcg_score_distilbart = get_mean_dcg_at_k(queries, results_distilbart, doc_mappings, 10) # get DCG score\n",
    "\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Distil Bart), Retrieval model: BM25: {map_score_distilbart:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Distil Bart), Retrieval model: BM25: {mrr_score_distilbart:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Distil Bart), Retrieval model: BM25: {dcg_score_distilbart:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Model Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_bm25(queries, corpus, doc_mappings, k1_values, b_values):\n",
    "    best_score = 0\n",
    "    best_k1 = None\n",
    "    best_b = None\n",
    "    model = BM25Okapi(corpus)\n",
    "    results = get_query_results(model, queries)\n",
    "    map_score = get_mean_average_precision(queries, results, doc_mappings, get_average_precision)\n",
    "    mrr_score = get_mean_average_precision(queries, results, doc_mappings, get_average_mrr)\n",
    "    dcg_score = get_mean_dcg_at_k(queries, results, doc_mappings, 10)\n",
    "    print(f\"k1=1.5 (default), b=0.75 (default), MAP={map_score:.4f}, MRR={mrr_score:.4f}, DCG={dcg_score:.4f}\")\n",
    "\n",
    "    for k1 in k1_values:\n",
    "        for b in b_values:\n",
    "            model = BM25Okapi(corpus, k1=k1, b=b)\n",
    "            results = get_query_results(model, queries)\n",
    "            map_score = get_mean_average_precision(queries, results, doc_mappings, get_average_precision)\n",
    "            mrr_score = get_mean_average_precision(queries, results, doc_mappings, get_average_mrr)\n",
    "            dcg_score = get_mean_dcg_at_k(queries, results, doc_mappings, 10)\n",
    "            # print(f\"BM25Okapi: k1={k1}, b={b}, MAP={map_score:.4f}, MRR={mrr_score:.4f}, DCG={dcg_score:.4f}\")\n",
    "            if map_score > best_score:\n",
    "                best_score = map_score\n",
    "                best_k1 = k1\n",
    "                best_b = b\n",
    "    return best_k1, best_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Led Large\n",
      "k1=1.5 (default), b=0.75 (default), MAP=0.8996, MRR=0.9600, DCG=0.8105\n",
      "Best k1: 1.4, Best b: 0.5, MAP: 0.9231, MRR: 0.9800, DCG: 0.8108\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Led Large\")\n",
    "k1_values = [i / 10 for i in range(1, 15, 1)]\n",
    "b_values = [i / 20 for i in range(1, 21, 1)]\n",
    "best_k1, best_b = tune_bm25(queries, tokenized_corpus_led_large, doc_mappings, k1_values, b_values)\n",
    "\n",
    "model = BM25Okapi(tokenized_corpus_led_large, k1=best_k1, b=best_b)\n",
    "results_led_large_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_led_large_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_led_large_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_led_large_tuned, doc_mappings, 10)\n",
    "print(f\"Best k1: {best_k1}, Best b: {best_b}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Pegasus\n",
      "k1=1.5 (default), b=0.75 (default), MAP=0.8049, MRR=0.8667, DCG=0.7068\n",
      "Best k1: 1.4, Best b: 0.1, MAP: 0.8081, MRR: 0.8600, DCG: 0.7067\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Pegasus\")\n",
    "k1_values = [i / 10 for i in range(1, 15, 1)]\n",
    "b_values = [i / 20 for i in range(1, 21, 1)]\n",
    "best_k1, best_b = tune_bm25(queries, tokenized_corpus_pegasus, doc_mappings, k1_values, b_values)\n",
    "\n",
    "model = BM25Okapi(tokenized_corpus_pegasus, k1=best_k1, b=best_b)\n",
    "results_pegasus_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_pegasus_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_pegasus_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_pegasus_tuned, doc_mappings, 10)\n",
    "print(f\"Best k1: {best_k1}, Best b: {best_b}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Distil Bart\n",
      "k1=1.5 (default), b=0.75 (default), MAP=0.8946, MRR=0.9800, DCG=0.8469\n",
      "Best k1: 1.4, Best b: 0.55, MAP: 0.9019, MRR: 0.9800, DCG: 0.8466\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Distil Bart\")\n",
    "k1_values = [i / 10 for i in range(1, 15, 1)]\n",
    "b_values = [i / 20 for i in range(1, 21, 1)]\n",
    "best_k1, best_b = tune_bm25(queries, tokenized_corpus_distilbart, doc_mappings, k1_values, b_values)\n",
    "\n",
    "model = BM25Okapi(tokenized_corpus_distilbart, k1=best_k1, b=best_b)\n",
    "results_distilbart_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_distilbart_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_distilbart_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_distilbart_tuned, doc_mappings, 10)\n",
    "print(f\"Best k1: {best_k1}, Best b: {best_b}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Likelihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryLikelihoodModel:\n",
    "    def __init__(self, documents, mu=2000):\n",
    "        self.documents = documents\n",
    "        # Build a vocabulary\n",
    "        self.vocab = set(term for doc in self.documents for term in doc)\n",
    "        # Compute document frequencies\n",
    "        self.doc_frequencies = Counter(term for doc in self.documents for term in set(doc))\n",
    "        # Smoothing parameter (Jelinek-Mercer smoothing)\n",
    "        # self.smoothing_lambda = smoothing_lambda\n",
    "        self.mu = mu\n",
    "\n",
    "    def document_likelihood(self, doc, query):\n",
    "        # Compute the likelihood of generating the query given the document with Jelinek-Mercer smoothing\n",
    "        # P(query | doc) = (1 - λ) * P(term | doc) + λ * P(term | collection)\n",
    "        likelihood = 0.0\n",
    "        total_terms_in_doc = len(doc)\n",
    "        for term in query:\n",
    "            # Estimate P(term | doc)\n",
    "            term_likelihood_doc = doc.count(term) / total_terms_in_doc if total_terms_in_doc > 0 else 0.0\n",
    "            # Estimate P(term | collection)\n",
    "            term_likelihood_collection = self.doc_frequencies[term] / sum(self.doc_frequencies.values())\n",
    "            # Combine using Jelinek-Mercer smoothing\n",
    "            smoothed_term_likelihood = (doc.count(term) + (self.mu / sum(self.doc_frequencies.values()) * self.doc_frequencies[term])) / (total_terms_in_doc + self.mu)\n",
    "            smoothed_term_likelihood = math.log(smoothed_term_likelihood) if smoothed_term_likelihood > 0 else 0.0\n",
    "            likelihood += smoothed_term_likelihood\n",
    "        return likelihood\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        # Rank documents based on the query likelihood with Jelinek-Mercer smoothing\n",
    "        scores = []\n",
    "\n",
    "        for doc in self.documents:\n",
    "            likelihood = self.document_likelihood(doc, query)\n",
    "            scores.append(likelihood)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Led Large, Retrieval model: Query Likelihood): 0.8979\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Led Large, Retrieval model: Query Likelihood): 0.9800\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Led Large, Retrieval model: Query Likelihood): 0.8004\n"
     ]
    }
   ],
   "source": [
    "ql_led_large = QueryLikelihoodModel(tokenized_corpus_led_large)\n",
    "results_led_large_ql = get_query_results(ql_led_large, queries)\n",
    "map_score_led_large_ql = get_mean_average_precision(queries, results_led_large_ql, doc_mappings, get_average_precision)\n",
    "mrr_score_led_large_ql = get_mean_average_precision(queries, results_led_large_ql, doc_mappings, get_average_mrr)\n",
    "dcg_score_led_large_ql = get_mean_dcg_at_k(queries, results_led_large_ql, doc_mappings, 10)\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Led Large, Retrieval model: Query Likelihood): {map_score_led_large_ql:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Led Large, Retrieval model: Query Likelihood): {mrr_score_led_large_ql:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Led Large, Retrieval model: Query Likelihood): {dcg_score_led_large_ql:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Pegasus, Retrieval model: Query Likelihood): 0.7704\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Pegasus, Retrieval model: Query Likelihood): 0.8267\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Pegasus, Retrieval model: Query Likelihood): 0.6976\n"
     ]
    }
   ],
   "source": [
    "ql_pegasus = QueryLikelihoodModel(tokenized_corpus_pegasus)\n",
    "results_pegasus_ql = get_query_results(ql_pegasus, queries)\n",
    "map_score_pegasus_ql = get_mean_average_precision(queries, results_pegasus_ql, doc_mappings, get_average_precision)\n",
    "mrr_score_pegasus_ql = get_mean_average_precision(queries, results_pegasus_ql, doc_mappings, get_average_mrr)\n",
    "dcg_score_pegasus_ql = get_mean_dcg_at_k(queries, results_pegasus_ql, doc_mappings, 10)\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Pegasus, Retrieval model: Query Likelihood): {map_score_pegasus_ql:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Pegasus, Retrieval model: Query Likelihood): {mrr_score_pegasus_ql:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Pegasus, Retrieval model: Query Likelihood): {dcg_score_pegasus_ql:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): 0.9267\n",
      "Mean Reciprocal Rank (MRR) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): 0.9800\n",
      "Mean Discounted Cumulative Gain (DCG) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): 0.8626\n"
     ]
    }
   ],
   "source": [
    "ql_distilbart = QueryLikelihoodModel(tokenized_corpus_distilbart)\n",
    "results_distilbart_ql = get_query_results(ql_distilbart, queries)\n",
    "map_score_distilbart_ql = get_mean_average_precision(queries, results_distilbart_ql, doc_mappings, get_average_precision)\n",
    "mrr_score_distilbart_ql = get_mean_average_precision(queries, results_distilbart_ql, doc_mappings, get_average_mrr)\n",
    "dcg_score_distilbart_ql = get_mean_dcg_at_k(queries, results_distilbart_ql, doc_mappings, 10)\n",
    "print(f\"Mean Average Precision (MAP) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): {map_score_distilbart_ql:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): {mrr_score_distilbart_ql:.4f}\")\n",
    "print(f\"Mean Discounted Cumulative Gain (DCG) (Summarization model: Distil Bart, Retrieval model: Query Likelihood): {dcg_score_distilbart_ql:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Likelihood Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_querylikelihood(queries, corpus, doc_mappings, mu_values):\n",
    "    best_score = 0\n",
    "    best_mu = None\n",
    "    model = QueryLikelihoodModel(corpus)\n",
    "    results = get_query_results(model, queries)\n",
    "    map_score = get_mean_average_precision(queries, results, doc_mappings, get_average_precision)\n",
    "    mrr_score = get_mean_average_precision(queries, results, doc_mappings, get_average_mrr)\n",
    "    dcg_score = get_mean_dcg_at_k(queries, results, doc_mappings, 10)\n",
    "    print(f\"mu=2000 (default), MAP={map_score:.4f}, MRR={mrr_score:.4f}, DCG={dcg_score:.4f}\")\n",
    "\n",
    "    for mu in mu_values:\n",
    "        model = QueryLikelihoodModel(corpus, mu=mu)\n",
    "        results = get_query_results(model, queries)\n",
    "        map_score = get_mean_average_precision(queries, results, doc_mappings, get_average_precision)\n",
    "        mrr_score = get_mean_average_precision(queries, results, doc_mappings, get_average_mrr)\n",
    "        dcg_score = get_mean_dcg_at_k(queries, results, doc_mappings, 10)\n",
    "        # print(f\"Query Likelihood: mu={mu}, MAP={map_score:.4f}, MRR={mrr_score:.4f}, DCG={dcg_score:.4f}\")\n",
    "        if map_score > best_score:\n",
    "            best_score = map_score\n",
    "            best_mu = mu\n",
    "    return best_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Led Large\n",
      "mu=2000 (default), MAP=0.8979, MRR=0.9800, DCG=0.8004\n",
      "Best mu: 650, MAP: 0.9097, MRR: 0.9800, DCG: 0.8142\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Led Large\")\n",
    "mu_values = [i for i in range(0, 2001, 10)]\n",
    "best_mu = tune_querylikelihood(queries, tokenized_corpus_led_large, doc_mappings, mu_values)\n",
    "\n",
    "model = QueryLikelihoodModel(tokenized_corpus_led_large, mu=best_mu)\n",
    "results_led_large_ql_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_led_large_ql_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_led_large_ql_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_led_large_ql_tuned, doc_mappings, 10)\n",
    "print(f\"Best mu: {best_mu}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Pegasus\n",
      "mu=2000 (default), MAP=0.7704, MRR=0.8267, DCG=0.6976\n",
      "Best mu: 520, MAP: 0.7877, MRR: 0.8333, DCG: 0.6999\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Pegasus\")\n",
    "mu_values = [i for i in range(0, 2001, 10)]\n",
    "best_mu = tune_querylikelihood(queries, tokenized_corpus_pegasus, doc_mappings, mu_values)\n",
    "\n",
    "model = QueryLikelihoodModel(tokenized_corpus_pegasus, mu=best_mu)\n",
    "results_pegasus_ql_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_pegasus_ql_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_pegasus_ql_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_pegasus_ql_tuned, doc_mappings, 10)\n",
    "print(f\"Best mu: {best_mu}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Distil Bart\n",
      "mu=2000 (default), MAP=0.9267, MRR=0.9800, DCG=0.8626\n",
      "Best mu: 1640, MAP: 0.9267, MRR: 0.9800, DCG: 0.8623\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Distil Bart\")\n",
    "mu_values = [i for i in range(0, 2001, 10)]\n",
    "best_mu = tune_querylikelihood(queries, tokenized_corpus_distilbart, doc_mappings, mu_values)\n",
    "\n",
    "model = QueryLikelihoodModel(tokenized_corpus_distilbart, mu=best_mu)\n",
    "results_distilbart_ql_tuned = get_query_results(model, queries)\n",
    "map_score_tuned = get_mean_average_precision(queries, results_distilbart_ql_tuned, doc_mappings, get_average_precision)\n",
    "mrr_score_tuned = get_mean_average_precision(queries, results_distilbart_ql_tuned, doc_mappings, get_average_mrr)\n",
    "dcg_score_tuned = get_mean_dcg_at_k(queries, results_distilbart_ql_tuned, doc_mappings, 10)\n",
    "print(f\"Best mu: {best_mu}, MAP: {map_score_tuned:.4f}, MRR: {mrr_score_tuned:.4f}, DCG: {dcg_score_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_documents(model, query, doc_mappings, k = 10):\n",
    "    tokenized_query = query.lower().split()  # tokenize query\n",
    "    doc_scores = model.get_scores(tokenized_query)  # get BM25 scores\n",
    "    ranked_documents = [{\n",
    "        'id': i + 1,\n",
    "        'score': doc_scores[i],\n",
    "        'title': doc_mappings[i][1],\n",
    "    } for i in range(len(doc_scores))]  # add document id, score, and title\n",
    "    ranked_documents = sorted(ranked_documents, key=lambda k: k['score'], reverse=True)  # Sort documents by score (highest to lowest)\n",
    "    topk_ranked_documents = ranked_documents[:k]\n",
    "    topk_ranked_documents = [doc['title'] for doc in topk_ranked_documents]\n",
    "    return topk_ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Led Large\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media\n",
      "Social media marketing\n",
      "Virtual community\n",
      "Digital marketing \n",
      "Content creation\n",
      "Social network\n",
      "Streaming media\n",
      "History of film technology\n",
      "Stock exchange\n",
      "History of women's cricket\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Led Large\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "bm25_led_large_tuned = BM25Okapi(tokenized_corpus_led_large, k1=best_k1, b=best_b)\n",
    "top_10_documents = get_top_k_documents(bm25_led_large_tuned, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Pegasus\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media marketing\n",
      "Social media\n",
      "Digital marketing \n",
      "Social network\n",
      "Virtual community\n",
      "Power\n",
      "Streaming media\n",
      "Franklin D Roosevelt\n",
      "Chemical energy\n",
      "Content creation\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Pegasus\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "top_10_documents = get_top_k_documents(bm25_pegasus, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: BM25, Summarization model: Distil Bart\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media\n",
      "Virtual community\n",
      "Social media marketing\n",
      "Digital marketing \n",
      "Content creation\n",
      "Filmmaking\n",
      "Social network\n",
      "Power\n",
      "Franklin D Roosevelt\n",
      "Quantum algorithm\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: BM25, Summarization model: Distil Bart\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "top_10_documents = get_top_k_documents(bm25_distilbart, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Led Large\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media\n",
      "Virtual community\n",
      "Social media marketing\n",
      "Content creation\n",
      "Digital marketing \n",
      "Social network\n",
      "Stock exchange\n",
      "History of women's cricket\n",
      "Streaming media\n",
      "History of film technology\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Led Large\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "top_10_documents = get_top_k_documents(ql_led_large, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Pegasus\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media marketing\n",
      "Social media\n",
      "Digital marketing \n",
      "Social network\n",
      "Virtual community\n",
      "Power\n",
      "Streaming media\n",
      "Franklin D Roosevelt\n",
      "Chemical energy\n",
      "Content creation\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Pegasus\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "top_10_documents = get_top_k_documents(ql_pegasus, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval model: Query Likelihood, Summarization model: Distil Bart\n",
      "\n",
      "Query:  Evolution of Social Media Platforms\n",
      "\n",
      "Top 10 documents: \n",
      "Social media\n",
      "Social media marketing\n",
      "Virtual community\n",
      "Digital marketing \n",
      "Content creation\n",
      "Social network\n",
      "Filmmaking\n",
      "Streaming media\n",
      "Power\n",
      "Streaming television\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieval model: Query Likelihood, Summarization model: Distil Bart\")\n",
    "query = queries[20]['query']\n",
    "print(\"\\nQuery: \", query)\n",
    "top_10_documents = get_top_k_documents(ql_distilbart, query, doc_mappings, 10)\n",
    "print(\"\\nTop 10 documents: \");\n",
    "for doc in top_10_documents:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[{"id": 1, "title": "Solar energy", "content": "Solar energy is radiant light and heat from the Sun that is harnessed using a range of technologies such as solar power to generate electricity, solar thermal energy (including solar water heating), and solar architecture. It is an essential source of renewable energy, and its technologies are broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power, and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light-dispersing properties, and designing spaces that naturally circulate air.\nIn 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible, and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming .... these advantages are global\".\n\n\n== Potential ==\n\nThe Earth receives 174 petawatts (PW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest, 122 PW, is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most of the world's population live in areas with insolation levels of 150\u2013300 watts/m2, or 3.5\u20137.0 kWh/m2 per day.Solar radiation is absorbed by the Earth's land surface, oceans \u2013 which cover about 71% of the globe \u2013 and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anticyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 \u00b0C. By photosynthesis, green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived.The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 122 PW\u00b7year = 3,850,000 exajoules (EJ) per year. In 2002 (2019), this was more energy in one hour (one hour and 25 minutes) than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass.\nThe potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limit the amount of solar energy that we can acquire. In 2021, Carbon Tracker Initiative estimated the land area needed to generate all our energy from solar alone was 450,000 km2 \u2014 or about the same as the area of Sweden, or the area of Morocco, or the area of California (0.3% of the Earth's total land area).Solar technologies are characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on the distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than Geothermal power and Tidal power, derive their energy either directly or indirectly from the Sun.\nActive solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand-side technologies.In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of 1,600 to 49,800 exajoules (4.4\u00d71014 to 1.4\u00d71016 kWh) per year (see table below).\n\n\n== Thermal energy ==\n\nSolar thermal technologies can be used for water heating, space heating, space cooling and process heat generation.\n\n\n=== Early commercial adaptation ===\nIn 1878, at the Universal Exposition in Paris, Augustin Mouchot successfully demonstrated a solar steam engine but could not continue development because of cheap coal and other factors.\n\nIn 1897, Frank Shuman, a US inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.\nShuman built the world's first solar thermal power station in Maadi, Egypt, between 1912 and 1913. His plant used parabolic troughs to power a 45\u201352 kilowatts (60\u201370 hp) engine that pumped more than 22,000 litres (4,800 imp gal; 5,800 US gal) of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman's vision, and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:\n\nWe have proved the commercial profit of sun power in the tropics and have more particularly proved that after our stores of oil and coal are exhausted the human race can receive unlimited power from the rays of the Sun.\n\n\n=== Water heating ===\n\nSolar hot water systems use sunlight to heat water. In middle geographical latitudes (between 40 degrees north and 40 degrees south), 60 to 70% of the domestic hot water use, with water temperatures up to 60 \u00b0C (140 \u00b0F), can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.As of 2015, the total installed capacity of solar hot water systems was approximately 436 thermal gigawatt (GWth), and China is the world leader in their deployment with 309 GWth installed, taken up 71% of the market. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada, and Australia, heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GWth as of 2005.\n\n\n=== Heating, cooling and ventilation ===\n\nIn the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.  Use of solar for heating can roughly be divided into passive solar concepts and active solar concepts, depending on whether active elements such as sun tracking and solar concentrator optics are used.\n\nThermal mass is any material that can be used to store heat\u2014heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement, and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting, and shading conditions. When duly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated, causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.\nDeciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator-facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.\n\n\n=== Cooking ===\n\nSolar cookers use sunlight for cooking, drying, and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers, and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of 90\u2013150 \u00b0C (194\u2013302 \u00b0F). Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of 315 \u00b0C (599 \u00b0F) and above but require direct light to function properly and must be repositioned to track the Sun.\n\n\n=== Process heat ===\n\nSolar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, US where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from seawater is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams.Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the \"right to dry\" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to 22 \u00b0C (40 \u00b0F) and deliver outlet temperatures of 45\u201360 \u00b0C (113\u2013140 \u00b0F). The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of 35,000 square metres (380,000 sq ft) had been installed worldwide, including an 860 m2 (9,300 sq ft) collector in Costa Rica used for drying coffee beans and a 1,300 m2 (14,000 sq ft) collector in Coimbatore, India, used for drying marigolds.\n\n\n=== Water treatment ===\n\nSolar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of 4,700 m2 (51,000 sq ft), could produce up to 22,700 L (5,000 imp gal; 6,000 US gal) per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.Solar energy may be used in a water stabilization pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.\n\n\n=== Molten salt technology ===\nMolten salt can be employed as a thermal energy storage method to retain thermal energy collected by a solar tower or solar trough of a concentrated solar power plant so that it can be used to generate electricity in bad weather or at night. It was demonstrated in the Solar Two project from 1995 to 1999. The system is predicted to have an annual efficiency of 99%, a reference to the energy retained by storing heat before turning it into electricity, versus converting heat directly into electricity. The molten salt mixtures vary. The most extended mixture contains sodium nitrate, potassium nitrate and calcium nitrate. It is non-flammable and non-toxic, and has already been used in the chemical and metals industries as a heat-transport fluid. Hence, experience with such systems exists in non-solar applications.\nThe salt melts at 131 \u00b0C (268 \u00b0F). It is kept liquid at 288 \u00b0C (550 \u00b0F) in an insulated \"cold\" storage tank. The liquid salt is pumped through panels in a solar collector where the focused irradiance heats it to 566 \u00b0C (1,051 \u00b0F). It is then sent to a hot storage tank. This is so well insulated that the thermal energy can be usefully stored for up to a week.When electricity is needed, the hot salt is pumped to a conventional steam-generator to produce superheated steam for a turbine/generator as used in any conventional coal, oil, or nuclear power plant. A 100-megawatt turbine would need a tank about 9.1 metres (30 ft) tall and 24 metres (79 ft) in diameter to drive it for four hours by this design.\nSeveral parabolic trough power plants in Spain and solar power tower developer SolarReserve use this thermal energy storage concept. The Solana Generating Station in the U.S. has six hours of storage by molten salt. In Chile, The Cerro Dominador power plant has a 110 MW solar-thermal tower, the heat is transferred to molten salts.\nThe molten salts then transfer their heat in a heat exchanger to water, generating superheated steam, which feeds a turbine that transforms the kinetic energy of the steam into electric energy using the Rankine cycle. In this way, the Cerro Dominador plant is capable of generating around 110 MW of power.\nThe plant has an advanced storage system enabling it to generate electricity for up to 17.5 hours without direct solar radiation, which allows it to provide a stable electricity supply without interruptions if required. The Project secured up to 950 GW\u00b7h per year sale. Another project is the Mar\u00eda Elena plant  is a 400 MW thermo-solar complex in the northern Chilean region of Antofagasta employing molten salt technology.\n\n\n== Electricity production ==\n\n\n== Concentrated solar power ==\n\nConcentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the solar tower collectors, the concentrating linear Fresnel reflector, and the Stirling dish. Various techniques are used to track the Sun and focus light. In all of these systems, a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.  Designs need to account for the risk of a dust storm, hail, or another extreme weather event that can damage the fine glass surfaces of solar power plants.  Metal grills would allow a high percentage of sunlight to enter the mirrors and solar panels while also preventing most damage.\n\n\n== Architecture and urban planning ==\n\nSunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment, they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans, and switchable windows can complement passive design and improve system performance.\nUrban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures result from increased absorption of solar energy by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and to plant trees in the area. Using these methods, a hypothetical \"cool communities\" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 \u00b0C at an estimated cost of US$1  billion, giving estimated total annual benefits of US$530  million from reduced air-conditioning costs and healthcare savings.\n\n\n== Agriculture and horticulture ==\nAgriculture and horticulture seek to optimize the capture of solar energy to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vintners, who use the energy generated by solar panels to power grape presses.Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today. Plastic transparent materials have also been used to similar effect in polytunnels and row covers.\n\n\n== Transport ==\n\nDevelopment of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over 3,021 kilometres (1,877 mi) across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was 67 kilometres per hour (42 mph) and by 2007 the winner's average speed had improved to 90.87 kilometres per hour (56.46 mph).\nThe North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.Some vehicles use solar panels for auxiliary power, such as for air conditioning, to keep the interior cool, thus reducing fuel consumption.In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar-powered crossing of the Pacific Ocean, and the Sun21 catamaran made the first solar-powered crossing of the Atlantic Ocean in the winter of 2006\u20132007. There were plans to circumnavigate the globe in 2010.In 1974, the unmanned AstroFlight Sunrise airplane made the first solar flight. On 29 April 1979, the Solar Riser made the first flight in a solar-powered, fully controlled, man-carrying flying machine, reaching an altitude of 40 ft (12 m). In 1980, the Gossamer Penguin made the first piloted flights powered solely by photovoltaics. This was quickly followed by the Solar Challenger which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the Pathfinder (1997) and subsequent designs, culminating in the Helios which set the altitude record for a non-rocket-propelled aircraft at 29,524 metres (96,864 ft) in 2001. The Zephyr, developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. From March 2015 to July 2016, Solar Impulse, an electric aircraft, successfully circumnavigated the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The design allows the aircraft to remain airborne for several days.A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands, causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.\n\n\n== Fuel production ==\n\nSolar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 \u2013  the splitting of seawater providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. In addition, chemical energy storage is another solution to solar energy storage.Hydrogen production technologies have been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (2,300\u20132,600 \u00b0C or 4,200\u20134,700 \u00b0F). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute of Science uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above 1,200 \u00b0C (2,200 \u00b0F). This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.\n\n\n== Energy storage methods ==\n\nThermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.Phase change materials such as paraffin wax and Glauber's salt are another thermal storage medium. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately 64 \u00b0C or 147 \u00b0F). The \"Dover House\" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity, and can deliver heat at temperatures compatible with conventional power systems. The Solar Two project used this method of energy storage, allowing it to store 1.44 terajoules (400,000 kWh) in its 68 m\u00b3 storage tank with an annual storage efficiency of about 99%.Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt-hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.\nPumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.\n\n\n== Development, deployment and economics ==\n\nBeginning with the surge in coal use, which accompanied the Industrial Revolution, energy consumption steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th  century in the face of the increasing availability, economy, and utility of coal and petroleum.The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world. It brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s, but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s, and annual growth rates have averaged 20% since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154  GW as of 2007.The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces:\n\nThe development of affordable, inexhaustible, and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible, and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.\n\nIn 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water, and concentrated solar power could provide a third of the world's energy by 2060 if politicians commit to limiting climate change and transitioning to renewable energy. The energy from the Sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. \"The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale\".\nWe have proved ... that after our stores of oil and coal are exhausted the human race can receive unlimited power from the rays of the Sun.In 2021 Lazard estimated the levelized cost of new build unsubsidized utility scale solar electricity at less than 37 dollars per MWh and existing coal-fired power above that amount. The 2021 report also said that new solar was also cheaper than new gas-fired power, but not generally existing gas power.\n\n\n=== Emerging technologies ===\n\n\n==== Experimental solar power ====\n\nConcentrated photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electricity generation. Thermoelectric, or \"thermovoltaic\" devices convert a temperature difference between dissimilar materials into an electric current.\n\n\n==== Floating solar arrays ====\n\n\n==== Solar-assisted heat pump ====\n\nA heat pump is a device that provides heat energy from a source of heat to a destination called a \"heat sink\". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A solar-assisted heat pump represents the integration of a heat pump and thermal solar panels in a single integrated system. Typically these two technologies are used separately (or only placing them in parallel) to produce hot water. In this system the solar thermal panel performs the function of the low temperature heat source and the heat produced is used to feed the heat pump's evaporator. The goal of this system is to get high COP and then produce energy in a more efficient and less expensive way.\nIt is possible to use any type of solar thermal panel (sheet and tubes, roll-bond, heat pipe, thermal plates) or hybrid (mono/polycrystalline, thin film) in combination with the heat pump. The use of a hybrid panel is preferable because it allows covering a part of the electricity demand of the heat pump and reduces the power consumption and consequently the variable costs of the system.\n\n\n==== Solar aircraft ====\n\nAn electric aircraft is an aircraft that runs on electric motors rather than internal combustion engines, with electricity coming from fuel cells, solar cells, ultracapacitors, power beaming, or batteries.\nCurrently, flying manned electric aircraft are mostly experimental demonstrators, though many small unmanned aerial vehicles are powered by batteries. Electrically powered model aircraft have been flown since the 1970s, with one report in 1957. The first man-carrying electrically powered flights were made in 1973. Between 2015 and 2016, a manned, solar-powered plane, Solar Impulse 2, completed a circumnavigation of the Earth.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading =="}, {"id": 2, "title": "First President", "content": "George Washington (February 22, 1732 \u2013 December 14, 1799) was an American military officer, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797. Appointed by the Second Continental Congress as commander of the Continental Army in June 1775, Washington led Patriot forces to victory in the American Revolutionary War and then served as president of the Constitutional Convention in 1787, which drafted and ratified the Constitution of the United States and established the American federal government. Washington has thus been called the \"Father of his Country\".\nWashington's first public office, from 1749 to 1750, was as surveyor of Culpeper County in the Colony of Virginia. He subsequently received military training and was assigned command of the Virginia Regiment during the French and Indian War. He was later elected to the Virginia House of Burgesses and was named a delegate to the Continental Congress in Philadelphia, which appointed him Commander-in-Chief of the Continental Army. Washington led American forces to a decisive victory over the British in the Revolutionary War, leading the British to sign the Treaty of Paris, which acknowledged the sovereignty and independence of the United States. He resigned his commission in 1783 after the conclusion of the Revolutionary War.\nWashington played an indispensable role in adopting and ratifying the Constitution, which replaced the Articles of Confederation in 1789. He was then twice elected president by the Electoral College unanimously. As the first U.S. president, Washington implemented a strong, well-financed national government while remaining impartial in a fierce rivalry that emerged between cabinet members Thomas Jefferson and Alexander Hamilton. During the French Revolution, he proclaimed a policy of neutrality while sanctioning the Jay Treaty. He set enduring precedents for the office of president, including use of the title \"Mr. President\" and the two-term tradition. His 1796 farewell address became a preeminent statement on republicanism in which he wrote about the importance of national unity and the dangers regionalism, partisanship, and foreign influence pose to it.\nWashington has been memorialized by monuments, a federal holiday, various media depictions, geographical locations including the national capital, the State of Washington, stamps, and currency. He is ranked among the greatest U.S. presidents. In 1976, Washington was posthumously promoted to the rank of General of the Armies, the highest rank in the U.S. Army. His legacy has become increasingly controversial over time, however, as a result of his ownership of slaves and his relationship with slavery. His historical reputation has also been complicated in modern discourse by his policy of assimilating Native Americans into Anglo-American culture and waging war against indigenous peoples during the Revolutionary War and the Northwest Indian War.\n\n\n== Early life (1732\u20131752) ==\n\nGeorge Washington was born on February 22, 1732, at Popes Creek in Westmoreland County, Virginia. He was the first of six children of Augustine and Mary Ball Washington. His father was a justice of the peace and a prominent public figure who had four additional children from his first marriage to Jane Butler. The family moved to Little Hunting Creek in 1734 before eventually settling in Ferry Farm near Fredericksburg, Virginia. When Augustine died in 1743, Washington inherited Ferry Farm and ten slaves; his older half-brother Lawrence inherited Little Hunting Creek and renamed it Mount Vernon.Washington did not have the formal education his elder brothers received at Appleby Grammar School in England, but he did attend the Lower Church School in Hartfield. He learned mathematics, trigonometry, and land surveying, and became a talented draftsman and mapmaker. By early adulthood, he was writing with \"considerable force\" and \"precision\". As a teenager, to practice his penmanship, Washington compiled over a hundred rules for social interaction styled Rules of Civility and Decent Behaviour in Company and Conversation, copied from an English translation of a French book of manners.Washington often visited Mount Vernon and Belvoir, the plantation of William Fairfax, Lawrence's father-in-law. Fairfax became Washington's patron and surrogate father, and Washington spent a month in 1748 with a team surveying Fairfax's Shenandoah Valley property. The following year, he received a surveyor's license from the College of William & Mary. Even though Washington had not served the customary apprenticeship, Fairfax appointed him surveyor of Culpeper County, Virginia, where he took his oath of office July 20, 1749. He subsequently familiarized himself with the frontier region, and though he resigned from the job in 1750, he continued to do surveys west of the Blue Ridge Mountains. By 1752, he had bought almost 1,500 acres (600 ha) in the Valley and owned 2,315 acres (937 ha).In 1751, Washington made his only trip abroad when he accompanied Lawrence to Barbados, hoping the climate would cure his brother's tuberculosis. Washington contracted smallpox during that trip, which left his face slightly scarred. Lawrence died in 1752, and Washington leased Mount Vernon from his widow Anne; he inherited it outright after her death in 1761.\n\n\n== Colonial military career (1752\u20131758) ==\nLawrence Washington's service as adjutant general of the Virginia militia inspired George to seek a commission. Virginia's lieutenant governor, Robert Dinwiddie, appointed Washington as a major and commander of one of the four militia districts. The British and French were competing for control of the Ohio Valley: the British were constructing forts along the Ohio River, and the French between the Ohio River and Lake Erie.In October 1753, Dinwiddie appointed Washington as a special envoy. He had sent Washington to demand French forces to vacate land that was claimed by the British. Washington was also appointed to make peace with the Iroquois Confederacy, and to gather further intelligence about the French forces. Washington met with Half-King Tanacharison, and other Iroquois chiefs, at Logstown, and gathered information about the numbers and locations of the French forts, as well as intelligence concerning individuals taken prisoner by the French. Washington was nicknamed Conotocaurius by Tanacharison. The name, meaning \"devourer of villages\", had been given to his great-grandfather John Washington in the late 17th century by the Susquehannock.Washington's party reached the Ohio River in November 1753, and was intercepted by a French patrol. The party was escorted to Fort Le Boeuf, where Washington was received in a friendly manner. He delivered the British demand to vacate to the French commander Saint-Pierre, but the French refused to leave. Saint-Pierre gave Washington his official answer after a few days' delay, as well as food and winter clothing for his party's journey back to Virginia. Washington completed the precarious mission in 77 days, in difficult winter conditions, achieving a measure of distinction when his report was published in Virginia and London.\n\n\n=== French and Indian War ===\n\nIn February 1754, Dinwiddie promoted Washington to lieutenant colonel and second-in-command of the 300-strong Virginia Regiment, with orders to confront French forces at the Forks of the Ohio. Washington set out with half the regiment in April and soon learned a French force of 1,000 had begun construction of Fort Duquesne there. In May, having set up a defensive position at Great Meadows, he learned that the French had made camp seven miles (11 km) away; he decided to take the offensive.The French detachment proved to be only about 50 men, so Washington advanced on May 28 with a small force of Virginians and Indian allies to ambush them. During the ambush, French forces were killed outright with muskets and hatchets, including French commander Joseph Coulon de Jumonville, who had been carrying a diplomatic message for the British. The French later found their countrymen dead and scalped, blaming Washington, who had retreated to Fort Necessity.The full Virginia Regiment joined Washington at Fort Necessity the following month with news that he had been promoted to command of the regiment and colonel upon the regimental commander's death. The regiment was reinforced by an independent company of a hundred South Carolinians led by Captain James Mackay; his royal commission outranked Washington's and a conflict of command ensued. On July 3, a French force attacked with 900 men, and the ensuing battle ended in Washington's surrender. He signed a surrender document in which he unwittingly took responsibility for \"assassinating\" Jumonville, later blaming the translator for not properly translating it.In the aftermath, Colonel James Innes took command of intercolonial forces, the Virginia Regiment was divided, and Washington was offered a captaincy in one of the newly formed regiments. He refused, however, as it would have been a demotion and instead resigned his commission. The \"Jumonville affair\" became the incident which ignited the French and Indian War, later to become part of the Seven Years' War.\nIn 1755, Washington served voluntarily as an aide to General Edward Braddock, who led a British expedition to expel the French from Fort Duquesne and the Ohio Country. On Washington's recommendation, Braddock split the army into one main column and a lightly equipped \"flying column\". Suffering from severe dysentery, Washington was left behind, and when he rejoined Braddock at Monongahela the French and their Indian allies ambushed the divided army. Two-thirds of the British force became casualties, including the mortally wounded Braddock. Under the command of Lieutenant Colonel Thomas Gage, Washington, still very ill, rallied the survivors and formed a rear guard, allowing the remnants of the force to disengage and retreat.During the engagement, he had two horses shot from under him, and his hat and coat were bullet-pierced. His conduct under fire redeemed his reputation among critics of his command in the Battle of Fort Necessity, but he was not included by the succeeding commander (Colonel Thomas Dunbar) in planning subsequent operations.The Virginia Regiment was reconstituted in August 1755, and Dinwiddie appointed Washington its commander, again with the rank of colonel. Washington clashed over seniority almost immediately, this time with John Dagworthy, another captain of superior royal rank, who commanded a detachment of Marylanders at the regiment's headquarters in Fort Cumberland. Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor as Commander-in-Chief, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. Shirley ruled in Washington's favor only in the matter of Dagworthy; Loudoun humiliated Washington, refused him a royal commission and agreed only to relieve him of the responsibility of manning Fort Cumberland.In 1758, the Virginia Regiment was assigned to the British Forbes Expedition to capture Fort Duquesne. Washington disagreed with General John Forbes' tactics and chosen route. Forbes nevertheless made Washington a brevet brigadier general and gave him command of one of the three brigades that would assault the fort. The French had abandoned the fort and the valley before the assault, however, and Washington only saw a friendly fire incident which left 14 dead and 26 injured. Frustrated, he resigned his commission soon afterwards and returned to Mount Vernon.Under Washington, the Virginia Regiment had defended 300 miles (480 km) of frontier against twenty Indian attacks in ten months. He increased the professionalism of the regiment as it grew from 300 to 1,000 men, and Virginia's frontier population suffered less than other colonies. Though he failed to realize a royal commission, he gained self-confidence, leadership skills, and knowledge of British military tactics. The destructive competition Washington witnessed among colonial politicians fostered his later support of a strong central government.\n\n\n== Marriage, civilian, and political life (1755\u20131775) ==\n\nOn January 6, 1759, Washington, at age 26, married Martha Dandridge Custis, the 27-year-old widow of wealthy plantation owner Daniel Parke Custis. The marriage took place at Martha's estate; she was intelligent, gracious, and experienced in managing a planter's estate, and the couple had a happy marriage. They moved to Mount Vernon, near Alexandria, where he lived as a planter of tobacco and wheat and emerged as a political figure.Washington's 1751 bout with smallpox is thought to have rendered him sterile, though it is equally likely that \"Martha may have sustained injury during the birth of Patsy, her final child, making additional births impossible.\" The couple lamented not having any children together. Despite this, the two raised Martha's two children John Parke Custis (Jacky) and Martha Parke Custis (Patsy), and later Jacky's two youngest children Eleanor Parke Custis (Nelly) and George Washington Parke Custis (Washy), along with numerous nieces and nephews.The marriage gave Washington control over Martha's one-third dower interest in the 18,000-acre (7,300 ha) Custis estate, and he managed the remaining two-thirds for Martha's children; the estate also included 84 slaves. As a result, he became one of the wealthiest men in Virginia, which increased his social standing.At Washington's urging, Governor Lord Botetourt fulfilled Dinwiddie's 1754 promise of land bounties to all-volunteer militia during the French and Indian War. In late 1770, Washington inspected the lands in the Ohio and Great Kanawha regions, and he engaged surveyor William Crawford to subdivide it. Crawford allotted 23,200 acres (9,400 ha) to Washington; Washington told the veterans that their land was hilly and unsuitable for farming, and he agreed to purchase 20,147 acres (8,153 ha), leaving some feeling they had been duped. He also doubled the size of Mount Vernon to 6,500 acres (2,600 ha) and, by 1775, had increased its slave population by more than a hundred.As a respected military hero and large landowner, Washington held local offices and was elected to the Virginia provincial legislature, representing Frederick County in the House of Burgesses for seven years beginning in 1758. He first ran for the seat in 1755 but was soundly beaten by Hugh West. When he ran in 1758, Washington plied voters with beer, brandy, and other beverages. Despite being away serving on the Forbes Expedition, he won the election with roughly 40 percent of the vote, defeating three opponents with the help of local supporters.Early in his legislative career, Washington rarely spoke or even attended legislative sessions. He would later become a prominent critic of Britain's taxation policy and mercantilist policies towards the American colonies and became more politically active starting in the 1760s.Washington imported luxuries and other goods from England, paying for them by exporting tobacco. His profligate spending combined with low tobacco prices left him \u00a31,800 in debt by 1764, prompting him to diversify his holdings. In 1765, because of erosion and other soil problems, he changed Mount Vernon's primary cash crop from tobacco to wheat and expanded operations to include corn flour milling and fishing.Washington soon was counted among the political and social elite in Virginia. From 1768 to 1775, he invited some 2,000 guests to Mount Vernon, mostly those whom he considered people of rank, and was known to be exceptionally cordial toward guests. Washington also took time for leisure with fox hunting, fishing, dances, theater, cards, backgammon, and billiards.Washington's stepdaughter Patsy suffered from epileptic attacks from age 12, and she died at Mount Vernon in 1773. The following day, he wrote to Burwell Bassett: \"It is easier to conceive, than to describe, the distress of this Family\". He canceled all business activity and remained with Martha every night for three months.\n\n\n=== Opposition to the British Parliament and Crown ===\n\nWashington played a central role before and during the American Revolution. His distrust of the British military had begun when he was passed over for promotion into the Regular Army. Opposed to taxes imposed by the British Parliament on the Colonies without proper representation, he and other colonists were also angered by the Royal Proclamation of 1763 which banned American settlement west of the Allegheny Mountains and protected the British fur trade.Washington believed the Stamp Act 1765 was an \"Act of Oppression\" and celebrated its repeal the following year. In March 1766, Parliament passed the Declaratory Act asserting that Parliamentary law superseded colonial law. In the late 1760s, the interference of the British Crown in American lucrative western land speculation spurred the American Revolution. Washington was a prosperous land speculator, and in 1767, he encouraged \"adventures\" to acquire backcountry western lands. Washington helped lead widespread protests against the Townshend Acts passed by Parliament in 1767, and he introduced a proposal in May 1769 which urged Virginians to boycott British goods; the Acts were mostly repealed in 1770.Parliament sought to punish Massachusetts colonists for their role in the Boston Tea Party in 1774 by passing the Coercive Acts, which Washington saw as \"an invasion of our rights and privileges\". He said Americans must not submit to acts of tyranny since \"custom and use shall make us as tame and abject slaves, as the blacks we rule over with such arbitrary sway\". That July, he and George Mason drafted a list of resolutions for the Fairfax County committee, including a call to end the Atlantic slave trade, which were adopted.On August 1, Washington attended the First Virginia Convention. There, he was selected as a delegate to the First Continental Congress. As tensions rose in 1774, he helped train militias in Virginia and organized enforcement of the Continental Association boycott of British goods instituted by the Congress.The American Revolutionary War broke out on April 19, 1775, with the Battles of Lexington and Concord and the Siege of Boston. Upon hearing the news, Washington was \"sobered and dismayed\", and he hastily departed Mount Vernon on May 4, 1775, to join the Second Continental Congress in Philadelphia.\n\n\n== Commander in chief (1775\u20131783) ==\n\nOn June 14, 1775, Congress created the Continental Army and John Adams nominated Washington as its commander-in-chief, mainly because of his military experience and the belief that a Virginian would better unite the colonies. He was unanimously elected by Congress the next day. Washington appeared before Congress in uniform and gave an acceptance speech on June 16, declining a salary, though he was later reimbursed expenses.Washington was commissioned on June 19 and officially appointed by Congress as \"General & Commander in chief of the army of the United Colonies and of all the forces raised or to be raised by them\". He was instructed to take charge of the Siege of Boston on June 22, 1775.Congress chose his primary staff officers, including Major General Artemas Ward, Adjutant General Horatio Gates, Major General Charles Lee, Major General Philip Schuyler, and Major General Nathanael Greene.  Henry Knox, a young bookkeeper, impressed Adams and Washington with ordnance knowledge and was subsequently promoted to colonel and chief of artillery. Similarly, Washington was impressed by Alexander Hamilton's intelligence and bravery. He would later promote him to colonel and appoint him his aide-de-camp.Washington initially banned the enlistment of blacks, both free and enslaved, into the Continental Army. The British saw an opportunity to divide the colonies, and the colonial governor of Virginia issued a proclamation, which promised freedom to slaves if they joined the British. Desperate for manpower by late 1777, Washington relented and overturned his ban. By the end of the war, around one-tenth of Washington's army were blacks. Following the British surrender, Washington sought to enforce terms of the preliminary Treaty of Paris (1783) by reclaiming slaves freed by the British and returning them to servitude. He arranged to make this request to Sir Guy Carleton on May 6, 1783. Instead, Carleton issued 3,000 freedom certificates and all former slaves in New York City were able to leave before the city was evacuated by the British in late November 1783.\n\n\n=== Siege of Boston ===\n\nEarly in 1775, in response to the growing rebellious movement, London sent British troops to occupy Boston, led by General Thomas Gage, commander of British forces in America. They set up fortifications, making the city impervious to attack. Local militias surrounded the city and effectively trapped the British troops, resulting in a standoff.\nAs Washington headed for Boston, word of his march preceded him, and he was greeted everywhere; gradually, he became a symbol of the Patriot cause. Upon arrival on July 2, 1775, two weeks after the Battle of Bunker Hill, he set up headquarters in Cambridge. When he went to inspect the army, he found undisciplined militia. After consultation, he initiated Benjamin Franklin's suggested reforms: drilling the soldiers and imposing strict discipline. Washington ordered his officers to identify the skills of recruits to ensure military effectiveness, while removing incompetent officers. He petitioned Gage, his former superior, to release captured Patriot officers from prison and treat them humanely. In October 1775, King George III declared that the colonies were in open rebellion and relieved Gage of command for incompetence, replacing him with General William Howe.The Continental Army, reduced to only 9,600 men by January 1776 due to expiring short-term enlistments, had to be supplemented with militia. Soon, they were joined by Knox with heavy artillery captured from Fort Ticonderoga. When the Charles River froze over, Washington was eager to cross and storm Boston, but General Gates and others were opposed to untrained militia striking well-garrisoned fortifications. Instead, he agreed to secure the Dorchester Heights, 100 feet above Boston, with Knox's artillery to try to force the British out.On March 9, under cover of darkness, Washington's troops bombarded British ships in Boston harbor. On March 17, 9,000 British troops and Loyalists began a chaotic ten-day evacuation aboard 120 ships. Soon after, Washington entered the city with 500 men, with explicit orders not to plunder the city. He refrained from exerting military authority in Boston, leaving civilian matters in the hands of local authorities.\n\n\n=== New York and New Jersey ===\n\n\n==== Battle of Long Island ====\n\nAfter the victory at Boston, Washington correctly guessed that the British would return to New York City, a Loyalist stronghold, and retaliate. He arrived there on April 13, 1776, and ordered the construction of fortifications to thwart the expected British attack. He also ordered his occupying forces to treat civilians and their property with respect, to avoid the abuses Bostonians suffered at the hands of British troops.Howe transported his resupplied army, with the British fleet, from Halifax to New York City. George Germain, who ran the British war effort in England, believed it could be won with one \"decisive blow\". The British forces, including more than a hundred ships and thousands of troops, began arriving on Staten Island on July 2 to lay siege to the city. After the Declaration of Independence was unanimously adopted on July 4, Washington informed his troops on July 9 that Congress had declared the united colonies to be \"free and independent states\".Howe's troop strength totaled 32,000 regulars and Hessian auxiliaries, and Washington's consisted of 23,000, mostly raw recruits and militia. In August, Howe landed 20,000 troops at Gravesend, Brooklyn, and approached Washington's fortifications Opposing his generals, Washington chose to fight, based on inaccurate information that Howe's army had only 8,000-plus troops. In the Battle of Long Island, Howe assaulted Washington's flank and inflicted 1,500 Patriot casualties, the British suffering 400. Washington retreated, instructing General William Heath to acquire river craft. On August 30, General William Alexander held off the British and gave cover while the army crossed the East River under darkness to Manhattan without loss of life or materiel, although Alexander was captured.\nHowe was emboldened by his Long Island victory and dispatched Washington as \"George Washington, Esq.\" in futility to negotiate peace. Washington declined, demanding to be addressed with diplomatic protocol, as general and fellow belligerent, not as a \"rebel\", lest his men be hanged as such if captured. The Royal Navy bombarded the unstable earthworks on lower Manhattan Island. Despite misgivings, Washington heeded the advice of Generals Greene and Putnam to defend Fort Washington. They were unable to hold it; Washington abandoned the fort and ordered his army north to the White Plains.Howe's pursuit forced Washington to retreat across the Hudson River to Fort Lee to avoid encirclement. Howe landed his troops on Manhattan in November and captured Fort Washington, inflicting high casualties on the Americans. Washington was responsible for delaying the retreat, though he blamed Congress and General Greene. Loyalists in New York City considered Howe a liberator and spread a rumor that Washington had set fire to the city. Patriot morale reached its lowest when Lee was captured. Now reduced to 5,400 troops, Washington's army retreated through New Jersey, and Howe broke off pursuit to set up winter quarters in New York.\n\n\n==== Crossing the Delaware, Trenton, and Princeton ====\n\nWashington crossed the Delaware River into Pennsylvania, where Lee's replacement General John Sullivan joined him with 2,000 more troops. The future of the Continental Army was in doubt due to lack of supplies, a harsh winter, expiring enlistments, and desertions. Washington was disappointed that many New Jersey residents were Loyalists or skeptical about independence.Howe split up his army and posted a Hessian garrison at Trenton to hold western New Jersey and the east shore of the Delaware. Desperate for a victory, Washington and his generals devised a surprise attack on Trenton. The army was to cross the Delaware in three divisions: one led by Washington (2,400 troops), another by General James Ewing (700), and the third by Colonel John Cadwalader (1,500). The force was to then split, with Washington taking the Pennington Road and General Sullivan traveling south on the river's edge.Washington ordered a 60-mile search for Durham boats to transport his army, and the destruction of vessels that could be used by the British. He personally risked capture while staking out the Jersey shoreline alone leading up to the crossing. Washington crossed the Delaware on Christmas night, 1776. His men followed across the ice-obstructed river from McConkey's Ferry, with 40 men per vessel. The wind churned up the waters, and they were pelted with hail, but by 3:00 a.m. on December 26, they made it across with no losses. Knox was delayed, managing frightened horses and about 18 field guns on flat-bottomed ferries. Cadwalader and Ewing failed to cross due to the ice and heavy currents. Once Knox arrived, Washington proceeded to Trenton, rather than risk being spotted returning his army to Pennsylvania.The troops spotted Hessian positions a mile from Trenton, so Washington split his force into two columns, rallying his men: \"Soldiers keep by your officers. For God's sake, keep by your officers.\" The two columns were separated at the Birmingham crossroads. General Greene's column took the upper Ferry Road, led by Washington, and General Sullivan's column advanced on River Road. The Americans marched in sleet and snowfall. Many were shoeless with bloodied feet, and two died of exposure. At sunrise, Washington, aided by Colonel Knox and artillery, led his men in a surprise attack on the unsuspecting Hessians and their commander, Colonel Johann Rall. The Hessians had 22 killed, including Colonel Rall, 83 wounded, and 850 captured with supplies.Washington retreated across the Delaware to Pennsylvania and returned to New Jersey on January 3, 1777, launching an attack on British regulars at Princeton, with 40 Americans killed or wounded and 273 British killed or captured. American Generals Hugh Mercer and John Cadwalader were being driven back by the British when Mercer was mortally wounded. Washington arrived and led the men in a counterattack which advanced to within 30 yards (27 m) of the British line.Some British troops retreated after a brief stand, while others took refuge in Nassau Hall, which became the target of Colonel Alexander Hamilton's cannons. Washington's troops charged, the British surrendered in less than an hour, and 194 soldiers laid down their arms. Howe retreated to New York City where his army remained inactive until early the next year. Washington took up winter headquarters in Jacob Arnold's Tavern in Morristown, New Jersey, while he received munition from the Hibernia mines. While in Morristown, Washington's troops disrupted British supply lines and expelled them from parts of New Jersey.During his stay in Morristown, Washington ordered the inoculation of Continental troops against smallpox. This went against the wishes of the Continental Congress who had issued a proclamation prohibiting it, but Washington feared the spread of smallpox in the army. The mass inoculation proved successful, with only isolated infections occurring and no regiments incapacitated by the disease.The British still controlled New York, and many Patriot soldiers did not re-enlist or deserted after the harsh winter campaign. Congress instituted greater rewards for re-enlisting and punishments for desertion to effect greater troop numbers. Strategically, Washington's victories at Trenton and Princeton were pivotal; they revived Patriot morale and quashed the British strategy of showing overwhelming force followed by offering generous terms, changing the course of the war. In February 1777, word of the American victories reached London, and the British realized the Patriots were in a position to demand unconditional independence.\n\n\n=== Philadelphia ===\n\n\n==== Brandywine, Germantown, and Saratoga ====\n\nIn July 1777, British General John Burgoyne led the Saratoga campaign south from Quebec through Lake Champlain and recaptured Fort Ticonderoga intending to divide New England, including control of the Hudson River. However, General Howe in British-occupied New York City blundered, taking his army south to Philadelphia rather than up the Hudson River to join Burgoyne near Albany.Washington and Gilbert du Motier, Marquis de Lafayette rushed to Philadelphia to engage Howe. In the Battle of Brandywine, on September 11, 1777, Howe outmaneuvered Washington and marched unopposed into the nation's capital at Philadelphia. A Patriot attack failed against the British at Germantown in October.In Upstate New York, the Patriots were led by General Horatio Gates. Concerned about Burgoyne's movements southward, Washington sent reinforcements north with Generals Benedict Arnold, his most aggressive field commander, and Benjamin Lincoln. On October 7, 1777, Burgoyne tried to take Bemis Heights but was isolated from support by Howe. He was forced to retreat to Saratoga and ultimately surrendered after the Battles of Saratoga. As Washington suspected, Gates' victory emboldened his critics.Biographer John Alden maintains, \"It was inevitable that the defeats of Washington's forces and the concurrent victory of the forces in upper New York should be compared.\" Admiration for Washington was waning, including little credit from John Adams.\n\n\n==== Valley Forge and Monmouth ====\n\nWashington and his Continental Army of 11,000 men went into winter quarters at Valley Forge north of Philadelphia in December 1777. There they lost between 2,000 and 3,000 men as a result of disease and lack of food, clothing, and shelter. The British were comfortably quartered in Philadelphia, paying for supplies in pounds sterling, while Washington struggled with a devalued American paper currency. The woodlands were soon exhausted of game. By February, Washington was facing lowered morale and increased desertions among his troops.An internal revolt by his officers, led by Major General Thomas Conway, prompted some members of Congress to consider removing Washington from command. Washington's supporters resisted, and the matter was dropped after much deliberation. Once the plot was exposed, Conway wrote an apology to Washington, resigned, and returned to France.Washington made repeated petitions to Congress for provisions. He received a congressional delegation to check the Army's conditions and expressed the urgency of the situation, proclaiming: \"Something must be done. Important alterations must be made.\" He recommended that Congress expedite supplies, and Congress agreed to strengthen and fund the army's supply lines by reorganizing the commissary department. By late February, supplies began arriving. Meanwhile, Baron Friedrich Wilhelm von Steuben's incessant drilling transformed Washington's recruits into a disciplined fighting force by the end of winter camp. For his services, Washington promoted Von Steuben to Major General and made him chief of staff.In early 1778, the French responded to Burgoyne's defeat and entered into a Treaty of Alliance with the Americans. Congress ratified the treaty in May, which amounted to a French declaration of war against Britain. In May 1778, Howe resigned and was replaced by Sir Henry Clinton.The British evacuated Philadelphia for New York that June and Washington summoned a war council of American and French generals. He chose a partial attack on the retreating British at the Battle of Monmouth. Generals Charles Lee and Lafayette moved with 4,000 men, without Washington's knowledge, and bungled their first attack on June 28. Washington relieved Lee and achieved a draw after an expansive battle. At nightfall, the British continued their retreat to New York, and Washington moved his army outside the city. Monmouth was Washington's last battle in the North.\n\n\n=== West Point espionage ===\n\nWashington became America's first spymaster by designing an espionage system against the British. In 1778, Major Benjamin Tallmadge formed the Culper Ring at Washington's direction to covertly collect information about the British in New York. Washington had disregarded incidents of disloyalty by Benedict Arnold, who had distinguished himself in many campaigns, including the Invasion of Quebec and the Battle of Saratoga.In 1780, Arnold began supplying British spymaster John Andr\u00e9 with sensitive information intended to compromise Washington and capture West Point, a key American defensive position on the Hudson River. Historians Nathaniel Philbrick and Ron Chernow noted possible reasons for Arnold's defection to be his anger at losing promotions to junior officers, or repeated slights from Congress. He was also deeply in debt, profiteering from the war, and disappointed by Washington's lack of support during his eventual court-martial.After repeated requests, Washington agreed to give Arnold command of West Point in August. On September 21, Arnold met Andr\u00e9 and gave him plans to take over the garrison. While returning to British lines, Andr\u00e9 was captured by militia who discovered the plans; upon hearing the news of Andr\u00e9's capture on September 24, while waiting to greet and have breakfast with Washington, Arnold immediately fleed to the HMS Vulture, the ship that had brought Andr\u00e9 to West Point, and escaped to New York.Upon being told about Arnold's treason, Washington recalled the commanders positioned under Arnold at key points around the fort to prevent any complicity. He assumed personal command at West Point and reorganized its defenses. Andr\u00e9's trial for espionage ended in a death sentence, and Washington offered to return him to the British in exchange for Arnold, but Clinton refused. Andr\u00e9 was hanged on October 2, 1780, despite his request for a firing squad, to deter other spies.\n\n\n=== Southern theater and Yorktown ===\n\nIn late 1778, General Clinton shipped 3,000 troops from New York to Georgia and launched a Southern invasion against Savannah, reinforced by 2,000 British and Loyalist troops. They repelled an attack by American patriots and French naval forces, which bolstered the British war effort.In June 1778, Iroquois warriors joined with Loyalist rangers led by Walter Butler and killed more than 200 frontiersmen, laying waste to the Wyoming Valley in Northeastern Pennsylvania. In mid-1779, in response to this and other attacks on New England towns, Washington ordered General John Sullivan to lead an expedition to force the Iroquois out of New York by effecting \"the total destruction and devastation\" of their villages and taking their women and children hostage. The expedition systematically destroyed Iroquois villages and food stocks, and forced at least 5,036 Iroquois to flee to British Canada. The campaign directly killed a few hundred Iroquois, but according to historian Rhiannon Koehler, the net effect was to reduce the Iroquois by half. They became unable to survive the harsh winter of 1779\u20131780; some historians now described the campaign as a genocide.Washington's troops went into quarters at Morristown, New Jersey for their worst winter of the war, with temperatures well below freezing. New York Harbor was frozen, snow covered the ground for weeks, and the troops again lacked provisions.In January 1780, Clinton assembled 12,500 troops and attacked Charles Town, South Carolina, defeating General Benjamin Lincoln. By June, they occupied the South Carolina Piedmont. Clinton returned to New York and left 8,000 troops under the command of General Charles Cornwallis. Congress replaced Lincoln with Horatio Gates; after his defeat in the Battle of Camden, Gates was replaced by Nathanael Greene, Washington's initial choice, but the British had firm control of the South. Washington was reinvigorated, however, when Lafayette returned from France with more ships, men, and supplies, and 5,000 veteran French troops led by Marshal Rochambeau arrived at Newport, Rhode Island in July 1780. French naval forces then landed, led by Admiral de Grasse.Washington's army went into winter quarters at New Windsor, New York in December 1780; he urged Congress and state officials to expedite provisions so the army would not \"continue to struggle under the same difficulties they have hitherto endured\". On March 1, 1781, Congress ratified the Articles of Confederation, but the government that took effect on March 2 did not have the power to levy taxes, and it loosely held the states together.General Clinton sent Benedict Arnold, now a British Brigadier General with 1,700 troops, to Virginia to capture Portsmouth and conduct raids on Patriot forces; Washington responded by sending Lafayette south to counter Arnold's efforts. Washington initially hoped to bring the fight to New York, drawing off British forces from Virginia and ending the war there, but Rochambeau advised him that Cornwallis in Virginia was the better target. De Grasse's fleet arrived off the Virginia coast, cutting off British retreat. Seeing the advantage, Washington made a feint towards Clinton in New York, then headed south to Virginia.\n\n\n==== Yorktown ====\n\nThe siege of Yorktown was a decisive victory by the combined forces of the Continental Army commanded by Washington, the French Army commanded by General Comte de Rochambeau, and the French Navy commanded by Admiral de Grasse. On August 19, the march to Yorktown led by Washington and Rochambeau began, which is known now as the \"celebrated march\". Washington was in command of an army of 7,800 Frenchmen, 3,100 militia, and 8,000 Continentals. Inexperienced in siege warfare, he often deferred to the judgment of General Rochambeau and relied on his advice. Despite this, Rochambeau never challenged Washington's authority as the battle's commanding officer.By late September, Patriot-French forces surrounded Yorktown, trapped the British Army, and prevented British reinforcements from Clinton in the North, while the French navy emerged victorious at the Battle of the Chesapeake. The final American offensive began with a shot fired by Washington. The siege ended with a British surrender on October 19, 1781; over 7,000 British soldiers became prisoners of war. Washington negotiated the terms of surrender for two days, and the official signing ceremony took place on October 19; Cornwallis claimed illness and was absent, sending General Charles O'Hara as his proxy.  As a gesture of goodwill, Washington held a dinner for the American, French, and British generals, all of whom fraternized on friendly terms and identified with one another as members of the same professional military caste.Afterwards, Washington moved the army to New Windsor, New York where they remained stationed until the Treaty of Paris was signed on September 3, 1783, formally ending the war. Although the peace treaty did not happen for two years following the end of the battle, Yorktown proved to be the last significant battle or campaign of the Revolutionary War, with the British Parliament agreeing to cease hostilities in March 1782.\n\n\n=== Demobilization and resignation ===\n\nWhen peace negotiations began in April 1782, both the British and French began gradually evacuating their forces. With the American treasury empty, unpaid and mutinous soldiers forced the adjournment of Congress. In March 1783, Washington successfully calmed the Newburgh Conspiracy, a planned munity by American officers; Congress promised each a five-year bonus. Washington submitted an account of $450,000 in expenses which he had advanced to the army, equivalent to $9.15 million in 2022. The account was settled, though it was allegedly vague about large sums and included expenses his wife had incurred through visits to his headquarters.The following month, a Congressional committee led by Alexander Hamilton began adapting the army for peacetime. In August 1783, Washington gave the Army's perspective to the committee in his Sentiments on a Peace Establishment, which advised Congress to keep a standing army, create a \"national militia\" of separate state units, and establish a navy and a national military academy.The Treaty of Paris was signed on September 3, 1783, and Britain officially recognized American independence. Washington disbanded his army, giving a farewell address to his soldiers on November 2. During this time, Washington oversaw the evacuation of British forces in New York and was greeted by parades and celebrations. Along with Governor George Clinton, he took formal possession of the city on November 25.In early December 1783, Washington bade farewell to his officers at Fraunces Tavern and resigned as commander-in-chief soon thereafter. In a final appearance in uniform, he gave a statement to the Congress: \"I consider it an indispensable duty to close this last solemn act of my official life, by commending the interests of our dearest country to the protection of Almighty God, and those who have the superintendence of them, to his holy keeping.\" Washington's resignation was acclaimed at home and abroad and showed a skeptical world that the new republic would not degenerate into chaos.The same month, Washington was appointed president-general of the Society of the Cincinnati, a newly established hereditary fraternity of Revolutionary War officers. He served in this capacity for the remainder of his life.\n\n\n== Early republic (1783\u20131789) ==\n\n\n=== Return to Mount Vernon ===\n\nWashington was longing to return home after spending just ten days at Mount Vernon out of 8+1\u20442 years of war. He arrived on Christmas Eve, delighted to be \"free of the bustle of a camp and the busy scenes of public life\". He was a celebrity and was f\u00eated during a visit to his mother at Fredericksburg in February 1784, and he received a constant stream of visitors wishing to pay their respects at Mount Vernon.Washington reactivated his interests in the Great Dismal Swamp and Potomac canal projects begun before the war, though neither paid him any dividends, and he undertook a 34-day, 680-mile (1,090 km) trip to check on his land holdings in the Ohio Country. He oversaw the completion of the remodeling work at Mount Vernon, which transformed his residence into the mansion that survives to this day\u2014although his financial situation was not strong. Creditors paid him in depreciated wartime currency, and he owed significant amounts in taxes and wages. Mount Vernon had made no profit during his absence, and he saw persistently poor crop yields due to pestilence and poor weather. His estate recorded its eleventh year running at a deficit in 1787, and there was little prospect of improvement.To make his estate profitable again, Washington undertook a new landscaping plan and succeeded in cultivating a range of fast-growing trees and native shrubs. He also began breeding mules after being gifted a Spanish jack by King Charles III of Spain in 1784. There were few mules in the United States at that time, and he believed that they would revolutionize agriculture and transportation.\n\n\n=== Constitutional Convention of 1787 ===\n\nBefore returning to private life in June 1783, Washington called for a strong union. Though he was concerned that he might be criticized for meddling in civil matters, he sent a circular letter to the states, maintaining that the Articles of Confederation was no more than \"a rope of sand\". He believed the nation was on the verge of \"anarchy and confusion\", was vulnerable to foreign intervention, and that a national constitution would unify the states under a strong central government.When Shays' Rebellion erupted in Massachusetts over taxation, Washington was further convinced that a national constitution was needed. Some nationalists feared that the new republic had descended into lawlessness, and they met on September 11, 1786, at Annapolis to ask Congress to revise the Articles of Confederation. One of their biggest efforts was getting Washington to attend. Congress agreed to a Constitutional Convention to be held in Philadelphia in Spring 1787, with each state to send delegates.On December 4, 1786, Washington was chosen to lead the Virginia delegation, but he declined on December 21. He had concerns about the legality of the convention and consulted James Madison, Henry Knox, and others. They persuaded him to attend as his presence might induce reluctant states to send delegates and smooth the way for the ratification process while also giving legitimacy to the convention. On March 28, Washington told Governor Edmund Randolph that he would attend the convention but made it clear that he was urged to attend.Washington arrived in Philadelphia on May 9, 1787, though a quorum was not attained until May 25. Benjamin Franklin nominated Washington to preside over the convention, and he was unanimously elected to serve as president general. The convention's state-mandated purpose was to revise the Articles of Confederation, and the new government would be established when the resulting document was \"duly confirmed by the several states\". Randolph introduced Madison's Virginia Plan on May 27, the third day of the convention. It called for an entirely new constitution and a sovereign national government, which Washington highly recommended.On July 10, Washington wrote to Alexander Hamilton: \"I almost despair of seeing a favorable issue to the proceedings of our convention and do therefore repent having had any agency in the business.\" Nevertheless, he lent his prestige to the work of the other delegates, unsuccessfully lobbying many to support ratification of the Constitution, such as anti-federalists Edmund Randolph and George Mason. The final version was voted on and signed by 39 of 55 delegates on September 17, 1787.\n\n\n=== Chancellor of William & Mary ===\nIn 1788, the Board of Visitors of the College of William & Mary decided to re-establish the position of Chancellor, and elected Washington to the office on January 18. The College Rector Samuel Griffin wrote to Washington inviting him to the post, and in a letter dated April 30, 1788, Washington accepted the position of the 14th Chancellor of the College of William & Mary. He continued to serve through his presidency until his death on December 14, 1799.\n\n\n=== First presidential election ===\n\nThe delegates to the Convention anticipated a Washington presidency and left it to him to define the office once elected.The state electors under the Constitution voted for the president on February 4, 1789, and Washington suspected that most republicans had not voted for him. The mandated March 4 date passed without a Congressional quorum to count the votes, but a quorum was reached on April 5. The votes were tallied the next day, and Washington won the majority of every state's electoral votes. He was informed of his election as president by Congressional Secretary Charles Thomson. John Adams received the next highest number of votes and was elected vice president. Despite feeling \"anxious and painful sensations\" about leaving Mount Vernon, he departed for New York City on April 16 to be inaugurated.\n\n\n== Presidency (1789\u20131797) ==\n\nWashington was inaugurated on April 30, 1789, taking the oath of office at Federal Hall in New York City. His coach was led by militia and a marching band and followed by statesmen and foreign dignitaries in an inaugural parade, with a crowd of 10,000. Chancellor Robert R. Livingston administered the oath, using a Bible provided by the Masons, after which the militia fired a 13-gun salute. Washington read a speech in the Senate Chamber, asking \"that Almighty Being ... consecrate the liberties and happiness of the people of the United States\". Though he wished to serve without a salary, Congress insisted that he accept it, later providing Washington $25,000 per year to defray costs of the presidency, equivalent to $6.14 million today.\nWashington wrote to James Madison: \"As the first of everything in our situation will serve to establish a precedent, it is devoutly wished on my part that these precedents be fixed on true principles.\" To that end, he preferred the title \"Mr. President\" over more majestic names proposed by the Senate, including \"His Excellency\" and \"His Highness the President\". His executive precedents included the inaugural address, messages to Congress, and the cabinet form of the executive branch.Washington planned to resign after his first term, but political strife convinced him to remain in office. He was an able administrator and a judge of talent and character, and he regularly talked with department heads to get their advice. He tolerated opposing views, despite fears that a democratic system would lead to political violence, and he conducted a smooth transition of power to his successor. He remained non-partisan throughout his presidency and opposed the divisiveness of political parties, but he favored a strong central government, was sympathetic to a Federalist form of government, and leery of the Republican opposition.Washington dealt with major problems. The old Confederation lacked the powers to handle its workload and had weak leadership, no executive, a small bureaucracy of clerks, large debt, worthless paper money, and no power to establish taxes. He had the task of assembling an executive department and relied on Tobias Lear for advice selecting its officers. Britain refused to relinquish its forts in the American West, and Barbary pirates preyed on American merchant ships in the Mediterranean before the United States even had a navy.\n\n\n=== Cabinet and executive departments ===\n\nCongress created executive departments in 1789, including the State Department in July, the War Department in August, and the Treasury Department in September. Washington appointed Edmund Randolph as Attorney General, Samuel Osgood as Postmaster General, Thomas Jefferson as Secretary of State, Henry Knox as Secretary of War, and Alexander Hamilton as Secretary of the Treasury. Washington's cabinet became a consulting and advisory body, not mandated by the Constitution.Washington's cabinet members formed rival parties with sharply opposing views, most fiercely illustrated between Hamilton and Jefferson. Washington restricted cabinet discussions to topics of his choosing, without participating in the debate. He occasionally requested cabinet opinions in writing and expected department heads to agreeably carry out his decisions.\n\n\n=== Domestic issues ===\nWashington was apolitical and opposed the formation of parties, suspecting that conflict would undermine republicanism. He exercised great restraint in using his veto power, writing that \"I give my Signature to many Bills with which my Judgment is at variance...\"His closest advisors formed two factions, portending the First Party System. Secretary of the Treasury Alexander Hamilton formed the Federalist Party to promote national credit and a financially powerful nation. Secretary of State Thomas Jefferson opposed Hamilton's agenda and founded the Jeffersonian Republicans. Washington favored Hamilton's agenda, however, and it ultimately went into effect\u2014resulting in bitter controversy.Washington proclaimed November 26, 1789, as a day of Thanksgiving to encourage national unity. \"It is the duty of all nations to acknowledge the providence of Almighty God, to obey His will, to be grateful for His benefits, and humbly to implore His protection and favor.\" He spent that day fasting and visiting debtors in prison to provide them with food and beer.\n\n\n==== African Americans ====\nIn response to two antislavery petitions that were presented to Congress in 1790, slaveholders in Georgia and South Carolina threatened to \"blow the trumpet of civil war\". Washington and Congress responded with a series of racist measures: naturalization was denied to black immigrants; blacks were barred from serving in state militias; the Southwest Territory (later the state of Tennessee) was permitted to maintain slavery; and two more slave states were admitted (Kentucky in 1792 and Tennessee in 1796). On February 12, 1793, Washington signed into law the Fugitive Slave Act, which overrode state laws and courts, allowing agents to cross state lines to return escaped slaves. Many free blacks in the north decried the law believing it would allow bounty hunting and kidnapping. The Fugitive Slave Act gave effect to the Constitution's Fugitive Slave Clause, and the Act was passed overwhelmingly in Congress.At the same time, Washington signed a reenactment of the Northwest Ordinance in 1789, which had freed all slaves brought after 1787 into a vast expanse of federal territory north of the Ohio River, except for slaves escaping from slave states. The 1787 law lapsed when the new U.S. Constitution was ratified in 1789. He also signed the Slave Trade Act of 1794, which sharply limited American involvement in the Atlantic slave trade. On February 18, 1791, Congress admitted the free state of Vermont into the Union as the 14th state as of March 4, 1791.\n\n\n==== National Bank ====\nWashington's first term was largely devoted to economic concerns. Establishment of public credit became a primary challenge for the federal government. Hamilton submitted a report to a deadlocked Congress, and he, Madison, and Jefferson reached the Compromise of 1790 in which Jefferson agreed to Hamilton's debt proposals in exchange for moving the nation's capital temporarily to Philadelphia and then south near Georgetown on the Potomac River. The terms were legislated in the Funding Act of 1790 and the Residence Act, both of which Washington signed into law. Congress authorized the assumption and payment of the nation's debts, with funding provided by customs duties and excise taxes.Hamilton caused controversy in Cabinet by advocating for the establishment of the First Bank of the United States. Madison and Jefferson objected to the idea, but legislation creating the bank easily passed Congress. Jefferson and Randolph insisted the federal government was going beyond its constitutional authority. Hamilton argued the government could charter the bank under the implied powers granted by the constitution. Washington sided with Hamilton and signed the bank legislation on February 25, 1791. The rift between Hamilton and Jefferson, meanwhile, became openly hostile.The nation's first financial crisis occurred in March 1792. Hamilton's Federalists exploited large loans to gain control of U.S. debt securities, causing a run on the national bank; the markets returned to normal by mid-April. Jefferson believed Hamilton was part of the scheme, despite Hamilton's efforts to ameliorate.\n\n\n==== Jefferson\u2013Hamilton feud ====\n\nJefferson and Hamilton adopted diametrically opposed political principles. Hamilton believed in a strong national government requiring a national bank and foreign loans to function, while Jefferson believed the states and the farm element should primarily direct the government; he also resented the idea of banks and foreign loans. To Washington's dismay, the two men persistently entered into disputes and infighting. Hamilton demanded that Jefferson resign if he could not support Washington, and Jefferson told Washington that Hamilton's fiscal system would lead to the overthrow of the republic. Washington urged them to call a truce for the sake of the nation, but they ignored him.Jefferson's political actions, his support of Freneau's National Gazette, and his attempts to undermine Hamilton nearly led Washington to dismiss him from the cabinet; he ultimately resigned his position in December 1793, and Washington forsook him.The feud led to the well-defined Federalist and Republican parties, and party affiliation became necessary for election to Congress by 1794. Washington remained aloof from congressional attacks on Hamilton, but did not publicly protect him. The Hamilton\u2013Reynolds sex scandal opened Hamilton to disgrace, but Washington continued to hold him in \"very high esteem\".\n\n\n==== Whiskey Rebellion ====\n\nIn March 1791, at Hamilton's urging, with support from Madison, Congress imposed an excise tax on distilled spirits to help curtail the national debt, which took effect in July. Grain farmers strongly protested in Pennsylvania's frontier districts; they argued that they were unrepresented and were shouldering too much of the debt, comparing their situation to British taxation pre-Revolution.\nOn August 2, Washington assembled his cabinet to discuss the situation. Unlike Washington, who had reservations about using force, Hamilton was eager to suppress the rebellion with federal authority. Wanting to avoid involving the federal government, Washington first called on Pennsylvania state officials to take the initiative, but they declined. On August 7, Washington issued his first proclamation for calling up state militias. After appealing for peace, he reminded the protestors that, unlike the rule of the British crown, the Federal law was issued by state-elected representatives.Threats and violence against tax collectors, however, escalated into defiance against federal authority in 1794 and gave rise to the Whiskey Rebellion. Washington issued a final proclamation on September 25, threatening the use of military force to no avail. The federal army was not up to the task, so Washington invoked the Militia Act of 1792 to summon state militias. Governors sent troops, initially commanded by Washington, who handed over command to Henry Lee to lead them into the rebellious districts. They took 150 prisoners, and the remaining rebels dispersed. Two of the prisoners were condemned to death, but Washington exercised his Constitutional authority for the first time and pardoned them.Washington's forceful action demonstrated that the new government could protect itself and its tax collectors. This represented the first use of federal military force against the states and citizens. Washington justified his action against \"certain self-created societies\", which he regarded as \"subversive organizations\" that threatened the national union. He did not dispute their right to protest, but he insisted that their dissent must not violate federal law. Congress agreed and extended their congratulations to him; only Madison and Jefferson expressed indifference.\n\n\n=== Foreign affairs ===\nIn April 1792, the French Revolutionary Wars began between Britain and France, and Washington declared America's neutrality. The revolutionary government of France sent diplomat Edmond-Charles Gen\u00eat to America, and he was welcomed with great enthusiasm. He created a network of new Democratic-Republican Societies promoting France's interests, but Washington denounced them and demanded that the French recall Gen\u00eat. The National Assembly of France granted Washington honorary French citizenship on August 26, 1792, during the early stages of the French Revolution.Hamilton formulated the Jay Treaty to normalize trade relations with Britain while removing them from western forts, and also to resolve financial debts remaining from the Revolution. Chief Justice John Jay acted as Washington's negotiator and signed the treaty on November 19, 1794; critical Jeffersonians, however, supported France. Washington deliberated, then supported the treaty because it avoided war with Britain, but was disappointed that its provisions favored Britain. He mobilized public opinion and secured ratification in the Senate but faced frequent public criticism.The British agreed to abandon their forts around the Great Lakes, and the United States modified the boundary with Canada. The government liquidated numerous pre-Revolution debts, and the British opened the British West Indies to American trade. The treaty secured peace with Britain and a decade of prosperous trade. Jefferson claimed that it angered France and \"invited rather than avoided\" war. Relations with France deteriorated afterward and, two days before Washington's term ended, the French Directory declared the authority to seize American ships, leaving succeeding president John Adams with prospective war.\n\n\n=== Native American affairs ===\n\nDuring the fall of 1789, Washington had to contend with the British refusing to evacuate their forts in the Northwest frontier and their concerted efforts to incite Indian tribes to attack American settlers. The Northwest tribes under Miami chief Little Turtle allied with the British to resist American expansion, and killed 1,500 settlers between 1783 and 1790.\nWashington declared that \"the Government of the United States are determined that their Administration of Indian Affairs shall be directed entirely by the great principles of Justice and humanity\", and provided that treaties should negotiate their land interests. The administration regarded powerful tribes as foreign nations, and Washington even smoked a peace pipe and drank wine with them at the President's House in Philadelphia. He made numerous attempts to conciliate them; he equated killing indigenous peoples with killing whites and sought to integrate them into European American culture.In the Southwest, negotiations failed between federal commissioners and raiding Indian tribes seeking retribution. Washington invited Creek Chief Alexander McGillivray and 24 leading chiefs to New York to negotiate a treaty and treated them like foreign dignitaries. Knox and McGillivray concluded the Treaty of New York on August 7, 1790, which provided the tribes with agricultural supplies and McGillivray with the rank of Brigadier General and an annual salary of $1,200, equivalent to $28,404 in 2022.In 1790, Washington sent Brigadier General Josiah Harmar to pacify the Northwest tribes, but Little Turtle routed him twice and forced him to withdraw. The Northwestern Confederacy of tribes used guerrilla tactics and were an effective force against the sparsely manned American Army. Washington sent Major General Arthur St. Clair from Fort Washington on an expedition to restore peace in the territory in 1791. On November 4, St. Clair's forces were ambushed and soundly defeated by tribal forces with few survivors.Washington replaced the disgraced St. Clair with the Revolutionary War hero Anthony Wayne. From 1792 to 1793, Wayne instructed his troops on Native American warfare tactics and instilled discipline which was lacking under St. Clair. In August 1794, Washington sent Wayne into tribal territory with authority to drive them out by burning their villages and crops in the Maumee Valley. On August 24, the American army defeated the Northwestern Confederacy at the Battle of Fallen Timbers, and the Treaty of Greenville in August 1795 opened two-thirds of the Ohio Country for American settlement.\n\n\n=== Second term ===\nWashington initially planned to retire after his first term, weary of office and in poor health. After dealing with the infighting in his own cabinet and with partisan critics, he showed little enthusiasm for a second term, while Martha also wanted him not to run. Washington's nephew George Augustine Washington, managing Mount Vernon in his absence, was critically ill, further increasing Washington's desire to retire.Many, however, urged him to run for a second term. Madison told him that his absence would only allow the dangerous political rift in his cabinet and the House to worsen. Jefferson also pleaded with him not to retire, agreeing to drop his attacks on Hamilton, and stating that he would also retire if Washington did. Hamilton maintained that Washington's absence would be \"deplored as the greatest evil\" to the country. With the election of 1792 nearing, Washington relented and agreed to run.On February 13, 1793, the Electoral College unanimously re-elected Washington president, and John Adams as vice president by a vote of 77 to 50. He was sworn into office by Associate Justice William Cushing on March 4, 1793, in the Senate Chamber of Congress Hall in Philadelphia. Afterwards, Washington gave a brief address before immediately retiring to the President's House.On April 22, 1793, when the French Revolutionary Wars broke out, Washington issued a proclamation which declared American neutrality. He was resolved to pursue \"a conduct friendly and impartial toward the belligerent Powers\" while also warning Americans not to intervene in the conflict. Although Washington recognized France's revolutionary government, he would eventually ask French minister to the United States Edmond-Charles Gen\u00eat be recalled over the Citizen Gen\u00eat affair. Gen\u00eat was a diplomatic troublemaker who was openly hostile toward Washington's neutrality policy. He procured four American ships as privateers to strike at Spanish forces (British allies) in Florida while organizing militias to strike at other British possessions. However, his efforts failed to draw the United States into the conflict.On July 31, 1793, Jefferson submitted his resignation from cabinet. Hamilton, desiring more income for his family, resigned from office in January 1795 and was replaced by Oliver Wolcott Jr.. While his relationship with Washington would remain friendly, Washington's relationship with his Secretary of War Henry Knox deteriorated after rumors that Knox had profited from contracts for the construction of U.S. frigates which had been commissioned under the Naval Act of 1794 in order to combat Barbary pirates, forcing Knox to resign.In the final months of his presidency, Washington was assailed by his political foes and a partisan press who accused him of being ambitious and greedy. He came to regard the press as a disuniting, \"diabolical\" force of falsehoods. At the end of his second term, Washington retired for personal and political reasons, dismayed with personal attacks, and to ensure that a truly contested presidential election could be held. He did not feel bound to a two-term limit, but his retirement set a significant precedent.\n\n\n=== Farewell Address ===\n\nIn 1796, Washington declined to run for a third term of office. In May 1792, in anticipation of his retirement, Washington instructed James Madison to prepare a \"valedictory address\", an initial draft of which was entitled the \"Farewell Address\". In May 1796, Washington sent the manuscript to Alexander Hamilton who did an extensive rewrite, while Washington provided final edits. On September 19, 1796, David Claypoole's American Daily Advertiser published the final version.Washington stressed that national identity was paramount, as a united America would safeguard freedom and prosperity. He warned the nation of three eminent dangers: regionalism, partisanship, and foreign entanglements, and said the \"name of AMERICAN, which belongs to you, in your national capacity, must always exalt the just pride of patriotism\". Washington called for men to move beyond partisanship for the common good, stressing that the United States must concentrate on its own interests. He warned against foreign alliances and their influence in domestic affairs, and bitter partisanship and the dangers of political parties. He counseled friendship and commerce with all nations, but advised against involvement in European wars. He stressed the importance of religion, asserting that \"religion and morality are indispensable supports\" in a republic. Washington's address favored Hamilton's Federalist ideology and economic policies.He closed the address by reflecting on his legacy:\n\nThough in reviewing the incidents of my Administration I am unconscious of intentional error, I am nevertheless too sensible of my defects not to think it probable that I may have committed many errors. Whatever they may be, I fervently beseech the Almighty to avert or mitigate the evils to which they may tend. I shall also carry with me the hope that my country will never cease to view them with indulgence, and that, after forty-five years of my life dedicated to its service with an upright zeal, the faults of incompetent abilities will be consigned to oblivion, as myself must soon be to the mansions of rest.\nAfter initial publication, many Republicans, including Madison, criticized the Address and described it as an anti-French campaign document, with Madison believing that Washington was strongly pro-British.In 1839, Washington biographer Jared Sparks maintained that Washington's \"Farewell Address was printed and published with the laws, by order of the legislatures, as an evidence of the value they attached to its political precepts, and of their affection for its author.\" In 1972, Washington scholar James Flexner referred to the Farewell Address as receiving as much acclaim as Thomas Jefferson's Declaration of Independence and Abraham Lincoln's Gettysburg Address. In 2010, historian Ron Chernow called the Farewell Address one of the most influential statements on republicanism.\n\n\n== Post-presidency (1797\u20131799) ==\n\n\n=== Retirement ===\nWashington retired to Mount Vernon in March 1797 and devoted time to his plantations and other business interests. His plantation operations were only minimally profitable, and his lands in the west (Piedmont) were under Indian attacks and yielded little income, with squatters there refusing to pay rent. He attempted to sell these but without success. He became an even more committed Federalist. He vocally supported the Alien and Sedition Acts and convinced Federalist John Marshall to run for Congress to weaken the Jeffersonian hold on Virginia.Washington grew restless in retirement, prompted by tensions with France; in a continuation of the French Revolutionary Wars, French privateers began seizing American ships in 1798, and relations deteriorated with France and led to the \"Quasi-War\". Washington wrote to Secretary of War James McHenry offering to organize President Adams' army. Adams nominated him for a lieutenant general commission on July 4, 1798, and the position of commander-in-chief of the armies. Washington served as the commanding general from July 13, 1798, until his death 17 months later. He participated in planning for a provisional army, but avoided involvement in details. In advising McHenry of potential officers for the army, he appeared to make a complete break with Jefferson's Democratic-Republicans: \"you could as soon scrub the blackamoor white, as to change the principles of a profest Democrat; and that he will leave nothing unattempted to overturn the government of this country.\" Washington delegated the active leadership of the army to Hamilton, a major general. No army invaded the United States during this period, and Washington did not assume a field command.Washington was known to be rich because of the well-known \"glorified fa\u00e7ade of wealth and grandeur\" at Mount Vernon, but nearly all his wealth was in the form of land and slaves rather than ready cash. To supplement his income, he erected a distillery for substantial whiskey production. He bought land parcels to spur development around the new Federal City named in his honor, and he sold individual lots to middle-income investors rather than multiple lots to large investors, believing they would more likely commit to making improvements.\n\n\n=== Final days and death ===\nOn December 12, 1799, Washington inspected his farms on horseback. He returned home late and had guests for dinner. He had a sore throat the next day but was well enough to mark trees for cutting. That evening, Washington complained of chest congestion. The next morning, however, he awoke to an inflamed throat and difficulty breathing. He ordered estate overseer George Rawlins to remove nearly a pint of his blood; bloodletting was a common practice of the time. His family summoned doctors James Craik, Gustavus Richard Brown, and Elisha C. Dick. A fourth doctor, William Thornton, arrived some hours after Washington died.Brown initially believed Washington had quinsy; Dick thought the condition was a more serious \"violent inflammation of the throat\". They continued the process of bloodletting to approximately five pints, but Washington's condition deteriorated further. Dick proposed a tracheotomy, but the other physicians were not familiar with that procedure and disapproved. Washington instructed Brown and Dick to leave the room, while he assured Craik, \"Doctor, I die hard, but I am not afraid to go.\"Washington's death came more swiftly than expected. On his deathbed, out of fear of being entombed alive, he instructed his private secretary Tobias Lear to wait three days before his burial. According to Lear, Washington died between 10 p.m. and 11 p.m. on December 14, 1799, with Martha seated at the foot of his bed. His last words were \"'Tis well\", from his conversation with Lear about his burial. He was 67.Congress immediately adjourned for the day upon news of Washington's death, and the Speaker's chair was shroud in black the next morning. The funeral was held four days after his death on December 18, 1799, at Mount Vernon, where his body was interred. Cavalry and foot soldiers led the procession, and six colonels served as the pallbearers. The Mount Vernon funeral service was restricted mostly to family and friends. Reverend Thomas Davis read the funeral service by the vault with a brief address, followed by a ceremony performed by members of Washington's Masonic lodge in Alexandria, Virginia. Word of his death traveled slowly; church bells rang in the cities, and many businesses closed. Memorial processions were held in major cities of the United States. Martha wore a black mourning cape for one year, and she burned their correspondence to protect their privacy. Only five letters between the couple are known to have survived: two from Martha to George and three from him to her.The diagnosis of Washington's illness and the immediate cause of his death have been subjects of debate since his death. The published account of doctors Craik and Brown stated that his symptoms were consistent with cynanche trachealis, a term then used to describe severe inflammation of the upper windpipe, including quinsy. Accusations have persisted since Washington's death concerning medical malpractice. Modern medical authors have concluded that he likely died from severe epiglottitis complicated by the treatments, including multiple doses of calomel, a purgative, and extensive bloodletting which almost certainly caused hypovolemic shock.\n\n\n== Burial, net worth, and aftermath ==\n\nWashington was buried in the old Washington family vault at Mount Vernon. At the time of his death, his estate was worth an estimated $780,000 in 1799, equivalent to $13.72 million in 2022. Washington's peak net worth was $587 million, including 300 slaves. Washington held title to more than 65,000 acres of land in 37 different locations.In 1830, a disgruntled ex-employee of the estate attempted to steal what he thought was Washington's skull, prompting the construction of a more secure vault. In his will, Washington had left instructions for the construction of a new vault as the old family vault was crumbling and needed repair even before his death. A new vault was constructed at Mount Vernon the following year to receive the remains of George and Martha and other relatives.In 1832, a joint Congressional committee debated moving his body from Mount Vernon to a crypt in the Capitol. The crypt had been built by architect Charles Bulfinch in the 1820s during the reconstruction of the burned-out capital, after the Burning of Washington by the British during the War of 1812. Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on \"a shore foreign to his native soil\" if the country became divided, and Washington's remains stayed in Mount Vernon.On October 7, 1837, Washington's remains, still in the original lead coffin, were placed within a marble sarcophagus designed by William Strickland and constructed by John Struthers. The sarcophagus was sealed and encased with planks, and an outer vault was constructed around it. The outer vault has the sarcophagi of both George and Martha Washington; the inner vault has the remains of other Washington family members and relatives.\n\n\n== Personal life ==\nWashington was somewhat reserved in personality, but was known for having a strong presence. He made speeches and announcements when required, but he was not a noted orator or debater. He was taller than most of his contemporaries; accounts of his height vary from 6 ft (1.83 m) to 6 ft 3.5 in (1.92 m) tall, he weighed between 210\u2013220 pounds (95\u2013100 kg) as an adult, and was known for his great strength.He had grey-blue eyes and long reddish-brown hair. He did not wear a powdered wig; instead he wore his hair curled, powdered, and tied in a queue in the fashion of the day.Washington frequently suffered from severe tooth decay and ultimately lost all his teeth but one. He had several sets of false teeth during his presidency. Contrary to common lore, these were not made of wood, but of metal, ivory, bone, animal teeth, and human teeth possibly obtained from slaves. These dental problems left him in constant pain, which he treated with laudanum.Washington was a talented equestrian, with Thomas Jefferson describing him as \"the best horseman of his age\". He collected thoroughbreds at Mount Vernon, his two favorite horses being Blueskin and Nelson. He enjoyed hunting foxes, deer, ducks, and other game. He was an excellent dancer and frequently attended the theater. He drank alcohol in moderation but was morally opposed to excessive drinking, smoking tobacco, gambling, and profanity.\n\n\n=== Religion and Freemasonry ===\n\nWashington was descended from Anglican minister Lawrence Washington, whose troubles with the Church of England may have prompted his heirs to emigrate to America. He was baptized as an infant in April 1732 and became a devoted member of the Anglican Church. He served more than 20 years as a vestryman and churchwarden at Fairfax Parish and Truco Parish in Virginia. He privately prayed and read the Bible daily, and publicly encouraged people and the nation to pray. He may have taken communion on a regular basis prior to the Revolution, but he did not do so following the war.Washington believed in a \"wise, inscrutable, and irresistible\" Creator God who was active in the Universe, contrary to deistic thought. He referred to God in Enlightenment terms, including Providence, the Creator, or the Almighty, and the Divine Author or Supreme Being. He believed in a divine power who watched over battlefields, was involved in the outcome of war, protected his life, and was involved in American politics and specifically the creation of the United States. Historian Ron Chernow has argued that Washington avoided evangelistic Christianity or hellfire-and-brimstone speech along with communion or anything inclined to \"flaunt his religiosity\", saying that he \"never used his religion as a device for partisan purposes or in official undertakings\". No mention of Jesus Christ appears in his private correspondence, and such references are rare in his public writings. At the same time, Washington frequently quoted from the Bible or paraphrased it, and often referred to the Anglican Book of Common Prayer.Washington emphasized religious toleration in a nation with numerous denominations and religions. He publicly attended services of different Christian denominations and prohibited anti-Catholic celebrations in the Army. He engaged workers at Mount Vernon without regard for religious belief or affiliation. While president, he acknowledged major religious sects and gave speeches on religious toleration. He was distinctly rooted in the ideas, values, and modes of thinking of the Enlightenment, but he harbored no contempt of organized Christianity and its clergy, \"being no bigot myself to any mode of worship\". In 1793, speaking to members of the New Church in Baltimore, Washington said, \"We have abundant reason to rejoice that in this Land the light of truth and reason has triumphed over the power of bigotry and superstition.\"Freemasonry was a widely accepted institution in the late 18th century, known for advocating moral teachings. Washington was attracted to the Masons' dedication to the Enlightenment principles of rationality, reason, and brotherhood. American Masonic lodges did not share the anti-clerical views of the controversial European lodges.A Masonic lodge was established in Fredericksburg, Virginia in September 1752, and Washington was initiated two months later at the age of 20 as one of its first Entered Apprentices. Within a year, he progressed through its ranks to become a Master Mason. Washington had high regard for the Masonic Order, but his lodge attendance was sporadic. In 1777, a convention of Virginia lodges asked him to be the Grand Master of the newly established Grand Lodge of Virginia, but he declined due to his commitments leading the Continental Army. After 1782, he frequently corresponded with Masonic lodges and members, and he was listed as Master in the Virginia charter of Alexandria Lodge No. 22 in 1788.\n\n\n== Slavery ==\n\nIn Washington's lifetime, slavery was deeply ingrained in the economic and social fabric of the Colony of Virginia, which continued after the Revolution and the establishment of Virginia as a state. Slavery was legal in all of the Thirteen Colonies prior to the American Revolution.\n\n\n=== Washington's slaves ===\nWashington owned and rented enslaved African Americans, and during his lifetime over 577 slaves lived and worked at Mount Vernon. He acquired them through inheritance, gaining control of 84 dower slaves upon his marriage to Martha, and purchased at least 71 slaves between 1752 and 1773. From 1786, he rented slaves; at the time of his death he was renting 41.Prior to the Revolutionary War, Washington's view on slavery was the same as most Virginia planters of the time. Beginning in the 1760s, however, Washington gradually grew to oppose it. His first doubts were prompted by his transition from tobacco to grain crops, which left him with a costly surplus of slaves, causing him to question the system's economic efficiency. His growing disillusionment with the institution was spurred by the principles of the Revolution and revolutionary friends such as Lafayette and Hamilton. Most historians agree the Revolution was central to the evolution of Washington's attitudes on slavery; \"After 1783,\" Kenneth Morgan writes, \"... [Washington] began to express inner tensions about the problem of slavery more frequently, though always in private\". Regardless, Washington would remain dependent on slave labor to work his farms.The many contemporary reports of slave treatment at Mount Vernon are varied and conflicting. Historian Kenneth Morgan maintains that Washington was frugal on spending for clothes and bedding for his slaves, and only provided them with just enough food, and that he maintained strict control over his slaves, instructing his overseers to keep them working hard from dawn to dusk year-round. In contrast, historian Dorothy Twohig said: \"Food, clothing, and housing seem to have been at least adequate\".Washington faced growing debts involved with the costs of supporting slaves. He held an \"engrained sense of racial superiority\" towards African Americans but harbored no ill feelings toward them. Some enslaved families worked at different locations on the plantation but were allowed to visit one another on their days off. Washington's slaves received two hours off for meals during the workday and were given time off on Sundays and religious holidays.Some accounts report that Washington opposed flogging but at times sanctioned its use, generally as a last resort, on both men and women slaves. Washington used both reward and punishment to encourage discipline and productivity in his slaves. He tried appealing to an individual's sense of pride, gave better blankets and clothing to the \"most deserving\", and motivated his slaves with cash rewards. He believed \"watchfulness and admonition\" were better deterrents against transgressions but would punish those who \"will not do their duty by fair means\". Punishment ranged in severity from demotion back to fieldwork, through whipping and beatings, to permanent separation from friends and family by sale. Historian Ron Chernow maintains that overseers were required to warn slaves before resorting to the lash and required Washington's written permission before whipping, though his extended absences did not always permit this.\nDuring his presidency, Washington brought several of his slaves to the federal capital. When the capital moved from New York City to Philadelphia in 1791, the president began rotating his slave household staff periodically between the capital and Mount Vernon. This was done deliberately to circumvent Pennsylvania's Slavery Abolition Act, which stated that any slave who lived there for more than six months was automatically freed.In May 1796, Martha's personal and favorite slave Ona Judge escaped to Portsmouth, New Hampshire. At Martha's behest, Washington attempted to capture Ona, using a Treasury agent, but failed. In February 1797, around the time of his 65th birthday, Washington's personal slave Hercules Posey escaped from Mount Vernon to Philadelphia and was never found.In February 1786, Washington took a census of Mount Vernon and recorded 224 slaves. By 1799, the slave population at Mount Vernon totaled 317, including 143 children. Washington owned 124 slaves, leased 40, and held 153 for his wife's dower interest. Washington supported many slaves who were too young or too old to work, greatly increasing Mount Vernon's slave population and causing the plantation to operate at a loss.\n\n\n=== Abolition and manumission ===\n\nBased on his private papers and on accounts from his contemporaries, Washington slowly developed a cautious sympathy toward abolitionism that eventually ended with his will freeing his long-time valet Billy Lee, and then subsequently freeing the rest of his personally owned slaves outright upon Martha's death. As president, he remained publicly silent on the topic of slavery, believing it was a nationally divisive issue that could undermine the union.During the Revolutionary War, Washington's views on slavery began to change. In a 1778 letter to Lund Washington, he made clear his desire \"to get quit of Negroes\" when discussing the exchange of slaves for the land he wanted to buy. The next year, Washington stated his intention not to separate enslaved families as a result of \"a change of masters\". During the 1780s, Washington privately expressed his support for gradual emancipation. In the 1780s, he gave moral support to a plan proposed by Lafayette to purchase land and free slaves to work on it, but declined to participate in the experiment.Washington privately expressed support for emancipation to prominent Methodists Thomas Coke and Francis Asbury in 1785 but declined to sign their petition. In personal correspondence the next year, he made clear his desire to see the institution of slavery ended by a gradual legislative process, a view that correlated with the mainstream antislavery literature published in the 1780s that Washington possessed. He significantly reduced his purchases of slaves after the war but continued to acquire them in small numbers.In 1788, Washington declined a suggestion from a leading French abolitionist, Jacques Brissot, to establish an abolitionist society in Virginia, stating that although he supported the idea, the time was not yet right. Historian Philip D. Morgan wrote that Washington was determined not to risk national unity. Washington never responded to any of the antislavery petitions he received, and the subject was not mentioned in either his last address to Congress or his Farewell Address.\nThe first clear indication that Washington seriously intended to free his slaves appears in a letter written to his secretary, Tobias Lear, in 1794. Washington instructed Lear to find buyers for his land in western Virginia, explaining in a private coda that he was doing so \"to liberate a certain species of property which I possess, very repugnantly to my own feelings\". The plan, along with others Washington considered in 1795 and 1796, could not be realized because he failed to find buyers for his land, his reluctance to break up slave families, and the refusal of the Custis heirs to help prevent such separations by freeing their dower slaves at the same time.On July 9, 1799, Washington finished making his last will; the longest provision concerned slavery. All his slaves were to be freed after the death of his wife. Washington said he did not free them immediately because his slaves intermarried with his wife's dower slaves. He forbade their sale or transportation out of Virginia. The provision also provided that old and young freed people be taken care of indefinitely; younger ones were to be taught to read and write and placed in suitable occupations. Washington emancipated 123 slaves, one of the few large slave-holding Virginians during the Revolutionary Era to do so.On January 1, 1801, one year after George Washington's death, Martha Washington signed an order to free his slaves. Many of them, having never strayed far from Mount Vernon, were reluctant to leave; others refused to abandon spouses or children still held as dower slaves by the Custis estate and also stayed with or near Martha. Following Washington's instructions in his will, funds were used to feed and clothe the young, aged, and infirm slaves until the early 1830s.\n\n\n== Historical reputation and legacy ==\n\nWashington's legacy endures as one of the most influential in American history since he served as commander-in-chief of the Continental Army, a hero of the Revolution, and the first president of the United States. Various historians maintain that he also was a dominant factor in America's founding. Revolutionary War comrade Henry Lee eulogized him as \"First in war, first in peace, and first in the hearts of his countrymen\". Lee's words became the hallmark by which Washington's reputation was impressed upon the American memory, with some biographers regarding him as the great exemplar of republicanism. He set many precedents for the national government and the presidency in particular, and he was called the \"Father of His Country\" as early as 1778.Washington became an international symbol for liberation and nationalism as the leader of the first successful revolution against a colonial empire. The Federalists made him the symbol of their party, but the Jeffersonians continued to distrust his influence for years and delayed building the Washington Monument. Washington was elected a member of the American Academy of Arts and Sciences on January 31, 1781.In 1879, Congress proclaimed Washington's Birthday to be a federal holiday. Through a congressional joint resolution Public Law 94-479, passed on January 19, 1976, with an effective appointment date of July 4, 1976, he was posthumously appointed to the grade of General of the Armies of the United States during the American Bicentennial. President Gerald Ford stated that Washington would \"rank first among all officers of the Army, past and present\". On March 13, 1978, Washington was militarily promoted to the rank of General of the Armies.In 1809, Mason Locke Weems wrote a hagiographic biography to honor Washington. Historian Ron Chernow maintains that Weems attempted to humanize Washington, making him look less stern, and to inspire \"patriotism and morality\" and to foster \"enduring myths\", such as Washington's refusal to lie about damaging his father's cherry tree. Weems' accounts have never been proven or disproven. Historian John Ferling, however, maintains that Washington remains the only founder and president ever to be referred to as \"godlike\", and points out that his character has been the most scrutinized by historians. Biographer Douglas Southall Freeman concluded, \"The great big thing stamped across that man is character.\" Expanding on Freeman's assessment, historian David Hackett Fischer defined Washington's character as \"integrity, self-discipline, courage, absolute honesty, resolve, and decision, but also forbearance, decency, and respect for others\".In the 21st century, Washington's reputation has been critically scrutinized.\nRon Chernow describes Washington as always trying to be even-handed in dealing with the Natives. He states that Washington hoped they would abandon their itinerant hunting life and adapt to fixed agricultural communities in the manner of white settlers. He also maintains that Washington never advocated outright confiscation of tribal land or the forcible removal of tribes and that he berated American settlers who abused natives, admitting that he held out no hope for peaceful relations as long as \"frontier settlers entertain the opinion that there is not the same crime (or indeed no crime at all) in killing a native as in killing a white man.\"By contrast, Colin G. Calloway wrote that, \"Washington had a lifelong obsession with getting Indian land, either for himself or for his nation, and initiated policies and campaigns that had devastating effects in Indian country.\" He stated:\n\nThe growth of the nation demanded the dispossession of Indian people. Washington hoped the process could be bloodless and that Indian people would give up their lands for a \"fair\" price and move away. But if Indians refused and resisted, as they often did, he felt he had no choice but to \"extirpate\" them and that the expeditions he sent to destroy Indian towns were therefore entirely justified.\nAlong with other Founding Fathers, Washington has been condemned for holding enslaved people. Though he expressed the desire to see the abolition of slavery come through legislation, he did not initiate or support any initiatives for bringing about its end. This has led to calls from some activists to remove his name from public buildings and his statue from public spaces. Nonetheless, Washington maintains his place among the highest-ranked U.S. Presidents.\n\n\n=== Places, namesakes, and monuments ===\n\nMany places and monuments have been named in honor of Washington, most notably Washington, D.C., the capital of the United States, and the state of Washington, the only U.S. state to be named after a president.On February 21, 1885, the Washington Monument was dedicated. The 555-foot marble obelisk, which stands on the National Mall in Washington, D.C., was built between 1848\u20131854 and 1879\u20131884 and was the tallest structure in the world between 1884 and 1889.Washington appears as one of four U.S. presidents on the Shrine of Democracy, a colossal statue by Gutzon Borglum on Mount Rushmore in South Dakota.A number of secondary schools and universities are named in honor of Washington, including George Washington University and Washington University in St. Louis.\n\n\n=== Currency and postage ===\n\nWashington appears on contemporary U.S. currency, including the one-dollar bill, the Presidential one-dollar coin and the quarter-dollar coin (the Washington quarter). Washington and Benjamin Franklin appeared on the nation's first postage stamps in 1847. Washington has since appeared on many postage issues, more than any other person.\n\n\n== See also ==\n\nFounders Online\nList of American Revolutionary War battles\nList of Continental Forces in the American Revolutionary War\nTimeline of the American Revolution\nThe Washington Papers\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== Further reading ==\nCurtis, Wayne (May 1, 2006). \"The Father of the Pina Colada?\". The Atlantic.\nCohen, Andrew (November 1, 2011). \"What George Washington Thought About the Constitution\". The Atlantic.\nLewis, Danny (September 22, 2016). \"George Washington's Biracial Family Is Getting New Recognition\". Smithsonian.\nGood, Cassandra (September 28, 2018). \"Did George Washington 'Have a Couple of Things in His Past'?\". The Atlantic.\nZegart, Amy (November 25, 2018). \"George Washington Was a Master of Deception\". The Atlantic.\nWulf, Karin (April 7, 2020). \"The President's Cabinet Was an Invention of America's First President\". Smithsonian.\nCoe, Alexis (February 12, 2020). \"George Washington Saw a Future for America: Mules\". Smithsonian.\nImmerwahr, Daniel (January 31, 2023). \"Did George Washington Burn New York?\". The Atlantic.\n\n\n== External links ==\n\nGeorge Washington's Mount Vernon\nThe Papers of George Washington, subset of Founders Online from the National Archives\nWorks by George Washington at Project Gutenberg\nWorks by George Washington at Biodiversity Heritage Library \nIn Our Time: Washington & the American Revolution, BBC Radio 4 discussion with Carol Berkin, Simon Middleton, & Colin Bonwick (June 24, 2004)\nGreat Lives: George Washington, BBC Radio 4 discussion with Matthew Parris, Michael Rose, & Frank Grizzard (October 21, 2016)\nGeorge Washington on C-SPAN\nScholarly coverage of Washington at the Miller Center, University of Virginia"}, {"id": 3, "title": "History of cricket", "content": "The sport of cricket has a known history beginning in the late 16th century England. It became an established sport in the country in the 18th century and developed globally in the 19th and 20th centuries. International matches have been played since the 19th-century and formal Test cricket matches are considered to date from 1877. Cricket is the world's second most popular spectator sport after association football (soccer).Internationally, cricket is governed by the International Cricket Council (ICC), which has over one hundred countries and territories in membership although only twelve currently play Test cricket.\nThe game's rules are defined in the \"Laws of cricket\". The game has various formats, ranging from T-10(Ten-10) played in around 90 minutes to Test matches which can last up to five days.\n\n\n== Early cricket ==\n\n\n=== Origin ===\nCricket was created during Saxon or Norman times by children living in the Weald, an area of dense woodlands and clearings in south-east England that lies across Kent and Sussex. The first definite written reference is from the end of the 16th century.\nThere have been several speculations about the game's origins, including some that it was created in France or Flanders. The earliest of these speculative references is from 1300 and concerns the future King Edward II playing at \"creag and other games\" in both Westminster and Newenden. It has been suggested that \"creag\" was an Old English word for cricket, but expert opinion is that it was an early spelling of \"craic\", meaning \"fun and games in general\".It is generally believed that cricket survived as a children's game for many generations before it was increasingly taken up by adults around the beginning of the 17th century. Possibly cricket was derived from bowls, assuming bowls is the older sport, by the intervention of a batsman trying to stop the ball from reaching its target by hitting it away. Playing on sheep-grazed land or in clearings, the original implements may have been a matted lump of sheep's wool (or even a stone or a small lump of wood) as the ball; a stick or a crook or another farm tool as the bat; and a stool or a tree stump or a gate (e.g., a wicket gate) as the wicket.\n\n\n=== First definite reference ===\nIn 1597 (Old Style \u2013 1598 New Style) a court case in England concerning an ownership dispute over a plot of common land in Guildford, Surrey, mentions the game of creckett. A 59-year-old coroner, John Derrick, testified that he and his school friends had played creckett on the site fifty years earlier when they attended the Free School. Derrick's account proves beyond reasonable doubt that the game was being played in Surrey circa 1550, and is the earliest universally accepted reference to the game.The first reference to cricket being played as an adult sport was in 1611, when two men in Sussex were prosecuted for playing cricket on Sunday instead of going to church. In the same year, a dictionary defined cricket as a boys' game, and this suggests that adult participation was a recent development.\n\n\n=== Derivation of the name of \"cricket\" ===\nA number of words are thought to be possible sources for the term \"cricket\". In the earliest definite reference, it was spelled creckett. The name may have been derived from the Middle Dutch krick(-e), meaning a stick; or the Old English cricc or cryce meaning a crutch or staff, or the French word criquet meaning a wooden post. The Middle Dutch word krickstoel means a long low stool used for kneeling in church; this resembled the long low wicket with two stumps used in early cricket. According to Heiner Gillmeister, a European language expert of the University of Bonn, \"cricket\" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., \"with the stick chase\").It is more likely that the terminology of cricket was based on words in use in south-east England at the time and, given trade connections with the County of Flanders, especially in the 15th century when it belonged to the Duchy of Burgundy, many Middle Dutch words found their way into southern English dialects.\n\n\n=== The Commonwealth ===\nAfter the Civil War ended in 1648, the new Puritan government clamped down on \"unlawful assemblies\", in particular the more raucous sports such as football. Their laws also demanded a stricter observance of the Sabbath than there had been previously. As the Sabbath was the only free time available to the lower classes, cricket's popularity may have waned during the Commonwealth. However, it did flourish in public fee-paying schools such as Winchester and St Paul's. There is no actual evidence that Oliver Cromwell's regime banned cricket specifically and there are references to it during the interregnum that suggest it was acceptable to the authorities provided that it did not cause any \"breach of the Sabbath\". It is believed that the nobility in general adopted cricket at this time through involvement in village games.\n\n\n=== Gambling and press coverage ===\nCricket thrived after the Restoration in 1660 and is believed to have first attracted gamblers making large bets at this time. It is possible, as believed by some historians, that top-class matches began. In 1664, the \"Cavalier\" Parliament passed the Gaming Act 1664 which limited stakes to \u00a3100, although that was still a fortune at the time, equivalent to about \u00a316,000 in present-day terms . Cricket had become a significant gambling sport by the end of the 17th century, as evidenced in 1697 by a newspaper report of a \"great match\" played in Sussex which was 11-a-side and played for high stakes of 50 guineas a side.With freedom of the press having been granted in 1696, cricket for the first time could be reported in the newspapers. But it was a long time before the newspaper industry adapted sufficiently to provide frequent, let alone comprehensive, coverage of the game. During the first half of the 18th century, press reports tended to focus on the betting rather than on the play.\n\n\n== 18th-century cricket ==\n\n\n=== Patronage and players ===\nGambling introduced the first patrons because some of the gamblers decided to strengthen their bets by forming their own teams and it is believed the first \"county teams\" were formed in the aftermath of the Restoration in 1660, especially as members of the nobility were employing \"local experts\" from village cricket as the earliest professionals. The first known game in which the teams use county names is in 1709 but there can be little doubt that these sort of fixtures were being arranged long before that. The match in 1697 was probably Sussex versus another county.\nThe most notable of the early patrons were a group of aristocrats and businessmen who were active from about 1725, which is the time that press coverage became more regular, perhaps as a result of the patrons' influence. These men included the 2nd Duke of Richmond, Sir William Gage, Alan Brodrick and Edwin Stead. For the first time, the press mentions individual players like Thomas Waymark.\n\n\n=== Cricket expands beyond England ===\nCricket was introduced to North America via the English colonies in the 17th century, probably before it had even reached the north of England. In the 18th century it arrived in other parts of the globe. It was introduced to the West Indies by colonists and to the Indian subcontinent by East India Company mariners in the first half of the century. It arrived in Australia almost as soon as colonisation began in 1788. New Zealand and South Africa followed in the early years of the 19th century.Cricket never caught on in Canada, despite efforts by the upper class to promote the game as a way of identifying with the \"mother country\". Canada, unlike Australia and the West Indies, witnessed a continual decline in the popularity of the game during 1860 to 1960. Linked in the public consciousness to an upper-class sport, the game never became popular with the general public. In the summer season it had to compete with baseball. During the First World War, Canadian units stationed in France played baseball instead of cricket.\n\n\n=== Development of the Laws ===\n\nIt's not clear when the basic rules of cricket such as bat and ball, the wicket, pitch dimensions, overs, how out, etc. were originally formulated. In 1728, the Duke of Richmond and Alan Brodick drew up Articles of Agreement to determine the code of practice in a particular game and this became a common feature, especially around payment of stake money and distributing the winnings given the importance of gambling.In 1744, the Laws of Cricket were codified for the first time and then amended in 1774, when innovations such as lbw, middle stump and maximum bat width were added. These laws stated that \"the principals shall choose from amongst the gentlemen present two umpires who shall absolutely decide all disputes\". The codes were drawn up by the so-called \"Star and Garter Club\" whose members ultimately founded the Marylebone Cricket Club at Lord's in 1787. The MCC immediately became the custodian of the Laws and has made periodic revisions and recodifications subsequently.\n\n\n=== Continued growth in England ===\nThe game continued to spread throughout England, and, in 1751, Yorkshire is first mentioned as a venue. The original form of bowling (i.e., rolling the ball along the ground as in bowls) was superseded sometime after 1760 when bowlers began to pitch the ball and study variations in line, length and pace. Scorecards began to be kept on a regular basis from 1772; since then, an increasingly clear picture has emerged of the sport's development.The first famous clubs were London and Dartford in the early 18th century. London played its matches on the Artillery Ground, which still exists. Others followed, particularly Slindon in Sussex, which was backed by the Duke of Richmond and featured the star player Richard Newland. There were other prominent clubs at Maidenhead, Hornchurch, Maidstone, Sevenoaks, Bromley, Addington, Hadlow and Chertsey.\nBut far and away the most famous of the early clubs was Hambledon in Hampshire. It started as a parish organisation that first achieved prominence in 1756. The club itself was founded in the 1760s and was well patronised to the extent that it was the focal point of the game for about thirty years until the formation of MCC and the opening of Lord's Cricket Ground in 1787. Hambledon produced several outstanding players including the master batsman John Small and the first great fast bowler Thomas Brett. Their most notable opponent was the Chertsey and Surrey bowler Edward \"Lumpy\" Stevens, who is believed to have been the main proponent of the flighted delivery.\n\nIt was in answer to the flighted, or pitched, delivery that the straight bat was introduced. The old \"hockey stick\"\u2013style of bat was only really effective against the ball being trundled or skimmed along the ground.\nFirst-class cricket began in 1772. Three surviving scorecards exist of 1772 matches organised by the Hambledon Club which commence a continuous statistical record. Those three matches were all between a Hampshire XI and an England XI, the first played at Broadhalfpenny Down on 24 and 25 June. The two leading online archives begin their first-class coverage with this match which is numbered \"first-class no. 1\" by ESPNcricinfo and \"f1\" by CricketArchive. Broadhalfpenny Down continued in regular use by Hambledon/Hampshire teams until 1781.\n\n\n== 19th-century cricket ==\n\nThe game also underwent a fundamental change of organisation with the formation for the first time of county clubs. All the modern county clubs, starting with Sussex in 1839, were founded during the 19th century. No sooner had the first county clubs established themselves than they faced what amounted to \"player action\" as William Clarke created the travelling All-England Eleven in 1846. Though a commercial venture, this team did much to popularise the game in districts which had never previously been visited by high-class cricketers. Other similar teams were created and this vogue lasted for about thirty years. But the counties and MCC prevailed.\n\nThe growth of cricket in the mid and late 19th century was assisted by the development of the railway network. For the first time, teams from a long distance apart could play one other without a prohibitively time-consuming journey. Spectators could travel longer distances to matches, increasing the size of crowds. Army units around the Empire had time on their hands, and encouraged the locals so they could have some entertaining competition. Most of the Empire embraced cricket, with the exception of Canada.In 1864, another bowling revolution resulted in the legalisation of overarm and in the same year Wisden Cricketers' Almanack was first published. W. G. Grace began his long and influential career at this time, his feats doing much to increase cricket's popularity. He introduced technical innovations which revolutionised the game, particularly in batting.\n\n\n=== International cricket begins ===\nThe first ever international cricket game was between the US and Canada in 1844. The match was played at the grounds of the St George's Cricket Club in New York.\nIn 1859, a team of leading English professionals set off to North America on the first-ever overseas tour and, in 1862, the first English team toured Australia. Between May and October 1868, a team of Aboriginal Australians toured England in what was the first Australian cricket team to travel overseas.\nIn 1877, an England touring team in Australia played two matches against full Australian XIs that are now regarded as the inaugural Test matches. The following year, the Australians toured England for the first time and the success of this tour ensured a popular demand for similar ventures in future. No Tests were played in 1878 but more soon followed and, at The Oval in 1882, the Australian victory in a tense finish gave rise to The Ashes.\nSouth Africa became the third Test nation in 1889.\nCricket grew significantly in many of the colonies during this time, aided by the fact that many colonised peoples saw it and other British sports as a source of nationalistic pride and one of the few venues in which they could defeat their colonisers.\n\n\n=== Decline in North America ===\nCricket started off as one of the more popular sports in America, aided by the invention of informal cricket variants such as wicket which resulted in higher-scoring matches that could be completed in an afternoon, rather than over the course of a few days. However, baseball overtook cricket's popularity in the United States during the American Civil War, as soldiers who had played baseball during the war went back to their homes across the country and took the game with them. Some factors in favour of baseball's rise were that it had a much shorter playing duration and that it could be played on any patch of land (rather than requiring special preparations such as the cricket pitch), which was essential for troops who needed to be able to move at a moment's notice during the war.\n\n\n=== National championships ===\nA significant development in domestic cricket occurred in 1890 when the official County Championship was constituted in England. Soon afterwards, in May 1894, the sport's first-class standard was officially defined. This organisational initiative has been repeated in other countries. Australia established the Sheffield Shield in 1892\u201393. Other national competitions to be established were the Currie Cup in South Africa, the Plunket Shield in New Zealand and the Ranji Trophy in India. The ICC re-defined first-class status in 1947 as a global concept.The period from 1890 to the outbreak of the First World War has become one of nostalgia, ostensibly because the teams played cricket according to \"the spirit of the game\", but more realistically because it was a peacetime period that was shattered by the First World War. The era has been called The Golden Age of cricket and it featured numerous great names such as Grace, Wilfred Rhodes, C. B. Fry, Ranjitsinhji and Victor Trumper.\n\n\n=== Balls per over ===\nDuring most of the 19th-century standard overs were made up of four deliveries. In 1889 five-ball overs were introduced in first-class cricket, with a move to generally use six-ball overs in 1900.In the 20th century, eight-ball overs were used at times in a number of countries, primarily Australia, where eight-balls were the standard over length between 1918/19 and 1978/79, South Africa and New Zealand. Since the 1979/80 Australian and New Zealand seasons, six balls per over have been used worldwide, and the most recent version of the Laws only permits six-ball overs.\n\n\n== 20th-century cricket ==\n\n\n=== Growth of international cricket ===\nThe game of cricket was exported to other colonies around the world. The Imperial Cricket Conference was founded in 1909 with England, Australia and South Africa as the founder members. It had been set up with the Marylebone Cricket club, the Australian Board of Control for International Cricket, and the South African Cricket Association as the original associations of the ICC. This aimed to regulate international cricket between the three sides, considered the only three of equal status at the time. In 1926, both New Zealand and the West Indies were admitted as members, allowing them to play Test cricket against the other sides. However, at this time in the West Indies, cricket was primarily dominated by the white population. Originally, the ICC was not interested in broadening the international popularity of cricket. The organization was reluctant to invite non-commonwealth nations to play. New Zealand was restricted to play three-day test matches. New Zealand and India both became Test playing nations before World War II and Pakistan joined soon afterwards in 1952.At the initial suggestion of Pakistan, the ICC was expanded to include non-Test playing countries from 1965, with Associate members being admitted. At the same time the organisation changed its name to the International Cricket Conference. The first limited-overs World Cups were played during the 1970s and Sri Lanka became the first Associate member to be raised to Test playing status in 1982. Because the ICC was predominantly a Western organization, the founding countries decided who was allowed to join the conference or engage in test cricket. There was no desire or attempt to create a set of Associate nations that would play in Test status, which is why countries such as Sri Lanka were not permitted to partake until the 1980s.The international game continued to grow with the introduction of Affiliate Member status in 1984, a level of membership designed for sides with less history of playing cricket. In 1989 the ICC renamed itself the International Cricket Council. Zimbabwe became Full Members in 1992 and Bangladesh in 2000 before Afghanistan and Ireland were both admitted as Test sides in 2018, bringing the number of full members of the ICC to 12.\n\n\n=== Suspension of South Africa (1970\u20131991) ===\n\nThe greatest crisis to hit international cricket was brought about by apartheid, the South African policy of racial segregation. The situation began to crystallise after 1961 when South Africa left the Commonwealth of Nations and so, under the rules of the day, its cricket board had to leave the International Cricket Conference (ICC). Cricket's opposition to apartheid intensified in 1968 with the cancellation of England's tour to South Africa by the South African authorities, due to the inclusion in the England team of Basil D'Oliveira, a Cape Coloured player. In 1970, the ICC members voted to suspend South Africa indefinitely from international cricket competition.Starved of top-level competition for its best players, the South African Cricket Board began funding so-called \"rebel tours\", offering large sums of money for international players to form teams and tour South Africa. The ICC's response was to blacklist any rebel players who agreed to tour South Africa, banning them from officially sanctioned international cricket. As players were poorly remunerated during the 1970s, several accepted the offer to tour South Africa, particularly players getting towards the end of their careers for which a blacklisting would have little effect.\nThe rebel tours continued into the 1980s but then progress was made in South African politics and it became clear that apartheid was ending. South Africa, now a \"Rainbow Nation\" under Nelson Mandela, was welcomed back into international sport in 1991.\n\n\n=== World Series Cricket ===\n\nThe money problems of top cricketers were also the root cause of another cricketing crisis that arose in 1977 when the Australian media magnate Kerry Packer fell out with the Australian Cricket Board over TV rights. Taking advantage of the low remuneration paid to players, Packer retaliated by signing several of the best players in the world to a privately run cricket league outside the structure of international cricket. World Series Cricket hired some of the banned South African players and allowed them to show off their skills in an international arena against other world-class players. The schism lasted only until 1979 and the \"rebel\" players were allowed back into established international cricket, though many found that their national teams had moved on without them. Long-term results of World Series Cricket have included the introduction of significantly higher player salaries and innovations such as coloured kit and night games.\n\n\n=== Limited-overs cricket ===\nIn the 1960s, English county teams began playing a version of cricket with games of only one innings each and a maximum number of overs per innings. Starting in 1963 as a knockout competition only, limited-overs cricket grew in popularity and, in 1969, a national league was created which consequently caused a reduction in the number of matches in the County Championship. The status of limited overs matches is governed by the official List A categorisation. Although many \"traditional\" cricket fans objected to the shorter form of the game, limited-overs cricket did have the advantage of delivering a result to spectators within a single day; it did improve cricket's appeal to younger or busier people; and it did prove commercially successful.\nThe first limited-overs international match took place at Melbourne Cricket Ground in 1971 as a time-filler after a Test match had been abandoned because of heavy rain on the opening days. It was tried simply as an experiment and to give the players some exercise, but turned out to be immensely popular. limited-overs internationals (LOIs or ODIs\u2014one-day internationals) have since grown to become a massively popular form of the game, especially for busy people who want to be able to see a whole match. The International Cricket Council reacted to this development by organising the first Cricket World Cup in England in 1975, with all the Test-playing nations taking part.\n\n\n=== Analytic and graphic technology ===\nLimited-overs cricket increased television ratings for cricket coverage. Innovative techniques introduced in coverage of limited-over matches were soon adopted for Test coverage. The innovations included presentation of in-depth statistics and graphical analysis, placing miniature cameras in the stumps, multiple usage of cameras to provide shots from several locations around the ground, high-speed photography and computer graphics technology enabling television viewers to study the course of a delivery and help them understand an umpire's decision.\nIn 1992, the use of a third umpire to adjudicate run-out appeals with television replays was introduced in the Test series between South Africa and India. The third umpire's duties have subsequently expanded to include decisions on other aspects of play such as stumpings, catches and boundaries. From 2011, the third umpire was being called upon to moderate review of umpires' decisions, including lbw, with the aid of virtual-reality tracking technologies (e.g., Hawk-Eye and Hot Spot), though such measures still could not free some disputed decisions from heated controversy.\n\n\n== 21st-century cricket ==\nIn June 2001, the ICC introduced a \"Test Championship Table\" and, in October 2002, a \"One-day International Championship Table\". As indicated by ICC rankings, the various cricket formats have continued to be a major competitive sport in most former British Empire countries, notably the Indian subcontinent, and new participants including the Netherlands. In 2017, the number of countries with full ICC membership was increased to twelve by the addition of Afghanistan and Ireland.The ICC expanded its development programme, aiming to produce more national teams capable of competing at the various formats. Development efforts are focused on African and Asian nations, and on the United States. In 2004, the ICC Intercontinental Cup brought first-class cricket to 12 nations, mostly for the first time. Cricket's newest innovation is Twenty20, essentially an evening entertainment. It has so far enjoyed enormous popularity and has attracted large attendances at matches as well as good TV audience ratings. The inaugural ICC Twenty20 World Cup tournament was held in 2007. The formation of Twenty20 leagues in India \u2013 the unofficial Indian Cricket League, which started in 2007, and the official Indian Premier League, starting in 2008 \u2013 raised much speculation in the cricketing press about their effect on the future of cricket.Formats shorter than Twenty20 have also arisen at the domestic level, such as the T10 format, which is played in leagues organised by various Associate and Full Members, as well as the 100-ball format, which is played in The Hundred, a major limited-overs competition in England.\n\n\n== See also ==\nHistory of women's cricket\n\n\n== References ==\n\n\n== Bibliography ==\nAltham, H.S. (1962). A History of Cricket, Volume 1 (to 1914). George Allen & Unwin.\nBateman, Anthony. Cricket, literature and culture: symbolising the nation, destabilising empire (Routledge, 2016).\nBirley, Derek (1999). A Social History of English Cricket. Aurum.\nBowen, Rowland (1970). Cricket: A History of its Growth and Development. Eyre & Spottiswoode.\nBox, Charles (1868). The Theory and Practice of Cricket, from its origin to the present time. Frederick Warne.\nHarte, Chris (1993). A History of Australian Cricket. London: Andre Deutsch. ISBN 0-233-98825-4.\nLang, Andrew. \"The History of Cricket\" in Steel A. G.; Lyttelton R. H. Cricket, Longmans 1898 (6th edn)\nLight, Rob. \"'In a Yorkshire Like Way': Cricket and the Construction of Regional Identity in Nineteenth-century Yorkshire.\" Sport in History 29.3 (2009): 500\u2013518.\nMcCann, Tim (2004). Sussex Cricket in the Eighteenth Century. Sussex Record Society.\nMcKibbin, Ross.  Classes and Cultures. England 1918\u20131951 (Oxford: Oxford University Press, 1998). online pp. 332\u201339\nRussell, Dave. \"Sport and identity: the case of Yorkshire County Cricket Club, 1890\u20131939.\" Twentieth Century British History 7.2 (1996): 206\u201330.\nStone, Duncan. \"Cricket's regional identities: the development of cricket and identity in Yorkshire and Surrey.\" Sport in Society 11.5 (2008): 501\u201316. online\nUnderdown, David (2000). Start of Play. Allen Lane.\nWisden Cricketers' Almanack (annual): various editions\nWynne-Thomas, Peter (1997). From the Weald to the World. Stationery Office Books.\n\n\n== External links ==\n\"BBC News \u2013 Today \u2013 Audio slideshow: 'Swinging Away'\". BBC Online. 20 May 2010.\n\"Cric History\". CricHistory.in. 31 August 2022."}, {"id": 4, "title": "Names for association football", "content": "There are many terms used to describe association football, the sport most commonly referred to in the English-speaking world as \"football\" or \"soccer\".\n\n\n== Background ==\nThe rules of Association football were codified in England by the Football Association in 1863. The alternative name soccer was first coined in late 19th century England to help distinguish between several codes of football that were growing in popularity at that time, in particular rugby football. The word soccer is an abbreviation of association (from assoc.) and first appeared in English Public Schools and universities in the 1880s (sometimes using the variant spelling \"socker\") where it retains some popularity of use to this day. The word is sometimes credited to Charles Wreford-Brown, an Oxford University student said to have been fond of shortened forms such as brekkers for breakfast and rugger for rugby football (see Oxford -er). However, the attribution to Wreford-Brown in particular is generally considered to be spurious. Clive Toye noted \"they took the third, fourth and fifth letters of Association and called it SOCcer.\"The sport's full name Association football has never been widely used, although in Britain some clubs in rugby football strongholds adopted the suffix Association Football Club (A.F.C.) to avoid confusion with the dominant sport in their area, and FIFA, the world governing body for the sport, is a French-language acronym of \"F\u00e9d\u00e9ration Internationale de Football Association\" \u2013 the International Association Football Federation. \"Soccer football\" is used less often than it once was: the United States Soccer Federation was known as the United States Soccer Football Association from 1945 until 1974, when it adopted its current name; and the Canadian Soccer Association was known as the Canadian Soccer Football Association from 1958 to 1971.\n\n\n=== Transition away from soccer in Britain ===\nFor nearly a hundred years after it was first coined, soccer was used as an uncontroversial alternative in Britain to football, often in colloquial and juvenile contexts, but was also widely used in formal speech and in writing about the game. \"Soccer\" was a term used by the upper class whereas the working and middle classes preferred the word \"football\"; as the upper class lost influence in British society from the 1960s on, \"football\" supplanted \"soccer\" as the most commonly used and accepted word. The use of soccer is declining in Britain and is now considered (albeit incorrectly, due to the word's British origin) to be an exclusively American English term. Since the early twenty-first century, the peak association football authorities in soccer-labeling Australia and New Zealand have actively promoted the use of football to mirror international usage and, at least in the Australian case, to rebrand a sport that had been experiencing difficulties. Both bodies dropped soccer from their names. These efforts have met with considerable success in New Zealand, but have not taken effect well in Australia or Papua New Guinea.\n\n\n== English-speaking countries ==\nUsage of the various names of association football vary among the countries or territories who hold the English language as an official or de facto official language. The brief survey of usage below addresses places which have some level of autonomy in the sport and their own separate federation but are not actually independent countries: for example the constituent countries of the United Kingdom and some overseas territories each have their own federation and national team. Not included are places such as Cyprus, where English is widely spoken on the ground but is not amongst the country's specifically stated official languages.\n\n\n=== Countries where it is called football ===\nAssociation football is known as \"football\" in the majority of countries where English is an official language, such as the United Kingdom, the Commonwealth Caribbean (including Trinidad and Tobago, Jamaica, Belize, Barbados, and others), Nepal, Malta, India, Bangladesh, Nigeria, Cameroon, Pakistan, Liberia, Singapore, Hong Kong and others, stretching over many regions including parts of Europe, Asia, Africa, the Caribbean and Central America.  In North America and Australia (where approximately 70 per cent of native English speakers reside), soccer is the primary term.\nFitbaa, fitba or fitbaw is a rendering of the Scots pronunciation of \"football\", often used in a humorous or ironic context.\n\n\n=== North America ===\nIn the United States, where American football is more popular, the word football is used to refer only to that sport. Association football is most commonly referred to as soccer.\nAs early as 1911 there were several names in use for the sport in the Americas. A 29 December 1911 New York Times article reporting on the addition of the game as an official collegiate sport in the US referred to it as \"association football\", \"soccer\" and \"soccer football\" all in a single article.The sport's governing body is the United States Soccer Federation; however, it was originally called the U.S. Football Association, and was formed in 1913 by the merger of the American Football Association and the American Amateur Football Association. The word \"soccer\" was added to the name in 1945, making it the U.S. Soccer Football Association, and it did not drop the word \"football\" until 1974, when it assumed its current name.\nIn Canada, similar to the US, the term \"football\" refers to gridiron football (either Canadian football or American football; le football canadien or le football am\u00e9ricain in Standard French). \"Soccer\" is the name for association football in Canadian English (similarly, in Canadian French, le soccer). Likewise, in majority francophone Quebec, the provincial governing body is the F\u00e9d\u00e9ration de Soccer du Qu\u00e9bec. This is unusual compared to francophone countries, where football is generally used. Canada's national body government of the sport is named the Canada Soccer Association, although at first its original name was the Dominion of Canada Football Association.\nSome teams based in the two countries have adopted FC as a suffix or prefix in their names; in Major League Soccer, these include Austin FC, Minnesota United FC, Chicago Fire FC, Atlanta United FC, FC Dallas, Seattle Sounders FC, Toronto FC, Vancouver Whitecaps FC, New York City FC, Los Angeles FC, FC Cincinnati and Charlotte FC, while two MLS teams (Inter Miami CF and CF Montr\u00e9al) use CF as a suffix or prefix in their names, reflecting the Spanish-speaking and Francophone communities where the respective teams are from. All teams in the Canadian Premier League (with the notable exception of Atl\u00e9tico Ottawa due to the team being owned by Atl\u00e9tico Madrid) use FC as a suffix while FC Edmonton is the only team in the league that uses it as a prefix.\nIn Central America, the only English-speaking nation is Belize, and like the other six Central American nations, the unqualified term football refers to association football, as used in the Football Federation of Belize and in the Belize Premier Football League. However, the term soccer is sometimes used in vernacular speech and media coverage.In the Caribbean, most of the English-speaking members use the word football for their federations and leagues, the exception being the U.S. Virgin Islands, where both federation and league use the word soccer.\nAn exceptional case is the largely Spanish-speaking Puerto Rico, where the word football is used in the Puerto Rican Football Federation, while the word soccer is used in the Puerto Rico Soccer League, the Puerto Rican 1st division; however, its 2nd division is named Liga Nacional de Futbol de Puerto Rico. Soccer is the most common term in vernacular speech, however. Another case is the Dutch island of Sint Maarten, where soccer is used in Sint Maarten Soccer Association, but neither football nor soccer appears in its league name: instead, the Dutch voetbal is used.\n\n\n=== Australia ===\n\nTraditionally, the sport has been mainly referred to as soccer in Australia. This is primarily due to Australian rules football and rugby league taking precedence of the name in conversation due to their greater cultural prominence and popularity - similarly to North America and gridiron football. However, in 2005, the Australia Soccer Association changed its name to Football Federation Australia, and it now encourages the use of \"football\" to describe the association code in line with international practice. All state organisations, many clubs, and most media outlets have followed its example. The Macquarie Dictionary observed, writing prior to 2010: \"While it is still the case that, in general use, soccer is the preferred term in Australia for what most of the world calls football, the fact that the peak body in Australia has officially adopted the term football for this sport will undoubtedly cause a shift in usage.\" This was highlighted shortly afterwards when then-Prime Minister Julia Gillard, speaking in Melbourne, referred to the sport as football, emphasising her choice when questioned. The Australian men's team is still known by its long-standing nickname, the Socceroos, the Soccer Ashes is still referred to as such, and \"soccer\" is still the most popular term for the sport in Australia.\nHistorically, the derogatory term \"wogball\" has been used to refer to the sport. This is due to \"wog\" being a derogatory (but since appropriated in some contexts) term referring to Australians of Mediterranean background (particularly Croatians, Egyptians, Greeks, Italians, Lebanese, Macedonians, Maltese and Turks), among whom the sport was most popular. It was also derogatorily described as a game for \"sheilas, wogs and poofters\" (with \"sheilas\" being women and \"poofters\" being homosexuals). Former Australian soccer player Johnny Warren later released a book titled Sheilas, Wogs and Poofters.\n\n\n==== Australian media ====\nThe debate over whether \"soccer\" or \"football\" should be used has extended to the media. Many media outlets use different terminology.\nAssociation football is referred to as \"football\" by many outlets, including ABC News,  News.com.au and The Australian. However, many others still use the term \"soccer\", including The Sydney Morning Herald and The West Australian.\n\n\n=== New Zealand ===\n\nIn New Zealand English, association football has historically been called \"soccer\". As late as 2005, the New Zealand Oxford Dictionary suggested that in that country \"football\" referred especially to rugby union; it also noted that rugby union was commonly called \"rugby\", while rugby league was called \"league\". A year earlier, New Zealand Soccer had reorganised its leading competition as the New Zealand Football Championship, and in 2007 it changed its own name to New Zealand Football. The wider language community appears to have embraced the new terminology\u2014influenced, among other things, by television coverage of association football in other parts of the world\u2014so that today, according to The New Zealand Herald, \"most people no longer think or talk of rugby as 'football'. A transformation has quietly occurred, and most people are happy to apply that name to the world's most popular game, dispensing with 'soccer' in the process.\"\n\n\n=== Papua New Guinea ===\nIn Papua New Guinea and other parts of Melanesia, the term \"soccer\" is the preferred term for the sport, due to the large Australian influence in the region. In Papua New Guinea, the national association is the Papua New Guinea Football Association but the national league is the Papua New Guinea National Soccer League. In Tok Pisin, \"soka\" refers to \"soccer\", \"ragbi\" refers to rugby and \"futbal\" refers to other codes of football (i.e. Australian rules football, or \"futbal bilong Ostrelia\").\n\n\n=== Other English-speaking countries ===\nOn the island of Ireland, \"football\" or \"footballer\" most often refers to association football or Gaelic football. It may also refer to rugby union. The association football federations are called the Football Association of Ireland (FAI) and the Irish Football Association (IFA) and the top clubs are called \"Football Club\". Furthermore, those whose primary interest lies in this game often call their sport \"football\" and refer to Gaelic football as \"Gaelic football\" or \"Gaelic\" (although they may also use \"soccer\"). The terms \"football\" and \"soccer\" are used interchangeably in Ireland's media.In most of Ulster, the northern province in Ireland, especially in Northern Ireland, East Donegal and Inishowen, association football is usually referred to as 'football' while Gaelic football is usually referred to as 'Gaelic'.\nIn Pakistan, Liberia, Nigeria and other English-speaking countries, both football and soccer are used both officially and commonly.\n\n\n== Non-English-speaking countries ==\nAssociation football, in its modern form, was exported by the British to much of the rest of the world and many of these nations adopted this common English term for the sport into their own language. This was usually done in one of two ways: either by directly importing the word itself, or as a calque by translating its constituent parts, foot and ball. In English, the word football was known in writing by the 14th century, as laws which prohibit similar games date back to at least that century.\n\n\n=== From English football ===\nAlbanian: futboll\nArmenian: \u0586\u0578\u0582\u057f\u0562\u0578\u056c (futbol)\nBangla: \u09ab\u09c1\u099f\u09ac\u09b2 (futbol)\nBelarusian: \u0444\u0443\u0442\u0431\u043e\u043b (futbol)\nBulgarian: \u0444\u0443\u0442\u0431\u043e\u043b (futbol), the sport was initially called \u0440\u0438\u0442\u043d\u0438\u0442\u043e\u043f (ritnitop, literally \"kickball\"); footballers are still sometimes mockingly called \u0440\u0438\u0442\u043d\u0438\u0442\u043e\u043f\u043a\u043e\u0432\u0446\u0438 (ritnitopkovtsi \"ball kickers\") today.\nCatalan: futbol\nCzech: fotbal (kopan\u00e1 for \"kick game\" is also used)\nFilipino: futbol (\u1709\u1713\u1706\u1714\u170a\u1713\u170e\u1714 in baybayin)\nFrench: football (except in French Canada where it is soccer)\nGalician: f\u00fatbol\nHindi: \u092b\u093c\u0941\u091f\u092c\u0949\u0932 (futbol)\nJapanese:\u30d5\u30c3\u30c8\u30dc\u30fc\u30eb (futtob\u014dru: represents \"football\") is a variant, but \u30b5\u30c3\u30ab\u30fc (sakk\u0101: represents \"soccer\") is most commonly used in Japanese, as in \u65e5\u672c\u30b5\u30c3\u30ab\u30fc\u5354\u4f1a (lit. Japan Soccer Association, the official English name of which is the Japan Football Association). From 1885 to around 1908 in the Meiji era, f\u016btob\u014dru (\u30d5\u30fc\u30c8\u30dc\u30fc\u30eb) was the most common and assoshieshon (\u30a2\u30c3\u30bd\u30b7\u30a8\u30fc\u30b7\u30e7\u30f3) was also used, and these were often written together with kemari (\u8e74\u97a0), a game of the Heian period. From the Taisho era to the early Showa era, ashiki futtob\u014dru (\u30a2\u5f0f\u30d5\u30c3\u30c8\u30dc\u30fc\u30eb), ashiki sh\u016bky\u016b (\u30a2\u5f0f\u8e74\u7403) and sh\u016bky\u016b (\u8e74\u7403) were often used.\nKannada: \u0cab\u0cc1\u0c9f\u0ccd\u200c\u0cac\u0cbe\u0cb2\u0ccd (phutball)\nKazakh: \u0444\u0443\u0442\u0431\u043e\u043b (futbol)\nKyrgyz: \u0444\u0443\u0442\u0431\u043e\u043b (futbol)\nLatvian: futbols\nLithuanian: futbolas\nMacedonian: \u0444\u0443\u0434\u0431\u0430\u043b (fudbal)\nMalayalam: \u0d2b\u0d41\u0d1f\u0d4d\u0d2c\u0d4b\u0d7e (phutball)\nMaltese: futbol\nMarathi: \u092b\u0941\u091f\u094d\u092c\u0949\u0932\u094d (phutball)\nPersian: \u0641\u0648\u062a\u0628\u0627\u0644 (futb\u00e2l)\nPolish: futbol, as well as the native term pi\u0142ka no\u017cna literally \"foot-ball\"\nPortuguese: futebol\nRomanian: fotbal\nRussian: \u0444\u0443\u0442\u0431\u043e\u043b (futbol)\nSerbian: \u0444\u0443\u0434\u0431\u0430\u043b (fudbal)\nSlovak: futbal\nSpanish: f\u00fatbol or futbol; the calque balompi\u00e9, from the words \"bal\u00f3n\" (ball) and \"pie\" (foot), is seldom used.\nTajik: \u0444\u0443\u0442\u0431\u043e\u043b (futbol)\nTelugu: \u0c2b\u0c41\u0c1f\u0c4d\u200c\u0c2c\u0c3e\u0c32\u0c4d (phutball)\nThai: \u0e1f\u0e38\u0e15\u0e1a\u0e2d\u0e25 (f\u00fat-bol)\nTurkish: futbol\nUkrainian: \u0444\u0443\u0442\u0431\u043e\u043b (futbol), occasionally called \u043a\u043e\u043f\u0430\u043d\u0438\u0439 \u043c'\u044f\u0447 (kopanyi myach), literally \"kicked ball\" or simply \u043a\u043e\u043f\u0430\u043d\u0438\u0439 (kopanyi)\nUzbek: futbol\nYiddish: \u05e4\u05d5\u05d8\u05d1\u05d0\u05b8\u05dc (futbol)This commonality is reflected in the auxiliary languages Esperanto and Interlingua, which utilize futbalo and football, respectively.\n\n\n=== Literal translations of foot ball (calques) ===\nArabic: \u0643\u0631\u0629 \u0627\u0644\u0642\u062f\u0645 (kurat al-qadam; however, in vernacular Arabic, \u0643\u0631\u0629 (kura), meaning \"ball\", is far more common. \u0641\u0648\u062a\u0628\u0648\u0644 (f\u016btb\u014dl) is also fairly common, particularly in the former French colonies of Morocco, Algeria, and Tunisia.)\nBreton: mell-droad\nBulgarian: \u0440\u0438\u0442\u043d\u0438\u0442\u043e\u043f (ritnitop) literally \"kickball\")\nChinese: \u8db3\u7403 (Hanyu Pinyin: z\u00faqi\u00fa, Jyutping: zuk1 kau4) from \u8db3 = foot and \u7403 = ballHong Kong daily Cantonese:  \u8e22\u6ce2 (tek3 bo1) where \u8e22 means kick, and \u6ce2 is a phonetic imitation of ball, (literally \u6ce2 means sea wave in Chinese).Danish: fodbold\nDutch: voetbal\nEstonian: jalgpall\nFaroese: f\u00f3tb\u00f3ltur\nFinnish: jalkapallo\nGeorgian: \u10e4\u10d4\u10ee\u10d1\u10e3\u10e0\u10d7\u10d8 (pekhburti), from \u10e4\u10d4\u10ee\u10d8 (pekhi = foot) and \u10d1\u10e3\u10e0\u10d7\u10d8 (burti = ball).\nGerman: Fu\u00dfball\nGreek: \u03c0\u03bf\u03b4\u03cc\u03c3\u03c6\u03b1\u03b9\u03c1\u03bf (podosphero), from \u03c0\u03cc\u03b4\u03b9 (podi) = \"foot\" and \u03c3\u03c6\u03b1\u03af\u03c1\u03b1 (sphera) = \"sphere\" or \"ball\". In Greek-Cypriot, the sport is called \"mappa\" (\u03bc\u03ac\u03c0\u03c0\u03b1), which means \"ball\" in this dialect.\nHebrew:  \u05db\u05d3\u05d5\u05e8\u05d2\u05dc (kaduregel), a portmanteau of the words \"\u05db\u05d3\u05d5\u05e8\" (kadur: ball) and \"\u05e8\u05d2\u05dc\" (regel: foot, leg).\nIcelandic: f\u00f3tbolti, but knattspyrna (from kn\u00f6ttur (\"ball\") + spyrna (\"kicking\")) is almost equally used.\nKarelian: jalgami\u00e4\u010d\u010dy\nKinyarwanda: umupira w'amaguru(from umupira (\"ball\") + amaguru (\"legs\"), literally \"ball of legs\")\nLatvian: k\u0101jbumba (the historic name in the first half of the 20th century, a literal translation from English).\nMalayalam: Kaalppanthu, from \"Kaal\" (foot) and \"Panthu\" (ball).\nManx: bluckan coshey\nNorwegian: fotball\nPolish: pi\u0142ka no\u017cna, from pi\u0142ka (ball) and noga (leg).\nScottish Gaelic: ball-coise\nSinhala: \u0db4\u0dcf \u0db4\u0db1\u0dca\u0daf\u0dd4 = paa pandu\nSomali: kubada cagta - kubada \"ball\" and cagta\"feet or foot\".\nSwahili: mpira wa miguu, from mpira (ball), wa (of) and miguu (feet/legs).\nSwedish: fotboll\nTamil: \u0b95\u0bbe\u0bb2\u0bcd\u0baa\u0ba8\u0bcd\u0ba4\u0bc1, \u0b95\u0bbe\u0bb2\u0bcd (kaal) = foot and \u0baa\u0ba8\u0bcd\u0ba4\u0bc1 (pandhu) = ball\nUkrainian: occasionally called \u043a\u043e\u043f\u0430\u043d\u0438\u0439 \u043c'\u044f\u0447 (kopanyi myach), literally \"kicked ball\" or simply \u043a\u043e\u043f\u0430\u043d\u0438\u0439 (kopanyi)\nVietnamese: b\u00f3ng \u0111\u00e1 (ball - kick)\nWelsh: p\u00eal-droedIn the first half of the 20th century, in Spanish and Portuguese, new words were created to replace \"football\" (f\u00fatbol in Spanish and futebol in Portuguese), balompi\u00e9 (bal\u00f3n and pie meaning \"ball\" and \"foot\") and ludop\u00e9dio (from words meaning \"game\" and \"foot\") respectively. However, these words were not widely accepted and are now only used in club names such as Real Betis Balompi\u00e9 and Albacete Balompi\u00e9.\n\n\n=== From soccer ===\nAfrikaans: sokker, echoing the predominant use of \"soccer\" in South African English.\nAustralian Kriol: soka\nBislama: soka\nBulgarian: \u0441\u043e\u043a\u044a\u0440 (sokur)\nCanadian French: soccer, pronounced like the English word. In Quebec, in New-Brunswick, etc. the word football refers either to American or Canadian football, following the usage of English-speaking North America.\nFijian: soka\nJapanese: sakk\u0101 (\u30b5\u30c3\u30ab\u30fc) is more common than futtob\u014dru (\u30d5\u30c3\u30c8\u30dc\u30fc\u30eb) because of American influence following World War II. While the Japan Football Association uses the word \"football\" in its official English name, the Association's Japanese name uses sakk\u0101.\nIrish: sacar\nManx Gaelic: soccar or sackyr\nM\u0101ori: hoka\nPijin: soka\nSamoan: soka\nSwahili: soka\nTok Pisin: soka\nTongan: soka\nTorres Strait Creole: soka\n\n\n=== Other forms ===\nItalian: calcio (from calciare, meaning to kick), although football is also widely understood, as many clubs include Football Club in their official denomination. This is due to the game's resemblance to Calcio Fiorentino, a 16th-century ceremonial Florentine court ritual, that has now been revived under the name calcio storico or calcio in costume (historical kick or kick in costume).\nBosnian, Croatian, Slovene: nogomet. The word is derived from \"noga\" (meaning \"leg\") and \"met\" (meaning \"to throw\"), hence \"throwing the ball using legs\".\nIn Erzya: \u043f\u0438\u043b\u044c\u0433\u0435\u043e\u0441\u043a\u0430 (pilgeoska).\nIn Komi: \u043a\u043e\u043a\u0441\u044f\u0440 (koksyar).\nIn Hungarian, futball or labdar\u00fag\u00e1s (meaning ball-kicking), but foci is used in the common language.\nIn Burmese, where the game was introduced in the 1880s by Sir James George Scott, it is called ball-pwe, a pwe being a rural all-night dance party, something like a rave.\nIn Lao, the term \"\u0e9a\u0eb2\u0e99\u0ec0\u0e95\u0eb0:ban-te\", literally meaning \"ball-kicking\", is used to denote \"football\".\nIn Navajo: joo\u0142 nab\u00edzn\u00edltaa\u0142\u00ed, meaning \"ball is kicked around\".\nIn Vietnamese, the terms \"b\u00f3ng \u0111\u00e1\" and \"\u0111\u00e1 banh\" (the latter is only used in certain regions), both literally meaning \"ball-kicking\", are used to denote \"football\". Sometime Sino-Vietnamese term \"t\u00fac c\u1ea7u\" (\u8db3\u7403) is used.\nIn Indonesian, the term sepak bola (\"ball kicking\") is used whereas Malaysian and Singaporean Malay use bola sepak (\"kickball\"); the latter is famously attested in the 1859 Jawi booklet Inilah Risalat Peraturan Bola Sepak Yang Dinamai oleh Inggeris Football (\"This is a Rulebook for Kick-ball that the English call Football\") printed in Singapore.\nIn Korean, the Sino-Korean derived term chukku (\u8e74\u7403 \ucd95\uad6c [t\u0255\u02b0ukk\u0348u]), \"kick-ball\", is used.\nIn Swahili, the word kandanda which has no transparent etymology, is used alongside mpira wa miguu and soka.\nIn Khmer, the term \"\u1794\u17b6\u179b\u17cb\u1791\u17b6\u178f\u17cb\" (kick-ball) is used.\n\n\n=== Other terminology ===\nAside from the name of the game itself, other foreign words based on English football terms include versions in many languages of the word goal (often gol in Romance languages). In German-speaking Switzerland, sch\u00fatte (Basel) or tschuutte (Z\u00fcrich), derived from the English shoot, means 'to play football'. Also, words derived from kick have found their way into German (noun Kicker) and Swedish (verb kicka). In France le penalty means a penalty kick. However, the phrase tir au but (lit. shot(s) on the goal) is often used in the context of a penalty shootout. In Brazilian Portuguese, because of the pervasive presence of football in Brazilian culture, many words related to the sport have found their way into everyday language, including the verb chutar (from shoot) \u2013 which originally meant \"to kick a football\", but is now the most widespread equivalent of the English verb \"to kick\". In Bulgaria a penalty kick is called duzpa (\u0434\u0443\u0437\u043f\u0430, from French words douze pas \u2013 twelve steps). In Italy, alongside the term calcio, is often used pallone (literally ball in Italian), especially in Sicily (u palluni). In Hong Kong, \u5341\u4e8c\u78bc (literally ten two yard, where ten two means twelve) is referring to the penalty kick, which is at 12 yards away from the goal line.\n\n\n== Notes ==\n\n\n== References =="}, {"id": 5, "title": "American football rules", "content": "Gameplay in American football consists of a series of downs, individual plays of short duration, outside of which the ball is dead or not in play. These can be plays from scrimmage \u2013 passes, runs, punts or field goal attempts (from either a place kick or a drop kick) \u2013 or free kicks such as kickoffs and fair catch kicks. Substitutions can be made between downs, which allows for a great deal of specialization as coaches choose the players best suited for each particular situation. During a play, each team should have no more than 11 players on the field, and each of them has specific tasks assigned for that specific play.\n\n\n== Objective of the game ==\nThe objective of this game is to score more points than the other team during the allotted time. The team with the ball (the offense) has 4 plays (downs) to advance at least 10 yards, and can score points once they reach the opposite end of the field, which is home to a scoring zone called the end zone, as well as the goalposts. If the offense succeeds in advancing at least 10 yards, they earn a \"first down\" and the number of tries allotted is reset and then they are again given 4 tries to advance an additional 10 yards, starting from the spot to which they last advanced. If the offense does not advance at least 10 yards during their 4 downs, the team without the ball (the defense) regains control of the ball (called turnover on downs).\nOn offense, points are scored by advancing the ball into the opponent's end zone for a touchdown (worth six points), or by kicking the ball from the playing field through the raised vertical posts (the goalposts) which are most commonly situated on the end line of the end zone for a field goal (worth three points). After scoring a touchdown, the offense is given an additional opportunity from the 2-yard line (3-yard line in amateur football) to attempt to score (in the NFL, 15-yard line on 1-point conversions). Conversion attempts are used to score 1 or 2 points as follows:\n\nThe offense may attempt a field goal kick which is worth 1 point.\nThe offense may attempt to re-advance the ball into the opponent's end zone for a two-point conversion worth 2 points.While the opposing team has possession, the defense attempts to prevent the offense from advancing the ball and scoring. If an offensive player loses the ball during play (a fumble) or the ball is caught by a defensive player while still in the air (an interception), the defense may attempt to run into the offense's end zone for a touchdown. The defense may also score points by tackling the ball carrier in the offense's own end zone, called a safety (which is worth two points).\n\n\n== Time of play ==\nCollegiate and professional football games are 1 hour long, divided into four quarters of 15 minutes each. In high school football, 12 minute quarters are usually played. The clock is stopped frequently, however, with the result that a typical college or professional game can exceed three hours in duration. The referee controls the game clock and stops the clock after any incomplete pass or any play that ends out of bounds. In addition, each team is allowed 3 timeouts in each half that they may use at their own discretion. The clock normally runs during the action of plays, with a few exceptions known as untimed plays. Some high schools employ a mercy rule in which the clock runs continuously after one team's lead over the other achieves a certain number of points. In these instances, the clock only stops for injuries, or time outs called by a team or a referee.\nThe clock may also be stopped for an officials' time-out, after which, if the clock was running, it is restarted. For example: if there is a question whether or not a team has moved the ball far enough for a first down, the officials may use a measuring device (the chains) to determine the distance. While this measurement is taking place, the officials will signal for a stoppage of the clock. Once the measurement is finished and the ball is placed at the proper location (spotted), the referee will then signal for the clock to restart. Additional situations where officials may take a time-out are to administer a penalty or for an injured player to be removed from the field.\nIn addition to the game clock, a separate play clock is also used. This counts down the time the offense has to start the next play before it is assessed a penalty for delay of game (see below). This clock is typically 25 seconds from when the referee marks the ball ready for play. The NFL and NCAA use a 40-second play clock that starts immediately after the previous play ends, though, for certain delays, such as penalty enforcement, the offense has 25 seconds from when the ball is marked ready. The purpose of the play clock is to ensure that the game progresses at a consistent pace, preventing unnecessary delays. Overall, clock management is a significant part of the game; teams leading toward the end of the game will often try to run out the clock via kneeldown while trailing teams attempt the opposite.\nOfficials also call for media time-outs, which allow time for television and radio advertising. They also stop the clock after a change of possession of the ball from one team to the other. Successful PATs (Point(s) After Touchdown), a field goal try, or a kickoff may also warrant stopping the clock. If an instant replay challenge is called during the game, the referees signal for a media time out. The referee signals these media time-outs by first using the time out signal, then extending both arms in a horizontal position.\nTeams change ends of the field at the end of the first quarter and the end of the third quarter, though otherwise, the situation on the field regarding possession, downs remaining and distance-to-goal does not change at these occasions (so a team with possession 5 yards from the opponent's endzone at the end of the first quarter would resume playing 5 yards from the endzone at the other end of the field, which they would then be attacking). Separating the first and second halves is halftime. Both halves, and any overtime, begin with kick-offs \u2014 the kicking team is decided by a coin toss (see below).\nIn the NFL, an automatic timeout is called by the officials once the ball is dead and there are two minutes or less left in both the second and the fourth quarters, and overtime; this is most commonly referred to as the two-minute warning. No such warning is normally given in amateur football, though if there is no visible stadium clock, the referee will give a two-minute warning (four minutes in high school).\n\n\n== Overtime ==\n\n\n=== NFL ===\nIn the preseason, prior to 1973 and since 2021, games that are tied at the end of four quarters end in a tie.\nIn the regular season and playoffs, if a game is tied at the end of four quarters, overtime is played. In overtime, a coin toss is used to determine which team will possess the ball first. The winner of the coin toss can choose to give the ball or receive the ball. In a regular season, if the first possession results in a touchdown (by the receiving team or by the defensive team on a turnover) or the defensive team scores a safety, the scoring team wins. If the receiving team fails to score and loses possession, the game goes into sudden death, and the first to score wins. However, if the initial receiving team only scores a field goal, the game is not automatically over and the other team is given an opportunity to possess the ball as well. The other team can win with a touchdown, tie with a field goal leading to sudden death or lose if they fail to score.\nIn a playoff, both teams will have the opportunity to possess the ball. If the score is tied after each team has possessed the ball, the next score wins. However, if the team kicking off to start the overtime scores a safety on the receiving team's initial possession, the team that kicked off is the winner.During the regular season in the NFL, one overtime period is played (with each team receiving two-time outs). If the game is still tied after the 10-minute overtime, the game officially ends in a tie. In the playoffs, overtime periods continue until a winner is determined. Overtime follows a three-minute intermission after the end of the regulation game. Prior to the start of overtime, a coin flip is performed in which the captain of the visiting team calls the toss. The team that wins the coin flip has the option either to receive the kickoff or choose the side of the field they wish to defend. Ties are rare in the NFL; the most recent being a game between the Washington Commanders and New York Giants on December 4, 2022, which ended in a 20\u2013all tie. See List of NFL tied games for more games.\n\n\n==== Rule changes ====\nFrom the 2017 season, the overtime period was shortened from 15 minutes to 10 minutes for the preseason and regular season. The overtime for the postseason remains 15 minutes.Prior to the 2010\u201311 playoffs, the overtime winner was simply the first team to score any points; however, the rules were changed to reduce the advantage obtained by the team that won the overtime coin toss. Under the prior rules, the team that won the coin toss would usually elect to receive the ball, then gain just enough yardage to win the game by kicking a field goal without the other team ever touching the ball. The coin toss winner won approximately 60% of overtime games under that rule, rather than the 50% which would be expected by random chance.The first overtime game played under a trial of the new overtime rules occurred in a 2012 AFC wild card game between the Denver Broncos and Pittsburgh Steelers at Sports Authority Field at Mile High, Denver, Colorado. Denver won the game on the first play in overtime, an 80-yard touchdown pass from Tim Tebow to Demaryius Thomas. The rule was formally adopted for the 2012 season, and the first game in which both teams scored in overtime was a 43\u201337 victory by the Houston Texans over the Jacksonville Jaguars on November 18, 2012.The rules for overtime changed for the 2016\u20132017 season and were tweaked again for the 2017\u20132018 season. The NFL's overtime rules are still subject to criticism, as a team that loses the coin toss and goes on to concede the touchdown does not get a chance for their offense to take the field.Super Bowl LI was the first Super Bowl to go into overtime with a 28-all tie between the Atlanta Falcons and New England Patriots, which the Patriots eventually won with James White scoring a touchdown on the Patriots' first drive.The 2019 NFC and AFC championship games both went to overtime, the first time for such an occurrence. In the NFC title game, the New Orleans Saints won the coin toss but an interception allowed the Los Angeles Rams to drive into range to kick the winning field goal. In the AFC Championship held later that day, the New England Patriots won the coin toss and on their first drive scored the winning touchdown over the Kansas City Chiefs.\n\n\n==== NFL Europa ====\nNFL Europa, a defunct league run by the NFL, used a 10-minute overtime period, with the constraint that each team must have the opportunity of possession; once both teams have had such an opportunity, the overtime proceeds in a manner similar to the NFL's. Thus, if Team A has the first possession of overtime and scores a touchdown and converts their kick (thus being 7 points ahead of Team B), Team A would then kick off to Team B (In the NFL, the game would have ended with the touchdown, without a conversion being attempted). Team B would have to match or exceed the 7 point difference within this ensuing possession; exceeding it would end the game immediately while matching the difference would result in a kickoff to Team A. From this point, the overtime is sudden death. The defunct United Football League had also used this rule.\n\n\n=== World Football League ===\nThe defunct World Football League, in its first season of 1974, used an overtime system more analogous to the system long used in international soccer. The overtime consisted of one 15-minute period, which was played in its entirety and divided into two halves of 7\u00bd minutes each, with each half starting with a kickoff by one of the teams. The league changed to the NFL's sudden-death format for its final season in 1975.\n\n\n=== College and high school ===\nIn college and high school football, an overtime procedure (the Kansas plan) ensures that each team has equal opportunity to score. In college, both teams are granted possession of the ball at their opponents' 25 yard-line in succession; the procedure repeats for next possession if needed; all possessions thereafter will be from the opponent's 3-yard line. A coin flip takes place, with the winning team having the option either 1) to declare that they will take the ball first or second, or 2) to decide on which end of the field the series will occur (both teams' series occur on the same end of the field). The losing team will have the first option in any subsequent even-numbered overtime. In the first overtime, the team with the first series attempts to score either a touchdown or a field goal; their possession ends when either a touchdown or a field goal have been scored, they turn the ball over via a fumble or an interception, or they fail to gain a first down. After a touchdown, a team may attempt either an extra-point or a two-point conversion. However, if the team on defense during the first series recovers a fumble and returns it for a touchdown, or returns an interception for a touchdown, the defensive team wins the game. (This is the only way for a college overtime game to end without both teams having possession.) Otherwise, regardless of the outcome of the first team's series (be it a touchdown, field goal, or turnover), the other team begins their series. If the score remains tied after both teams have completed a series, the procedure is repeated, but if a touchdown is scored, a two-point conversion will be required. Since 2021, if the game is still tied after double overtime, each team attempts one 2-point conversion per period rather than getting the ball at the 25-yard line.In high school football, individual state associations can choose any overtime format they want, or even elect to not play overtime at all (ties stand in this case). However, most states use the Kansas Plan. In a majority of states, each team is granted possession of the ball at the 10-yard line, meaning that a team cannot make a first down without scoring except via a defensive penalty that carries an automatic first down (such as defensive pass interference or roughing the passer). As is the case with the college overtime rule, the team that wins the coin toss will have the choice as to whether to take the ball first or second, or decide at which end of the field the overtime will be played. The other major difference between overtime in college football and high school football is that in some states, if the defense forces a turnover, the ball is dead immediately, thus eliminating the possibility of scoring. However, in Texas, the college overtime rule is used, as both the University Interscholastic League, which governs interscholastic activities for Texas public high schools, and the Texas Association of Private and Parochial Schools, the largest analogous body for Texas private high schools, play by NCAA football rules with a few modifications for the high school level.\n\n\n=== XFL ===\nThe original incarnation of the XFL used a modified Kansas Plan which, upon the first team scoring, required the opponent to score the same or greater number of points in the same or fewer downs (i.e., if the first team scored a touchdown, and converted the one-point conversion in three downs, the opponent would have to match that touchdown and conversion in three downs as well). Each team started at the 20-yard line, but like high school, there were no opportunities for first downs. The league also banned field goals except on a fourth down.\nThe XFL's current incarnation uses a five-round shootout of two-point conversions similar to a penalty shootout in soccer or ice hockey. Such a shootout had never been attempted in organized football at the time the rule was proposed; in April 2019, the NCAA adopted a similar concept for games that reach quintuple  overtime starting with the 2019 FBS season, two seasons later, triple overtime. The defense is not able to score, as should a turnover occur, the play would be dead. Defensive penalties result in the ball moving up to the 1-yard line, while a second defensive penalty on any play, even in future rounds, results in a score awarded to the offensive team. To speed up the overtime process, both teams' offense and defense are on the field at the appropriate end zone. Once one team's offense has completed its round of the shootout, the other team's offense plays its round from the opposite end zone. These overtime rules ensure that both teams have an opportunity to win the game and would limit overtime to 5 or 6 minutes. If both teams remain tied after five rounds, multiple rounds of conversions will be played until one team succeeds, thus ensuring that no game can end in a draw.\n\n\n=== USFL (2022) ===\nThe United States Football League settles ties this way: teams get three (at least two) rounds of two-point conversions from the three-yard line. Coin toss is called by the visiting team; winner of the toss can choose to possess the ball first or defend. Whoever scores more points in the three rounds wins it; otherwise, teams play sudden-death rounds until one team scores. One timeout can be called per overtime round. Although no game clock is used, the play clock of 35 seconds is still used.\n\n\n== Playing the game ==\n\n\n=== Coin toss ===\nThree minutes before the start of the game, the referee meets with captains from both teams for a coin toss. The visiting team calls the toss before the coin is flipped, since 1996. The winner of the toss may defer their choice to the start of the second half, or they may take the first choice of:\n\nReceiving the kickoff to start the game, or kicking off to start the game\nChoosing an end of the field to defend in the first quarter (with the teams switching directions at the end of the first quarter and at the end of the third quarter)The loser of the toss gets the remaining option. Typically, if the winner of the toss defers, the loser will choose to receive the ball first. \nAt the start of the second half, the team that did not choose first (either because they deferred their choice or because they lost the toss) gets the first choice of options.\nAccording to USA Today, in college games, the team that wins the toss defers their choice to the start of the second half over 90% of the time.If a game goes to overtime, a coin toss is held before the start of overtime, but tosses are not held before the start of subsequent overtime periods. In college, for example, the loser of the toss to start overtime has the first choice in the second overtime period. The choices available to the captains in overtime vary among the NFL, college, and various states' high school rules.\nIn high school, the coin toss may be held between the captains or coaches earlier before the start of the game. At three minutes before kickoff, the captains meet for a simulated coin toss, where the referee announces the results of the earlier toss.\n\n\n==== XFL ====\nThe original incarnation of the XFL did not implement a coin toss; instead, an event took place called the \"opening scramble\", in which one player from each team fought to recover a football 20 yards away to determine possession. Both players lined up side-by-side on one of the 30-yard lines, with the ball being placed at the 50-yard line. At the whistle, the two players would run toward the ball and attempt to gain possession; whichever player gained possession first was allowed to choose possession (as if he had won a coin toss in other leagues).\nThe XFL's current incarnation also does not feature coin tosses. Instead, the home team is given the option to kick off, receive, select a goal, or defer to the second half. In the event of overtime, the visiting team is given the choice of going first or second or selecting which end zone to attack (with the home team getting the other choice).\n\n\n=== Downed player ===\nThe rules vary from the college level to the professional level. In the NFL, unless a player is tagged by an opposing player or gives himself up, he is not down. A player carrying the ball (the runner) is downed when any of the following occurs:\n\nAny part of the runner other than his hands or feet touches the ground. Ankles and wrists count as downed. This may be as a result of:\nContact by an opponent (called down by contact) where the opponent tackles the runner by pushing him, grasping him and pulling him to the ground, sliding into his legs, or touching him in any manner prior to any part of the runner other than his hands or feet touching the ground. Unlike the use of the word tackle in other sports, if the opposing player fails to down the ball carrier, it is merely an attempted tackle. If the ball carrier falls onto another player but he doesn't make contact with the ground, he can still get up and keep playing. A player on the ground is not considered part of the ground.\nIntentionally downing the ball: intentionally kneeling, verbally declaring \"I'm Down\" (except in college), or similar actions. For example, to protect himself from violent hits by opponents attempting to tackle him, the quarterback may choose to slide to the ground feet-first. This slide is interpreted as intentionally downing the ball, and opponents may then be penalized for hitting him.\nIn amateur football, a runner is downed when any part of his body other than his hands or feet touches the ground at any time (unless he is the holder for a place kick). In professional football, the runner is not down for such accidental contact; he must be down by contact with an opponent as described above.\nThe runner goes out of bounds: that is, any part of his body (including his hands or feet) touches the ground, or anything other than another player or an official, on or past a sideline or an endline. The sideline itself is out of bounds so that the runner is deemed out of bounds if he steps on or touches any part of it. A runner may carry the ball in such a manner that it is over the sideline, so long as the ball or runner does not touch anything out of bounds.\nThe runner's forward progress toward the opponents' goal line is stopped by contact with an opponent, with little chance to be resumed. The exact moment at which the player's forward progress stops is subject to the judgment of the officials. In particular, for the protection of the quarterback, he is considered down as soon as an official judge that he is in the grasp of an opponent behind the line of scrimmage, and the tackling defensive player(s) will be awarded a sack, if he is driven backward by the opponent, the ball will be spotted where his forward progress was stopped.\n\n\n== Scrimmage downs ==\nThe majority of a football game takes place on plays, or downs, that begin at the line of scrimmage. The officials spot the ball (place it in a designated spot on the field) on the line of scrimmage and declare it ready for play.\n\n\n=== Positions ===\n\nThe width of the spotted football defines the width of the neutral zone, an area of the field no player other than the snapper may position himself in or above before the snap. Each team has its own line of scrimmage, thought of as a vertical plane from sideline to sideline that passes through the point of the ball nearest its own goal line.\n\nA typical offense is made up of a quarterback, five offensive linemen, two wide receivers, a running back, a fullback, and a tight end, however teams will vary their personnel on the field to fit any given play. A quarterback is essentially the leader of the offense. It is most often their responsibility to pass along the play called to the rest of the players in the huddle before any given play. A quarterback is the primary ball-handler on offense. It is their responsibility to call the snap count for the ball to enter play. Once the ball is hiked into play, it is their job to either hand the ball off to one of their running backs, or scout the field for an open receiver to throw the ball to. In some instances, the quarterback will run the ball themselves. A quarterback is guarded by their offensive linemen. The offensive line is made up of a left and right tackle, a left and right guard, and a center. It is the center's responsibility to hike the ball to the quarterback. An offensive line has two different jobs. When the offense runs a pass play, it is their job to guard the quarterback from the defense that is rushing. When the offense runs a run play, it is their job to clear a path for the running back to run through. The running back also has multiple roles. They will either take the ball from the quarterback and run, move up and help the offensive line block or go out and catch a pass. While the role of the fullback is deteriorating currently among professional leagues, it is their primary responsibility to lead the running back. Running backs and fullbacks are sometimes also called a halfback, a wingback, or a slotback. Like the running back, the tight end also has multiple roles. They will either help the offensive line protect the quarterback, block on run plays, or run or catch the ball themselves. The wide receivers' primary role is to run out into the field of play and catch the ball, although they will also block in some instances.\nThe players on offense must arrange themselves in a formation, all behind their line of scrimmage (that is, on their side of the ball). For reasons of safety and competitive balance, there are strict rules which define the way in which the offensive players may line up. Seven players must line up directly on the line of scrimmage while four players line up behind the line of scrimmage. Within this formation, there are six eligible receivers who may receive a forward pass during play. These eligible receivers are either the running back, fullback, tight end, or wide receivers. The remaining five linemen, often called interior linemen do not normally handle the ball during a play. Because of these rules, various leagues of American football have enacted strict rules of uniform numbering so officials may more easily judge which players were eligible and which were not at the start of a play. For example, in college football, ineligible players wear numbers 50\u201379, while eligible receivers wear 1\u201349 or 80\u201399. Even within this structure, offenses can still present a wide number of formations, so long as they maintain the \"seven and four\" arrangement. Receivers, for example, may play close to the other linemen or they may play some distance down the line of scrimmage, where they would sometimes be called split ends. Of the four backs, they may play behind the linemen or may play \"split out\" to provide additional wide receivers. These additional receivers can be flankers (if they play split far wide, but still in the backfield) or slot receivers if they play in the \"slot\" between the split end and the rest of the offensive line.\nThe players on defense may arrange themselves in any manner, as long as all players are \"behind the line\" (that is, on the side of the line nearest their own end zone). Players who line up opposite the offensive line are called defensive linemen, usually with one or two defensive tackles in the middle (a single defensive tackle is often called the nose guard or nose tackle) and with one defensive end on each side. A defensive lineman's job is typically to put pressure on the opposing team's quarterback by rushing the offensive line. The defensive line is also most often the first set of players the opponent must get through should they choose to run the ball. Behind the linemen are the linebackers. A linebacker's job can be any number of things, including trying to rush the opposing team's quarterback, stopping the opponents running back on run plays, or covering the opponent's tight end or wide receivers. Positioned opposite the wide receivers are the cornerbacks. Their primary responsibility is to cover the wide receivers. Farthest back from the line are the safeties, usually in the middle of the field behind the linebackers. The safeties are the last line of defense against the opponent. Like a linebacker, a safety's role can vary, however, their most common role is to help the cornerbacks cover the opponent's wide receivers, which is called \"double coverage\". The linemen and linebackers close to the line of scrimmage, are often referred to as playing \"in the box\". Players outside \"the box\" (usually cornerbacks and safeties) are collectively referred to as the \"secondary\".\n\n\n=== Starting the down ===\nA scrimmage down begins with a snap, where the center throws or hands the ball back to one of the backs, usually the quarterback. The quarterback then either hands the ball off to a back, throws the ball, or runs with it himself. The down ends when the ball becomes dead (see below). The ball is typically next spotted where the ball became dead; however, if it became dead outside the hash marks, it is brought in on the same yard line to the nearest hash mark. This spot becomes the line of scrimmage for the next play. In the case of an incomplete forward pass, the ball is returned to the spot where it was last snapped to begin the next play. A fumbled ball that goes out of bounds is declared dead and possession remains with the team that most recently had control of the ball.\n\n\n=== Dead ball ===\nThe ball becomes dead, and the down ends, when:\n\nthe ball carrier is downed, as described above;\nunder college rules only, the ball carrier fakes a slide to the ground;\na forward pass falls incomplete (it touches the ground before possession is secured by a player);\nthe ball carrier or ball touches the sideline or end line or otherwise goes outside the field of play (\"out of bounds\");\nthe ball carrier or the ball, except on a scoring field goal attempt, hits any part of the goalpost (even if it bounces back onto the field);\na team scores;\na kick receiver makes a fair catch (waving his arm above his head to signal a fair catch, where the kicking team is not allowed to interfere with him or hit him after the catch, but in return, he is not allowed to run), or a member of the receiving team gains possession after a fair catch signal was given;\na member of the kicking team possesses a kicked ball beyond the line of scrimmage (e.g. \"downing\" a punt allowed to roll by the receiving team by holding it to stop its roll);\na kicked ball comes to rest;\na touchback occurs; or\nunder NFL or college rules, on fourth down (or, in the NFL, on any down after the two-minute warning in either half/overtime), a ball fumbled forward by the offensive team is recovered by an offensive team player other than the fumbler.The nearest official typically blows his whistle after the ball becomes dead to alert the players that the down has already ended. If the ball is alive and the official sounds an inadvertent whistle, then the ball still becomes dead, but the team in possession of the ball may elect to have the down replayed or take the spot where the ball was declared dead. If the ball was loose from a fumble, then the ball can be put into play at the spot of the fumble. If the ball was in flight from a kick or a pass, then the down is always replayed.\n\n\n== Free kick downs ==\nA free kick is a down that does not occur from scrimmage. The kicking team begins behind the ball, while the receiving team must remain at least 10 yards downfield before the ball is kicked.\n\n\n=== Kickoffs ===\nA kickoff is a type of free kick where the ball is placed on a tee (or held) at the kicking team's 35-yard line (40 for high school). In the 2011 NFL Season, changes were made regarding kickoffs to limit injuries. The spot from which the ball is kicked was restored to the 35 yard line, bringing to an end the 1994 designation of the 30 yard line, a change meant to increase the frequency of the option to elect a touchback. In addition, players on the kickoff coverage team (apart from the kicker) cannot line up more than a specified distance behind the kickoff line. This distance is 5 yards at most levels of the game and 1 yard in the NFL, minimizing running starts and thus reducing the speed of collisions. The kicking team's players may not cross this line until the ball is kicked; members of the non-kicking (or \"receiving\") team are similarly restrained behind a line 10 yards further downfield (the 45-yard line, or 50 for high school). A valid kickoff must travel at least this 10-yard distance to the receiving team's restraining line, after which any player of either team may catch or pick up the ball and try to advance it (a member of the kicking team may only recover a kickoff and may not advance it) before being downed (see \"Downed player,\" below). In most cases, the ball is kicked as far as possible (typically 40 to 70 yards), after which a player of the receiving team is usually able to secure possession (since the members of the kicking team cannot start downfield until after the ball is kicked). Occasionally, for tactical reasons, the kicking team may instead choose to attempt an onside kick, in which the kicker tries to kick the ball along the ground just over the required 10-yard distance in such a manner that one of his own teammates can recover the ball for the kicking side. If it is touched before ten yards, the ball is dead and a re-kick or spot of the ball will be rewarded to the receiving team.\n\n\n==== Receiving a kickoff ====\nA member of the receiving team gaining possession of the ball on a kickoff may attempt to advance it as far as he can toward the kicking team's goal line before being downed. Once the ball carrier is downed, the play is whistled dead and the ball is placed by the officials at the point where the play ended; this spot then becomes the line of scrimmage for the ensuing play. A kick that travels through or goes out of bounds within the end zone without being touched, or is caught by the receiving team in the end zone but not advanced out of it, results in a touchback; the ball is then placed at the receiving team's 25-yard line, which becomes the line of scrimmage. In college football only, a fair catch by the receiving team between its own 25-yard line and the goal line is treated as a touchback, with the ball placed at the 25.\nA kickoff that goes out of bounds anywhere other than the end zone before being touched by the receiving team is an illegal kick: the receiving team has the option of having the ball re-kicked from five yards closer to the kicking team's goal line, or they may choose to take possession of the ball at the point where it went out of bounds or 30 yards from the point of the kick (25 yards in high school, and in college as of 2012), whichever is more advantageous.\n\n\n=== Other free kicks ===\nA free-kick is also used to restart the game following a safety. The team that was trapped in its own end zone, therefore conceding two points to the other team, kicks the ball from its own 20-yard line. This can be a place kick (in the NFL, a tee cannot be used), drop-kick, or punt.\nIn the NFL and high school, a free kick may be taken on the play immediately after a fair catch; see \"fair catch kick\" below.\n\n\n== Scoring ==\n\n\n=== Scrimmage plays and kickoffs ===\nMost standard football plays are considered scrimmage plays, initiated from a line of scrimmage. Exceptions are kickoffs and try plays (below). Although similar rules apply during a try play, the number of points awarded for each score differs on a try play.\n\n\n==== Touchdown (6 points) ====\n\nA touchdown is earned when a player has legal possession of the ball and the ball touches or goes over the imaginary vertical plane above the opposing team's goal line. After a touchdown, the scoring team attempts a try play for 1 or 2 points (see below). A successful touchdown is signaled by an official extending both arms vertically above the head. A touchdown is worth six points, except in the defunct WFL where it was worth seven points.\nFor statistical purposes, the player who advances the ball into or catches it in the end zone is credited with the touchdown. If a forward pass was thrown on the play, the throwing player is credited with a passing touchdown.\n\n\n==== Field goal (3 points) ====\n\nA field goal is scored when the ball is place kicked, drop kicked, or free kicked after a fair catch or awarded fair catch (High School or NFL only) between the goalposts behind the opponent's end zone. The most common type of kick used is the place kick. For a place kick, the ball must first be snapped to a placeholder, who holds the ball upright on the ground with his fingertip so that it may be kicked. Three points are scored if the ball crosses between the two upright posts and above the crossbar and remains over. If a field goal is missed, the ball is returned to the original line of scrimmage (in the NFL, to the spot of the kick; in high school, to the 20-yard line if the ball enters the end zone, or otherwise where the ball becomes dead after the kick) or to the 20-yard line if that is further from the goal line, and possession is given to the other team. If the ball does not go out of bounds, the other team may catch the kicked ball and attempt to advance it, but this is usually not advantageous. One official is positioned under each goalpost; if either one rules the field goal no good, then the field goal is unsuccessful. A successful field goal is signaled by an official extending both arms vertically above the head. A team that successfully kicks a field goal kicks off to the opposing team on the next play.\n\n\n==== Safety (2 points) ====\n\nThe uncommon safety is scored if a player causes the ball to become dead in his own end zone; two points are awarded to the opposing (usually defending) team. This can happen if a player is either downed or goes out of bounds in the end zone while carrying the ball, or if he fumbles the ball, and it goes out of bounds in the end zone. Safety is also awarded to the defensive team if the offensive team commits a foul which is enforced in its own end zone. A safety is not awarded if a player intercepts a pass or receives a kick in his own end zone and is downed there. This situation, in which the opponent caused the ball to enter the end zone, is called a touchback; no points are scored, and the team that gained possession of the ball is awarded possession at its own 25-yard line. If the interception or reception occurs outside the end zone, and the player is carried into the end zone by momentum, the ball is placed at the spot of the catch, and no safety is awarded. A safety is signaled by a referee holding both palms together above the head, fingertips pointing upwards. After a safety, the team that conceded the safety kicks a free kick (which may be a punt, place kick, or drop-kick) from its 20-yard line.\n\n\n=== Try plays ===\nA try play (as opposed to a regular scrimmage play or kickoff), more commonly referred to as an extra-point attempt, PAT (abbreviation of \"point after touchdown\"), conversion attempt, or two-point conversion attempt, based on the scoring team's intentions on the play, is awarded to the scoring team immediately following a touchdown. This un-timed down is an opportunity to score additional points.\nAlthough the game clock is not advanced during a try play, the play clock is enforced. A delay of game penalty, false start, or similar penalty, by the offense results in a 5-yard penalty assessed for the try. Typically, penalties charged against the defense give the offensive two options: half the distance to the goal for the try, or assessing the full penalty on the ensuing kickoff. Since the trial is not timed by the game clock, if a touchdown is scored as regulation time expires (and game clock subsequently reads 0:00.0), the try is still allowed to be conducted. After the Minneapolis Miracle in 2018, the NFL implemented a rule that if a team scores on the final play of the game, and extra points would not change the result, the PAT will no longer be conducted.\n\n\n==== Extra point (field goal - 1 point) ====\nThe offensive team may attempt to kick the ball through the goalposts, in the same manner, that a field goal is kicked during a scrimmage play. In the NFL, the ball is spotted at the 15-yard line. In college and high school, the ball is spotted at the 3-yard line. If successful, the team is awarded 1 point, referred to as an extra point. This option is almost always chosen because a two-point conversion attempt is much riskier. Since the extra point is almost always successful, sportscasters will often refer to a team-up or trailing by seven (not six) points as being \"up/trailing by a touchdown\".\n\n\n==== Two-point conversion (touchdown - 2 points) ====\nThe offensive team may attempt to advance the ball via run or pass into the end zone, much like a touchdown on the extra-point attempt, except that it receives two points. This is called a two-point conversion. If the offense elects to attempt a two-point conversion on the try play, the ball is spotted at the 2-yard line in the NFL and on the 3-yard line for college and high school. The success rate for two-point conversions is about 48 percent in the NFL, making the two-point conversion attempt a risky tactic; thus it is usually attempted only when two points will help the team but one point will not.\n\nFor example, suppose that it is late in the game with a score of 21\u201310 and the losing team scores a touchdown, making the score 21\u201316. The scoring team will usually attempt the two-point conversion because if successful, a three-point deficit later could be matched with one field goal; failure to convert would result in a five-point deficit that could be surmounted with another touchdown \u2013 a situation no worse than the four-point deficit achieved with a kicked extra point.\nAnother example would be if a team scores a late-game touchdown, and as a result is down by two points. A successful two-point conversion would tie the game and likely force overtime. In very rare and risky instances, a trailing team who scored a touchdown, and as a result is down by 1 point, may attempt a two-point conversion to gamble on a win and avoid overtime (or, under NCAA or NFHS rules, a subsequent overtime period). Two famous examples of this gamble were by Nebraska in the last minute of the 1984 Orange Bowl (unsuccessful) and by Boise State in the first overtime of the 2007 Fiesta Bowl (successful). Under NCAA rules, teams are required to \"go for two\" starting with double overtime.\n\n\n===== Defensive conversion =====\nUnder college, NFL & USFL rules, if the defensive team gains possession and advances the ball the length of the field into the opposite end zone on the try play (via interception or a fumble recovery, or by blocking a kick and legally recovering the ball), they score two points. This is officially recorded as a defensive conversion scored by the defense. The NCAA adopted this rule in 1988; the NFL added this in 2015; the USFL, 2022.\n\nThis scenario cannot occur under high school football rules except in Texas, which bases its rules on the college ruleset. Outside of Texas, the ball is ruled dead and the try is over immediately when the defense gains possession.\n\n\n==== Safety (1 point) ====\nA safety scored on a try play is worth one point. This can occur when, for example, the defense gains control of the ball and advances it into the field of play, but then retreats into its own end zone when play is stopped. Similarly, the defense could recover a fumble in its own end zone before play is stopped. A safety on a try play could also be awarded to the defense if the defense takes possession of the ball during a try play, advances it all the way down to the opposite end of the field, where the offensive team then regains possession before the play is declared dead in that end zone.\nThis has never been achieved in the NFL.\n\n\n==== Officials signals on try plays ====\nThe officials' signal for a successful try, whether an extra point or a two-point conversion, is the same as for a touchdown. The officials' signal for a safety on a try play is also the same as on a scrimmage play.\nAfter the try, the team that scored the touchdown kicks off to the opposing team. Unlike a safety that occurs on a scrimmage play, no free-kick is awarded following a safety on a try play.\n\n\n==== Try play rules in overtime ====\nDuring sudden-death over time, particularly in the NFL, if a team scores a touchdown in the overtime period, the game is immediately over, and the try is ignored.\nIn NCAA overtime, if the second team to possess the ball in the overtime scores a touchdown which puts them ahead of the opponent in points, the game is immediately over, and the try is ignored.\n\n\n=== Fair catch kick ===\n\nA free-kick (see above) may be taken on the play immediately after any fair catch of a punt. In the NFL, if the receiving team elects to attempt this and time expired during the punt, the half/overtime is extended with an untimed down. The ball must be held on the ground by a member of the kicking team or drop kicked; a tee may not be used. (High school kickers may use a tee). This is both a field goal attempt and a free-kick; if the ball is kicked between the goalposts, three points are scored for the kicking team. This is the only case where a free kick may score points. This method of scoring is extremely rare, last successfully completed in the NFL by Ray Wersching in 1976. It is only advantageous when a team catches a very short punt with very little time left. A team is unlikely to be punting with only a few seconds left in a half or overtime, and it is rarer still for punts to be caught near field goal range. The officials' signal for a successful fair catch kick is the same as for a field goal.\n\n\n==== Defunct leagues ====\nIn the WFL, PAT's were called \"Action Points\" and could only be scored via a run or pass play (as opposed to by kick as in the NFL), and were worth one point. The ball was placed on the two-and-a-half -yard line for an Action Point. This rule was a revival of a 1968 preseason experiment by the NFL and American Football League. The XFL's first incarnation employed a similar rule in which teams ran a single offensive down from the two-yard line (functionally identical to the NFL/NCAA/CFL two-point conversion), also for one point. By the playoffs, two-point and three-point conversions had been added to the rules. Teams could opt for the bonus points by playing the conversion farther back from the goal line. This rule remains intact in the current XFL.\n\n\n== Officiating ==\n\nThe game is officiated by a crew of three to seven officials. Every crew will consist of a referee, who is generally in charge of the game and watches the action on the quarterback and in the offensive backfield; an umpire, who handles spotting the ball and watches the action on the offensive line; and a head linesman, who supervises the placement of the down box and line-to-gain chains. The crew may also consist of a line judge, back judge, field judge and side judge, in the order listed: i.e. a crew of five officials have a referee, umpire, head linesman, line judge, and back judge.\nOfficials are selected by the teams in advance or appointed by the governing league. While the majority of officials at lower levels only officiate games on a part-time basis, the NFL is implementing a new system where seven officials will become full-time employees of the league, one for each official position (i.e. back judge, field judge, side judge, etc.). In the other three major North American professional sports leagues \u2013 Major League Baseball, the NBA and NHL \u2013 officials are employed by their respective leagues. The sheer volume of games in the other three sports necessitates full-time officials; since 2021, the NFL regular season is only 17 games long, compared to 162 games for MLB and 82 for the NBA and NHL.\nDuring the game, the officials are assisted in the administration of the game by other persons, including a clock operator to start and stop the game clock (and possibly also the play clock); a chain crew who hold the  down indicator and the line-to-gain chains on the sideline; and ball boys, who provide footballs to officials between downs (e.g. a dry ball each down on a wet day). These individuals may be provided by the teams involved \u2013 it is common for a high school coach's son or daughter to act as a ball boy for the team.\n\n\n== Fouls and their penalties ==\n\nBecause football is a high-contact sport requiring a balance between offense and defense, many rules exist that regulate equality, safety, contact, and actions of players on each team. It is very difficult to always avoid violating these rules without giving up too much of an advantage. Thus, an elaborate system of fouls and penalties has been developed to \"let the punishment fit the crime\" and maintain a balance between following the rules and keeping a good flow of the game. Players are constantly looking for ways to find an advantage that stretches the limitations imposed by the rules. Also, the frequency and severity of fouls can make a large difference in the outcome of a game, so coaches are constantly looking for ways to minimize the number and severity of infractions committed by their players.\nIt is a common misconception that the term \"penalty\" is used to refer both to an infraction and the penal consequence of that infraction. A foul is a rule infraction for which a penalty is prescribed. Some of the more common fouls are listed below. In most cases when a foul occurs, the offending team will be assessed a penalty of 5, 10, or 15 yards, depending on the foul. Also, in most cases, if the foul is committed while the ball is in play, the down will be replayed from the new position (for example, if the offense commits a foul on a first-down play, the next play will still be first down, but the offense may have to go 15 yards, or farther, to achieve another first down.) But if a defensive foul results in the ball advancing beyond the offense's first-down objective, the next play will be the first down of a new series. Some penalties (typically for more serious fouls), however, require a loss of down for the offense; and some defensive fouls may result in an automatic first down regardless of the ball position.\nIn all cases (except for ejection of a player or, in rare cases, forfeiture of the game), the non-offending team is given the option of declining the penalty and letting the result of the play stand (although the Referee may exercise this option on their behalf when it is obvious), if they believe it to be more to their advantage. For some fouls by the defense, the penalty is applied in addition to the yardage gained on the play. Most personal fouls, which involve danger to another player, carry 15-yard penalties; in rare cases, they result in offending players being ejected from the game. In the NFL, if a defensive foul occurs after time has expired at the end of a half, the half will be continued for a single, untimed play from scrimmage. Under college rules, any accepted penalty when the time has expired at the end of any quarter results in an extension for one untimed down.\nIn the NFL, with three exceptions, no penalty may move the ball more than half the distance toward the penalized team's goal line. These exceptions are defensive pass interference (see the discussion of that foul for more details), intentional grounding, and offensive holding \u2013 but in this last case, the exception pertains only if the infraction occurs within the offensive team's own end zone, in which case an automatic safety is assessed (intentional grounding from the end zone also carries an automatic safety). Under college rules, the same half-the-distance principle applies, but any offensive fouls involving contact in their end zone (e.g. holding, illegal blocking or personal fouls) result in a safety.\n\nThe neutral zone is the space between the two free-kick lines during a free-kick down and between the two scrimmage lines during a scrimmage down. For a free-kick down, the neutral zone is 10 yards wide and for a scrimmage down it is as wide as the length of the football. It is established when the ball is marked ready for play. No player may legally be in the neutral zone except for the snapper on scrimmage downs, and no one except the kicker and the holder for free kick downs.\n\n\n== Timeouts ==\nEach team receives three timeouts per half (if the game goes to overtime, each team receives additional timeouts), making for a total of six timeouts per team in a regulation game. Unused timeouts may not carry over to the second half or overtime. In professional football, a team must have at least one remaining timeout to challenge an official's call.\n\n\n== Instant replay ==\n\nIn the NFL, a number of rulings can be reviewed by officials or challenged by coaches. If a coach wants to challenge a play, he must do so before the next play begins, and he does so by throwing a red flag similar to the officials' yellow flags. Coaches are allowed two challenges per game and are granted a third if their first two are successful. The team loses a timeout if they lose the challenge. Therefore, they cannot challenge if they do not have timeouts. Plays within the two-minute-warning and overtime cannot be challenged; any review must be initiated by a replay official off-field. The referee performs the actual review via a video screen on the sideline. The referee will announce the result of instant replay reviews over his wireless microphone.\nBeginning in the 2011 NFL Season, an instant replay review by the booth official will now be automatic for every play ruled by the referees on the field to have scored points. This is seen as another step in the \"modernization\" of sports. Every scoring play will be reviewed now, which saves coaches from using up their challenges on close plays in the endzone. And since the 2012 season, the booth official also reviews all turnovers during the game.\nIn college, coaches are allowed one challenge per game by first requesting a timeout. Otherwise, a replay official in the press box observes all plays. If he deems a ruling may be in error, he notifies the officials on the field to interrupt the game before the beginning of the next play. The replay official performs the review and relays the decision to the referee, who announces the result. Not every conference employs replay, which is optional.\nHigh school rules generally do not provide for a video review of any decisions by officials during a game. By state adoption, replay may be used in a state championship game. At all times, the use of television or videotape for coaching purposes during the game is prohibited. If a coach feels a rule has been misinterpreted, he may call a timeout and request a coach-referee conference to discuss the ruling with the referee, but no replay equipment will be consulted during the conference.\n\n\n== Major rule differences between NFL and college football ==\nSome of the major rule differences between NFL and college football include:\n\n\n== See also ==\n1941 Oklahoma City vs. Youngstown football game, first use of the penalty flag\n\n\n== References ==\n\n\n== External links ==\nNational Football League Official Playing Rules and Casebook\nNational Football League Digest of Rules\nNational Football League Beginner's Guide to Football\nNational Football League Official Signals\nNational Collegiate Athletic Association Football Rules and Interpretations"}, {"id": 6, "title": "Stock exchange", "content": "A stock exchange, securities exchange, or bourse is an exchange where stockbrokers and traders can buy and sell securities, such as shares of stock, bonds and other financial instruments. Stock exchanges may also provide facilities for the issue and redemption of such securities and instruments and capital events including the payment of income and dividends. Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as \"continuous auction\" markets with buyers and sellers consummating transactions via open outcry at a central location such as the floor of the exchange or by using an electronic trading platform.To be able to trade a security on a particular stock exchange, the security must be listed there. Usually, there is a central location for record keeping, but trade is increasingly less linked to a physical place as modern markets use electronic communication networks, which give them advantages of increased speed and reduced cost of transactions. Trade on an exchange is restricted to brokers who are members of the exchange. In recent years, various other trading venues such as electronic communication networks, alternative trading systems and \"dark pools\" have taken much of the trading activity away from traditional stock exchanges.Initial public offerings of stocks and bonds to investors is done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).\nThere is usually no obligation for stock to be issued through the stock exchange itself, nor must stock be subsequently traded on an exchange. Such trading may be off exchange or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global securities market. Stock exchanges also serve an economic function in providing liquidity to shareholders in providing an efficient means of disposing of shares.\n\n\n== History ==\nStock market-based economies launched with BC Phoenicia's large trade network. The beginnings of lending were in Italy in the late Middle Ages.\nIn the 1300s, Venetian lenders would carry slates with information on the various issues for sale and meet with clients, much like a broker does today.\n\nVenetian merchants introduced the principle of exchanging debts between moneylenders; a lender looking to unload a high-risk, high-interest loan might exchange it for a different loan with another lender. These lenders also bought government debt issues.\nAs the natural evolution of their business continued, the lenders began to sell debt issues to the first individual investors in the late 1900s. The Venetians were the leaders in the field and the first to start trading securities from other governments, yet did not embark on private trade with India. Nor did the Italians connect on land with the Chinese Silk Road. Along the potential overland trade route, Habsburg (Austrian) emperor Frederick II repulsed advances by Mongol Batu Kahn (Golden Horde) in 1241. \nThere is little consensus among scholars as to when corporate stock was first traded. Some view the key event as the Dutch East India Company's founding in 1602, while others point to much earlier developments (Bruges, Antwerp in 1531 and in Lyon in 1548). The first book in history of securities exchange, the Confusion of Confusions, was written by the Dutch-Jewish trader Joseph de la Vega and the Amsterdam Stock Exchange is often considered the oldest \"modern\" securities market in the world. On the other hand, economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome, that derives from Etruscan \"Argentari\". In the Roman Republic, which existed for centuries before the Empire was founded, there were societates publicanorum, organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had partes or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions \"shares that had a very high price at the time\". Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The societas declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.\nTradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.\nJoseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book Confusion of Confusions explained the workings of the city's stock market. It was the earliest book about stock trading and inner workings of a stock market, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.\n\nIn England, the Dutch King William III sought to modernize the kingdom's finances to pay for its wars, and thus the first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public.\nLondon's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698, a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.One of history's greatest financial bubbles occurred around 1720. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of \"a company for carrying out an undertaking of great advantage, but nobody to know what it is\".\nBy the end of that same year, share prices had started collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States. On May 17, 1792, the New York Stock Exchange opened under a Platanus occidentalis (buttonwood tree) in New York City, as 24 stockbrokers signed the Buttonwood Agreement, agreeing to trade five securities under that buttonwood tree.\nBombay Stock Exchange was started by Premchand Roychand in 1875. While BSE Limited is now synonymous with Dalal Street, it was not always so. In the 1850s, five stock brokers gathered together under a Banyan tree in front of Mumbai Town Hall, where Horniman Circle is now situated. A decade later, the brokers moved their location to another leafy setting, this time under banyan trees at the junction of Meadows Street and what was then called Esplanade Road, now Mahatma Gandhi Road. With a rapid increase in the number of brokers, they had to shift places repeatedly. At last, in 1874, the brokers found a permanent location, the one that they could call their own. The brokers group became an official organization known as \"The Native Share & Stock Brokers Association\" in 1875.The Bombay Stock Exchange continued to operate out of a building near the Town Hall until 1928. The present site near Horniman Circle was acquired by the exchange in 1928, and a building was constructed and occupied in 1930. The street on which the site is located came to be called Dalal Street in Hindi (meaning \"Broker Street\") due to the location of the exchange.\nOn 31 August 1957, the BSE became the first stock exchange to be recognized by the Indian Government under the Securities Contracts Regulation Act. Construction of the present building, the Phiroze Jeejeebhoy Towers at Dalal Street, Fort area, began in the late 1970s and was completed and occupied by the BSE in 1980. Initially named the BSE Towers, the name of the building was changed soon after occupation, in memory of Sir Phiroze Jamshedji Jeejeebhoy, chairman of the BSE since 1966, following his death.\nIn 1986, the BSE developed the S&P BSE SENSEX index, giving the BSE a means to measure the overall performance of the exchange. In 2000, the BSE used this index to open its derivatives market, trading S&P BSE SENSEX futures contracts. The development of S&P BSE SENSEX options along with equity derivatives followed in 2001 and 2002, expanding the BSE's trading platform.\nHistorically an open outcry floor trading exchange, the Bombay Stock Exchange switched to an electronic trading system developed by Cmc ltd. in 1995. It took the exchange only 50 days to make this transition. This automated, screen-based trading platform called BSE On-Line Trading (BOLT) had a capacity of 8 million orders per day. Now BSE has raised capital by issuing shares and as on 3 May 2017 the BSE share which is traded in NSE only closed with \u20b9999.\n\n\n== Roles ==\nStock exchanges have multiple roles in the economy. This may include the following:\n\n\n=== Raising capital for businesses ===\nBesides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, a stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.Capital intensive companies, particularly high tech companies, typically need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. In the 1990s and early 2000s, hi-tech listed companies experienced a boom and bust in the world's major stock exchanges. Since then, it has been much more demanding for the high-tech entrepreneur to take his/her company public, unless either the company is already generating sales and earnings, or the company has demonstrated credibility and potential from successful outcomes: clinical trials, market research, patent registrations, etc. This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world in the total absence of sales, earnings, or any type of well-documented promising outcome. Though it is not as common, it still happens that highly speculative and financially unpredictable hi-tech startups are listed for the first time in a major stock exchange. Additionally, there are smaller, specialized entry markets for these kind of companies with stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX).\n\n\n==== Alternatives to stock exchanges for raising capital ====\nAlternative investment funds refer to funds that include hedge funds, venture capital, private equity, angel funds, real estate, commodities, collectibles, structured products, etc. Alternative investment funds are an alternative to traditional investment options (stocks, bonds, and cash).\n\n\n===== Research and Development limited partnerships =====\nCompanies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships. In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.\n\n\n===== Venture capital =====\nA general source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).\n\n\n===== Corporate partners =====\nAnother alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.\n\n\n=== Mobilizing savings for investment ===\nWhen people draw their savings and invest in shares (through an initial public offering or the seasoned equity offering of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.\n\n\n=== Facilitating acquisitions ===\nCompanies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or mergers and acquisitions through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.\n\n\n=== Profit sharing ===\nBoth casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.\n\n\n=== Corporate governance ===\nBy having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders and the more stringent rules for public corporations imposed by public stock exchanges and the government. This improvement can be attributed in some cases to the price mechanism exerted through shares of stock, wherein the price of the stock falls when management is considered poor (making the firm vulnerable to a takeover by new management) or rises when management is doing well (making the firm less vulnerable to a takeover).  In addition, publicly listed shares are subject to greater transparency so that investors can make informed decisions about a purchase.  Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders, their families and heirs, or otherwise by a small group of investors).\nDespite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies, particularly in the cases of accounting scandals. The policies that led to the dot-com bubble in the late 1990s and the subprime mortgage crisis in 2007\u201308 are also examples of corporate mismanagement. The mismanagement of companies such as Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam Products (2001), Webvan (2001), Adelphia Communications Corporation (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) all received plenty of media attention.\nMany banks and companies worldwide utilize securities identification numbers (ISIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.\nHowever, when poor financial, ethical or managerial records become public, stock investors tend to lose money as the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.\n\n\n=== Creating investment opportunities for small investors ===\nAs opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors as minimum investment amounts are minimal. Therefore, the stock exchange provides the opportunity for small investors to own shares of the same companies as large investors.\n\n\n=== Government capital-raising for development projects ===\nGovernments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate, in the short term, direct taxation of citizens to finance development\u2014though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.\n\n\n=== Barometer of the economy ===\nAt the stock exchange, share prices rise and decreases depending, largely, on economic forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. A recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore, the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.\n\n\n== Listing requirements ==\nEach stock exchange imposes its own listing requirements upon companies that want to be listed on that exchange. Such conditions may include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.\n\n\n=== Examples of listing requirements ===\nThe listing requirements imposed by some stock exchanges include:\n\nNew York Stock Exchange: the New York Stock Exchange (NYSE) requires a company to have issued at least 1.1 million shares of stock worth $40 million and must have earned more than $10 million over the last three years.\nNASDAQ Stock Exchange: NASDAQ requires a company to have issued at least 1.25 million shares of stock worth at least $70 million and must have earned more than $11 million over the last three years.\nLondon Stock Exchange: the main market of the London Stock Exchange requires a minimum market capitalization (\u00a3700,000), three years of audited financial statements, minimum public float (25%) and sufficient working capital for at least 12 months from the date of listing.\nBombay Stock Exchange: Bombay Stock Exchange (BSE) requires a minimum market capitalization of \u20b9250 million (US$3.1 million) and minimum public float equivalent to \u20b9100 million (US$1.3 million).\n\n\n== Ownership ==\nStock exchanges originated as mutual organizations, owned by its member stockbrokers. However, the major stock exchanges have demutualized, where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Espa\u00f1oles, and the S\u00e3o Paulo Stock Exchange (2007).\nThe Shenzhen Stock Exchange and Shanghai Stock Exchange can be characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission.\nAnother example is Tashkent Stock Exchange established in 1994, three years after the collapse of the Soviet Union, mainly state-owned but has a form of a public corporation (joint-stock company). Korea Exchange (KRX) owns 25% less one share of the Tashkent Stock Exchange.In 2018, there were 15 licensed stock exchanges in the United States, of which 13 actively traded securities. All of these exchanges were owned by three publicly traded multinational companies, Intercontinental Exchange, Nasdaq, Inc., and Cboe Global Markets, except one, IEX. In 2019, a group of financial corporations announced plans to open a members owned exchange, MEMX, an ownership structure similar to the mutual organizations of earlier exchanges.\n\n\n== Other types of exchanges ==\nIn the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These commodity markets later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.\n\n\n== See also ==\nAuction\nCapital market\nCommodities exchange\nCorporate governance\nFederation of Euro-Asian Stock Exchanges\nFinancial regulation\nHistoire des bourses de valeurs (French)\nInternational Organization of Securities Commissions\nSecurities market participants (United States)\nStag profit\nStock exchanges for developing countries\nStock market data systems\nWorld Federation of ExchangesLists:\n\nList of stock exchanges\nList of European stock exchanges\nList of stock exchanges in the Americas\nList of African stock exchanges\nList of stock exchanges in Western Asia\nList of South Asian stock exchanges\nList of East Asian stock exchanges\nList of Southeast Asian stock exchanges\nList of stock exchanges in Oceania\nList of countries without a stock exchange\nList of stock market indices\nList of financial regulatory authorities by country\nList of Swiss financial market legislation\n\n\n== References ==\n\n\n== External links ==\n\nStock exchange at Curlie"}, {"id": 7, "title": "Jaguar Cars", "content": "Jaguar (UK: , US: ) is the luxury vehicle brand of Jaguar Land Rover, a British  multinational car manufacturer with its headquarters in Whitley, Coventry, England. Jaguar Cars was the company that was responsible for the production of Jaguar cars until its operations were fully merged with those of Land Rover to form Jaguar Land Rover on 1 January 2013.\nJaguar's business was founded as the Swallow Sidecar Company in 1922, originally making motorcycle sidecars before developing bodies for passenger cars. Under the ownership of SS Cars, the business extended to complete cars made in association with Standard Motor Company, many bearing Jaguar as a model name. The company's name was changed from SS Cars to Jaguar Cars in 1945. A merger with the British Motor Corporation followed in 1966, the resulting enlarged company now being renamed as British Motor Holdings (BMH), which in 1968 merged with Leyland Motor Corporation and became British Leyland, itself to be nationalised in 1975.\nJaguar was spun off from British Leyland and was listed on the London Stock Exchange in 1984 until it was acquired by Ford in 1990. Since the late 1970s, Jaguar manufactured cars for the Prime Minister of the United Kingdom, the most recent prime ministerial car delivery being an XJ (X351) in May 2010. The company also held royal warrants from Queen Elizabeth II and Prince Charles.Ford owned Jaguar Cars, also buying Land Rover in 2000, until 2008 when it sold both to Tata Motors. Tata created Jaguar Land Rover as a subsidiary holding company. At operating company level, Jaguar Cars was merged in 2013 with Land Rover to form Jaguar Land Rover as the single design, manufacture, sales company, and brand owner for both Jaguar and Land Rover vehicles.\nSince the Ford ownership era, Jaguar and Land Rover have used joint design facilities in engineering centres at Whitley in Coventry and Gaydon in Warwickshire and Jaguar cars have been assembled in plants at Castle Bromwich and Solihull. On 15 February 2021, Jaguar Land Rover announced that all cars made under the Jaguar brand will be fully electric by 2025.\n\n\n== History ==\n\n\n=== Founding ===\nThe Swallow Sidecar Company was founded in 1922 by two motorcycle enthusiasts, William Lyons and William Walmsley. In 1934, Walmsley elected to sell-out and in order to buy the Swallow business (but not the company which was liquidated) Lyons formed SS Cars, finding new capital by issuing shares to the public.\n\nJaguar first appeared in September 1935 as a model name on an SS 2\u00bd-litre sports saloon. A matching open two seater sports model with a 3\u00bd-litre engine was named SS Jaguar 100.\nOn 23 March 1945, the S. S. Cars shareholders in general meeting agreed to change the company's name to Jaguar Cars Limited. Said chairman William Lyons \"Unlike S. S. the name Jaguar is distinctive and cannot be connected or confused with any similar foreign name.\"Though five years of pent-up demand ensured plenty of buyers production was hampered by shortage of materials, particularly steel, issued to manufacturers until the 1950s by a central planning authority under strict government control.  Jaguar sold Motor Panels, a pressed steel body manufacturing company bought in the late 1930s, to steel and components manufacturer Rubery Owen, and Jaguar bought from John Black's Standard Motor Company the plant where Standard built Jaguar's six-cylinder engines. From this time Jaguar was entirely dependent for their bodies on external suppliers, in particular then independent Pressed Steel and in 1966 that carried them into BMC, BMH and British Leyland.\n\nJaguar made its name by producing a series of successful eye-catching sports cars, the Jaguar XK120 (1948\u201354), Jaguar XK140 (1954\u201357), Jaguar XK150 (1957\u201361), and Jaguar E-Type (1961\u201375), all embodying Lyons' mantra of \"value for money\". The sports cars were successful in international motorsport, a path followed in the 1950s to prove the engineering integrity of the company's products.\nJaguar's sales slogan for years was \"Grace, Space, Pace\", a mantra epitomised by the record sales achieved by the MK VII, IX, Mks I and II saloons and later the XJ6. During the time this slogan was used, but the exact text varied.The core of Bill Lyons' success following the Second World War was the twin-cam straight six engine, conceived pre-war and realised while engineers at the Coventry plant were dividing their time between fire-watching and designing the new power plant.  It had a hemispherical cross-flow cylinder head with valves inclined from the vertical; originally at 30 degrees (inlet) and 45 degrees (exhaust) and later standardised to 45 degrees for both inlet and exhaust.\n\nAs fuel octane ratings were relatively low from 1948 onwards, three piston configuration were offered: domed (high octane), flat (medium octane), and dished (low octane).\nThe main designer, William Heynes, assisted by Walter Hassan, was determined to develop the Twin OHC unit. Bill Lyons agreed over misgivings from Hassan. It was risky to take what had previously been considered a racing or low-volume and cantankerous engine needing constant fettling and apply it to reasonable volume production saloon cars.\nThe subsequent engine (in various versions) was the mainstay powerplant of Jaguar, used in the XK 120, Mk VII Saloon, Mk I and II Saloons and XK 140 and 150. It was also employed in the E Type, itself a development from the race winning and Le Mans conquering C and D Type Sports Racing cars refined as the short-lived XKSS, a road-legal D-Type.\nFew engine types have demonstrated such ubiquity and longevity: Jaguar used the Twin OHC XK Engine, as it came to be known, in the Jaguar XJ6 saloon from 1969 through 1992, and employed in a J60 variant as the power plant in such diverse vehicles as the British Army's Combat Vehicle Reconnaissance (Tracked) family of vehicles, as well as the Fox armoured reconnaissance vehicle, the Ferret Scout Car, and the Stonefield four-wheel-drive all-terrain lorry. Properly maintained, the standard production XK Engine would achieve 200,000 miles of useful life.\nTwo of the proudest moments in Jaguar's long history in motor sport involved winning the Le Mans 24 hours race, firstly in 1951 and again in 1953. Victory at the 1955 Le Mans was overshadowed by it being the occasion of the worst motorsport accident in history. Later in the hands of the Scottish racing team Ecurie Ecosse two more wins were added in 1956 and 1957.\nIn spite of such a performance orientation, it was always Lyons' intention to build the business by producing world-class sporting saloons in larger numbers than the sports car market could support. Jaguar secured financial stability and a reputation for excellence with a series of elegantly styled luxury saloons that included the 3-litre and 3\u00bd litre cars, the Mark VII, VIII, and IX, the compact Mark I and 2, and the XJ6 and XJ12. All were deemed very good values, with comfortable rides, good handling, high performance, and great style.\nCombined with the trend-setting XK 120, XK 140, and XK 150 series of sports car, and nonpareil E-Type, Jaguar's elan as a prestige motorcar manufacturer had few rivals. The company's post-War achievements are remarkable, considering both the shortages that drove Britain (the Ministry of Supply still allocated raw materials) and the state of metallurgical development of the era.\n\n\n=== Daimler ===\nIn 1950, Jaguar agreed to lease from the Ministry of Supply the Daimler Shadow 2 factory in Browns Lane, Allesley, Coventry, which at the time was being used by Daimler and moved to the new site from Foleshill over the next 12 months. Jaguar purchased Daimler, not to be confused with Daimler-Benz or Daimler AG, in 1960 from BSA. From the late 1960s, Jaguar used the Daimler marque as a brand name for their most luxurious saloons.\n\n\n=== Ownership ===\n\n\n==== An end to independence ====\nPressed Steel Company Limited made all Jaguar's (monocoque) bodies leaving provision and installation of the mechanicals to Jaguar. In mid-1965 British Motor Corporation (BMC), the Austin-Morris combine, bought Pressed Steel. Lyons became concerned about the future of Jaguar, partly because of the threat to ongoing supplies of bodies, and partly because of his age and lack of an heir. He therefore accepted BMC's offer to merge with Jaguar to form British Motor (Holdings) Limited. At a press conference on 11 July 1965 at the Great Eastern Hotel in London, Lyons and BMC chairman George Harriman announced, \"Jaguar Group of companies is to merge with The British Motor Corporation Ltd., as the first step towards the setting up of a joint holding company to be called British Motor (Holdings) Limited\". In due course BMC changed its name to British Motor Holdings at the end of 1966.\nBMH was pushed by the Government to merge with Leyland Motor Corporation Limited, manufacturer of Leyland bus and truck, Standard-Triumph and, since 1967, Rover vehicles. The result was British Leyland Motor Corporation, a new holding company which appeared in 1968, but the combination was not a success. A combination of poor decision making by the board along with the financial difficulties of, especially, the Austin-Morris division (previously BMC) led to the Ryder Report and to effective nationalisation in 1975.\n\n\n==== Temporary return to independence ====\nOver the next few years it became clear that because of the low regard for many of the group's products insufficient capital could be provided to develop and begin manufacture of new models, including Jaguars, particularly if Jaguar were to remain a part of the group.\nIn July 1984, Jaguar was floated off as a separate company on the London Stock Exchange;\u2013 one of the Thatcher government's many privatisations\u2013 to create its own track record.Installed as chairman in 1980, Sir John Egan is credited for Jaguar's unprecedented prosperity immediately after privatisation. In early 1986 Egan reported he had tackled the main problems that were holding Jaguar back from selling more cars: quality control, lagging delivery schedules, poor productivity. He laid off about one third of the company's roughly 10,000 employees to cut costs. Commentators later pointed out he exploited an elderly model range (on which all development costs had been written off) and raised prices. He also intensified the effort to improve Jaguar's quality. In the US the price increases were masked by a favourable exchange rate.\n\n\n==== Ford era ====\nFord made offers to Jaguar's US and UK shareholders to buy their shares in November 1989; Jaguar's listing on the London Stock Exchange was removed on 28 February 1990. In 1999 it became part of Ford's new Premier Automotive Group along with Aston Martin, Volvo Cars and, from 2000, Land Rover. Under Ford's ownership, Jaguar never made a profit.Under Ford's ownership Jaguar expanded its range of products with the launch of the S-Type in 1999 and X-type in 2001. After PAG acquired Land Rover in May 2000 purchase by Ford, the brand became closely associated with Jaguar. In many countries they shared a common sales and distribution network (including shared dealerships), and some models shared components, although the only shared production facility was Halewood Body & Assembly \u2013 which manufactured the technically related X-Type and the Freelander 2. Operationally the two companies were effectively integrated under a common management structure within Ford's PAG.\nOn 11 June 2007, Ford announced that it planned to sell Jaguar, along with Land Rover and retained the services of Goldman Sachs, Morgan Stanley and HSBC to advise it on the deal. The sale was initially expected to be announced by September 2007, but was delayed until March 2008. Private equity firms such as Alchemy Partners of the UK, TPG Capital, Ripplewood Holdings (which hired former Ford Europe executive Sir Nick Scheele to head its bid), Cerberus Capital Management and One Equity Partners (owned by JPMorgan Chase and managed by former Ford executive Jacques Nasser) of the US, Tata Motors of India and a consortium comprising Mahindra & Mahindra (an automobile manufacturer from India) and Apollo Management all initially expressed interest in purchasing the marques from Ford.Before the sale was announced, Anthony Bamford, chairman of British excavator manufacturer JCB had expressed interest in purchasing the company in August 2006, but backed out upon learning that the sale would also involve Land Rover, which he did not wish to buy. On Christmas Eve of 2007, Mahindra and Mahindra backed out of the race for both brands, citing complexities in the deal.\n\n\n==== Tata Motors era ====\nOn 1 January 2008, Ford announced Tata as the preferred bidder. Tata Motors also received endorsements from the Transport And General Worker's Union (TGWU)-Amicus combine as well as from Ford. According to the rules of the auction process, this announcement would not automatically disqualify any other potential suitor. However, Ford (as well as representatives of Unite) would now be able to enter into detailed discussions with Tata concerning issues ranging from labour concerns (job security and pensions), technology (IT systems and engine production) and intellectual property, as well as the final sale price. Ford would also open its books for a more comprehensive due diligence by Tata. On 18 March 2008, Reuters reported that American bankers Citigroup and JP Morgan would finance the deal with a US$3 billion loan.On 26 March 2008, Ford announced that it had agreed to sell its Jaguar and Land Rover operations to Tata Motors of India, and that they expected to complete the sale by the end of the second quarter of 2008. Included in the deal were the rights to three other British brands, Jaguar's own Daimler, as well as two dormant brands Lanchester and Rover. On 2 June 2008, the sale to Tata was completed at a cost of \u00a31.7 billion.On 18 January 2008, Tata Motors, a part of the Tata Group, established Jaguar Land Rover as a British-registered and wholly owned subsidiary. The company was to be used as a holding company for the acquisition of the two businesses from Ford \u2013 Jaguar Cars Limited and Land Rover. That acquisition was completed on 2 June 2008. On 1 January 2013, the group, which had been operating as two separate companies (Jaguar Cars Limited and Land Rover), although on an integrated basis, underwent a fundamental restructuring. The parent company was renamed to Jaguar Land Rover Automotive PLC, Jaguar Cars Limited was renamed to Jaguar Land Rover Limited and the assets (excluding certain Chinese interests) of Land Rover were transferred to it. The consequence was that Jaguar Land Rover Limited became responsible in the UK for the design, manufacture and marketing of both Jaguar and Land Rover products.\n\n\n=== Plants ===\nFrom 1922 the Swallow Sidecar company (SSC) was located in Blackpool. The company moved to Holbrook Lane, Coventry in 1928 when demand for the Austin Swallow became too great for the factory's capacity. The company started using the Jaguar name whilst based in Holbrooks Lane.\nIn 1951, having outgrown the original Coventry site they moved to Browns Lane, which had been a wartime \"shadow factory\" run by The Daimler Company. The Browns Lane plant ceased trim and final operations in 2005, the X350 XJ having already moved to Castle Bromwich two years prior, with the XK and S-Type following. The Browns Lane plant, which continued producing veneer trim for a while and housed the Jaguar Daimler Heritage centre until it moved to the British Motor Museum site, has now been demolished and is being redeveloped.\nJaguar acquired the Whitley engineering centre from Peugeot in 1986, the facility having been part of Chrysler Europe which the French firm had owned since the late 1970s.  The decision to offload the site to Jaguar came as Peugeot discontinued the Talbot brand for passenger cars.  In 2016, Jaguar also moved into part of the old Peugeot/Chrysler/Rootes site in Ryton-on-Dunsmore which closed a decade earlier \u2013 this now is the home of Jaguar Land Rover's classic restoration operation.\nJaguar's Radford plant, originally a Daimler bus plant but later a Jaguar engine and axle plant, was closed by Ford in 1997 when it moved all Jaguar engine production to its Bridgend facility.\nIn 2000, Ford turned its Halewood plant over to Jaguar following the discontinuation of its long running Escort that year for Jaguar's new X-Type model. It was later joined by the second-generation Land Rover Freelander 2, from 2007.  Jaguars ceased being produced at Halewood in 2009 following the discontinuation of the X-Type; Halewood now becoming a Land Rover-only plant.\nSince Jaguar Land Rover was formed following the merger of Jaguar Cars with Land Rover, facilities have been shared across several JLR sites, most of which are used for work on both the Jaguar and Land Rover brands.\n\n\n== Current cars ==\n\n\n=== E-Pace ===\nThe Jaguar E-Pace is a compact SUV, officially revealed on 13 July 2017.\n\n\n=== F-Pace ===\nThe F-Pace is a compact luxury crossover SUV \u2013 the first SUV from Jaguar. It was unveiled at the International Motor Show Germany in Frankfurt in September 2015.\n\n\n=== F-Type ===\nThe F-Type convertible was launched at the 2012 Paris Motor Show, following its display at the Goodwood Festival of Speed in June 2012, and is billed as a successor to the legendary E-Type. In fact, the Series III E-Type already had a successor, in the form of the XJS, which was in turn replaced by the XK8 and XKR. The F-Type nevertheless returns to the 2-seat plan that was lost with the introduction of the Series III E-Type, which was available only in a 2+2-seat configuration. It was developed following the positive reaction to Jaguar's C-X16 concept car at the 2011 Frankfurt Auto Show. Sales began in 2013 with three engine choices; two variants of the AJ126 V6 petrol engine and the AJ133 V8 petrol engine.\n\n\n=== I-Pace ===\nThe Jaguar I-Pace is an electric SUV, officially revealed on 1 March 2018. It is Jaguar's first electric car.\n\n\n=== XE ===\nThe XE is the first compact executive Jaguar since the 2009 model year X-Type and is the first of several Jaguar models to be built using Jaguar's new modular aluminium architecture, moving the company away from the Ford derived platforms that were used in the past for the X-Type and XF. The use of Jaguar's own platform allows the XE to feature either rear-wheel drive or all-wheel drive configurations, and it is the first car in its segment with an aluminium monocoque structure. Originally announced at the 2014 Geneva Motor Show with sales scheduled for 2015.\n\n\n=== XF ===\nThe Jaguar XF is a mid-size executive car introduced in 2008 to replace the S-Type. In January 2008, the XF was awarded the What Car? 'Car of the Year' and 'Executive Car of the Year' awards. The XF was also awarded Car of the Year 2008 from What Diesel? magazine. Engines available in the XF are 2.2-litre I4 and 3.0-litre V6 diesel engines, or 3.0 litre V6 and 5.0-litre V8 petrol engines. The 5.0 Litre engine is available in supercharged form in the XFR. From 2011, the 2.2-litre diesel engine from the Land Rover Freelander was added to the range as part of a facelift.\n\n\n== R models ==\n\nJaguar began producing R models in 1995 with the introduction of the first XJR, and the first XKR was introduced in 1997. Jaguar R, R-S and SVR models are designated to compete with the likes of Mercedes-AMG, BMW M and Audi S and RS.\n\n\n== Historic car models ==\nThe renamed Jaguar company started production with the pre-war 1.5, 2.5 and 3.5-litre models, which used engines designed by the Standard Motor Company. The 1.5-litre four-cylinder engine was still supplied by Standard but the two larger six-cylinder ones were made in house. These cars have become known unofficially as Mark IVs.\nThe first post-war model was the September 1948 Mark V available with either 2.5 or 3.5-litre engines. It had a slightly more streamlined appearance than pre-war models, but more important was the change to torsion bar independent front suspension and hydraulic brakes. In the spring of 1948 Lyons had returned from USA reporting Jaguar's individuality and perceived quality attracted the admiration of American buyers accustomed to the virtual uniformity of their home-grown vehicles.\nThe first big breakthrough was the launch in October 1948 of their new record-breaking engine design in their XK120 sportscar to replace the prewar SS Jaguar 100. It was powered by a new twin overhead camshaft (DOHC) 3.5-litre hemi-head six-cylinder engine designed by William Heynes, Walter Hassan and Claude Baily. The XK100 4-cylinder 2-Litre version had broken records in Belgium travelling at 177 mph. This XK engine had been designed at night during the war when they would be on fire watch in the factory. After several attempts a final design was achieved. That is until owner William Lyons said \"make it quieter\".\nThe sportscar bearing its prefix X had originally been intended as a short production model of about 200 vehicles. A test bed for the new engine until its intended home, the new Mark VII saloon, was ready.\nThe second big breakthrough was the large Mark VII saloon in 1950, a car especially conceived for the American market, Jaguar was overwhelmed with orders. The Mark VII and its successors gathered rave reviews from magazines such as Road & Track and The Motor. In 1956 a Mark VII won the prestigious Monte Carlo Rally. The XK120's exceptional reception was followed in 1954 by an improved XK140 then in May 1957 a fully revised XK150.\n\nIn 1955, the Two-point-four or 2.4-litre saloon (named by enthusiasts 2.4 Mark 1) was the first monocoque (unitary) car from Jaguar. Its 2.4-litre short-stroke version of the XK engine provided 100 mph (160 km/h) performance. In 1957, the 3.4-litre version with disk brakes, wire wheels and other options was introduced, with a top speed of 120 mph (190 km/h). In October 1959, an extensively revised version of the car with wider windows and 2.4, 3.4, and 3.8-litre engine options became the Mark 2. The 3.8 Mark 2 was popular with British police forces for its small size and 125 mph (201 km/h) performance.\nThe Mark VIII of 1956 and Mark IX of 1958 were essentially updates of the Mark VII, but the oversize Mark X of 1961 was a completely new design of large saloon with all round independent suspension and unitary construction.\n\nJaguar launched the E-Type in 1961.\nThe independent rear suspension from the Mark X was incorporated in the 1963 S-Type, a Mark 2 lengthened to contain the complex rear suspension, and in 1967 the Mark 2 name was dropped when the small saloons became the 240/340 range. The 420 of 1966, also sold as the Daimler Sovereign, put a new front onto the S-type, although both cars continued in parallel until the S-Type was dropped in 1968. The slow-selling Mark X became the 420G in 1966 and was dropped at the end of the decade. Jaguar was saved by its new equally capacious but very much trimmer new XJ6.\n\nOf the more recent saloons, the most significant is the XJ (1968\u20131992). From 1968 on, the Series I XJ saw minor changes, first in 1973 (to Series II), 1979 (Series III), a complete redesign for 1986/1987 in XJ40, further modifications in 1995 (X300), in 1997 with V8-power (X308), and a major advance in 2003 with an industry-first aluminium monocoque-chassis (X350). The most luxurious XJ models carried either the Vanden Plas (US) or Daimler (rest of world) nameplates. In 1972, the 12-cylinder engine was introduced in the XJ, while simultaneously being offered in the E Type.\n\n1992 saw the introduction of the mid-engined, twin-turbo XJ220, powered by a 542 bhp (404 kW; 550 PS) V6 engine. The XJ220 was confirmed the fastest production car in the world at the time after Martin Brundle recorded a speed of 217 mph (349 km/h) on the Nardo track in Italy.Over the years many Jaguar models have sported the famous chrome plated Leaping Jaguar, traditionally forming part of the radiator cap. Known as \"The Leaper\", this iconic mascot has been the subject of controversy in recent times when banned for safety reasons from cars supplied to Europe whilst it continued to be fitted on cars destined for the United States, Middle East and Far East. It has now been dropped from all the latest Jaguar models, although some customers add it to their car as a customization.\n\nThe Jaguar S-Type, first appeared in 1999 and stopped production in 2008. It has now been replaced by the Jaguar XF. Early S-Types suffered from reliability problems but those were mostly resolved by the 2004 model year.The Jaguar X-Type was a compact executive car launched in 2001, while the company was under Ford ownership, sharing its platform with the Ford Mondeo. X-Type production ended in 2009.The Jaguar XK was a luxury grand tourer introduced in 2006, where it replaced the XK8. The XK introduced an aluminium monocoque bodyshell, and was available both as a two-door coup\u00e9 and two-door cabriolet/convertible. Production ceased in 2014.\n\nThe Jaguar XJ was a full-size luxury saloon. The model was in production since 1968, with production ceasing in 2019, with the first generation being the last Jaguar car to have creative input by the company's founder, Sir William Lyons, although this is disputed as some Jaguar historians claim that the second generation XJ \u2013 the XJ40 series  \u2013 was the last car which Lyons had influenced. The XJ40 originally launched in 1986 and went through two major revamps in 1994 (X300) and 1997 (X308) for a total production run of 17 years.\nIn early 2003, the third generation XJ \u2013 the X350 \u2013 arrived in showrooms and while the car's exterior and interior styling were traditional in appearance, the car was completely re-engineered. Its styling attracted much criticism from many motoring journalists who claimed that the car looked old-fashioned and barely more modern than its predecessor, many even citing that the 'Lyons line' had been lost in the translation from XJ40 into X350 XJ, even though beneath the shell lay a highly advanced aluminium construction that put the XJ very near the top of its class.Jaguar responded to the criticism with the introduction of the fourth generation XJ, launched in 2009. Its exterior styling is a departure from previous XJs, with a more youthful, contemporary stance, following the design shift that came into effect previously with the company's XF and XK models. The 5-litre V8 engine in the XJ Supersport can accelerate the car from 0 to 60 mph (0\u201397 km/h) in 4.7 seconds, and has a UK CO2 emission rating of 289 g/km. To cater to the limousine market, all XJ models are offered with a longer wheelbase (LWB) as an option, which increases the rear legroom.\n\n\n=== List ===\n\n\n== Concept cars ==\nE1A \u2013 The 1950s E-Type concept vehicle\nE2 A \u2013 The second E-Type concept vehicle, which raced at LeMans and in the USA\nXJ13 (1966) \u2013 Built to race at LeMans, never run\nPirana (1967) \u2013 Designed by Bertone\nAscot (1977)\nXJ41/XJ42 (1982-1990) \u2013 the first F-Type; cancelled due to the Ford's takeover of Jaguar\nXJ90 (1988-1991) \u2013 planned XJ40 replacement; cancelled due to Ford's takeover of Jaguar\nKensington (1990)\nXK 180 (1998) \u2013 Roadster concept based on the XK8\nF-Type (2000) \u2013 Roadster, similar to the XK8 but smaller\nR-Coup\u00e9 (2001) \u2013 Large four-seater coup\u00e9\nFuore XF 10 (2003)\nR-D6 (2003) \u2013 Compact four-seat coup\u00e9\nXK-RR \u2013 A high-performance version of last generation XK coup\u00e9\nXK-RS \u2013 Another performance-spec version of last generation XK convertible\nConcept Eight (2004) \u2013 Super-luxury version of the long-wheelbase model of the XJ\nC-XF (2007) \u2013 Precursor to the production model XF saloon\nC-X75 (2010) \u2013 Hybrid-electric sports car, originally intended for production but cancelled in 2012\nB99 (2011)\nC-X16 (2011) \u2013 Precursor to the production model F-Type\nC-X17 (2013) \u2013 First ever Jaguar SUV concept\nProject 7 \u2013 a 542 bhp V8-powered speedster based on the F-Type and inspired by the D-Type (2013)\nFuture-Type (2017)\n\n\n== Engines ==\nJaguar has designed in-house six generations of engines:\n\nHistoric:\nXK6 \u2013 Inline-6\nV12 \u2013 60\u00b0 V12\nAJ6/AJ16 \u2013 22\u00b0 Inline-6\nAJ-V6 \u2013 60\u00b0 V6 (Ford designed, Jaguar modified)\nCurrent:\nAJ-V8 \u2013 90\u00b0 V8\nAJ126 \u2013 90\u00b0 V6\nAJD-V6 \u2013 60\u00b0 V6 (Ford designed)\nIngenium \u2013 Inline-4\n\n\n== Motorsport ==\n\nJaguar has had major success in sports car racing, particularly in the Le Mans 24 Hours. Victories came in 1951 and 1953 with the C-Type, then in 1955, 1956 and 1957 with the D-Type. The manager of the racing team during this period, Lofty England, later became CEO of Jaguar in the early 1970s. Although the prototype XJ13 was built in the mid-1960s it was never raced, and the famous race was then left for many years.\nIn 1982, a successful relationship with Tom Walkinshaw Racing commenced with the XJ-S competing in the European Touring Car Championship, which it won in 1984. A TWR XJ-S won the 1985 Bathurst 1000. In the mid-1980s TWR started designing and preparing Jaguar V12-engined Group C cars for World Sports Prototype Championship races. The team started winning regularly from 1987, and won Le Mans in 1988 and 1990 with the XJR series sports cars. The Jaguar XJR-14 was the last of the XJRs to win, taking the 1991 World Sportscar Championship.\nIn 1999, Ford decided that Jaguar would be the corporation's Formula One entry. Ford bought out the Milton Keynes-based Stewart Grand Prix team and rebranded it as Jaguar Racing for the 2000 season. The Jaguar F1 program was not a success however, achieving only two podium finishes in five seasons of competition between 2000 and 2004. At the end of 2004, with costs mounting and Ford's profits dwindling, the F1 team was seen as an unneeded expense and was sold to Red Bull  and rebranded Red Bull Racing.On 15 December 2015, it was announced that Jaguar would return to motorsport for the third season of Formula E.\nOn 15 June 2018, Jaguar Vector Racing broke the world speed record for an electric battery powered boat. The Jaguar Vector V20E recorded an average speed of 88.61 mph across the two legs of the 1 km course on Coniston Water, England.Notable sports racers:\n\nJaguar C-Type (1951\u20131953)\nJaguar D-Type (1954\u20131957)\nJaguar Lightweight E-Type\nJaguar XJ13\nJaguar XJR Sportscars\nJaguar XJR-9 (1988)\nXJ220 (1988)\nXJR-15 (1990)\n\n\n== Jaguar and the arts ==\nFor some time now, Jaguar has been active in the international arts scene. In particular, the company has collaborated with the artist Stefan Szczesny, implementing major art projects. In 2011, Jaguar presented the exhibition series \"Shadows\", which involved the installation of Szczesny's shadow sculptures in Sankt-Moritz, on Sylt and in Saint-Tropez. In 2012, a large number of sculptures, ceramics and paintings were shown in Frankfurt (and mainly in Frankfurt's Palmengarten).\nAs part of the collaboration with Szczesny, Jaguar has released the \"Jaguar Art Collection\".\n\n\n== See also ==\nList of car manufacturers of the United Kingdom\n\n\n== References ==\n\n\n== External links ==\n\nJaguar marque official website\nJaguar Daimler Heritage Trust website"}, {"id": 8, "title": "List of FIFA World Cup finals", "content": "The FIFA World Cup is an international association football competition contested by the senior men's national teams of the F\u00e9d\u00e9ration Internationale de Football Association (FIFA), the sport's global governing body. The championship has been awarded every four years since 1930, except in 1942 and 1946, when it was not held because of World War II.\nThe World Cup final match is the last of the competition, played by the only two teams remaining in contention, and the result determines which country is declared the world champion. It is a one-off match decided in regulation time. In case of a draw, extra time is used. If scores are then still level, a penalty shoot-out determines the winner, under the rules in force since 1986; prior to that, finals still tied after extra time would have been replayed, though this never proved necessary. The golden goal rule would have applied during extra time in 1998 and 2002, but was not put in practice either.\nThe only exception to this type of format was the 1950 World Cup, which featured a final round-robin group of four teams; the decisive match of that group is often regarded as the de facto final of that tournament, including by FIFA itself.The team that wins the final receives the FIFA World Cup Trophy, and its name is engraved on the bottom side of the trophy. Of 80 different nations that have appeared in the tournament, 13 have made it to the final, and 8 have won. Brazil, the only team that has participated in every World Cup, is also the most successful team in the competition, having won five titles and finished second twice. Italy and Germany have four titles each, with Germany having reached more finals than any other team, eight. Current champion Argentina has three titles, Uruguay and France have two each, while England and Spain have one each. Czechoslovakia, Hungary, Sweden, the Netherlands and Croatia have played in the final without winning. Only teams from Europe (UEFA) and South America (CONMEBOL) have ever competed in the final.\nArgentina defeated France on penalties in the latest final, staged at Qatar's Lusail Stadium in 2022.\n\n\n== List of final matches ==\n\n\n== Results ==\n\n\n== See also ==\nList of FIFA Confederations Cup finals\nList of FIFA Women's World Cup finals\nList of FIFA World Cup final stadiums\nList of UEFA European Championship finals\nList of Copa Am\u00e9rica finals\nList of AFC Asian Cup finals\nList of Africa Cup of Nations finals\nList of CONCACAF Gold Cup finals\nList of FIFA World Cup final goalscorers\n\n\n== Footnotes ==\n\n\n== References ==\nGeneral\n\n\"World Cup 1930-2018\". Rec. Sport. Soccer Statistics Foundation (RSSSF). 9 August 2018.Specific\n\n\n== External links ==\nFIFA World Cup\u2122 at FIFA.com"}, {"id": 9, "title": "Kinetic Energy", "content": "In physics, the kinetic energy of an object is the form of energy that it possesses due to its motion.In classical mechanics, the kinetic energy of a non-rotating object of mass m traveling at a speed v is \n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\textstyle {\\frac {1}{2}}mv^{2}}\n  .It can be shown that the kinetic energy of an object is equal to the work needed to accelerate an object of mass m from rest to its stated velocity. Having gained this energy during its acceleration, the object maintains this kinetic energy unless its speed changes. The same amount of work is done by the object when decelerating from its current speed to a state of rest.The SI unit of kinetic energy is the joule, while the English unit of kinetic energy is the foot-pound.\nIn relativistic mechanics, \n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\textstyle {\\frac {1}{2}}mv^{2}}\n   is a good approximation of kinetic energy only when v is much less than the speed of light.\n\n\n== History and etymology ==\nThe adjective kinetic has its roots in the Greek word \u03ba\u03af\u03bd\u03b7\u03c3\u03b9\u03c2 kinesis, meaning \"motion\". The dichotomy between kinetic energy and potential energy can be traced back to Aristotle's concepts of actuality and potentiality.The principle in classical mechanics that E \u221d mv2 was first developed by Gottfried Leibniz and Johann Bernoulli, who described kinetic energy as the living force, vis viva.:\u200a227\u200a Willem 's Gravesande of the Netherlands provided experimental evidence of this relationship in 1722. By dropping weights from different heights into a block of clay, Willem 's Gravesande determined that their penetration depth was proportional to the square of their impact speed. \u00c9milie du Ch\u00e2telet recognized the implications of the experiment and published an explanation.The terms kinetic energy and work in their present scientific meanings date back to the mid-19th century. Early understandings of these ideas can be attributed to Gaspard-Gustave Coriolis, who in 1829 published the paper titled Du Calcul de l'Effet des Machines outlining the mathematics of kinetic energy. William Thomson, later Lord Kelvin, is given the credit for coining the term \"kinetic energy\" c. 1849\u20131851. Rankine, who had introduced the term \"potential energy\" in 1853, and the phrase \"actual energy\" to complement it, later cites William Thomson and Peter Tait as substituting the word \"kinetic\" for \"actual\".\n\n\n== Overview ==\nEnergy occurs in many forms, including chemical energy, thermal energy, electromagnetic radiation, gravitational energy, electric energy, elastic energy, nuclear energy, and rest energy. These can be categorized in two main classes: potential energy and kinetic energy. Kinetic energy is the movement energy of an object. Kinetic energy can be transferred between objects and transformed into other kinds of energy.Kinetic energy may be best understood by examples that demonstrate how it is transformed to and from other forms of energy. For example, a cyclist uses chemical energy provided by food to accelerate a bicycle to a chosen speed. On a level surface, this speed can be maintained without further work, except to overcome air resistance and friction. The chemical energy has been converted into kinetic energy, the energy of motion, but the process is not completely efficient and produces heat within the cyclist.\nThe kinetic energy in the moving cyclist and the bicycle can be converted to other forms.  For example, the cyclist could encounter a hill just high enough to coast up, so that the bicycle comes to a complete halt at the top.  The kinetic energy has now largely been converted to gravitational potential energy that can be released by freewheeling down the other side of the hill.  Since the bicycle lost some of its energy to friction, it never regains all of its speed without additional pedaling. The energy is not destroyed; it has only been converted to another form by friction. Alternatively, the cyclist could connect a dynamo to one of the wheels and generate some electrical energy on the descent.  The bicycle would be traveling slower at the bottom of the hill than without the generator because some of the energy has been diverted into electrical energy.  Another possibility would be for the cyclist to apply the brakes, in which case the kinetic energy would be dissipated through friction as heat.\nLike any physical quantity that is a function of velocity, the kinetic energy of an object depends on the relationship between the object and the observer's frame of reference. Thus, the kinetic energy of an object is not invariant.\nSpacecraft use chemical energy to launch and gain considerable kinetic energy to reach orbital velocity.  In an entirely circular orbit, this kinetic energy remains constant because there is almost no friction in near-earth space. However, it becomes apparent at re-entry when some of the kinetic energy is converted to heat. If the orbit is elliptical or hyperbolic, then throughout the orbit kinetic and potential energy are exchanged; kinetic energy is greatest and potential energy lowest at closest approach to the earth or other massive body, while potential energy is greatest and kinetic energy the lowest at maximum distance. Disregarding loss or gain however, the sum of the kinetic and potential energy remains constant.\nKinetic energy can be passed from one object to another. In the game of billiards, the player imposes kinetic energy on the cue ball by striking it with the cue stick. If the cue ball collides with another ball, it slows down dramatically, and the ball it hit accelerates as the kinetic energy is passed on to it. Collisions in billiards are effectively elastic collisions, in which kinetic energy is preserved. In inelastic collisions, kinetic energy is dissipated in various forms of energy, such as heat, sound and binding energy (breaking bound structures).\nFlywheels have been developed as a method of energy storage.  This illustrates that kinetic energy is also stored in rotational motion.\nSeveral mathematical descriptions of kinetic energy exist that describe it in the appropriate physical situation. For objects and processes in common human experience, the formula \u00bdmv\u00b2 given by classical mechanics is suitable. However, if the speed of the object is comparable to the speed of light, relativistic effects become significant and the relativistic formula is used. If the object is on the atomic or sub-atomic scale, quantum mechanical effects are significant, and a quantum mechanical model must be employed.\n\n\n== Kinetic energy for non-relativistic velocity ==\nTreatments of kinetic energy depend upon the relative velocity of objects compared to the fixed speed of light. Speeds experienced directly by humans are non-relativisitic; higher speeds require the theory of relativity. \n\n\n=== Kinetic energy of rigid bodies ===\nIn classical mechanics, the kinetic energy of a point object (an object so small that its mass can be assumed to exist at one point), or a non-rotating rigid body depends on the mass of the body as well as its speed. The kinetic energy is equal to 1/2 the product of the mass and the square of the speed. In formula form:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}={\\frac {1}{2}}mv^{2}}\n  where \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is the mass and \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   is the speed (magnitude of the velocity) of the body. In SI units, mass is measured in kilograms, speed in metres per second, and the resulting kinetic energy is in joules.\nFor example, one would calculate the kinetic energy of an 80 kg mass (about 180 lbs) traveling at 18 metres per second (about 40 mph, or 65 km/h) as\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        \u22c5\n        80\n        \n        \n          kg\n        \n        \u22c5\n        \n          \n            (\n            \n              18\n              \n              \n                m/s\n              \n            \n            )\n          \n          \n            2\n          \n        \n        =\n        12\n        ,\n        960\n        \n        \n          J\n        \n        =\n        12.96\n        \n        \n          kJ\n        \n      \n    \n    {\\displaystyle E_{\\text{k}}={\\frac {1}{2}}\\cdot 80\\,{\\text{kg}}\\cdot \\left(18\\,{\\text{m/s}}\\right)^{2}=12,960\\,{\\text{J}}=12.96\\,{\\text{kJ}}}\n  When a person throws a ball, the person does work on it to give it speed as it leaves the hand. The moving ball can then hit something and push it, doing work on what it hits. The kinetic energy of a moving object is equal to the work required to bring it from rest to that speed, or the work the object can do while being brought to rest: net force \u00d7 displacement = kinetic energy, i.e.,\n\n  \n    \n      \n        F\n        s\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle Fs={\\frac {1}{2}}mv^{2}}\n  Since the kinetic energy increases with the square of the speed, an object doubling its speed has four times as much kinetic energy. For example, a car traveling twice as fast as another requires four times as much distance to stop, assuming a constant braking force. As a consequence of this quadrupling, it takes four times the work to double the speed.\nThe kinetic energy of an object is related to its momentum by the equation:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}={\\frac {p^{2}}{2m}}}\n  where:\n\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is momentum\n\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is mass of the bodyFor the translational kinetic energy, that is the kinetic energy associated with rectilinear motion, of a rigid body with constant mass \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , whose center of mass is moving in a straight line with speed \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  , as seen above is equal to\n\n  \n    \n      \n        \n          E\n          \n            t\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{t}}={\\frac {1}{2}}mv^{2}}\n  where:\n\n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is the mass of the body\n\n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   is the speed of the center of mass of the body.The kinetic energy of any entity depends on the reference frame in which it is measured. However, the total energy of an isolated system, i.e. one in which energy can neither enter nor leave, does not change over time in the reference frame in which it is measured. Thus, the chemical energy converted to kinetic energy by a rocket engine is divided differently between the rocket ship and its exhaust stream depending upon the chosen reference frame. This is called the Oberth effect. But the total energy of the system, including kinetic energy, fuel chemical energy, heat, etc., is conserved over time, regardless of the choice of reference frame. Different observers moving with different reference frames would however disagree on the value of this conserved energy.\nThe kinetic energy of such systems depends on the choice of reference frame: the reference frame that gives the minimum value of that energy is the center of momentum frame, i.e. the reference frame in which the total momentum of the system is zero. This minimum kinetic energy contributes to the invariant mass of the system as a whole.\n\n\n==== Derivation ====\n\n\n===== Without vector calculus =====\nThe work W done by a force F on an object over a distance s parallel to F equals \n\n  \n    \n      \n        W\n        =\n        F\n        \u22c5\n        s\n      \n    \n    {\\displaystyle W=F\\cdot s}\n  .Using Newton's Second Law \n\n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  with m the mass and a the acceleration of the object and \n\n  \n    \n      \n        s\n        =\n        \n          \n            \n              a\n              \n                t\n                \n                  2\n                \n              \n            \n            2\n          \n        \n      \n    \n    {\\displaystyle s={\\frac {at^{2}}{2}}}\n  the distance traveled by the accelerated object in time t, we find with \n  \n    \n      \n        v\n        =\n        a\n        t\n      \n    \n    {\\displaystyle v=at}\n   for the velocity v of the object\n\n  \n    \n      \n        W\n        =\n        m\n        a\n        \n          \n            \n              a\n              \n                t\n                \n                  2\n                \n              \n            \n            2\n          \n        \n        =\n        \n          \n            \n              m\n              (\n              a\n              t\n              \n                )\n                \n                  2\n                \n              \n            \n            2\n          \n        \n        =\n        \n          \n            \n              m\n              \n                v\n                \n                  2\n                \n              \n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle W=ma{\\frac {at^{2}}{2}}={\\frac {m(at)^{2}}{2}}={\\frac {mv^{2}}{2}}.}\n  \n\n\n===== With vector calculus =====\nThe work done in accelerating a particle with mass m during the infinitesimal time interval dt is given by the dot product of force F and the infinitesimal displacement dx\n\n  \n    \n      \n        \n          F\n        \n        \u22c5\n        d\n        \n          x\n        \n        =\n        \n          F\n        \n        \u22c5\n        \n          v\n        \n        d\n        t\n        =\n        \n          \n            \n              d\n              \n                p\n              \n            \n            \n              d\n              t\n            \n          \n        \n        \u22c5\n        \n          v\n        \n        d\n        t\n        =\n        \n          v\n        \n        \u22c5\n        d\n        \n          p\n        \n        =\n        \n          v\n        \n        \u22c5\n        d\n        (\n        m\n        \n          v\n        \n        )\n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} \\cdot d\\mathbf {x} =\\mathbf {F} \\cdot \\mathbf {v} dt={\\frac {d\\mathbf {p} }{dt}}\\cdot \\mathbf {v} dt=\\mathbf {v} \\cdot d\\mathbf {p} =\\mathbf {v} \\cdot d(m\\mathbf {v} )\\,,}\n  where we have assumed the relationship p = m v and the validity of Newton's Second Law. (However, also see the special relativistic derivation below.)\nApplying the product rule we see that:\n\n  \n    \n      \n        d\n        (\n        \n          v\n        \n        \u22c5\n        \n          v\n        \n        )\n        =\n        (\n        d\n        \n          v\n        \n        )\n        \u22c5\n        \n          v\n        \n        +\n        \n          v\n        \n        \u22c5\n        (\n        d\n        \n          v\n        \n        )\n        =\n        2\n        (\n        \n          v\n        \n        \u22c5\n        d\n        \n          v\n        \n        )\n        .\n      \n    \n    {\\displaystyle d(\\mathbf {v} \\cdot \\mathbf {v} )=(d\\mathbf {v} )\\cdot \\mathbf {v} +\\mathbf {v} \\cdot (d\\mathbf {v} )=2(\\mathbf {v} \\cdot d\\mathbf {v} ).}\n  Therefore, (assuming constant mass so that dm = 0), we have,\n\n  \n    \n      \n        \n          v\n        \n        \u22c5\n        d\n        (\n        m\n        \n          v\n        \n        )\n        =\n        \n          \n            m\n            2\n          \n        \n        d\n        (\n        \n          v\n        \n        \u22c5\n        \n          v\n        \n        )\n        =\n        \n          \n            m\n            2\n          \n        \n        d\n        \n          v\n          \n            2\n          \n        \n        =\n        d\n        \n          (\n          \n            \n              \n                m\n                \n                  v\n                  \n                    2\n                  \n                \n              \n              2\n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} \\cdot d(m\\mathbf {v} )={\\frac {m}{2}}d(\\mathbf {v} \\cdot \\mathbf {v} )={\\frac {m}{2}}dv^{2}=d\\left({\\frac {mv^{2}}{2}}\\right).}\n  Since this is a total differential (that is, it only depends on the final state, not how the particle got there), we can integrate it and call the result kinetic energy:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \u222b\n          \n            \n              v\n              \n                1\n              \n            \n          \n          \n            \n              v\n              \n                2\n              \n            \n          \n        \n        \n          p\n        \n        d\n        \n          v\n        \n        =\n        \n          \u222b\n          \n            \n              v\n              \n                1\n              \n            \n          \n          \n            \n              v\n              \n                2\n              \n            \n          \n        \n        m\n        \n          v\n        \n        d\n        \n          v\n        \n        =\n        \n          \n            \n              m\n              \n                v\n                \n                  2\n                \n              \n            \n            2\n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            \n              v\n              \n                1\n              \n            \n          \n          \n            \n              v\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        (\n        \n          v\n          \n            2\n          \n          \n            2\n          \n        \n        \u2212\n        \n          v\n          \n            1\n          \n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}=\\int _{v_{1}}^{v_{2}}\\mathbf {p} d\\mathbf {v} =\\int _{v_{1}}^{v_{2}}m\\mathbf {v} d\\mathbf {v} ={mv^{2} \\over 2}{\\bigg \\vert }_{v_{1}}^{v_{2}}={1 \\over 2}m(v_{2}^{2}-v_{1}^{2}).}\n  This equation states that the kinetic energy (Ek) is equal to the integral of the dot product of the momentum (p) of a body and the infinitesimal change of the velocity (v) of the body. It is assumed that the body starts with no kinetic energy when it is at rest (motionless).\n\n\n=== Rotating bodies ===\nIf a rigid body Q is rotating about any line through the center of mass then it has rotational kinetic energy (\n  \n    \n      \n        \n          E\n          \n            r\n          \n        \n        \n      \n    \n    {\\displaystyle E_{\\text{r}}\\,}\n  ) which is simply the sum of the kinetic energies of its moving parts, and is thus given by:\n\n  \n    \n      \n        \n          E\n          \n            r\n          \n        \n        =\n        \n          \u222b\n          \n            Q\n          \n        \n        \n          \n            \n              \n                v\n                \n                  2\n                \n              \n              d\n              m\n            \n            2\n          \n        \n        =\n        \n          \u222b\n          \n            Q\n          \n        \n        \n          \n            \n              (\n              r\n              \u03c9\n              \n                )\n                \n                  2\n                \n              \n              d\n              m\n            \n            2\n          \n        \n        =\n        \n          \n            \n              \u03c9\n              \n                2\n              \n            \n            2\n          \n        \n        \n          \u222b\n          \n            Q\n          \n        \n        \n          \n            r\n            \n              2\n            \n          \n        \n        d\n        m\n        =\n        \n          \n            \n              \u03c9\n              \n                2\n              \n            \n            2\n          \n        \n        I\n        =\n        \n          \n            1\n            2\n          \n        \n        I\n        \n          \u03c9\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{r}}=\\int _{Q}{\\frac {v^{2}dm}{2}}=\\int _{Q}{\\frac {(r\\omega )^{2}dm}{2}}={\\frac {\\omega ^{2}}{2}}\\int _{Q}{r^{2}}dm={\\frac {\\omega ^{2}}{2}}I={\\frac {1}{2}}I\\omega ^{2}}\n  where:\n\n\u03c9 is the body's angular velocity\nr is the distance of any mass dm from that line\n\n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   is the body's moment of inertia, equal to \n  \n    \n      \n        \n          \u222b\n          \n            Q\n          \n        \n        \n          \n            r\n            \n              2\n            \n          \n        \n        d\n        m\n      \n    \n    {\\textstyle \\int _{Q}{r^{2}}dm}\n  .(In this equation the moment of inertia must be taken about an axis through the center of mass and the rotation measured by \u03c9 must be around that axis; more general equations exist for systems where the object is subject to wobble due to its eccentric shape).\n\n\n=== Kinetic energy of systems ===\nA system of bodies may have internal kinetic energy due to the relative motion of the bodies in the system. For example, in the Solar System the planets and planetoids are orbiting the Sun. In a tank of gas, the molecules are moving in all directions. The kinetic energy of the system is the sum of the kinetic energies of the bodies it contains.\nA macroscopic body that is stationary (i.e. a reference frame has been chosen to correspond to the body's center of momentum) may have various kinds of internal energy at the molecular or atomic level, which may be regarded as kinetic energy, due to molecular translation, rotation, and vibration, electron translation and spin, and nuclear spin. These all contribute to the body's mass, as provided by the special theory of relativity. When discussing movements of a macroscopic body, the kinetic energy referred to is usually that of the macroscopic movement only. However, all internal energies of all types contribute to a body's mass, inertia, and total energy.\n\n\n=== Fluid dynamics ===\nIn fluid dynamics, the kinetic energy per unit volume at each point in an incompressible fluid flow field is called the dynamic pressure at that point.\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}={\\frac {1}{2}}mv^{2}}\n  Dividing by V, the unit of volume:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      E\n                      \n                        k\n                      \n                    \n                    V\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    2\n                  \n                \n                \n                  \n                    m\n                    V\n                  \n                \n                \n                  v\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                q\n              \n              \n                \n                =\n                \n                  \n                    1\n                    2\n                  \n                \n                \u03c1\n                \n                  v\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\frac {E_{\\text{k}}}{V}}&={\\frac {1}{2}}{\\frac {m}{V}}v^{2}\\\\q&={\\frac {1}{2}}\\rho v^{2}\\end{aligned}}}\n  where \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   is the dynamic pressure, and \u03c1 is the density of the incompressible fluid.\n\n\n=== Frame of reference ===\nThe speed, and thus the kinetic energy of a single object is frame-dependent (relative): it can take any non-negative value, by choosing a suitable inertial frame of reference. For example, a bullet passing an observer has kinetic energy in the reference frame of this observer. The same bullet is stationary to an observer moving with the same velocity as the bullet, and so has zero kinetic energy. By contrast, the total kinetic energy of a system of objects cannot be reduced to zero by a suitable choice of the inertial reference frame, unless all the objects have the same velocity. In any other case, the total kinetic energy has a non-zero minimum, as no inertial reference frame can be chosen in which all the objects are stationary. This minimum kinetic energy contributes to the system's invariant mass, which is independent of the reference frame.\nThe total kinetic energy of a system depends on the inertial frame of reference: it is the sum of the total kinetic energy in a center of momentum frame and the kinetic energy the total mass would have if it were concentrated in the center of mass.\nThis may be simply shown: let \n  \n    \n      \n        \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle \\textstyle \\mathbf {V} }\n   be the relative velocity of the center of mass frame i in the frame k. Since\n\n  \n    \n      \n        \n          v\n          \n            2\n          \n        \n        =\n        \n          \n            (\n            \n              \n                v\n                \n                  i\n                \n              \n              +\n              V\n            \n            )\n          \n          \n            2\n          \n        \n        =\n        \n          (\n          \n            \n              \n                v\n              \n              \n                i\n              \n            \n            +\n            \n              V\n            \n          \n          )\n        \n        \u22c5\n        \n          (\n          \n            \n              \n                v\n              \n              \n                i\n              \n            \n            +\n            \n              V\n            \n          \n          )\n        \n        =\n        \n          \n            v\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          \n            v\n          \n          \n            i\n          \n        \n        +\n        2\n        \n          \n            v\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          V\n        \n        +\n        \n          V\n        \n        \u22c5\n        \n          V\n        \n        =\n        \n          v\n          \n            i\n          \n          \n            2\n          \n        \n        +\n        2\n        \n          \n            v\n          \n          \n            i\n          \n        \n        \u22c5\n        \n          V\n        \n        +\n        \n          V\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle v^{2}=\\left(v_{i}+V\\right)^{2}=\\left(\\mathbf {v} _{i}+\\mathbf {V} \\right)\\cdot \\left(\\mathbf {v} _{i}+\\mathbf {V} \\right)=\\mathbf {v} _{i}\\cdot \\mathbf {v} _{i}+2\\mathbf {v} _{i}\\cdot \\mathbf {V} +\\mathbf {V} \\cdot \\mathbf {V} =v_{i}^{2}+2\\mathbf {v} _{i}\\cdot \\mathbf {V} +V^{2},}\n  Then,\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \u222b\n        \n          \n            \n              v\n              \n                2\n              \n            \n            2\n          \n        \n        d\n        m\n        =\n        \u222b\n        \n          \n            \n              v\n              \n                i\n              \n              \n                2\n              \n            \n            2\n          \n        \n        d\n        m\n        +\n        \n          V\n        \n        \u22c5\n        \u222b\n        \n          \n            v\n          \n          \n            i\n          \n        \n        d\n        m\n        +\n        \n          \n            \n              V\n              \n                2\n              \n            \n            2\n          \n        \n        \u222b\n        d\n        m\n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}=\\int {\\frac {v^{2}}{2}}dm=\\int {\\frac {v_{i}^{2}}{2}}dm+\\mathbf {V} \\cdot \\int \\mathbf {v} _{i}dm+{\\frac {V^{2}}{2}}\\int dm.}\n  However, let \n  \n    \n      \n        \u222b\n        \n          \n            \n              v\n              \n                i\n              \n              \n                2\n              \n            \n            2\n          \n        \n        d\n        m\n        =\n        \n          E\n          \n            i\n          \n        \n      \n    \n    {\\textstyle \\int {\\frac {v_{i}^{2}}{2}}dm=E_{i}}\n   the kinetic energy in the center of mass frame, \n  \n    \n      \n        \u222b\n        \n          \n            v\n          \n          \n            i\n          \n        \n        d\n        m\n      \n    \n    {\\textstyle \\int \\mathbf {v} _{i}dm}\n   would be simply the total momentum that is by definition zero in the center of mass frame, and let the total mass: \n  \n    \n      \n        \u222b\n        d\n        m\n        =\n        M\n      \n    \n    {\\textstyle \\int dm=M}\n  . Substituting, we get:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          E\n          \n            i\n          \n        \n        +\n        \n          \n            \n              M\n              \n                V\n                \n                  2\n                \n              \n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}=E_{i}+{\\frac {MV^{2}}{2}}.}\n  Thus the kinetic energy of a system is lowest to center of momentum reference frames, i.e., frames of reference in which the center of mass is stationary (either the center of mass frame or any other center of momentum frame). In any different frame of reference, there is additional kinetic energy corresponding to the total mass moving at the speed of the center of mass. The kinetic energy of the system in the center of momentum frame is a quantity that is invariant (all observers see it to be the same).\n\n\n=== Rotation in systems ===\nIt sometimes is convenient to split the total kinetic energy of a body into the sum of the body's center-of-mass translational kinetic energy and the energy of rotation around the center of mass (rotational energy):\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          E\n          \n            t\n          \n        \n        +\n        \n          E\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}=E_{\\text{t}}+E_{\\text{r}}}\n  where:\n\nEk is the total kinetic energy\nEt is the translational kinetic energy\nEr is the rotational energy or angular kinetic energy in the rest frameThus the kinetic energy of a tennis ball in flight is the kinetic energy due to its rotation, plus the kinetic energy due to its translation.\n\n\n== Relativistic kinetic energy ==\n\nIf a body's speed is a significant fraction of the speed of light, it is necessary to use relativistic mechanics to calculate its kinetic energy. In relativity, the total energy is given by the energy-momentum relation:\n\nHere we use the relativistic expression for linear momentum: \n  \n    \n      \n        p\n        =\n        m\n        \u03b3\n        v\n      \n    \n    {\\displaystyle p=m\\gamma v}\n  , where \n  \n    \n      \n        \u03b3\n        =\n        1\n        \n          /\n        \n        \n          \n            1\n            \u2212\n            \n              v\n              \n                2\n              \n            \n            \n              /\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\textstyle \\gamma =1/{\\sqrt {1-v^{2}/c^{2}}}}\n  .\nwith \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   being an object's (rest) mass, \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   speed, and c the speed of light in vacuum.\nThen kinetic energy is the  total relativistic energy minus the rest energy:\n\nAt low speeds, the square root can be expanded and the rest energy drops out, giving the Newtonian kinetic energy. \n\n\n=== Derivation ===\nStart with the expression for linear momentum \n  \n    \n      \n        \n          p\n        \n        =\n        m\n        \u03b3\n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {p} =m\\gamma \\mathbf {v} }\n  , where \n  \n    \n      \n        \u03b3\n        =\n        1\n        \n          /\n        \n        \n          \n            1\n            \u2212\n            \n              v\n              \n                2\n              \n            \n            \n              /\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\textstyle \\gamma =1/{\\sqrt {1-v^{2}/c^{2}}}}\n  .\nIntegrating by parts yields\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \u222b\n        \n          v\n        \n        \u22c5\n        d\n        \n          p\n        \n        =\n        \u222b\n        \n          v\n        \n        \u22c5\n        d\n        (\n        m\n        \u03b3\n        \n          v\n        \n        )\n        =\n        m\n        \u03b3\n        \n          v\n        \n        \u22c5\n        \n          v\n        \n        \u2212\n        \u222b\n        m\n        \u03b3\n        \n          v\n        \n        \u22c5\n        d\n        \n          v\n        \n        =\n        m\n        \u03b3\n        \n          v\n          \n            2\n          \n        \n        \u2212\n        \n          \n            m\n            2\n          \n        \n        \u222b\n        \u03b3\n        d\n        \n          (\n          \n            v\n            \n              2\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle E_{\\text{k}}=\\int \\mathbf {v} \\cdot d\\mathbf {p} =\\int \\mathbf {v} \\cdot d(m\\gamma \\mathbf {v} )=m\\gamma \\mathbf {v} \\cdot \\mathbf {v} -\\int m\\gamma \\mathbf {v} \\cdot d\\mathbf {v} =m\\gamma v^{2}-{\\frac {m}{2}}\\int \\gamma d\\left(v^{2}\\right)}\n  Since \n  \n    \n      \n        \u03b3\n        =\n        \n          \n            (\n            \n              1\n              \u2212\n              \n                v\n                \n                  2\n                \n              \n              \n                /\n              \n              \n                c\n                \n                  2\n                \n              \n            \n            )\n          \n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\gamma =\\left(1-v^{2}/c^{2}\\right)^{-{\\frac {1}{2}}}}\n  ,\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  E\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                m\n                \u03b3\n                \n                  v\n                  \n                    2\n                  \n                \n                \u2212\n                \n                  \n                    \n                      \u2212\n                      m\n                      \n                        c\n                        \n                          2\n                        \n                      \n                    \n                    2\n                  \n                \n                \u222b\n                \u03b3\n                d\n                \n                  (\n                  \n                    1\n                    \u2212\n                    \n                      \n                        \n                          v\n                          \n                            2\n                          \n                        \n                        \n                          c\n                          \n                            2\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \u03b3\n                \n                  v\n                  \n                    2\n                  \n                \n                +\n                m\n                \n                  c\n                  \n                    2\n                  \n                \n                \n                  \n                    (\n                    \n                      1\n                      \u2212\n                      \n                        \n                          \n                            v\n                            \n                              2\n                            \n                          \n                          \n                            c\n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    )\n                  \n                  \n                    \n                      1\n                      2\n                    \n                  \n                \n                \u2212\n                \n                  E\n                  \n                    0\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}E_{\\text{k}}&=m\\gamma v^{2}-{\\frac {-mc^{2}}{2}}\\int \\gamma d\\left(1-{\\frac {v^{2}}{c^{2}}}\\right)\\\\&=m\\gamma v^{2}+mc^{2}\\left(1-{\\frac {v^{2}}{c^{2}}}\\right)^{\\frac {1}{2}}-E_{0}\\end{aligned}}}\n  \n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle E_{0}}\n   is a constant of integration for the indefinite integral.\nSimplifying the expression we obtain\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  E\n                  \n                    k\n                  \n                \n              \n              \n                \n                =\n                m\n                \u03b3\n                \n                  (\n                  \n                    \n                      v\n                      \n                        2\n                      \n                    \n                    +\n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \n                      (\n                      \n                        1\n                        \u2212\n                        \n                          \n                            \n                              v\n                              \n                                2\n                              \n                            \n                            \n                              c\n                              \n                                2\n                              \n                            \n                          \n                        \n                      \n                      )\n                    \n                  \n                  )\n                \n                \u2212\n                \n                  E\n                  \n                    0\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \u03b3\n                \n                  (\n                  \n                    \n                      v\n                      \n                        2\n                      \n                    \n                    +\n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \u2212\n                    \n                      v\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n                \u2212\n                \n                  E\n                  \n                    0\n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                m\n                \u03b3\n                \n                  c\n                  \n                    2\n                  \n                \n                \u2212\n                \n                  E\n                  \n                    0\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}E_{\\text{k}}&=m\\gamma \\left(v^{2}+c^{2}\\left(1-{\\frac {v^{2}}{c^{2}}}\\right)\\right)-E_{0}\\\\&=m\\gamma \\left(v^{2}+c^{2}-v^{2}\\right)-E_{0}\\\\&=m\\gamma c^{2}-E_{0}\\end{aligned}}}\n  \n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle E_{0}}\n   is found by observing that when \n  \n    \n      \n        \n          v\n        \n        =\n        0\n        ,\n         \n        \u03b3\n        =\n        1\n      \n    \n    {\\displaystyle \\mathbf {v} =0,\\ \\gamma =1}\n   and \n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle E_{\\text{k}}=0}\n  , giving\n\n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n        =\n        m\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{0}=mc^{2}}\n  resulting in the formula\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        m\n        \u03b3\n        \n          c\n          \n            2\n          \n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        =\n        \n          \n            \n              m\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              1\n              \u2212\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        =\n        (\n        \u03b3\n        \u2212\n        1\n        )\n        m\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}=m\\gamma c^{2}-mc^{2}={\\frac {mc^{2}}{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}-mc^{2}=(\\gamma -1)mc^{2}}\n  This formula shows that the work expended accelerating an object from rest approaches infinity as the velocity approaches the speed of light. Thus it is impossible to accelerate an object across this boundary.\nThe mathematical by-product of this calculation is the mass\u2013energy equivalence formula\u2014the body at rest must have energy content\n\n  \n    \n      \n        \n          E\n          \n            rest\n          \n        \n        =\n        \n          E\n          \n            0\n          \n        \n        =\n        m\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{rest}}=E_{0}=mc^{2}}\n  At a low speed (v \u226a c), the relativistic kinetic energy is approximated well by the classical kinetic energy. This is done by binomial approximation or by taking the first two terms of the Taylor expansion for the reciprocal square root:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        \u2248\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          (\n          \n            1\n            +\n            \n              \n                1\n                2\n              \n            \n            \n              \n                \n                  v\n                  \n                    2\n                  \n                \n                \n                  c\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}\\approx mc^{2}\\left(1+{\\frac {1}{2}}{\\frac {v^{2}}{c^{2}}}\\right)-mc^{2}={\\frac {1}{2}}mv^{2}}\n  So, the total energy \n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle E_{k}}\n   can be partitioned into the rest mass energy plus the non-relativistic kinetic energy at low speeds.\nWhen objects move at a speed much slower than light (e.g. in everyday phenomena on Earth), the first two terms of the series predominate. The next term in the Taylor series approximation\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        \u2248\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          (\n          \n            1\n            +\n            \n              \n                1\n                2\n              \n            \n            \n              \n                \n                  v\n                  \n                    2\n                  \n                \n                \n                  c\n                  \n                    2\n                  \n                \n              \n            \n            +\n            \n              \n                3\n                8\n              \n            \n            \n              \n                \n                  v\n                  \n                    4\n                  \n                \n                \n                  c\n                  \n                    4\n                  \n                \n              \n            \n          \n          )\n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n        +\n        \n          \n            3\n            8\n          \n        \n        m\n        \n          \n            \n              v\n              \n                4\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}\\approx mc^{2}\\left(1+{\\frac {1}{2}}{\\frac {v^{2}}{c^{2}}}+{\\frac {3}{8}}{\\frac {v^{4}}{c^{4}}}\\right)-mc^{2}={\\frac {1}{2}}mv^{2}+{\\frac {3}{8}}m{\\frac {v^{4}}{c^{2}}}}\n  is small for low speeds. For example, for a speed of 10 km/s (22,000 mph) the correction to the non-relativistic kinetic energy is 0.0417 J/kg (on a non-relativistic kinetic energy of 50 MJ/kg) and for a speed of 100 km/s it is 417 J/kg (on a non-relativistic kinetic energy of 5 GJ/kg).\nThe relativistic relation between kinetic energy and momentum is given by\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              c\n              \n                2\n              \n            \n            +\n            \n              m\n              \n                2\n              \n            \n            \n              c\n              \n                4\n              \n            \n          \n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{k}}={\\sqrt {p^{2}c^{2}+m^{2}c^{4}}}-mc^{2}}\n  This can also be expanded as a Taylor series, the first term of which is the simple expression from Newtonian mechanics:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        \u2248\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \u2212\n        \n          \n            \n              p\n              \n                4\n              \n            \n            \n              8\n              \n                m\n                \n                  3\n                \n              \n              \n                c\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}\\approx {\\frac {p^{2}}{2m}}-{\\frac {p^{4}}{8m^{3}c^{2}}}.}\n  This suggests that the formulae for energy and momentum are not special and axiomatic, but concepts emerging from the equivalence of mass and energy and the principles of relativity.\n\n\n=== General relativity ===\n\nUsing the convention that\n\n  \n    \n      \n        \n          g\n          \n            \u03b1\n            \u03b2\n          \n        \n        \n        \n          u\n          \n            \u03b1\n          \n        \n        \n        \n          u\n          \n            \u03b2\n          \n        \n        \n        =\n        \n        \u2212\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle g_{\\alpha \\beta }\\,u^{\\alpha }\\,u^{\\beta }\\,=\\,-c^{2}}\n  where the four-velocity of a particle is\n\n  \n    \n      \n        \n          u\n          \n            \u03b1\n          \n        \n        \n        =\n        \n        \n          \n            \n              d\n              \n                x\n                \n                  \u03b1\n                \n              \n            \n            \n              d\n              \u03c4\n            \n          \n        \n      \n    \n    {\\displaystyle u^{\\alpha }\\,=\\,{\\frac {dx^{\\alpha }}{d\\tau }}}\n  and \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n   is the proper time of the particle, there is also an expression for the kinetic energy of the particle in general relativity.\nIf the particle has momentum\n\n  \n    \n      \n        \n          p\n          \n            \u03b2\n          \n        \n        \n        =\n        \n        m\n        \n        \n          g\n          \n            \u03b2\n            \u03b1\n          \n        \n        \n        \n          u\n          \n            \u03b1\n          \n        \n      \n    \n    {\\displaystyle p_{\\beta }\\,=\\,m\\,g_{\\beta \\alpha }\\,u^{\\alpha }}\n  as it passes by an observer with four-velocity uobs, then the expression for total energy of the particle as observed (measured in a local inertial frame) is\n\n  \n    \n      \n        E\n        \n        =\n        \n        \u2212\n        \n        \n          p\n          \n            \u03b2\n          \n        \n        \n        \n          u\n          \n            obs\n          \n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle E\\,=\\,-\\,p_{\\beta }\\,u_{\\text{obs}}^{\\beta }}\n  and the kinetic energy can be expressed as the total energy minus the rest energy:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        \n        =\n        \n        \u2212\n        \n        \n          p\n          \n            \u03b2\n          \n        \n        \n        \n          u\n          \n            obs\n          \n          \n            \u03b2\n          \n        \n        \n        \u2212\n        \n        m\n        \n        \n          c\n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle E_{k}\\,=\\,-\\,p_{\\beta }\\,u_{\\text{obs}}^{\\beta }\\,-\\,m\\,c^{2}\\,.}\n  Consider the case of a metric that is diagonal and spatially isotropic (gtt, gss, gss, gss). Since\n\n  \n    \n      \n        \n          u\n          \n            \u03b1\n          \n        \n        =\n        \n          \n            \n              d\n              \n                x\n                \n                  \u03b1\n                \n              \n            \n            \n              d\n              t\n            \n          \n        \n        \n          \n            \n              d\n              t\n            \n            \n              d\n              \u03c4\n            \n          \n        \n        =\n        \n          v\n          \n            \u03b1\n          \n        \n        \n          u\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle u^{\\alpha }={\\frac {dx^{\\alpha }}{dt}}{\\frac {dt}{d\\tau }}=v^{\\alpha }u^{t}}\n  where v\u03b1 is the ordinary velocity measured w.r.t. the coordinate system, we get\n\n  \n    \n      \n        \u2212\n        \n          c\n          \n            2\n          \n        \n        =\n        \n          g\n          \n            \u03b1\n            \u03b2\n          \n        \n        \n          u\n          \n            \u03b1\n          \n        \n        \n          u\n          \n            \u03b2\n          \n        \n        =\n        \n          g\n          \n            t\n            t\n          \n        \n        \n          \n            (\n            \n              u\n              \n                t\n              \n            \n            )\n          \n          \n            2\n          \n        \n        +\n        \n          g\n          \n            s\n            s\n          \n        \n        \n          v\n          \n            2\n          \n        \n        \n          \n            (\n            \n              u\n              \n                t\n              \n            \n            )\n          \n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle -c^{2}=g_{\\alpha \\beta }u^{\\alpha }u^{\\beta }=g_{tt}\\left(u^{t}\\right)^{2}+g_{ss}v^{2}\\left(u^{t}\\right)^{2}\\,.}\n  Solving for ut gives\n\n  \n    \n      \n        \n          u\n          \n            t\n          \n        \n        =\n        c\n        \n          \n            \n              \n                \u2212\n                1\n              \n              \n                \n                  g\n                  \n                    t\n                    t\n                  \n                \n                +\n                \n                  g\n                  \n                    s\n                    s\n                  \n                \n                \n                  v\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle u^{t}=c{\\sqrt {\\frac {-1}{g_{tt}+g_{ss}v^{2}}}}\\,.}\n  Thus for a stationary observer (v = 0)\n\n  \n    \n      \n        \n          u\n          \n            obs\n          \n          \n            t\n          \n        \n        =\n        c\n        \n          \n            \n              \n                \u2212\n                1\n              \n              \n                g\n                \n                  t\n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle u_{\\text{obs}}^{t}=c{\\sqrt {\\frac {-1}{g_{tt}}}}}\n  and thus the kinetic energy takes the form\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \u2212\n        m\n        \n          g\n          \n            t\n            t\n          \n        \n        \n          u\n          \n            t\n          \n        \n        \n          u\n          \n            obs\n          \n          \n            t\n          \n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        =\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          \n            \n              \n                g\n                \n                  t\n                  t\n                \n              \n              \n                \n                  g\n                  \n                    t\n                    t\n                  \n                \n                +\n                \n                  g\n                  \n                    s\n                    s\n                  \n                \n                \n                  v\n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        \u2212\n        m\n        \n          c\n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}=-mg_{tt}u^{t}u_{\\text{obs}}^{t}-mc^{2}=mc^{2}{\\sqrt {\\frac {g_{tt}}{g_{tt}+g_{ss}v^{2}}}}-mc^{2}\\,.}\n  Factoring out the rest energy gives:\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        m\n        \n          c\n          \n            2\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    g\n                    \n                      t\n                      t\n                    \n                  \n                  \n                    \n                      g\n                      \n                        t\n                        t\n                      \n                    \n                    +\n                    \n                      g\n                      \n                        s\n                        s\n                      \n                    \n                    \n                      v\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n            \u2212\n            1\n          \n          )\n        \n        \n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}=mc^{2}\\left({\\sqrt {\\frac {g_{tt}}{g_{tt}+g_{ss}v^{2}}}}-1\\right)\\,.}\n  This expression reduces to the special relativistic case for the flat-space metric where\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  g\n                  \n                    t\n                    t\n                  \n                \n              \n              \n                \n                =\n                \u2212\n                \n                  c\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  g\n                  \n                    s\n                    s\n                  \n                \n              \n              \n                \n                =\n                1\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}g_{tt}&=-c^{2}\\\\g_{ss}&=1\\,.\\end{aligned}}}\n  In the Newtonian approximation to general relativity\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  g\n                  \n                    t\n                    t\n                  \n                \n              \n              \n                \n                =\n                \u2212\n                \n                  (\n                  \n                    \n                      c\n                      \n                        2\n                      \n                    \n                    +\n                    2\n                    \u03a6\n                  \n                  )\n                \n              \n            \n            \n              \n                \n                  g\n                  \n                    s\n                    s\n                  \n                \n              \n              \n                \n                =\n                1\n                \u2212\n                \n                  \n                    \n                      2\n                      \u03a6\n                    \n                    \n                      c\n                      \n                        2\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}g_{tt}&=-\\left(c^{2}+2\\Phi \\right)\\\\g_{ss}&=1-{\\frac {2\\Phi }{c^{2}}}\\end{aligned}}}\n  where \u03a6 is the Newtonian gravitational potential. This means clocks run slower and measuring rods are shorter near massive bodies.\n\n\n== Kinetic energy in quantum mechanics ==\n\nIn quantum mechanics, observables like kinetic energy are represented as operators. For one particle of mass m, the kinetic energy operator appears as a term in the Hamiltonian and is defined in terms of the more fundamental momentum operator \n  \n    \n      \n        \n          \n            \n              p\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}}\n  . The kinetic energy operator in the non-relativistic case can be written as\n\n  \n    \n      \n        \n          \n            \n              T\n              ^\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  \n                    p\n                    ^\n                  \n                \n              \n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {T}}={\\frac {{\\hat {p}}^{2}}{2m}}.}\n  Notice that this can be obtained by replacing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   by \n  \n    \n      \n        \n          \n            \n              p\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}}\n   in the classical expression for kinetic energy in terms of momentum,\n\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            \n              p\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E_{\\text{k}}={\\frac {p^{2}}{2m}}.}\n  In the Schr\u00f6dinger picture, \n  \n    \n      \n        \n          \n            \n              p\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}}\n   takes the form \n  \n    \n      \n        \u2212\n        i\n        \u210f\n        \u2207\n      \n    \n    {\\displaystyle -i\\hbar \\nabla }\n   where the derivative is taken with respect to position coordinates and hence\n\n  \n    \n      \n        \n          \n            \n              T\n              ^\n            \n          \n        \n        =\n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \u2207\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {T}}=-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}.}\n  The expectation value of the electron kinetic energy, \n  \n    \n      \n        \n          \u27e8\n          \n            \n              \n                T\n                ^\n              \n            \n          \n          \u27e9\n        \n      \n    \n    {\\displaystyle \\left\\langle {\\hat {T}}\\right\\rangle }\n  , for a system of N electrons described by the wavefunction \n  \n    \n      \n        |\n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle \\vert \\psi \\rangle }\n   is a sum of 1-electron operator expectation values:\n\n  \n    \n      \n        \n          \u27e8\n          \n            \n              \n                T\n                ^\n              \n            \n          \n          \u27e9\n        \n        =\n        \n          \u27e8\n          \n            \u03c8\n            \n              |\n              \n                \n                  \u2211\n                  \n                    i\n                    =\n                    1\n                  \n                  \n                    N\n                  \n                \n                \n                  \n                    \n                      \u2212\n                      \n                        \u210f\n                        \n                          2\n                        \n                      \n                    \n                    \n                      2\n                      \n                        m\n                        \n                          e\n                        \n                      \n                    \n                  \n                \n                \n                  \u2207\n                  \n                    i\n                  \n                  \n                    2\n                  \n                \n              \n              |\n            \n            \u03c8\n          \n          \u27e9\n        \n        =\n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              \n                m\n                \n                  e\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \u27e8\n          \n            \u03c8\n            \n              |\n              \n                \u2207\n                \n                  i\n                \n                \n                  2\n                \n              \n              |\n            \n            \u03c8\n          \n          \u27e9\n        \n      \n    \n    {\\displaystyle \\left\\langle {\\hat {T}}\\right\\rangle =\\left\\langle \\psi \\left\\vert \\sum _{i=1}^{N}{\\frac {-\\hbar ^{2}}{2m_{\\text{e}}}}\\nabla _{i}^{2}\\right\\vert \\psi \\right\\rangle =-{\\frac {\\hbar ^{2}}{2m_{\\text{e}}}}\\sum _{i=1}^{N}\\left\\langle \\psi \\left\\vert \\nabla _{i}^{2}\\right\\vert \\psi \\right\\rangle }\n  where \n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle m_{\\text{e}}}\n   is the mass of the electron and \n  \n    \n      \n        \n          \u2207\n          \n            i\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\nabla _{i}^{2}}\n   is the Laplacian operator acting upon the coordinates of the ith electron and the summation runs over all electrons.\nThe density functional formalism of quantum mechanics requires knowledge of the electron density only, i.e., it formally does not require knowledge of the wavefunction.  Given an electron density \n  \n    \n      \n        \u03c1\n        (\n        \n          r\n        \n        )\n      \n    \n    {\\displaystyle \\rho (\\mathbf {r} )}\n  , the exact N-electron kinetic energy functional is unknown; however, for the specific case of a 1-electron system, the kinetic energy can be written as\n\n  \n    \n      \n        T\n        [\n        \u03c1\n        ]\n        =\n        \n          \n            1\n            8\n          \n        \n        \u222b\n        \n          \n            \n              \u2207\n              \u03c1\n              (\n              \n                r\n              \n              )\n              \u22c5\n              \u2207\n              \u03c1\n              (\n              \n                r\n              \n              )\n            \n            \n              \u03c1\n              (\n              \n                r\n              \n              )\n            \n          \n        \n        \n          d\n          \n            3\n          \n        \n        r\n      \n    \n    {\\displaystyle T[\\rho ]={\\frac {1}{8}}\\int {\\frac {\\nabla \\rho (\\mathbf {r} )\\cdot \\nabla \\rho (\\mathbf {r} )}{\\rho (\\mathbf {r} )}}d^{3}r}\n  where \n  \n    \n      \n        T\n        [\n        \u03c1\n        ]\n      \n    \n    {\\displaystyle T[\\rho ]}\n   is known as the von Weizs\u00e4cker kinetic energy functional.\n\n\n== See also ==\nEscape velocity\nFoot-pound\nJoule\nKinetic energy penetrator\nKinetic energy per unit mass of projectiles\nKinetic projectile\nParallel axis theorem\nPotential energy\nRecoil\n\n\n== Notes ==\n\n\n== References ==\nPhysics Classroom (2000). \"Kinetic Energy\". Retrieved 2015-07-19.\nSchool of Mathematics and Statistics, University of St Andrews (2000). \"Biography of Gaspard-Gustave de Coriolis (1792-1843)\". Retrieved 2006-03-03.\nSerway, Raymond A.; Jewett, John W. (2004). Physics for Scientists and Engineers (6th ed.). Brooks/Cole. ISBN 0-534-40842-7.\nTipler, Paul (2004). Physics for Scientists and Engineers: Mechanics, Oscillations and Waves, Thermodynamics (5th ed.). W. H. Freeman. ISBN 0-7167-0809-4.\nTipler, Paul; Llewellyn, Ralph (2002). Modern Physics (4th ed.). W. H. Freeman. ISBN 0-7167-4345-0.\n\n\n== External links ==\n Media related to Kinetic energy at Wikimedia Commons"}, {"id": 10, "title": "Electric vehicle", "content": "An electric vehicle (EV) is a vehicle that uses one or more electric motors for propulsion. It can be powered by a collector system, with electricity from extravehicular sources, or it can be powered autonomously by a battery (sometimes charged by solar panels, or by converting fuel to electricity using fuel cells or a generator). EVs include but are not limited to road and rail vehicles, and broadly can also include electric boat and underwater vessels (submersibles, and technically also diesel- and turbo-electric submarines), electric aircraft and electric spacecraft.\nElectric road vehicles include electric passenger cars, electric buses, electric trucks and personal transporters such as electric buggy, electric tricycles, electric bicycles and electric motorcycles/scooters. Together with other emerging automotive technologies such as autonomous driving, connected vehicles and shared mobility, EVs form a future vision of transportation called Connected, Autonomous, Shared and Electric (CASE) mobility.Early electric vehicles first came into existence in the late 19th century, when the Second Industrial Revolution brought forth electrification. Using electricity was among the preferred methods for motor vehicle propulsion as it provides a level of quietness, comfort and ease of operation that could not be achieved by the gasoline engine cars of the time, but range anxiety due to the limited energy storage offered by contemporary battery technologies hindered any mass adoption of private electric vehicles throughout the 20th century. Internal combustion engines (both gasoline and diesel engines) were the dominant propulsion mechanisms for cars and trucks for about 100 years, but electricity-powered locomotion remained commonplace in other vehicle types, such as overhead line-powered mass transit vehicles like electric trains, trams, monorails and trolley buses, as well as various small, low-speed, short-range battery-powered personal vehicles such as mobility scooters. Hybrid electric vehicles, where electric motors are used as a supplementary propulsion to internal combustion engines, became more widespread in the late 1990s. Plug-in hybrid electric vehicles, where electric motors can be used as the predominant propulsion rather than a supplement, did not see any mass production until the late 2000s, and battery electric cars did not become practical options for the consumer market until the 2010s.\nGovernment incentives to increase adoption were first introduced by Norway in 1990, followed by larger markets in the 2000s, including in the United States and the European Union, leading to a growing market for vehicles in the 2010s. Increasing public interest and awareness and structural incentives, such as those being built into the green recovery from the COVID-19 pandemic, are expected to greatly increase the electric vehicle market. During the COVID-19 pandemic, lockdowns reduced the number of greenhouse gases in gasoline or diesel vehicles. The International Energy Agency has stated that governments should do more to meet climate goals, including policies for heavy electric vehicles. A total of 14% of all new cars sold were electric in 2022, up from 9% in 2021 and less than 5% in 2020. Electric vehicle sales may increase from 1% of the global share in 2016 to more than 35% by 2030. As of July 2022 the global EV market size was $280 billion and was expected to grow to $1 trillion by 2026. Much of this growth is expected in markets like North America, Europe, and China; a 2020 literature review suggested that growth in the use of four-wheeled electric vehicles appears economically unlikely in developing economies, but growth in electric two-wheeler and three-wheeler is likely. At more than 20%, two/three-wheelers are already the most electrified road transport segment today, and are projected to continue being the largest EV fleet among all transport modes. Bloomberg reports that in 2023, 292,423,403 bicycles and tricycles sold, representing 49% of the total market. The same report noted that 666,479 buses were sold, with 38% of the market (these are higher priced vehicles, so actual numbers are lower than the percentage of sales), 26,583,856 passenger cars at 14% of sales, and 965,442 vans and trucks with 3% of sales.\n\n\n== History ==\n\nElectric motive power started in 1827 when Hungarian priest \u00c1nyos Jedlik built the first crude but viable electric motor, which used a stator, rotor, and commutator; the next year he used it to power a small car. In 1835, professor Sibrandus Stratingh of the University of Groningen, in the Netherlands, built a small-scale electric car, and sometime between 1832 and 1839, Robert Anderson of Scotland invented the first crude electric carriage, powered by non-rechargeable primary cells. American blacksmith and inventor Thomas Davenport built a toy electric locomotive, powered by a primitive electric motor, in 1835. In 1838, a Scotsman named Robert Davidson built an electric locomotive that attained a speed of four miles per hour (6 km/h). In England, a patent was granted in 1840 for the use of rails as conductors of electric current, and similar American patents were issued to Lilley and Colten in 1847.\nThe first mass-produced electric vehicles appeared in America in the early 1900s. In 1902, the Studebaker Automobile Company entered the automotive business with electric vehicles, though it also entered the gasoline vehicles market in 1904. However, with the advent of cheap assembly line cars by Ford Motor Company, the popularity of electric cars declined significantly.Due to lack of electricity grids and the limitations of storage batteries at that time, electric cars did not gain much popularity; however, electric trains gained immense popularity due to their economies and achievable speeds. By the 20th century, electric rail transport became commonplace due to advances in the development of electric locomotives. Over time their general-purpose commercial use reduced to specialist roles as platform trucks, forklift trucks, ambulances, tow tractors, and urban delivery vehicles, such as the iconic British milk float. For most of the 20th century, the UK was the world's largest user of electric road vehicles.Electrified trains were used for coal transport, as the motors did not use the valuable oxygen in the mines. Switzerland's lack of natural fossil resources forced the rapid electrification of their rail network. One of the earliest rechargeable batteries \u2013 the nickel-iron battery \u2013 was favored by Edison for use in electric cars.\nEVs were among the earliest automobiles, and before the preeminence of light, powerful internal combustion engines (ICEs), electric automobiles held many vehicle land speed and distance records in the early 1900s. They were produced by Baker Electric, Columbia Electric, Detroit Electric, and others, and at one point in history outsold gasoline-powered vehicles. In 1900, 28 percent of the cars on the road in the US were electric. EVs were so popular that even President Woodrow Wilson and his secret service agents toured Washington, D.C., in their Milburn Electrics, which covered 60\u201370 miles (100\u2013110 km) per charge.\nMost producers of passenger cars opted for gasoline cars in the first decade of the 20th century, but electric trucks were an established niche well into the 1920s. A number of developments contributed to a decline in the popularity of electric cars.  Improved road infrastructure required a greater range than that offered by electric cars, and the discovery of large reserves of petroleum in Texas, Oklahoma, and California led to the wide availability of affordable gasoline/petrol, making internal combustion powered cars cheaper to operate over long distances. Electric vehicles were not seldom marketed as a women's luxury car, which may have been a stigma among male consumers. Also, internal combustion powered cars became ever-easier to operate thanks to the invention of the electric starter by Charles Kettering in 1912, which eliminated the need of a hand crank for starting a gasoline engine, and the noise emitted by ICE cars became more bearable thanks to the use of the muffler, which Hiram Percy Maxim had invented in 1897. As roads were improved outside urban areas, electric vehicle range could not compete with the ICE. Finally, the initiation of mass production of gasoline-powered vehicles by Henry Ford in 1913 reduced significantly the cost of gasoline cars as compared to electric cars.In the 1930s, National City Lines, which was a partnership of General Motors, Firestone, and Standard Oil of California purchased many electric tram networks across the country to dismantle them and replace them with GM buses. The partnership was convicted of conspiring to monopolize the sale of equipment and supplies to their subsidiary companies, but was acquitted of conspiring to monopolize the provision of transportation services.\nThe Copenhagen Summit, which was conducted in the midst of a severe observable climate change brought on by human-made greenhouse gas emissions, was held in 2009. During the summit, more than 70 countries developed plans to eventually reach net zero. For many countries, adopting more EVs will help reduce the use of gasoline.\n\n\n=== Experimentation ===\nIn January 1990, General Motors President introduced its EV concept two-seater, the \"Impact\", at the Los Angeles Auto Show. That September, the California Air Resources Board mandated major-automaker sales of EVs, in phases starting in 1998. From 1996 to 1998 GM produced 1117 EV1s, 800 of which were made available through three-year leases.Chrysler, Ford, GM, Honda, and Toyota also produced limited numbers of EVs for California drivers during this time period. In 2003, upon the expiration of GM's EV1 leases, GM discontinued them. The discontinuation has variously been attributed to:\n\nthe auto industry's successful federal court challenge to California's zero-emissions vehicle mandate,\na federal regulation requiring GM to produce and maintain spare parts for the few thousand EV1s and\nthe success of the oil and auto industries' media campaign to reduce public acceptance of EVs.A movie made on the subject in 2005\u20132006 was titled Who Killed the Electric Car? and released theatrically by Sony Pictures Classics in 2006. The film explores the roles of automobile manufacturers, oil industry, the U.S. government, batteries, hydrogen vehicles, and the general public, and each of their roles in limiting the deployment and adoption of this technology.\nFord released a number of their Ford Ecostar delivery vans into the market. Honda, Nissan and Toyota also repossessed and crushed most of their EVs, which, like the GM EV1s, had been available only by closed-end lease. After public protests, Toyota sold 200 of its RAV4 EVs; they later sold at over their original forty-thousand-dollar price. Later, BMW of Canada sold off a number of Mini EVs when their Canadian testing ended.\nThe production of the Citro\u00ebn Berlingo Electrique stopped in September 2005. Zenn started production in 2006 but ended by 2009.\n\n\n=== Reintroduction ===\nDuring the late 20th and early 21st century, the environmental impact of the petroleum-based transportation infrastructure, along with the fear of peak oil, led to renewed interest in electric transportation infrastructure. EVs differ from fossil fuel-powered vehicles in that the electricity they consume can be generated from a wide range of sources, including fossil fuels, nuclear power, and renewables such as solar power and wind power , or any combination of those.\nThe carbon footprint and other emissions of electric vehicles vary depending on the fuel and technology used for electricity generation. The electricity may be stored in the vehicle using a battery, flywheel, or supercapacitors. Vehicles using internal combustion engines usually only derive their energy from a single or a few sources, usually non-renewable fossil fuels. A key advantage of electric vehicles is regenerative braking, which recovers kinetic energy, typically lost during friction braking as heat, as electricity restored to the on-board battery.\n\n\n== Electricity sources ==\nThere are many ways to generate electricity, of varying costs, efficiency and ecological desirability.\n\n\n=== Connection to generator plants ===\nDirect connection to electric grids as is common among electric trains, trams, trolleybuses, and trolleytrucks (See also: overhead lines, third rail and conduit current collection)\nOnline electric vehicle collects power from electric power strips buried under the road surface through electromagnetic induction\n\n\n=== Onboard generators and hybrid EVs ===\n\nGenerated on-board using a diesel engine: diesel\u2013electric locomotive and diesel\u2013electric multiple unit (DEMU)\nGenerated on-board using a fuel cell: fuel cell vehicle\nGenerated on-board using nuclear energy: nuclear submarines and aircraft carriers\nRenewable sources such as solar power: solar vehicleIt is also possible to have hybrid EVs that derive electricity from multiple sources, such as:\n\nOn-board rechargeable electricity storage system (RESS) and a direct continuous connection to land-based generation plants for purposes of on-highway recharging with unrestricted highway range\nOn-board rechargeable electricity storage system and a fueled propulsion power source (internal combustion engine): plug-in hybridFor especially large EVs, such as submarines, the chemical energy of the diesel\u2013electric can be replaced by a nuclear reactor. The nuclear reactor usually provides heat, which drives a steam turbine, which drives a generator, which is then fed to the propulsion. See Nuclear marine propulsion.\nA few experimental vehicles, such as some cars and a handful of aircraft use solar panels for electricity.\n\n\n=== Onboard storage ===\nThese systems are powered from an external generator plant (nearly always when stationary), and then disconnected before motion occurs, and the electricity is stored in the vehicle until needed.\n\nFull Electric Vehicles (FEV). Power storage methods include:\nChemical energy stored on the vehicle in on-board batteries: Battery electric vehicle (BEV) typically with a lithium-ion battery\nKinetic energy storage: flywheels\nStatic energy stored on the vehicle in on-board electric double-layer capacitorsBatteries, electric double-layer capacitors and flywheel energy storage are forms of rechargeable on-board electricity storage systems. By avoiding an intermediate mechanical step, the energy conversion efficiency can be improved compared to hybrids by avoiding unnecessary energy conversions. Furthermore, electro-chemical batteries conversions are reversible, allowing electrical energy to be stored in chemical form.\n\n\n== Lithium-ion battery ==\n\nMost electric vehicles use lithium-ion batteries (Li-Ions or LIBs). Lithium-ion batteries have a higher energy density, longer life span , and higher power density than most other practical batteries. Complicating factors include safety, durability, thermal breakdown,  environmental impact , and cost. Li-ion batteries should be used within safe temperature and voltage ranges to operate safely and efficiently.Increasing the battery's lifespan decreases effective costs. One technique is to operate a subset of the battery cells at a time and switching these subsets.In the past, nickel\u2013metal hydride batteries were used in some electric cars, such as those made by General Motors. These battery types are considered outdated due to their tendencies to self-discharge in the heat. Furthermore, a patent for this type of battery was held by Chevron, which created a problem for their widespread development. These factors, coupled with their high cost, has led to lithium-ion batteries leading as the predominant battery for EVs.The prices of lithium-ion batteries have declined dramatically over the past decade, contributing to a reduction in price for electric vehicles, but an increase in the price of critical minerals such as lithium from 2021 to the end of 2022 has put pressure on historical battery price decreases.\n\n\n== Electric motor ==\n\nThe power of a vehicle's electric motor, as in other machines, is measured in kilowatts (kW). Electric motors can deliver their maximum torque over a wide RPM range. This means that the performance of a vehicle with a 100 kW electric motor exceeds that of a vehicle with a 100 kW internal combustion engine, which can only deliver its maximum torque within a limited range of engine speed.\nEfficiency of charging varies considerably depending on the type of charger, and energy is lost during the process of converting the electrical energy to mechanical energy.\nUsually, direct current (DC) electricity is fed into a DC/AC inverter where it is converted to alternating current (AC) electricity and this AC electricity is connected to a 3-phase AC motor.\nFor electric trains, forklift trucks, and some electric cars, DC motors are often used. In some cases, universal motors are used, and then AC or DC may be employed. In recent production vehicles, various motor types have been implemented; for instance, induction motors within Tesla Motor vehicles and permanent magnet machines in the Nissan Leaf and Chevrolet Bolt.\n\n\n== Vehicle types ==\nIt is generally possible to equip any kind of vehicle with an electric power-train.\n\n\n=== Ground vehicles ===\n\n\n==== Pure-electric vehicles ====\n\nA pure-electric vehicle or all-electric vehicle is powered exclusively through electric motors. The electricity may come from a battery (battery electric vehicle), solar panel (solar vehicle) or fuel cell (fuel cell vehicle).\n\n\n==== Hybrid EVs ====\n\nThere are different ways that a hybrid electric vehicle can combine the power from an electric motor and the internal combustion engine. The most common type is a parallel hybrid that connects the engine and the electric motor to the wheels through mechanical coupling. In this scenario, the electric motor and the engine can drive the wheels directly. Series hybrids only use the electric motor to drive the wheels and can often be referred to as extended-range electric vehicles (EREVs) or range-extended electric vehicles (REEVs). There are also series-parallel hybrids where the vehicle can be powered by the engine working alone, the electric motor on its own, or by both working together; this is designed so that the engine can run at its optimum range as often as possible.\n\n\n==== Plug-in electric vehicle ====\n\nA plug-in electric vehicle (PEV) is any motor vehicle that can be recharged from any external source of electricity, such as wall sockets, and the electricity stored in the Rechargeable battery packs drives or contributes to drive the wheels. PEV is a subcategory of electric vehicles that includes battery electric vehicles (BEVs), plug-in hybrid vehicles, (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles.\n\n\n==== Range-extended electric vehicle ====\n\nA range-extended electric vehicle (REEV) is a vehicle powered by an electric motor and a plug-in battery. An auxiliary combustion engine is used only to supplement battery charging and not as the primary source of power.\n\n\n==== On- and off-road EVs ====\nOn-road electric vehicles include electric cars, electric trolleybuses, electric buses, battery electric buses, electric trucks, electric bicycles, electric motorcycles and scooters, personal transporters, neighborhood electric vehicles, golf carts, milk floats, and forklifts. Off-road vehicles include electrified all-terrain vehicles and tractors.\n\n\n==== Railborne EVs ====\n\nThe fixed nature of a rail line makes it relatively easy to power EVs through permanent overhead lines or electrified third rails, eliminating the need for heavy onboard batteries. Electric locomotives, electric multiple units, electric trams (also called streetcars or trolleys), electric light rail systems, and electric rapid transit are all in common use today, especially in Europe and Asia.\nSince electric trains do not need to carry a heavy internal combustion engine or large batteries, they can have very good power-to-weight ratios. This allows high speed trains such as France's double-deck TGVs to operate at speeds of 320 km/h (200 mph) or higher, and electric locomotives to have a much higher power output than diesel locomotives. In addition, they have higher short-term surge power for fast acceleration, and using regenerative brakes can put braking power back into the electrical grid rather than wasting it.\nMaglev trains are also nearly always EVs.There are also battery electric passenger trains operating on non-electrified rail lines.\n\n\n=== Seaborne EVs ===\n\nElectric boats were popular around the turn of the 20th century. Interest in quiet and potentially renewable marine transportation has steadily increased since the late 20th century, as solar cells have given motorboats the infinite range of sailboats. Electric motors can and have also been used in sailboats instead of traditional diesel engines. Electric ferries operate routinely. Submarines use batteries (charged by diesel or gasoline engines at the surface), nuclear power, fuel cells or Stirling engines to run electric motor-driven propellers.\n\n\n=== Airborne EVs ===\n\nSince the beginnings of aviation, electric power for aircraft has received a great deal of experimentation. Currently, flying electric aircraft include piloted and unpiloted aerial vehicles.\n\n\n=== Electrically powered spacecraft ===\n\nElectric power has a long history of use in spacecraft. The power sources used for spacecraft are batteries, solar panels and nuclear power. Current methods of propelling a spacecraft with electricity include the arcjet rocket, the electrostatic ion thruster, the Hall-effect thruster, and Field Emission Electric Propulsion.\n\n\n==== Space rover vehicles ====\n\nCrewed and uncrewed vehicles have been used to explore the Moon and other planets in the Solar System. On the last three missions of the Apollo program in 1971 and 1972, astronauts drove silver-oxide battery-powered Lunar Roving Vehicles distances up to 35.7 kilometers (22.2 mi) on the lunar surface. Uncrewed, solar-powered rovers have explored the Moon and Mars.\n\n\n== Energy and motors ==\nMost large electric transport systems are powered by stationary sources of electricity that are directly connected to the vehicles through wires. Electric traction allows the use of regenerative braking, in which the motors are used as brakes and become generators that transform the motion of, usually, a train into electrical power that is then fed back into the lines. This system is particularly advantageous in mountainous operations, as descending vehicles can produce a large portion of the power required for those ascending. This regenerative system is only viable if the system is large enough to use the power generated by descending vehicles.\nIn the systems above, motion is provided by a rotary electric motor. However, it is possible to \"unroll\" the motor to drive directly against a special matched track. These linear motors are used in maglev trains which float above the rails supported by magnetic levitation. This allows for almost no rolling resistance of the vehicle and no mechanical wear and tear of the train or track. In addition to the high-performance control systems needed, switching and curving of the tracks becomes difficult with linear motors, which to date has restricted their operations to high-speed point to point services.\n\n\n== Records ==\nRimac Nevera, an electric hypercar, sets 23 world speed records in one day.\nFastest acceleration of an electric car, 0 to 100 km/h in 1.461 seconds by university students at the University of Stuttgart.\nElectric Land Speed Record 353 mph (568 km/h).\nElectric Car Distance Record 1,725 miles (2,776 km) in 24 hours by Bj\u00f8rn Nyland.\nGreatest distance by electric vehicle, single charge 999.5 miles (1,608.5 km).\nSolar-powered EV is fastest EV to go over 1,000 km without stopping to recharge, the Sunswift 7.\nElectric Motorcycle: 1,070 miles (1,720 km) under 24 hours. Michel von Tell on a Harley LiveWire.\nElectric flight: 439.5 miles (707.3 km) without charge.\n\n\n== Properties ==\n\n\n=== Components ===\nThe type of battery, the type of traction motor and the motor controller design vary according to the size, power and proposed application, which can be as small as a motorized shopping cart or wheelchair, through pedelecs, electric motorcycles and scooters, neighborhood electric vehicles, industrial fork-lift trucks and including many hybrid vehicles.\n\n\n=== Energy sources ===\nEVs are much more efficient than fossil fuel vehicles and have few direct emissions. At the same time, they do rely on electrical energy that is generally provided by a combination of non-fossil fuel plants and fossil fuel plants. Consequently, EVs can be made less polluting overall by modifying the source of electricity. In some areas, persons can ask utilities to provide their electricity from renewable energy.\nFossil fuel vehicle efficiency and pollution standards take years to filter through a nation's fleet of vehicles. New efficiency and pollution standards rely on the purchase of new vehicles, often as the current vehicles already on the road reach their end-of-life. Only a few nations set a retirement age for old vehicles, such as Japan or Singapore, forcing periodic upgrading of all vehicles already on the road.\n\n\n=== Batteries ===\n\nAn electric-vehicle battery (EVB) in addition to the traction battery speciality systems used for industrial (or recreational) vehicles, are batteries used to power the propulsion system of a battery electric vehicle (BEVs). These batteries are usually a secondary (rechargeable) battery, and are typically lithium-ion batteries.\nTraction batteries, specifically designed with a high ampere-hour capacity, are used in forklifts, electric golf carts, riding floor scrubbers, electric motorcycles, electric cars, trucks, vans, and other electric vehicles.\n\n\n=== Efficiency ===\nEVs convert over 59\u201362% of grid energy to the wheels. Conventional gasoline vehicles convert around 17\u201321%.\n\n\n=== Charging ===\n\n\n==== Grid capacity ====\nIf almost all road vehicles were electric it would increase global demand for electricity by up to 25% by 2050 compared to 2020. However, overall energy consumption and emissions would diminish because of the higher efficiency of EVs over the entire cycle, and the reduction in energy needed to refine fossil fuels.\n\n\n==== Charging stations ====\n\n\n==== Battery swapping ====\nInstead of recharging EVs from electric sockets, batteries could be mechanically replaced at special stations in a few minutes (battery swapping).\nBatteries with greater energy density such as metal-air fuel cells cannot always be recharged in a purely electric way, so some form of mechanical recharge may be used instead. A zinc\u2013air battery, technically a fuel cell, is difficult to recharge electrically so may be \"refueled\" by periodically replacing the anode or electrolyte instead.\n\n\n==== Dynamic charging ====\nTRL (formerly Transport Research Laboratory) lists three power delivery types for dynamic charging, or charging while the vehicle is in motion: overhead power lines, and ground level power through rail or induction. TRL lists overhead power as the most technologically mature solution which provides the highest levels of power, but the technology is unsuitable for non-commercial vehicles. Ground-level power is suitable for all vehicles, with rail being a mature solution with high transfer of power and easily accessible and inspected elements. Inductive charging delivers the least power and requires more roadside equipment than the alternatives.:\u200aAppendix D\u200aThe European Commission published in 2021 a request for regulation and standardization of electric road systems. Shortly afterward, a working group of the French Ministry of Ecology recommended adopting a European electric road standard formulated with Sweden, Germany, Italy, the Netherlands, Spain, Poland, and others. The first standard for electrical equipment on-board a vehicle powered by a rail electric road system (ERS), CENELEC Technical Standard 50717, has been approved in late 2022. Following standards, encompassing \"full interoperability\" and a \"unified and interoperable solution\" for ground-level power supply, are scheduled to be published by the end 2024, detailing complete \"specifications for communication and power supply through conductive rails embedded in the road\".\n\n\n=== Other in-development technologies ===\nConventional electric double-layer capacitors are being worked on to achieve the energy density of lithium-ion batteries, offering almost unlimited lifespans and no environmental issues. High-K electric double-layer capacitors, such as EEStor's EESU, could improve lithium ion energy density several times over if they can be produced. Lithium-sulphur batteries offer 250 Wh/kg. Sodium-ion batteries promise 400 Wh/kg with only minimal expansion/contraction during charge/discharge and a very high surface area, and rely on lower cost materials than Lithium-ion, Leading to Cheaper batteries that do not require critical minerals.\n\n\n=== Safety ===\nThe United Nations in Geneva (UNECE) has adopted the first international regulation (Regulation 100) on safety of both fully electric and hybrid electric cars, with the intent of ensuring that cars with a high voltage electric power train, such as hybrid and fully-electric vehicles, are as safe as combustion-powered cars. The EU and Japan have already indicated that they intend to incorporate the new UNECE Regulation in their respective rules on technical standards for vehicles.\n\n\n=== Environmental ===\n\nEVs release no tailpipe air pollutants, and reduce respiratory illnesses such as asthma. However, EVs are charged with electricity that may be generated by means that have health and environmental impacts.The carbon emissions from producing and operating an EV are typically less than those of producing and operating a conventional vehicle. EVs in urban areas almost always pollute less than internal combustion vehicles.One limitation of the environmental potential of EVs is that simply switching the existing privately owned car fleet from ICEs to EVs will not free up road space for active travel or public transport. Electric micromobility vehicles, such as e-bikes, may contribute to the decarbonisation of transport systems, especially outside of urban areas which are already well-served by public transport.Internal combustion engined vehicles use far more raw materials over their lifetime than EVs.Since their first commercial release in 1991, lithium-ion batteries have become an important technology for achieving low-carbon transportation systems.  The sustainability of production process of batteries has not been fully assessed in either economic, social or environmental terms.Business processes of raw material extraction in practice raise issues of transparency and accountability of the management of extractive resources. In the complex supply chain of lithium technology, there are diverse stakeholders representing corporate interests, public interest groups and political elites that are concerned with outcomes from the technology production and use. One possibility to achieve balanced extractive processes would be the establishment of commonly agreed standards on the governance of technology worldwide.The compliance of these standards can be assessed by the Assessment of Sustainability in Supply Chains Frameworks (ASSC). Hereby, the qualitative assessment consists of examining governance and social and environmental commitment. Indicators for the quantitative assessment are management systems and standards, compliance and social and environmental indicators.One source estimates that over a fifth of the lithium and about 65% of the cobalt needed for electric cars will be from recycled sources by 2035. On the other hand, when counting the large quantities of fossil fuel non-electric cars consume over their lifetime, electric cars can be considered to dramatically reduce raw-material needs.In 2022, the manufacturing of an EV emitted on average around 50% more CO2 than an equivalent internal combustion engine vehicle, but this difference is more than offset by the much higher emissions from the oil used in driving an internal combustion engine Vehicle over its lifetime compared to those from generating the electricity used for driving the EV.In 2023 Greenpeace issued a video, criticizing the view according to which EVs are \"silver bullet for climate\", saying there construction phase has a high environmental impact. For example, the rise in SUV sales by Hyundai almost eliminate the climate benefits of passing to EV in this company, because even electric SUVs have a high carbon footprint as they consume much raw materials and energy during construction. Greenpeace propose a mobility as a service concept instead, based on biking, public transport and ride sharing.\n\n\n=== Socio-economic ===\nA 2003 study in the United Kingdom found that \"[p]ollution is most concentrated in areas where young children and their parents are more likely to live and least concentrated in areas to which the elderly tend to migrate,\" and that \"those communities that are most polluted and which also emit the least pollution tend to be amongst the poorest in Britain.\" A 2019 UK study found that \"households in the poorest areas emit the least NOx and PM, whilst the least poor areas emitted the highest, per km, vehicle emissions per household through having higher vehicle ownership, owning more diesel vehicles and driving further.\"\n\n\n=== Mechanical ===\nElectric motors are mechanically very simple and often achieve 90% energy conversion efficiency over the full range of speeds and power output and can be precisely controlled. They can also be combined with regenerative braking systems that have the ability to convert movement energy back into stored electricity. This can be used to reduce the wear on brake systems (and consequent brake pad dust) and reduce the total energy requirement of a trip. Regenerative braking is especially effective for start-and-stop city use.\nThey can be finely controlled and provide high torque from stationary-to-moving, unlike internal combustion engines, and do not need multiple gears to match power curves. This removes the need for gearboxes and torque converters.\nEVs provide quiet and smooth operation and consequently have less noise and vibration than internal combustion engines. While this is a desirable attribute, it has also evoked concern that the absence of the usual sounds of an approaching vehicle poses a danger to blind, elderly and very young pedestrians. To mitigate this situation, many countries mandate warning sounds when EVs are moving slowly, up to a speed when normal motion and rotation (road, suspension, electric motor, etc.) noises become audible.Electric motors do not require oxygen, unlike internal combustion engines; this is useful for submarines and for space rovers.\n\n\n=== Energy resilience ===\nElectricity can be produced from a variety of sources; therefore, it gives the greatest degree of energy resilience.\n\n\n=== Energy efficiency ===\nEV 'tank-to-wheels' efficiency is about a factor of three higher than internal combustion engine vehicles. Energy is not consumed while the vehicle is stationary, unlike internal combustion engines which consume fuel while idling. However, looking at the well-to-wheel efficiency of EVs, their total emissions, while still lower, are closer to an efficient gasoline or diesel in most countries where electricity generation relies on fossil fuels.In 2022, EVs enabled a net reduction of about 80 Mt of GHG emissions, on a well to-wheels basis, and the net GHG benefit of EVs will increase over time as the electricity sector is decarbonised.Well-to-wheel efficiency of an EV has less to do with the vehicle itself and more to do with the method of electricity production. A particular EV would instantly become twice as efficient if electricity production were switched from fossil fuels to renewable energy, such as wind power, tidal power, solar power, and nuclear power. Thus, when \"well-to-wheels\" is cited, the discussion is no longer about the vehicle, but rather about the entire energy supply infrastructure \u2013 in the case of fossil fuels this should also include energy spent on exploration, mining, refining, and distribution.\nThe lifecycle analysis of EVs shows that even when powered by the most carbon-intensive electricity in Europe, they emit less greenhouse gases than a conventional diesel vehicle.\n\n\n=== Total cost ===\nAs of 2021 the purchase price of an EV is often more, but the total cost of ownership of an EV varies wildly depending on location and distance travelled per year: in parts of the world where fossil fuels are subsidized, lifecycle costs of diesel or gas-powered vehicle are sometimes less than a comparable EV.European carmakers face significant pressure from more affordable Chinese models and price cuts by US-based Tesla Motor. From 2021 to 2022, the European market share of Chinese EV manufacturers doubled to almost 9%, prompting the CEO of Stellantis to describe it as an \"invasion\".\n\n\n=== Range ===\nElectric vehicles may have shorter range compared to vehicles with internal combustion engines, which is why the electrification of long-distance transport, such as long-distance shipping, remains challenging.\nIn 2022, the sales-weighted average range of small BEVs sold in the United States was nearly 350 km, while in France, Germany and the United Kingdom it was just under 300 km, compared to under 220 km in China.\n\n\n=== Heating of EVs ===\nWell insulated cabins can heat the vehicle using the body heat of the passengers. This is not enough, however, in colder climates as a driver delivers only about 100 W of heating power. A heat pump system, capable of cooling the cabin during summer and heating it during winter, is an efficient way of heating and cooling EVs. For vehicles which are connected to the grid, battery EVs can be preheated, or cooled, with little or no need for battery energy, especially for short trips. Most new electric cars come with heat pumps as standard.\n\n\n== Electric public transit efficiency ==\nShifts from private to public transport (train, trolleybus, personal rapid transit or tram) have the potential for large gains in efficiency in terms of an individual's distance traveled per kWh.\nResearch shows people prefer trams to buses, because they are quieter and more comfortable and perceived as having higher status. Therefore, it may be possible to cut liquid fossil fuel consumption in cities through the use of electric trams. Trams may be the most energy-efficient form of public transportation, with rubber-wheeled vehicles using two-thirds more energy than the equivalent tram, and run on electricity rather than fossil fuels.\nIn terms of net present value, they are also the cheapest \u2013 Blackpool trams are still running after 100 years, but combustion buses only last about 15 years.\n\n\n== Polluter pays principle ==\n\nThe IEA suggests that taxing inefficient internal combustion engine vehicles could encourage adoption of EVs, with taxes raised being used to fund subsidies for EVs. Government procurement is sometimes used to encourage national EV manufacturers. Many countries will ban sales of fossil fuel vehicles between 2025 and 2040.Many governments offer incentives to promote the use of electric vehicles, with the goals of reducing air pollution and oil consumption. Some incentives intend to increase purchases of electric vehicles by offsetting the purchase price with a grant. Other incentives include lower tax rates or exemption from certain taxes, and investment in charging infrastructure.\nCompanies selling EVs have partnered with local electric utilities to provide large incentives on some electric vehicles.\n\n\n== Future ==\n\n\n=== Public perception ===\nA European survey based on climate found that as of 2022, 39% of European citizens tend to prefer hybrid vehicles, 33% prefer petrol or diesel vehicles, followed by electric cars which were preferred by 28% of Europeans. 44% Chinese car buyers are the most likely to buy an electric car, while 38% of Americans would opt for a hybrid car, 33% would prefer petrol or diesel, while only 29% would go for an electric car.In a 2023 survey concentrated specifically on electric car ownership in the US, 50% of adult respondents considered themselves unlikely to seriously consider buying an EV. \n\n\n=== Environmental considerations ===\n\nBy reducing air pollution, such as nitrogen dioxide, EVs could prevent hundreds of thousands of early deaths every year, especially from trucks and old cars in cities.In 2023 the US State Department said that the supply of lithium would need to increase 42-fold by 2050 globally to support a transition to clean energy. Rare-earth metals (neodymium, dysprosium) and other mined metals (copper, nickel, iron) are used by EV motors, while lithium, cobalt, manganese are used by the batteries. The full environmental impact of electric vehicles includes the life cycle impacts of carbon and sulfur emissions, as well as toxic metals entering the environment. Most of the lithium ion battery production occurs in China, where the bulk of energy used is supplied by coal burning power plants. A study of hundreds of cars on sale in 2021 concluded that the life cycle GHG emissions of full electric cars are slightly less than hybrids and that both are less than gasoline and diesel fuelled cars.An alternative method of sourcing essential battery materials being deliberated by the International Seabed Authority is deep sea mining, however carmakers are not using this as of 2023.\n\n\n=== Improved batteries ===\nAdvances in lithium-ion batteries, driven at first by the personal-use electronics industry, allow full-sized, highway-capable EVs to travel nearly as far on a single charge as conventional cars go on a single tank of gasoline. Lithium batteries have been made safe, can be recharged in minutes instead of hours (see recharging time), and now last longer than the typical vehicle (see lifespan). The production cost of these lighter, higher-capacity lithium-ion batteries is gradually decreasing as the technology matures and production volumes increase.\nMany companies and researchers are also working on newer battery technologies, including solid state batteries and \nalternate technologies.\n\n\n=== Battery management and intermediate storage ===\nAnother improvement is to decouple the electric motor from the battery through electronic control, using supercapacitors to buffer large but short power demands and regenerative braking energy. The development of new cell types combined with intelligent cell management improved both weak points mentioned above. The cell management involves not only monitoring the health of the cells but also a redundant cell configuration (one more cell than needed). With sophisticated switched wiring, it is possible to condition one cell while the rest are on duty.\n\n\n=== Electric trucks ===\n\n\n=== Hydrogen trains ===\nParticularly in Europe, fuel-cell electric trains are gaining in popularity to replace diesel-electric units. In Germany, several L\u00e4nder have ordered Alstom Coradia iLINT trainsets, in service since 2018, with France also planning to order trainsets. The United Kingdom, the Netherlands, Denmark, Norway, Italy, Canada and Mexico are equally interested. In France, the SNCF plans to replace all its remaining diesel-electric trains with hydrogen trains by 2035. In the United Kingdom, Alstom announced in 2018 their plan to retrofit British Rail Class 321 trainsets with fuel cells.\n\n\n=== Higher voltage outlets in garages of newly built homes ===\n\nIn New Mexico the government is looking to pass legislation mandating electrical receptacles that are higher voltage to be installed in garages of newly built homes.  The NEMA 14-50 outlets provide 240 volts and 50 Amps for a total of 12.5 Kilowatts for level 2 charging of electric vehicles. Level 2 charging can add up to 30 miles of range per hour of charging compared to up to 4 miles of range per hour for level 1 charging from 120 volt outlets.\n\n\n=== Bidirectional charging ===\nGeneral Motors (GM) is adding a capability called V2H, or bidirectional charging, to allow its new electric vehicles to send power from their batteries to the owner's home.  GM will start with 2024 models, including the Silverado and Blazer EVs, and promises to continue the feature through to model year 2026.  This could be helpful to the owner during unexpected power grid outages because an electric vehicle is a giant battery on wheels.\n\n\n== Infrastructure management ==\nWith the increase in number of electric vehicles, it is necessary to create an appropriate number of charging stations to supply the increasing demand, and a proper management system that coordinates the charging turn of each vehicle to avoid having some charging stations overloaded with vehicles and others empty.\n\n\n=== Stabilization of the grid ===\nSince EVs can be plugged into the electric grid when not in use, battery-powered vehicles could reduce the need for dispatchable generation by feeding electricity into the grid from their batteries during periods of high demand and low supply (such as just after sunset) while doing most of their charging at night or midday, when there is unused generating capacity. This vehicle-to-grid (V2G) connection has the potential to reduce the need for new power plants, as long as vehicle owners do not mind reducing the life of their batteries, by being drained by the power company during peak demand. Electric vehicle parking lots can provide demand response.Current electricity infrastructure may need to cope with increasing shares of variable-output power sources such as wind and solar. This variability could be addressed by adjusting the speed at which EV batteries are charged, or possibly even discharged.Some concepts see battery exchanges and battery charging stations, much like gas/petrol stations today. These will require enormous storage and charging potentials, which could be manipulated to vary the rate of charging, and to output power during shortage periods, much as diesel generators are used for short periods to stabilize some national grids.\n\n\n=== Repair Shops ===\nThe infrastructure for vehicle repairs after accidents is a concern for insurers and mechanics due to safety requirements. Batteries and other components must be carefully evaluated rather than being totally written off by insurers.\n\n\n== See also ==\nElectric rickshaw \u2013 E-tricycle\nNeighborhood Electric Vehicle \u2013 NEV\nPolluter pays principle\nAlternative fuel vehicle\nVehicle classification by propulsion system\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Electrically powered vehicles at Wikimedia Commons"}, {"id": 11, "title": "Symmetric-key algorithm", "content": "Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both the encryption of plaintext and the decryption of ciphertext. The keys may be identical, or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. The requirement that both parties have access to the secret key is one of the main drawbacks of symmetric-key encryption, in comparison to public-key encryption (also known as asymmetric-key encryption). However, symmetric-key encryption algorithms are usually better for bulk encryption. With exception of the one-time pad they have a smaller key size, which means less storage space and faster transmission. Due to this, asymmetric-key encryption is often used to exchange the secret key for symmetric-key encryption.\n\n\n== Types ==\nSymmetric-key encryption can use either stream ciphers or block ciphers.Stream ciphers encrypt the digits (typically bytes), or letters (in substitution ciphers) of a message one at a time. An example is ChaCha20. Substitution ciphers are well-known ciphers, but can be easily decrypted using a frequency table.Block ciphers take a number of bits and encrypt them in a single unit, padding the plaintext to achieve a multiple of the block size. The Advanced Encryption Standard (AES) algorithm, approved by NIST in December 2001, uses 128-bit blocks.\n\n\n== Implementations ==\nExamples of popular symmetric-key algorithms include Twofish, Serpent, AES (Rijndael), Camellia, Salsa20, ChaCha20, Blowfish, CAST5, Kuznyechik, RC4, DES, 3DES, Skipjack, Safer, and IDEA.\n\n\n== Use as a cryptographic primitive ==\nSymmetric ciphers are commonly used to achieve other cryptographic primitives than just encryption.Encrypting a message does not guarantee that it will remain unchanged while encrypted. Hence, often a message authentication code is added to a ciphertext to ensure that changes to the ciphertext will be noted by the receiver. Message authentication codes can be constructed from an AEAD cipher (e.g. AES-GCM).\nHowever, symmetric ciphers cannot be used for non-repudiation purposes except by involving additional parties. See the ISO/IEC 13888-2 standard.\nAnother application is to build hash functions from block ciphers. See one-way compression function for descriptions of several such methods.\n\n\n== Construction of symmetric ciphers ==\n\nMany modern block ciphers are based on a construction proposed by Horst Feistel. Feistel's construction makes it possible to build invertible functions from other functions that are themselves not invertible.\n\n\n== Security of symmetric ciphers ==\nSymmetric ciphers have historically been susceptible to known-plaintext attacks, chosen-plaintext attacks, differential cryptanalysis and linear cryptanalysis. Careful construction of the functions for each round can greatly reduce the chances of a successful attack. It is also possible to increase the key length or the rounds in the encryption process to better protect against attack. This, however, tends to increase the processing power and decrease the speed at which the process runs due to the amount of operations the system needs to do.Most modern symmetric-key algorithms appear to be resistant to the threat of post-quantum cryptography. Quantum computers would exponentially increase the speed at which these ciphers can be decoded; notably, Grover's algorithm would take the square-root of the time traditionally required for a brute-force attack, although these vulnerabilities can be  compensated for by doubling key length. For example, a 128 bit AES cipher would not be secure against such an attack as it would reduce the time required to test all possible iterations from over 10 quintillion years to about six months. By contrast, it would still take a quantum computer the same amount of time to decode a 256 bit AES cipher as it would a conventional computer to decode a 128 bit AES cipher. For this reason, AES-256 is believed to be \"quantum resistant\".\n\n\n== Key management ==\n\n\n== Key establishment ==\n\nSymmetric-key algorithms require both the sender and the recipient of a message to have the same secret key. All early cryptographic systems required either the sender or the recipient to somehow receive a copy of that secret key over a physically secure channel.\nNearly all modern cryptographic systems still use symmetric-key algorithms internally to encrypt the bulk of the messages, but they eliminate the need for a physically secure channel by using Diffie\u2013Hellman key exchange or some other public-key protocol to securely come to agreement on a fresh new secret key for each session/conversation (forward secrecy).\n\n\n== Key generation ==\n\nWhen used with asymmetric ciphers for key transfer, pseudorandom key generators are nearly always used to generate the symmetric cipher session keys. However, lack of randomness in those generators or in their initialization vectors is disastrous and has led to cryptanalytic breaks in the past. Therefore, it is essential that an implementation use a source of high entropy for its initialization.\n\n\n== Reciprocal cipher ==\nA reciprocal cipher is a cipher where, just as one enters the plaintext into the cryptography system to get the ciphertext, one could enter the ciphertext into the same place in the system to get the plaintext. A reciprocal cipher is also sometimes referred as self-reciprocal cipher.Practically all mechanical cipher machines implement a reciprocal cipher, a mathematical involution on each typed-in letter.\nInstead of designing two kinds of machines, one for encrypting and one for decrypting, all the machines can be identical and can be set up (keyed) the same way.Examples of reciprocal ciphers include:\n\nAtbash\nBeaufort cipher\nEnigma machine\nMarie Antoinette and Axel von Fersen communicated with a self-reciprocal cipher.\nthe Porta polyalphabetic cipher is self-reciprocal.\nPurple cipher\nRC4\nROT13\nXOR cipher\nVatsyayana cipherThe majority of all modern ciphers can be classified as either a stream cipher, most of which use a reciprocal XOR cipher combiner, or a block cipher, most of which use a Feistel cipher or Lai\u2013Massey scheme with a reciprocal transformation in each round.\n\n\n== Notes ==\n\n\n== References =="}, {"id": 12, "title": "List of Presidents of the United States by Home State", "content": "These lists give the states of primary affiliation and of birth for each president of the United States.\n\n\n== Birthplaces ==\nTwenty-one states have the distinction of being the birthplace of a president.\nOne president's birth state is in dispute; North and South Carolina (British colonies at the time) both lay claim to Andrew Jackson, who was born in 1767 in the Waxhaw region along their common border. Jackson himself considered South Carolina as his birth state.Born on December 5, 1782, Martin Van Buren was the first president born an American citizen (and not a British subject).The term Virginia dynasty is sometimes used to describe the fact that four of the first five U.S. presidents were from Virginia.\nThe number of presidents per state in which they were born, counting Jackson as being from South Carolina, are:\n\nOne: Arkansas, California, Connecticut, Georgia, Hawaii, Illinois, Iowa, Kentucky,  Missouri, Nebraska, New Hampshire, New Jersey, and South Carolina\nTwo: North Carolina, Pennsylvania, Texas, and Vermont\nFour: Massachusetts\nFive: New York\nSeven: Ohio\nEight: Virginia\n\n\n== Presidential birthplace and early childhood historic sites ==\nThe birthplaces and early childhood residences of many U.S. presidents have been preserved or replicated. In instances where a physical structure is absent, a monument or roadside marker has been erected to denote the site's historic significance. All sites in the table below are listed in the National Register of Historic Places.\nA dramatic shift in childbirth from home to hospital occurred in the United States in the early 20th century (mid\u20131920s to 1940). Reflective of this trend, Jimmy Carter and all presidents born during and after World War II (Bill Clinton and every president since) have been born in a hospital, not a private residence. This sortable table is ordered by the presidents' birthdates.\n\n\n== States of primary affiliation ==\n\nA list of U.S. presidents including the state with which each was primarily affiliated or most closely associated with, due to residence, professional career, and electoral history.\n\n\n=== Notes ===\n\n\n=== Presidents by state of primary affiliation ===\nA list of U.S. presidents grouped by primary state of residence and birth, with priority given to residence. Only 19 out of the 50 states are represented. Presidents with an asterisk (*) did not primarily reside in their respective birth states (they were not born in the state listed below).\n\n\n== References ==\n\n\n== External links ==\nAmerican Presidents Sites \u2013 Discover Our Shared Heritage Travel Itinerary from the National Park Service"}, {"id": 13, "title": "Citizenship of the United States", "content": "Citizenship of the United States is a legal status that entails Americans with specific rights, duties, protections, and benefits in the United States. It serves as a foundation of fundamental rights derived from and protected by the Constitution and laws of the United States, such as freedom of expression, due process, the rights to vote (however, not all citizens have the right to vote in all federal elections, for example, those living in Puerto Rico), live and work in the United States, and to receive federal assistance.There are two primary sources of citizenship: birthright citizenship, in which persons born within the territorial limits of the United States are presumed to be a citizen, or\u2014providing certain other requirements are met\u2014born abroad to a United States citizen parent, and naturalization, a process in which an eligible legal immigrant applies for citizenship and is accepted. The first of these two pathways to citizenship is specified in the Citizenship Clause of the Fourteenth Amendment of the Constitution which reads:\n\nAll persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside.\nThe second is provided for in U.S. law. In Article One of the Constitution, the power to establish a \"uniform rule of naturalization\" is granted explicitly to Congress.\nUnited States law permits multiple citizenship. Citizens of other countries who are naturalized as United States citizens may retain their previous citizenship, although they must renounce allegiance to the other country. A United States citizen retains United States citizenship when becoming the citizen of another country, should that country's laws allow it. United States citizenship can be renounced by Americans via a formal procedure at a United States embassy.National citizenship signifies membership in the country as a whole; state citizenship, in contrast, signifies a relation between a person and a particular state and has application generally limited to domestic matters. State citizenship may affect (1) tax decisions, (2) eligibility for some state-provided benefits such as higher education, and (3) eligibility for state political posts such as United States senator. At the time of the American Civil War, state citizenship assumed a much increased importance when it was widely deemed to have a prior claim over the citizens' loyalty in the seceding Southern states.\n\n\n== Rights, duties, and benefits ==\n\n\n=== Rights ===\nFreedom to reside and work. United States citizens have the right to reside and work in the United States. Certain non-citizens, such as lawful permanent residents, have similar rights; however, non-citizens, unlike citizens, may have the right taken away. For example, they may be deported if convicted of a serious crime.\nFreedom to enter and leave the United States. United States citizens have the right to enter and leave the United States freely. Certain non-citizens, such as permanent residents, have similar rights. Unlike permanent residents, United States citizens do not have an obligation to maintain residence in the United States \u2013 they can leave for any length of time and return freely at any time.\nVoting for federal office in all fifty states and the District of Columbia is restricted to citizens only. States are not required to extend the franchise to all citizens: for example, several states bar citizen felons from voting, even after they have completed any custodial sentence. The United States Constitution bars states from restricting citizens from voting on grounds of race, color, previous condition of servitude, sex, failure to pay any tax, or age (for citizens who are at least eighteen years old). Historically, many states and local jurisdictions have allowed non-citizens to vote; however, today this is limited to local elections in very few places. Citizens are not compelled to vote.\nFreedom to stand for public office. The United States Constitution requires that all members of the United States House of Representatives have been citizens for seven years, and that all senators have been citizens for nine years, before taking office. Most states have similar requirements: for example California requires that legislators have been citizens for three years, and the Governor has been a citizen for five years, upon taking office. The United States Constitution requires that one be \"a natural born Citizen\" and a United States resident for fourteen years in order to be president of the United States or vice president of the United States. The Constitution also stipulates that otherwise eligible citizens must meet certain age requirements for these offices.\nRight to apply for federal employment. Many federal government jobs require applicants to have United States citizenship. United States citizens can apply for federal employment within a government agency or department.\n\n\n=== Duties ===\nJury duty is only imposed upon citizens. Jury duty may be considered the \"sole differential obligation\" between non-citizens and citizens; the federal and state courts \"uniformly exclude non-citizens from jury pools today, and with the exception of a few states in the past, this has always been the case\".\nMilitary participation is not currently required in the United States, but a policy of conscription of men has been in place at various times (both in war and in peace) in American history, most recently during the Vietnam War. Currently, the United States Armed Forces are a professional all-volunteer force, although both male United States citizens and male non-citizen permanent residents are required to register with the Selective Service System and may be called up in the event of a future draft. Johns Hopkins University political scientist Benjamin Ginsberg writes, \"The professional military has limited the need for citizen soldiers\".\nTaxes. In the United States today, everyone except those whose income is derived from tax-exempt revenue (Subchapter N, Section 861 of the U.S. Tax Code) is required to file a federal income tax return. U.S. citizens are subject to federal income tax on worldwide income regardless of their country of residence.\nCensus. A response to the decennial United States census is mandated by Article I, Section 2 of the United States Constitution and by Title 13 of the United States Code of all residents. A response to the American Community Survey is also mandated by Title 13, U.S. Code, Sections 141, 193, and 221, as changed by Title 18.\n\n\n=== Benefits ===\nConsular protection outside the United States. While traveling abroad, if a person is arrested or detained by foreign authorities, the person can request to speak to somebody from the United States Embassy or Consulate. Consular officials can provide resources for Americans incarcerated abroad, such as a list of local attorneys who speak English. The United States government may even intervene on the person's behalf. Non-citizen United States nationals also have this benefit.\nIncreased ability to sponsor relatives living abroad. Several types of immigrant visas require that the person requesting the visa be directly related to a United States citizen. Having United States citizenship facilitates the granting of IR and F visas to family members.\nAbility to invest in United States real property without triggering FIRPTA. Perhaps the only quantifiable economic benefit of United States citizenship, citizens are not subject to additional withholding tax on income and capital gains derived from United States real estate under the Foreign Investment in Real Property Tax Act (FIRPTA).\nTransmission of United States citizenship to children born abroad. Generally, children born to two United States citizen parents abroad are automatically United States citizens at birth. When the parents are one United States citizen and one non-United States citizen, certain conditions about the United States citizen's parent's length of time spent in the United States need to be met. Non-citizen United States nationals also have a similar benefit (transmission of non-citizen United States nationality to children born abroad).\nProtection from deportation. Naturalized United States citizens are no longer considered aliens and cannot be placed into deportation proceedings.\nOther benefits. The USCIS sometimes honors the achievements of naturalized United States citizens. The Outstanding American by Choice Award was created by the USCIS to recognize the outstanding achievements of naturalized United States citizens, and past recipients include author Elie Wiesel who won the Nobel Peace Prize; Indra K. Nooyi who was CEO of PepsiCo; John Shalikashvili who was Chairman of the Joint Chiefs of Staff; and others. Further, citizenship status can affect which country an athlete can compete as a member of in competitions such as the Olympics.\n\n\n== Civic participation ==\nCivic participation is not required in the United States. There is no requirement to attend town meetings, belong to a political party, or vote in elections. However, a benefit of naturalization is the ability to \"participate fully in the civic life of the country\". Moreover, to be a citizen means to be vitally important to politics and not ignored. There is disagreement about whether popular lack of involvement in politics is helpful or harmful.\nVanderbilt professor Dana D. Nelson suggests that most Americans merely vote for president every four years, and sees this pattern as undemocratic. In her book Bad for Democracy, Nelson argues that declining citizen participation in politics is unhealthy for long term prospects for democracy.\nHowever, writers such as Robert D. Kaplan in The Atlantic see benefits to non-involvement; he wrote \"the very indifference of most people allows for a calm and healthy political climate\". Kaplan elaborated: \"Apathy, after all, often means that the political situation is healthy enough to be ignored. The last thing America needs is more voters\u2014particularly badly educated and alienated ones\u2014with a passion for politics\". He argued that civic participation, in itself, is not always a sufficient condition to bring good outcomes, and pointed to authoritarian societies such as Singapore which prospered because it had \"relative safety from corruption, from breach of contract, from property expropriation, and from bureaucratic inefficiency\".\n\n\n== Dual citizenship ==\nA person who is considered a citizen by more than one nation has dual citizenship. It is possible for a United States citizen to have dual citizenship; this can be achieved in various ways, such as by birth in the United States to a parent who is a citizen of a foreign country (or in certain circumstances the foreign nationality may be transmitted even by a grandparent) by birth in another country to a parent(s) who is/are a United States citizen/s, or by having parents who are citizens of different countries. Anyone who becomes a naturalized United States citizen is required to renounce any prior \"allegiance\" to other countries during the naturalization ceremony.The State Department states that \"A United States citizen may naturalize in a foreign state without any risk to his or her United States citizenship.\"The earliest recorded instances of dual citizenship began before the French Revolution when the British captured American ships and forced them back to Europe. The British Crown considered subjects from the United States as British by birth and forced them to fight in the Napoleonic Wars.Under certain circumstances there are relevant distinctions between dual citizens who hold a \"substantial contact\" with a country, for example by holding a passport or by residing in the country for a certain period of time, and those who do not. For example, under the Heroes Earnings Assistance and Relief Tax (HEART) Act of 2008, United States citizens in general are subject to an expatriation tax if they give up United States citizenship, but there are exceptions (specifically 26 U.S.C. \u00a7 877A(g)(1)(b)) for those who are either under age 18+1\u20442 upon giving up United States citizenship and have lived in the United States for less than ten years in their lives, or who are dual citizens by birth residing in their other country of citizenship at the time of giving up United States citizenship and have lived in the United States for less than ten out of the past fifteen years. Similarly, the United States considers holders of a foreign passport to have a substantial contact with the country that issued the passport, which may preclude security clearance.\nUnited States citizens are required by federal law to identify themselves with a United States passport, not with any other foreign passport, when entering or leaving the United States. The Supreme Court case of Afroyim v. Rusk, 387 U.S. 253 (1967) declared that a United States citizen did not lose his citizenship by voting in an election in a foreign country, or by acquiring foreign citizenship, if they did not intend to lose United States citizenship. United States citizens who have dual citizenship do not lose their United States citizenship unless they renounce it officially.\n\n\n== History of citizenship in the United States ==\n\nCitizenship began in colonial times as an active relation between men working cooperatively to solve municipal problems and participating actively in democratic decision-making, such as in New England town hall meetings. Men met regularly to discuss local affairs and make decisions. These town meetings were described as the \"earliest form of American democracy\" which was vital since citizen participation in public affairs helped keep democracy \"sturdy\", according to Alexis de Tocqueville in 1835. A variety of forces changed this relation during the nation's history. Citizenship became less defined by participation in politics and more defined as a legal relation with accompanying rights and privileges. While the realm of civic participation in the public sphere has shrunk, the citizenship franchise has been expanded to include not just propertied white adult men but black men and adult women.The Supreme Court affirmed in United States v. Wong Kim Ark, 169 U.S. 649 (1898), that per the Fourteenth Amendment's Citizenship Clause an ethnic Chinese person born in the United States becomes a citizen.  This is distinct from naturalized citizenship; in 1922 the Court held in Ozawa v. United States, 260 U.S. 178, that a Japanese person, born in Japan but resident in the United States for twenty years, could not be naturalized under the law of the time and in 1923 in United States v. Bhagat Singh Thind, 261 U.S. 204, that an Indian person could not be naturalized.  In the Ozawa decision it was noted that \"In all of the naturalization acts from 1790 to 1906 the privilege of naturalization was confined to white persons (with the addition in 1870 of those of African nativity and descent)\", 1906 being the most recent legislation in question at the time.\nThe Equal Nationality Act of 1934 allowed a foreign-born child of a US citizen mother and an alien father, who had entered US territory before age 18 and lived in the United States for five years, to apply for United States citizenship for the first time. It also made the naturalization process quicker for American women's alien husbands. This law equalized expatriation, immigration, naturalization, and repatriation rules between women and men. However, it was not applied retroactively, and was modified by later laws, such as the Nationality Act of 1940.\n\n\n== Birthright citizenship ==\n\nUnited States citizenship is usually acquired by birth when a child is born within the territory of the United States. For the purposes of birthright citizenship, the territory of the United States consists of the 50 U.S. states, the District of Columbia, Guam, Puerto Rico, the Northern Mariana Islands, the United States Virgin Islands, and the Palmyra Atoll. Citizenship, however, was not specified in the original Constitution. In 1868, the Fourteenth Amendment specifically defined persons who were either born or naturalized in the United States and subject to its jurisdiction as citizens. All babies born in the United States\u2014except those born to enemy aliens in wartime or the children of foreign diplomats\u2014enjoy United States citizenship under the Supreme Court's long-standing interpretation of the Fourteenth Amendment regardless of the citizenship or immigration status of their parents. The amendment states: \"All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside.\" There remains dispute as to who is \"subject to the jurisdiction\" of the United States at birth.By acts of Congress, every person born in Puerto Rico, the United States Virgin Islands, Guam, and the Northern Mariana Islands is a United States citizen by birth. Also, every person born in the former Panama Canal Zone whose father or mother (or both) are or were a citizen is a United States citizen by birth.Regardless of where they are born, children of United States citizens are United States citizens in most cases. Children born outside the United States with at least one United States citizen parent usually have birthright citizenship by parentage.\nA child of unknown parentage found in the United States while under the age of five is considered a US citizen unless and until it is proven, before that child reaches the age of twenty-two, the child had not been born in the US.While persons born in the United States are considered to be citizens and can obtain US passports, children under the age of eighteen are legally considered to be minors and cannot vote, stand for, or hold public office. Upon the person's eighteenth birthday, they are considered to be full citizens, although no official ceremony takes place and no correspondence between the government and the new citizen occurs to acknowledge the relation. Citizenship is assumed to exist, and the relation is assumed to remain viable until death or until it is renounced or dissolved by some other legal process. Secondary schools ideally teach the basics of citizenship and create \"informed and responsible citizens\" who are \"skilled in the arts of effective deliberation and action.\"Americans who live in foreign countries and become members of other governments have, in some instances, been stripped of citizenship, although there have been court cases where decisions regarding citizenship have been reversed.\n\n\n== Naturalized citizenship ==\nArticle I, Section 8 of the U.S. constitution gives Congress the power \"To establish an uniform Rule of Naturalization\". Acts of Congress provide for acquisition of citizenship by persons not born in the U.S.\n\n\n=== Agency in charge ===\nThe agency in charge of admitting new citizens is the United States Citizenship and Immigration Services, commonly abbreviated as USCIS. It is a bureau of the Department of Homeland Security. It offers web-based services. The agency depends on application fees for revenue; in 2009, with a struggling economy, applications were down sharply, and consequently there was much less revenue to upgrade and streamline services. There was speculation that if the administration of president Barack Obama passed immigration reform measures, then the agency could face a \"welcome but overwhelming surge of Americans-in-waiting\" and longer processing times for citizenship applications. The USCIS has made efforts to digitize records. A USCIS website says the \"United States Citizenship and Immigration Services (USCIS) is committed to offering the best possible service to you, our customer\" and which says \"With our focus on customer service, we offer you a variety of services both before and after you file your case\". The website allowed applicants to estimate the length of time required to process specific types of cases, to check application status, and to access a customer guide. The USCIS processes cases in the order they're received.\n\n\n=== Pathways to citizenship ===\nPeople applying to become United States citizens must satisfy certain requirements. For example, applicants must generally have been permanent residents for five years (three if married to a United States citizen), be of \"good moral character\" (meaning no felony convictions), be of \"sound mind\" in the judgment of immigration officials, have a knowledge of the Constitution, and be able to speak and understand English unless they are elderly or disabled. Applicants must also pass a citizenship test. Until recently, a test published by the Immigration and Naturalization Service asked questions such as \"How many stars are there in our flag?\" and \"What is the Constitution?\" and \"Who is the president of the United States today?\" At one point, the Government Printing Office sold flashcards for US$8.50 to help test-takers prepare for the test. In 2006, the government replaced the former trivia test with a ten-question oral test designed to \"shun simple historical facts about America that can be recounted in a few words, for more explanation about the principles of American democracy, such as freedom\". One reviewer described the new citizenship test as \"thoughtful\". While some have criticized the new version of the test, officials counter that the new test is a \"teachable moment\" without making it conceptually more difficult, since the list of possible questions and answers, as before, will be publicly available. Six correct answers constitute a passing grade. The new test probes for signs that immigrants \"understand and share American values\".\nOne way to become a permanent resident is to apply to the US government Diversity Visa (DV) lottery. This program permits foreigners to apply for a drawing to become a permanent resident.\nMilitary participation can also allow immigrant residents to become citizens. The military has had a tradition of \"filling out its ranks\" with aliens living in the United States. The financial and social benefits of citizenship can motivate persons to participate in potentially hazardous activities such as military service. For example, a 2009 article in The New York Times said that the United States Military was recruiting \"skilled immigrants who are living in this country with temporary visas\" by promising an opportunity to become citizens \"in as little as six months\" in exchange for service in Afghanistan and Iraq where United States forces are \"stretched thin\".  One estimate was that in 2009 the US military had 29,000 foreign-born people currently serving who were not American citizens. In 2003, of 1.4 million service members, 37,000 active-duty members were not citizens, and of these, 20% had applied for citizenship. In 2002, President Bush signed an executive order to eliminate the three-year waiting period and made service personnel immediately eligible for citizenship. In 2003, Congress voted to \"cut the waiting period to become a citizen from three years down to one year\" for immigrants who had served in the armed forces. Spouses of citizens or non-citizens who served in the military also become citizens more quickly. The option was not open to illegal immigrants. One analyst noted that \"many immigrants, not yet citizens, have volunteered to serve in the United States military forces ... Some have been killed and others wounded ... Perhaps this can be seen as a cynical attempt to qualify more easily for United States citizenship ... But I think that service in the United States military has to be taken as a pretty serious commitment to the United States\". By June 2003, twelve non-citizens had died while on active duty in the United States armed forces during the Iraqi war.\nGrandparent rule. Section 322 of the Immigration and Nationality Act of 1952 (INA), added in 1994, enabled children of a United States citizen who did not become citizens at birth, to use the physical presence period in the United States of a grandparent who was a citizen to qualify for United States citizenship. Under the Child Citizenship Act of 2000, Section 322 was amended to extend also to children who generally reside outside the United States with a United States citizen parent, whether biological or adopted. The child must be in the legal and physical custody of the United States citizen parent, the child and parent must be lawfully present in the United States for the interview, and the child must take the oath of allegiance before the age of 18 years (for those 14 years or older). The application (Form N-600K) may only be submitted by the United States citizen parent, or by the grandparent or legal guardian within 5 years of the parent's death. In 2006, there were 4,000 applications of citizenship using the physical presence of grandparents. Israel comprises 90% of those taking advantage of the clause.\n\n\n=== Strong demand ===\nAccording to a senior fellow at the Migration Policy Institute, \"citizenship is a very, very valuable commodity\". However, one study suggested legal residents eligible for citizenship, but who don't apply, tend to have low incomes (41%), do not speak English well (60%), or have low levels of education (25%). There is strong demand for citizenship based on the number of applications filed. From 1920 to 1940, the number of immigrants to the United States who became citizens numbered about 200,000 each year; there was a spike after World War II, and then the level reduced to about 150,000 per year until resuming to the 200,000 level beginning about 1980. In the mid-1990s to 2009, the levels rose to about 500,000 per year with considerable variation. In 1996, more than one million people became citizens through naturalization. In 1997, there were 1.41 million applications filed; in 2006, 1.38 million. The number of naturalized citizens in the United States rose from 6.5 million in the mid-1990s to 11 million in 2002. By 2003, the pool of immigrants eligible to become naturalized citizens was 8 million, and of these, 2.7 million lived in California. In 2003, the number of new citizens from naturalization was 463,204. In 2007, the number was 702,589. In 2007, 1.38 million people applied for citizenship creating a backlog. In 2008, applications decreased to 525,786.Naturalization fees were US$60 in 1989; US$90 in 1991; US$95 in 1994; US$225 in 1999; US$260 in 2002; US$320 in 2003; US$330 in 2005. In 2007 application fees were increased from US$330 to US$595 and an additional US$80 computerized fingerprinting fee was added. The biometrics fee was increased to US$85 in 2010. On December 23, 2014, the application fees were increased again from US$595 to US$640. The high fees have been criticized as putting up one more wall to citizenship. Increases in fees for citizenship have drawn criticism. Doris Meissner, a senior fellow at the Migration Policy Institute and former Immigration and Naturalization Service Commissioner, doubted that fee increases deter citizenship-seekers. In 2009, the number of immigrants applying for citizenship plunged 62%; reasons cited were the slowing economy and the cost of naturalization.\n\n\n=== Citizenship ceremonies ===\nThe citizenship process has been described as a ritual that is meaningful for many immigrants. Many new citizens are sworn in during Independence Day ceremonies. Most citizenship ceremonies take place at offices of the United States Citizenship and Immigration Services. However, one swearing-in ceremony was held at Arlington National Cemetery in Virginia in 2008. The judge who chose this venue explained: \"I did it to honor our country's warriors and to give the new citizens a sense for what makes this country great\". According to federal law, citizenship applicants who are also changing their names must appear before a federal judge.\n\n\n== Honorary citizenship ==\nThe title of \"Honorary Citizen of the United States\" has been granted eight times by an act of Congress or by a proclamation issued by the president pursuant to authorization granted by Congress. The eight individuals are Sir Winston Churchill, Raoul Wallenberg, William Penn, Hannah Callowhill Penn, Mother Teresa, the Marquis de Lafayette, Casimir Pulaski, and Bernardo de G\u00e1lvez y Madrid, Viscount of Galveston and Count of G\u00e1lvez.\nSometimes, the government awarded non-citizen immigrants who died fighting for American forces with the posthumous title of United States citizen, but this is not considered honorary citizenship. In June 2003, Congress approved legislation to help families of fallen non-citizen soldiers.\n\n\n== Corporate citizenship ==\nThere is a sense in which corporations can be considered \"citizens\". Since corporations are considered persons in the eyes of the law, it is possible to think of corporations as being like citizens. For example, the airline Virgin America asked the United States Department of Transportation to be treated as an American air carrier. The advantage of \"citizenship\" is having the protection and support of the United States government when jockeying with foreign governments for access to air routes and overseas airports. Alaska Airlines, a competitor of Virgin America, asked for a review of the situation; according to United States law, \"foreign ownership in a United States air carrier is limited to 25% of the voting interest in the carrier\", but executives at Virgin America insisted the airline met this requirement.For the purposes of diversity jurisdiction in the United States civil procedure, corporate citizenship is determined by the principal place of business of the corporation. There is some degree of disagreement among legal authorities as to how exactly this may be determined.Another sense of \"corporate citizenship\" is a way to show support for causes such as social issues and the environment and, indirectly, gain a kind of \"reputational advantage\".\n\n\n== Distinction between citizenship and nationality ==\nThe Immigration and Nationality Act of 1952 made a distinction between \"citizenship\" and \"nationality\" of the United States: all United States citizens are also United States nationals, but not all U.S. nationals are also U.S. citizens. Hence, it is possible for a person to be a national of the United States, but not a U.S. citizen.\n\n\n=== Historic and current grants of non-citizen nationality ===\nThe federal government of the United States takes the position that unincorporated territories of the United States are not \"in the United States\" for purposes of the Citizenship Clause of the Fourteenth Amendment to the U.S. Constitution, which grants U.S. citizenship at birth to people born in the United States. Hence, people born in an unincorporated territory of the United States are U.S. citizens at birth only if Congress has passed a citizenship statute for that territory; otherwise, they become non-citizen U.S. nationals at birth instead, as per 8 U.S.C. \u00a7 1408.\nCurrently, American Samoa is the only unincorporated territory of the United States where newborn infants become non-citizen U.S. nationals at birth. Although international law and Supreme Court dicta would regard persons born in a United States Minor Outlying Island as non-citizen nationals of the United States, the nationality status of these persons is not specifically mentioned by US law.The U.S. government position regarding American Samoa began to be challenged in court in the 2010s. A 2016 ruling by the D.C. Circuit Court upheld the government's position that American Samoa is not \"in the United States\" for purposes of the Fourteenth Amendment and thus American Samoans are nationals but not citizens at birth,  A 2021 ruling by the 10th Circuit Court of Appeals similarly upheld the government's position and reversed a lower court ruling that said American Samoan plaintiffs were United States citizens at birth.Unlike people born in American Samoa, people born in Puerto Rico, Guam, the United States Virgin Islands and the Northern Mariana Islands (on or after November 4, 1986) have United States citizenship at birth, as Congress has granted this status by law. People born in the Northern Mariana Islands before November 4, 1986, automatically gained U.S. citizenship on that date, but they could choose to give up U.S. citizenship and become non-citizen U.S. nationals within 6 months after the later of November 4, 1986 or the date they turned 18 years old.\n\n\n=== Legal distinctions between United States citizens and non-citizen nationals ===\nUnited States citizenship grants more privileges and rights than non-citizen United States nationality. For example, while non-citizen U.S. nationals can reside and work in the United States without restrictions, both they and foreign nationals and citizens are not allowed to vote in federal or state elections, although there is no constitutional prohibition against their doing so. By statute law, most non-citizen U.S. nationals pass their U.S. nationality to children born outside the United States, similarly to U.S. citizens.Non-citizen U.S. nationals can apply for naturalization if they want to become U.S. citizens. In order to be naturalized, non-citizen U.S. nationals must meet similar requirements to foreign nationals, meaning non-citizen nationals must pay a US$640 fee (as of May 29, 2023), pass a good moral character assessment, be fingerprinted and pass an English and civics examination. However, unlike foreign nationals, non-citizen U.S. nationals do not need to hold permanent residency of the U.S. when they apply for citizenship, and they can count their legal residence and physical presence in unincorporated U.S. territories the same as presence in the U.S. proper toward the naturalization requirements.The United States passport issued to non-citizen nationals of the United States contains the endorsement code 9 which states: \"The bearer is a United States national and not a United States citizen\" on the annotations page.\n\n\n== Controversies ==\nThe issue of citizenship naturalization is a highly contentious matter in United States politics, particularly regarding illegal immigrants. Candidates in the 2008 presidential election, such as Rudolph Giuliani, tried to \"carve out a middle ground\" on the issue of illegal immigration, but rivals such as John McCain advocated legislation requiring illegal immigrants to first leave the country before being eligible to apply as citizens. Some measures to require proof of citizenship upon registering to vote have met with controversy.Controversy can arise when citizenship affects political issues. Whether to include questions about current citizenship status in the United States Census questions has been debated in the Senate. Census data affects state electoral clout; it also affects budgetary allocations. Including non-citizens in Census counts also shifts political power to states that have large numbers of non-citizens due to the fact that reapportionment of congressional seats is based on Census data, and including non-citizens in the census is mandated by the United States Constitution.There have been controversies based on speculation about which way newly naturalized citizens are likely to vote. Since immigrants from many countries have been presumed to vote Democratic if naturalized, there have been efforts by Democratic administrations to streamline citizenship applications before elections to increase turnout; Republicans, in contrast, have exerted pressure to slow down the process. In 1997, there were efforts to strip the citizenship of 5,000 newly approved immigrants who, it was thought, had been \"wrongly naturalized\"; a legal effort to do this presented enormous challenges. An examination by the Immigration and Naturalization Service of 1.1 million people who were granted citizenship from September 1995 to September 1996 found 4,946 cases in which a criminal arrest should have disqualified an applicant or in which an applicant lied about his or her criminal history. Before the 2008 election, there was controversy about the speed of the USCIS in processing applications; one report suggested that the agency would complete 930,000 applications in time for the newly processed citizens to vote in the November 2008 election. Foreign-born naturalized citizens tend to vote at the same rates as natives. For example, in the state of New Jersey in the 2008 election, the foreign born represented 20.1% of the state's population of 8,754,560; of these, 636,000 were eighteen or older and hence eligible to vote; of eligible voters, 396,000 actually voted, which was about 62%. So foreign-born citizens vote in roughly the same proportion (62%) as native citizens (67%).There has been controversy about the agency in charge of citizenship. The USCIS has been criticized as being a \"notoriously surly, inattentive bureaucracy\" with long backlogs in which \"would-be citizens spent years waiting for paperwork\". Rules made by Congress and the federal government regarding citizenship are highly technical and often confusing, and the agency is forced to cope with enforcement within a complex regulatory milieu. There have been instances in which applicants for citizenship have been deported on technicalities. One Pennsylvania doctor and his wife, both from the Philippines, who applied for citizenship, and one Mr. Darnell from Canada who was married to an American with two children from this marriage, ran afoul of legal technicalities and faced deportation. The New York Times reported that \"Mr. Darnell discovered that a 10-year-old conviction for domestic violence involving a former girlfriend, even though it had been reduced to a misdemeanor and erased from his public record, made him ineligible to become a citizen \u2014 or even to continue living in the United States\". Overworked federal examiners under pressure to make \"quick decisions\" as well as \"weed out security risks\" have been described as preferring \"to err on the side of rejection\". In 2000, 399,670 applications were denied (about 1\u20443 of all applications); in 2007, 89,683 applications for naturalization were denied, about 12% of those presented.Generally, eligibility for citizenship is denied for the millions of people living in the United States illegally, although from time to time, there have been amnesties. In 2006, there were mass protests numbering hundreds of thousands of people throughout the United States demanding United States citizenship for illegal immigrants. Many carried banners which read \"We Have A Dream Too\". One estimate is that there were 12 million illegal immigrants in the United States in 2006. Many American high school students have citizenship issues. In 2008, it was estimated that there were 65,000 illegal immigrant students. The number was less clear for post-secondary education. A 1982 Supreme Court decision, Plyler v. Doe 457 U.S. 202 (1982), entitled illegal immigrants to free education from kindergarten through high school. Undocumented immigrants who get arrested face difficulties in the courtroom as they have no constitutional right to challenge the outcome of their deportation hearings. In 2009, writer Tom Barry of the Boston Review criticized the crackdown against illegal immigrants since it \"flooded the federal courts with nonviolent offenders, besieged poor communities, and dramatically increased the United States prison population, while doing little to solve the problem itself\". Barry criticized the United States' high incarceration rate as being \"fives times greater than the average rate in the rest of the world\". Virginia senator Jim Webb agreed that \"we are doing something dramatically wrong in our criminal justice system\".\n\n\n== Relinquishment of citizenship ==\n\nUnited States citizens can relinquish their citizenship, which involves abandoning the right to reside in the United States and all the other rights and responsibilities of citizenship. \"Relinquishment\" is the legal term covering all seven different potentially-expatriating acts (ways of giving up citizenship) under 8 U.S.C. \u00a7 1481(a). \"Renunciation\" refers to two of those acts: swearing an oath of renunciation before a United States diplomatic or consular officer abroad, or before an official designated by the attorney general within the United States during a state of war. Out of an estimated three to six million United States citizens residing abroad, between five and six thousand relinquished citizenship each year in 2015 and 2016. United States nationality law treats people who performs potentially-expatriating acts with intent to give up United States citizenship as ceasing to be United States citizens from the moment of the act, but United States tax law since 2004 treats such individuals as though they remain United States citizens until they notify the State Department and apply for a Certificate of Loss of Nationality (CLN).Renunciation requires an oath to be sworn before a State Department officer and thus involves in-person attendance at an embassy or consulate, but applicants for CLNs on the basis of other potentially-expatriating acts must attend an in-person interview as well. During the interview, a State Department official assesses whether the person acted voluntarily, intended to abandon all rights of United States citizenship, and understands the consequences of their actions. The State Department strongly recommends that Americans intending to relinquish citizenship have another citizenship, but will permit Americans to make themselves stateless if they understand the consequences. There is a US$2,350 administrative fee for the process. In addition, an expatriation tax is imposed on some individuals relinquishing citizenship, but payment of the tax is not a legal prerequisite for relinquishing citizenship; rather, the tax and its associated forms are due on the normal tax due date of the year following relinquishment of citizenship. State Department officials do not seek to obtain any tax information from the interviewee, and instruct the interviewee to contact the IRS directly with any questions about taxes.\n\n\n== Revocation of citizenship ==\nCitizenship can be revoked under certain circumstances. For instance, if held that a naturalized person has concealed material evidence, willfully misrepresented themselves, or engaged in subversive activities, then they may have their naturalization revoked.A citizen does not lose United States citizenship when they perform such acts like seeking office in a foreign state. However, the higher office and more important role a citizen holds in a foreign government, the more limited the exercise of consular rights of United States citizenship will be: \"Serving as a foreign head of state/government or foreign minister may affect the level of immunity from United States jurisdiction that a dual national may be afforded. All such cases should be referred to the Office of the Assistant Legal Adviser for Consular Affairs\".From September 22, 1922, to the passage of Nationality Act of 1940, a woman holding United States citizenship could lose it simply by marriage to an alien or certain aliens ineligible for citizenship.\n\n\n== See also ==\nAccidental American\nAnchor baby\nBirth tourism\nBirthright citizenship in the United States of America\nBirthright generation\nCitizenship (general discussion for all nations)\nCitizenship education\nDREAM Act\nHistory of citizenship\nJus soli\nNatural born citizen of the United States\nUndocumented students in the United States\nUndocumented youth in the United States\nUnited States nationality law\n\n\n== Explanatory footnotes ==\n\n\n== References ==\n\n\n== Further reading ==\nAbdelfatah, Rund; Ramtin Arablouei (June 9, 2022). \"By Accident of Birth\". Throughline. Retrieved September 3, 2023.\nFrost, Amanda (2021). You Are Not American: Citizenship Stripping from Dred Scott to the Dreamers. Boston: Beacon Press. ISBN 9780807051429. OCLC 1164826057.\nNackenoff, Carol (2021). American by Birth: Wong Kim Ark and the Battle for Citizenship (First/Unabridged ed.). Lawrence, Kansas: University Press of Kansas. ISBN 9780700631926. OCLC 1195815540.\n\n\n== External links ==\nCitizenship and the American Empire, Noted on the Legislative History of the United States Citizenship of Puerto Ricans"}, {"id": 14, "title": "Social media marketing", "content": "Social media marketing is the use of social media platforms and websites to promote a product or service. Although the terms e-marketing and digital marketing are still dominant in academia, social media marketing is becoming more popular for both practitioners and researchers. Most social media platforms have built-in data analytics tools, enabling companies to track the progress, success, and engagement of ad campaigns. Companies address a range of stakeholders through social media marketing, including current and potential customers, current and potential employees, journalists, bloggers, and the general public. On a strategic level, social media marketing includes the management of a marketing campaign, governance, setting the scope (e.g. more active or passive use) and the establishment of a firm's desired social media \"culture\" and \"tone\".\nWhen using social media marketing, firms can allow customers and Internet users to post user-generated content (e.g., online comments, product reviews, etc.), also known as \"earned media\", rather than use marketer-prepared advertising copy.\n\n\n== Platforms ==\n\n\n=== Social networking websites ===\nSocial networking websites allow individuals, businesses and other organizations to interact with one another and build relationships and communities online. When companies join these social channels, consumers can interact with them directly. That interaction can be more personal to users than traditional methods of outbound marketing and advertising. Social networking sites act as word of mouth or more precisely, e-word of mouth. The Internet's ability to reach billions across the globe has given online word of mouth a powerful voice and far reach. The ability to rapidly change buying patterns and product or service acquisition and activity to a growing number of consumers is defined as an influence network. Social networking sites and blogs allow followers to \"retweet\" or \"repost\" comments made by others about a product being promoted, which occurs quite frequently on some social media sites. By repeating the message, the user's connections are able to see the message, therefore reaching more people. Because the information about the product is being put out there and is getting repeated, more traffic is brought to the product/company.Social networking websites are based on building virtual communities that allow consumers to express their needs, wants and values, online. Social media marketing then connects these consumers and audiences to businesses that share the same needs, wants, and values. Through social networking sites, companies can keep in touch with individual followers. This personal interaction can instill a feeling of loyalty into followers and potential customers. Also, by choosing whom to follow on these sites, products can reach a very narrow target audience. Social networking sites also include much information about what products and services prospective clients might be interested in. Through the use of new semantic analysis technologies, marketers can detect buying signals, such as content shared by people and questions posted online. An understanding of buying signals can help sales people target relevant prospects and marketers run micro-targeted campaigns.\nIn 2014, over 80% of business executives identified social media as an integral part of their business. Business retailers have seen 133% increases in their revenues from social media marketing.Some examples of popular social networking websites over the years are Facebook, Instagram, Twitter, TikTok, Myspace, LinkedIn, and Snapchat.\n\n\n=== Mobile phones ===\nMore than three billion people in the world are active on the Internet. Over the years, the Internet has continually gained more and more users, jumping from 738 million in 2000 all the way to 3.2 billion in 2015. Roughly 81% of the current population in the United States has some type of social media profile that they engage with frequently. Mobile phone usage is beneficial for social media marketing because of their web browsing capabilities which allow individuals immediate access to social networking sites. Mobile phones have altered the path-to-purchase process by allowing consumers to easily obtain pricing and product information in real time. They have also allowed companies to constantly remind and update their followers. Many companies are now putting QR (Quick Response) codes along with products for individuals to access the company website or online services with their smart phones. Retailers use QR codes to facilitate consumer interaction with brands by linking the code to brand websites, promotions, product information, and any other mobile-enabled content. In addition, Real-time bidding use in the mobile advertising industry is high and rising due to its value for on-the-go web browsing. In 2012, Nexage, a provider of real time bidding in mobile advertising reported a 37% increase in revenue each month. Adfonic, another mobile advertisement publishing platform, reported an increase of 22 billion ad requests that same year.Mobile devices have become increasingly popular, where 5.7 billion people are using them worldwide. This has played a role in the way consumers interact with media and has many further implications for TV ratings, advertising, mobile commerce, and more. Mobile media consumption such as mobile audio streaming or mobile video are on the rise \u2013 In the United States, more than 100 million users are projected to access online video content via mobile device. Mobile video revenue consists of pay-per-view downloads, advertising and subscriptions. As of 2013, worldwide mobile phone Internet user penetration was 73.4%. In 2017, figures suggest that more than 90% of Internet users will access online content through their phones.\n\n\n== Strategies ==\nThere are two basic strategies for using social media as a marketing tool:\n\n\n=== Passive approach ===\nSocial media can be a useful source of market information and a way to hear customer perspectives. Blogs, content communities, and forums are platforms where individuals share their reviews and recommendations of brands, products, and services. Businesses are able to tap and analyze the customer voices and feedback generated in social media for marketing purposes; in this sense the social media is a relatively inexpensive source of market intelligence which can be used by marketers and managers to track and respond to consumer-identified problems and detect market opportunities. For example, the Internet erupted with videos and pictures of iPhone 6 \"bend test\" which showed that the coveted phone could be bent by hand pressure. The so-called \"bend gate\" controversy created confusion amongst customers who had waited months for the launch of the latest rendition of the iPhone. However, Apple promptly issued a statement saying that the problem was extremely rare and that the company had taken several steps to make the mobile device's case stronger and robust. Unlike traditional market research methods such as surveys, focus groups, and data mining which are time-consuming and costly, and which take weeks or even months to analyze, marketers can use social media to obtain 'live' or \"real time\" information about consumer behavior and viewpoints on a company's brand or products. This can be useful in the highly dynamic, competitive, fast-paced and global marketplace of the 2010s.\n\n\n=== Active approach ===\nSocial media can be used not only as public relations and direct marketing tools, but also as communication channels targeting very specific audiences with social media influencers and social media personalities as effective customer engagement tools.  This tactic is widely known as influencer marketing. Influencer marketing allows brands the opportunity to reach their target audience in a more genuine, authentic way via a special group of selected influencers advertising their product or service. In fact, brands are set to spend up to $15 billion on influencer marketing by 2022, per Business Insider Intelligence estimates, based on Mediakix data.Technologies predating social media, such as broadcast TV and newspapers can also provide advertisers with a fairly targeted audience, given that an ad placed during a sports game broadcast or in the sports section of a newspaper is likely to be read by sports fans. However, social media websites can target niche markets even more precisely. Using digital tools such as Google AdSense, advertisers can target their ads to very specific demographics, such as people who are interested in social entrepreneurship, political activism associated with a particular political party, or video gaming. Google AdSense does this by looking for keywords in social media user's online posts and comments. It would be hard for a TV station or paper-based newspaper to provide ads that are this targeted (though not impossible, as can be seen with \"special issue\" sections on niche issues, which newspapers can use to sell targeted ads).\nSocial networks are, in many cases, viewed as a great tool for avoiding costly market research. They are known for providing a short, fast, and direct way to reach an audience through a person who is widely known. For example, an athlete who gets endorsed by a sporting goods company also brings their support base of millions of people who are interested in what they do or how they play and now they want to be a part of this athlete through their endorsements with that particular company. At one point consumers would visit stores to view their products with famous athletes, but now you can view a famous athlete's, such as Cristiano Ronaldo, latest apparel online with the click of a button. He advertises them to you directly through his Twitter, Instagram, and Facebook accounts.\nFacebook and LinkedIn are leading social media platforms where users can hyper-target their ads. Hypertargeting not only uses public profile information but also information users submit but hide from others. There are several examples of firms initiating some form of online dialog with the public to foster relations with customers. According to Constantinides, Lorenzo and G\u00f3mez Borja (2008) \"Business executives like Jonathan Swartz, President and CEO of Sun Microsystems, Steve Jobs CEO of Apple Computers, and McDonald's Vice President Bob Langert post regularly in their CEO blogs, encouraging customers to interact and freely express their feelings, ideas, suggestions, or remarks about their postings, the company or its products\". Using customer influencers (for example popular bloggers) can be a very efficient and cost-effective method to launch new products or services\nAmong the political leaders in office, Prime Minister Narendra Modi has the highest number of followers at 40 million, and President Donald Trump ranks second with 25 million followers. Modi employed social media platforms to circumvent traditional media channels to reach out to the young and urban population of India which is estimated to be 200 million.\n\n\n=== Algorithms ===\nSocial media content that has been driven by algorithms has become an increasingly popular feature in recent years.One social media platform that has used this ground-changing strategy is TikTok. TikTok has become one of the fastest growing applications to date and currently has around 1.5 billion users, mainly consisting of children and teenagers. The algorithm used within this platform encourages creativity among TikTok users because of the platform's wide range of effects and challenges that change from day to day. Because of this feature, content creators big or small have increased chances of going viral by appearing on TikTok's \"for you\" page. The \"for you\" page algorithm allows users to have videos recommended to them based on their previous watches, likes and shares.This can be extremely beneficial for small businesses who are using this platform as a means of social media marketing. Although they may be starting off small, by following trends, using hashtags, and much more, anyone can promote themselves on this emerging application to attract new audiences from all around the world. Moreover, using algorithmically driven content within TikTok allows for a more positive response rate from users as the target audience tends to be young users, who are more susceptible to these increasingly popular marketing communications. With this in mind, TikTok is filled with rich content that include images and videos which can beneficially aid influencer marketing over platforms that are heavily text-based as they are less engaging for their audiences.When considering algorithms in respect to social media platforms, it is evident they can heavily impact the outcomes of this marketing strategy. Since it is a new content feature, younger audiences may respond more positively to it than others, but it is not something that should be disregarded for other audiences. Anyone is able to use this marketing strategy to get themselves out there and engage with new customers or users because of the effective algorithm technique.\n\n\n== Engagement ==\nEngagement with the social web means that customers and stakeholders are active participants rather than passive viewers. An example of these are consumer advocacy groups and groups that criticize companies (e.g., lobby groups or advocacy organizations). Social media use in a business or political context allows all consumers/citizens to express and share an opinion about a company's products, services, business practices, or a government's actions. Each participating customer, non-customer, or citizen who is participating online via social media becomes a part of the marketing department (or a challenge to the marketing effort) as other customers read their positive or negative comments or reviews. Getting consumers, potential consumers or citizens to be engaged online is fundamental to successful social media marketing. With the advent of social media marketing, it has become increasingly important to gain customer interest in products and services. This can eventually be translated into buying behavior, or voting and donating behavior in a political context. New online marketing concepts of engagement and loyalty have emerged which aim to build customer participation and brand reputation.Engagement in social media for the purpose of a social media strategy is divided into two parts. The first is proactive, regular posting of new online content. This can be seen through digital photos, digital videos, text, and conversations. It is also represented through sharing of content and information from others via weblinks. The second part is reactive conversations with social media users responding to those who reach out to your social media profiles through commenting or messaging. Traditional media such as TV news shows are limited to one-way interaction with customers or 'push and tell' where only specific information is given to the customer with few or limited mechanisms to obtain customer feedback. Traditional media such as physical newspapers, do give readers the option of sending a letter to the editor. Though, this is a relatively slow process, as the editorial board has to review the letter and decide if it is appropriate for publication. On the other hand, social media is participative and open; Participants are able to instantly share their views on brands, products, and services. Traditional media gave control of message to the marketer, whereas social media shifts the balance to the consumer or citizen.\n\n\n== Campaigns ==\n\n\n=== Local businesses ===\nSmall businesses also use social networking sites as a promotional technique. Businesses can follow individuals social networking site uses in the local area and advertise specials and deals. These can be exclusive and in the form of \"get a free drink with a copy of this tweet\". This type of message encourages other locals to follow the business on the sites in order to obtain the promotional deal. In the process, the business is getting seen and promoting itself (brand visibility).\nSmall businesses also use social networking sites to develop their own market research on new products and services.  By encouraging their customers to give feedback on new product ideas, businesses can gain valuable insights on whether a product may be accepted by their target market enough to merit full production, or not. In addition, customers will feel the company has engaged them in the process of co-creation\u2014the process in which the business uses customer feedback to create or modify a product or service the filling a need of the target market.  Such feedback can present in various forms, such as surveys, contests, polls, etc.\nSocial networking sites such as LinkedIn, also provide an opportunity for small businesses to find candidates to fill staff positions.Of course, review sites, such as Yelp, also help small businesses to build their reputation beyond just brand visibility. Positive customer peer reviews help to influence new prospects to purchase goods and services more than company advertising.\n\n\n=== Nike #MakeItCount ===\nIn early 2012, Nike introduced its Make It Count social media campaign. The campaign kickoff began YouTubers Casey Neistat and Max Joseph launching a YouTube video, where they traveled 34,000 miles to visit 16 cities in 13 countries. They promoted the #makeitcount hashtag, which millions of consumers shared via Twitter and Instagram by uploading photos and sending tweets. The #MakeItCount YouTube video went viral and Nike saw an 18% increase in profit in 2012, the year this product was released.\n\n\n== Benefits of social media marketing ==\nPossible benefits of social media marketing include:\n\nAllows companies to promote themselves to large, diverse audiences that could not be reached through traditional marketing such as phone and email-based advertising.\nMarketing on most social media platforms comes at little to no cost- making it accessible to virtually any size business.\nAccommodates personalized and direct marketing that targets specific demographics and markets.\nCompanies can engage with customers directly, allowing them to obtain feedback and resolve issues almost immediately.\nIdeal environment for a company to conduct market research.\nCan be used as a means of obtaining information about competitors and boost competitive advantage.\nSocial platforms can be used to promote brand events, deals, and news.\nSocial platforms can also be used to offer incentives in the form of loyalty points and discounts.\n\n\n== Purposes and tactics ==\nOne of the main purposes of employing social media in marketing is as a communications tool that makes the companies accessible to those interested in their product and makes them visible to those who have no knowledge of their products. These companies use social media to create buzz, and learn from and target customers. It's the only form of marketing that can finger consumers at each and every stage of the consumer decision journey. Marketing through social media has other benefits as well. Of the top 10 factors that correlate with a strong Google organic search, seven are social media dependent. This means that if brands are less or non-active on social media, they tend to show up less on Google searches. While platforms such as Twitter, Facebook and Google+ have a larger number of monthly users, the visual media sharing based mobile platforms, however, garner a higher interaction rate in comparison and have registered the fastest growth and have changed the ways in which consumers engage with brand content. Instagram has an interaction rate of 1.46% with an average of 130 million users monthly as opposed to Twitter which has a .03% interaction rate with an average of 210 million monthly users.  Unlike traditional media that are often cost-prohibitive to many companies, a social media strategy does not require astronomical budgeting.To this end, companies make use of platforms such as Facebook, Twitter, YouTube, TikTok and Instagram  to reach audiences much wider than through the use of traditional print/TV/radio advertisements alone at a fraction of the cost, as most social networking sites can be used at little or no cost (however, some websites charge companies for premium services). This has changed the ways that companies approach to interact with customers, as a substantial percentage of consumer interactions are now being carried out over online platforms with much higher visibility. Customers can now post reviews of products and services, rate customer service, and ask questions or voice concerns directly to companies through social media platforms. According to Measuring Success, over 80% of consumers use the web to research products and services. Thus social media marketing is also used by businesses in order to build relationships of trust with consumers. To this aim, companies may also hire personnel to specifically handle these social media interactions, who usually report under the title of Online community managers. Handling these interactions in a satisfactory manner can result in an increase of consumer trust. To both this aim and to fix the public's perception of a company, 3 steps are taken in order to address consumer concerns, identifying the extent of the social chatter, engaging the influencers to help, and developing a proportional response.\n\n\n=== Twitter ===\nTwitter allows companies to promote their products in short messages known as tweets limited to 280 characters which appear on followers' Home timelines. Tweets can contain text, Hashtag, photo, video, Animated GIF, Emoji, or links to the product's website and other social media profiles, etc. Twitter is also used by companies to provide customer service. Some companies make support available 24/7 and answer promptly, thus improving brand loyalty and appreciation.\n\n\n=== Facebook ===\nFacebook pages are far more detailed than Twitter accounts. They allow a product to provide videos, photos, longer descriptions, and testimonials where followers can comment on the product pages for others to see. Facebook can link back to the product's Twitter page, as well as send out event reminders. As of May 2015, 93% of businesses marketers use Facebook to promote their brand.  Facebooks original owner, Mark Zuckerberg created Meta in 2004 which is a new brand for Facebook apps and features. Meta is the parent company of Facebook. [1]\nA study from 2011 attributed 84% of \"engagement\" or clicks and likes that link back to Facebook advertising. By 2014, Facebook had restricted the content published from business and brand pages. Adjustments in Facebook algorithms have reduced the audience for non-paying business pages (that have at least 500,000 \"Likes\") from 16% in 2012 down to 2% in February 2014.\n\n\n=== LinkedIn ===\nLinkedIn, a professional business-related networking site, allows companies to create professional profiles for themselves as well as their business to network and meet others. Through the use of widgets, members can promote their various social networking activities, such as Twitter stream or blog entries of their product pages, onto their LinkedIn profile page.  LinkedIn provides its members the opportunity to generate sales leads and business partners.  Members can use \"Company Pages\" similar to Facebook pages to create an area that will allow business owners to promote their products or services and be able to interact with their customers.\n\n\n=== Whatsapp ===\nWhatsApp was founded by Jan Koum and Brian Acton. Joining Facebook in 2014, WhatsApp continues to operate as a separate app with a laser focus on building a messaging service that works fast and reliably anywhere in the world. Started as an alternative to SMS, WhatsApp now supports sending and receiving a variety of media including text, photos, videos, documents, and location, as well as voice calls. WhatsApp messages and calls are secured with end-to-end encryption, meaning that no third party including WhatsApp can read or listen to them. WhatsApp has a customer base of 1 billion people in over 180 countries. It is used to send personalised promotional messages to individual customers. It has plenty of advantages over SMS that includes ability to track how Message Broadcast Performs using blue tick option in WhatsApp. It allows sending messages to Do Not Disturb (DND) customers. WhatsApp is also used to send a series of bulk messages to their targeted customers using broadcast option. Companies started using this to a large extent because it is a cost-effective promotional option and quick to spread a message. As of 2019, WhatsApp still not allow businesses to place ads in their app.\n\n\n=== Yelp ===\nYelp consists of a comprehensive online index of business profiles. Businesses are searchable by location, similar to Yellow Pages.  The website is operational in seven different countries, including the United States and Canada. Business account holders are allowed to create, share, and edit business profiles. They may post information such as the business location, contact information, pictures, and service information. The website further allows individuals to write, post reviews about businesses, and rate them on a five-point scale. Messaging and talk features are further made available for general members of the website, serving to guide thoughts and opinions.\n\n\n=== Instagram ===\nIn May 2014, Instagram had over 200 million users. The user engagement rate of Instagram was 15 times higher than of Facebook and 25 times higher than that of Twitter. According to Scott Galloway, the founder of L2 and a professor of marketing at New York University's Stern School of Business, latest studies estimate that 93% of prestige brands have an active presence on Instagram and include it in their marketing mix. When it comes to brands and businesses, Instagram's goal is to help companies to reach their respective audiences through captivating imagery in a rich, visual environment. Moreover, Instagram provides a platform where user and company can communicate publicly and directly, making itself an ideal platform for companies to connect with their current and potential customers.Many brands are now heavily using this mobile app to boost their marketing strategy. Instagram can be used to gain the necessary momentum needed to capture the attention of the market segment that has an interest in the product offering or services. As Instagram is supported by Apple and android system, it can be easily accessed by smartphone users. Moreover, it can be accessed by the Internet as well. Thus, the marketers see it as a potential platform to expand their brands exposure to the public, especially the younger target group. On top of this, marketers do not only use social media for traditional Internet advertising, but they also encourage users to create attention for a certain brand. This generally creates an opportunity for greater brand exposure. Furthermore, marketers are also using the platform to drive social shopping and inspire people to collect and share pictures of their favorite products. Many big names have already jumped on board: Starbucks, MTV, Nike, Marc Jacobs, and Red Bull are a few examples of multinationals that adopted the mobile photo app early. Fashion blogger Danielle Bernstein, who goes by @weworewhat on Instagram, collaborated with Harper's Bazaar to do a piece on how brands are using Instagram to market their products, and how bloggers make money from it. Bernstein, who currently has one and a half million followers on Instagram, and whose \"outfit of the day\" photos on Snapchat get tens of thousands of screenshots, explained that for a lot of her sponsored posts, she must feature the brand in a certain number of posts, and often cannot wear a competitor's product in the same picture. According to Harper's Bazaar, industry estimates say that brands are spending more than $1 billion per year on consumer-generated advertising.Instagram has proven itself a powerful platform for marketers to reach their customers and prospects through sharing pictures and brief messages. According to a study by Simply Measured, 71% of the world's largest brands are now using Instagram as a marketing channel. For companies, Instagram can be used as a tool to connect and communicate with current\nand potential customers. The company can present a more personal picture of their brand, and by doing so the company conveys a better and true picture of itself. The idea of Instagram pictures lies on on-the-go, a sense that the event is happening right now, and that adds another layer to the personal and accurate picture of the company. In fact, Thomas Rankin, co-founder and CEO of the program Dash Hudson, stated that when he approves a blogger's Instagram post before it is posted on the behalf of a brand his company represents, his only negative feedback is if it looks too posed. \"It's not an editorial photo,\" he explained, \"We're not trying to be a magazine. We're trying to create a moment.\" Another option Instagram provides the opportunity for companies to reflect a true picture of the brand from the perspective of the customers, for instance, using the user-generated contents thought the hashtags encouragement. Other than the filters and hashtags functions, the Instagram's 15-second videos and the recently added ability to send private messages between users have opened new opportunities for brands to connect with customers in a new extent, further promoting effective marketing on Instagram.\n\n\n=== Snapchat ===\nSnapchat is a popular messaging and picture exchanging application that was created in 2011 by three students at Stanford University named Evan Spiegel, Bobby Murphy, and Reggie Brown. The application was first developed to allow users to message back and forth and to also send photographs that are only available from 1\u201310 seconds until they are no longer available. The app was an instant hit with social media members and today there are up to 158 million people using snapchat every single day. It is also estimated that Snapchat users are opening the application approximately 18 times per day, which means users are on the app for about 25\u201330 minutes per day.\n\n\n=== YouTube ===\nYouTube is another popular avenue; advertisements are done in a way to suit the target audience. The type of language used in the commercials and the ideas used to promote the product reflect the audience's style and taste. Also, the ads on this platform are usually in sync with the content of the video requested, this is another advantage YouTube brings for advertisers. Certain ads are presented with certain videos since the content is relevant. Promotional opportunities such as sponsoring a video is also possible on YouTube, \"for example, a user who searches for a YouTube video on dog training may be presented with a sponsored video from a dog toy company in results along with other videos.\" YouTube also enable publishers to earn money through its YouTube Partner Program. Companies can pay YouTube for a special \"channel\" which promotes the companies products or services.\n\n\n=== TikTok ===\nTikTok was first released in 2016 and became one of the most popular social media apps allowing users to post short video content. It is mainly mobile-based.\nIn 2016 it was launched under the name Musical.ly, and its main focus was on lip-syncing content. Then after the acquisition of the app by ByteDance in 2018, the name of the app was changed to TikTok.\nAs of current, users can upload up to ten-minute long videos. It is now a platform that houses all different types of content which is personalized to each viewer based on previously liked videos. TikTok now offers live-streaming with users that have larger fan bases. TikTok has brought new celebrities to the public eye for example, Charli Damelio and Alix Earle.\n\n\n=== ClubHouse ===\nClubHouse was launched in April 2020, it is an audio-based social media app that lets you create and join a room where you can talk to other users or listen to other users' conversations.\n\n\n=== Social bookmarking sites ===\nSocial bookmarking sites are used in social media promotion. Each of these sites is dedicated to the collection, curation, and organization of links to other websites that users deem to be of good quality.  This process is \"crowdsourced\", allowing amateur social media network members to sort and prioritize links by relevance and general category.  Due to the large user bases of these websites, any link from one of them to another, the smaller website may in a flash crowd, a sudden surge of interest in the target website. In addition to user-generated promotion, these sites also offer advertisements within individual user communities and categories.  Because ads can be placed in designated communities with a very specific target audience and demographic, they have far greater potential for traffic generation than ads selected simply through cookie and browser history.  Additionally, some of these websites have also implemented measures to make ads more relevant to users by allowing users to vote on which ones will be shown on pages they frequent.  The ability to redirect large volumes of web traffic and target specific, relevant audiences makes social bookmarking sites a valuable asset for social media marketers.\n\n\n=== Blogs ===\nPlatforms like LinkedIn create an environment for companies and clients to connect online. Companies that recognize the need for information, originality and accessibility employ blogs to make their products popular and unique/ and ultimately reach out to consumers who are privy to social media. Studies from 2009 show that consumers view coverage in the media or from bloggers as being more neutral and credible than print advertisements, which are not thought of as free or independent. Blogs allow a product or company to provide longer descriptions of products or services, can include testimonials and can link to and from other social network and blog pages. Blogs can be updated frequently and are promotional techniques for keeping customers, and also for acquiring followers and subscribers who can then be directed to social network pages. Online communities can enable a business to reach the clients of other businesses using the platform. To allow firms to measure their standing in the corporate world, sites enable employees to place evaluations of their companies.Some businesses opt out of integrating social media platforms into their traditional marketing regimen. There are also specific corporate standards that apply when interacting online.  To maintain an advantage in a business-consumer relationship, businesses have to be aware of four key assets that consumers maintain: information, involvement, community, and control.\n\n\n=== Tumblr ===\nBlogging website Tumblr first launched ad products on May 29, 2012. Rather than relying on simple banner ads, Tumblr requires advertisers to create a Tumblr blog so the content of those blogs can be featured on the site. In one year, four native ad formats were created on web and mobile, and had more than 100 brands advertising on Tumblr with 500 cumulative sponsored posts.\n\n\n==== Ad formats ====\nSponsored mobile post \u2013 Advertisements (Advertisers' blog posts) will show up on user's Dashboard when the user is on a mobile device such as smartphones and tablets, allowing them to like, reblog, and share the sponsored post.\nSponsored web post \u2013 \"Largest in-stream ad unit on the web\" that catches the users' attention when looking at their Dashboard through their computer or laptop. It also allows the viewers to like, reblog, and share it.\nSponsored radar \u2013 Radar picks up exceptional posts from the whole Tumblr community based on their originality and creativity. It is placed on the right side next to the Dashboard, and it typically earns 120 million daily impressions. Sponsored radar allows advertisers to place their posts there to have an opportunity to earn new followers, reblogs, and likes.\nSponsored spotlight \u2013 Spotlight is a directory of some of the popular blogs throughout the community and a place where users can find new blogs to follow. Advertisers can choose one category out of fifty categories that they can have their blog listed on there.These posts can be one or more of the following: images, photo sets, animated GIFs, video, audio, and text posts. For the users to differentiate the promoted posts to the regular users' posts, the promoted posts have a dollar symbol on the corner. On May 6, 2014, Tumblr announced customization and theming on mobile apps for brands to advertise.\n\n\n==== Advertising campaigns ====\nTo promote the 2013 film Monsters University, Disney/Pixar created a Tumblr account, MUGrumblr, saying that the account is maintained by a 'Monstropolis transplant' and 'self-diagnosed coffee addict' who is currently a sophomore at Monsters University. A \"student\" from Monsters University uploaded memes, animated GIFs, and Instagram-like photos related to the movie.\nIn 2014, Apple created a Tumblr page to promote the iPhone 5c, labeling it \"Every color has a story\" with the website name: \"ISee5c\". Upon opening the website, the page is covered with different colors representing the iPhone 5c phone colors and case colors. When a colored section is clicked, a 15-second video plays a song and \"showcases the dots featured on the rear of the iPhone 5c official cases and on the iOS 7 dynamic wallpapers\", concluding with words that are related to the video's theme.\n\n\n== Marketing techniques ==\nSocial media marketing involves the use of social networks, consumer's online brand-related activities (COBRA) and electronic word of mouth (eWOM) to successfully advertise online. Social networks such as Facebook and Twitter provide advertisers with information about the likes and dislikes of their consumers. This technique is crucial, as it provides the businesses with a \"target audience\". With social networks, information relevant to the user's likes is available to businesses; who then advertise accordingly. Activities such as uploading a picture of your \"new Converse sneakers to Facebook\" is an example of a COBRA. Electronic recommendations and appraisals are a convenient manner to have a product promoted via \"consumer-to-consumer interactions. An example of eWOM would be an online hotel review; the hotel company can have two possible outcomes based on their service. A good service would result in a positive review which gets the hotel free advertising via social media. However, a poor service will result in a negative consumer review which can potentially harm the company's reputation.Social networking sites such as Facebook, Instagram, Twitter, MySpace etc. have all influenced the buzz of word of mouth marketing. In 1999, Misner said that word-of mouth marketing is, \"the world's most effective, yet least understood marketing strategy\" (Trusov, Bucklin, & Pauwels, 2009, p. 3). Through the influence of opinion leaders, the increased online \"buzz\" of \"word-of-mouth\" marketing that a product, service or companies are experiencing is due to the rise in use of social media and smartphones. Businesses and marketers have noticed that, \"a person's behaviour is influenced by many small groups\" (Kotler, Burton, Deans, Brown, & Armstrong, 2013, p. 189). These small groups rotate around social networking accounts that are run by influential people (opinion leaders or \"thought leaders\") who have followers of groups. The types of groups (followers) are called: reference groups (people who know each other either face-to-face or have an indirect influence on a person's attitude or behaviour); membership groups (a person has a direct influence on a person's attitude or behaviour); and aspirational groups (groups which an individual wishes to belong to).\n\n\n=== Influencer marketing ===\nMarketers target influential people, referred to as influencers, on social media who are recognized as being opinion leaders and opinion-formers to send messages to their target audiences and amplify the impact of their message. A social media post by an opinion leader can have a much greater impact (via the forwarding of the post or \"liking\" of the post) than a social media post by a regular user. Marketers have come to the understanding that \"consumers are more prone to believe in other individuals\" who they trust (Sepp, Liljander, & Gummerus, 2011). OL's and OF's can also send their own messages about products and services they choose (Fill, Hughes, & De Francesco, 2013, p. 216). The reason the opinion leader or formers have such a strong following base is because their opinion is valued or trusted (Clement, Proppe, & Rott, 2007). They can review products and services for their followings, which can be positive or negative towards the brand. OL's and OF's are people who have a social status and because of their personality, beliefs, values etc. have the potential to influence other people (Kotler, Burton, Deans, Brown, & Armstrong, 2013, p. 189). They usually have a large number of followers otherwise known as their reference, membership or aspirational group (Kotler, Burton, Deans, Brown, & Armstrong), 2013, p. 189. By having an OL or OF support a brands product by posting a photo, video or written recommendation on a blog, the following may be influenced and because they trust the OL/OF a high chance of the brand selling more products or creating a following base. Having an OL/OF helps spread word of mouth talk amongst reference groups and/or memberships groups e.g. family, friends, work-friends etc. (Kotler, Burton, Deans, Brown, & Armstrong, 2013, p. 189). The adjusted communication model shows the use of using opinion leaders and opinion formers. The sender/source gives the message to many, many OL's/OF's who pass the message on along with their personal opinion, the receiver (followers/groups) form their own opinion and send their personal message to their group (friends, family etc.)  (Dahlen, Lange, & Smith, 2010, p. 39).\n\n\n=== Organic Social Media ===\nOwned social media channels are an essential extension of businesses and brands in today's world. Brand must seek to create their brand image on each platform, and cater to the type of consumer demographics on each respective platform. In contrast with pre-Internet marketing, such as TV ads and newspaper ads, in which the marketer controlled all aspects of the ad, with social media, users are free to post comments right below an online ad or an online post by a company about its product. Companies are increasing using their social media strategy as part of their traditional marketing effort using magazines, newspapers, radio advertisements, television advertisements. Since in the 2010s, media consumers are often using multiple platforms at the same time (e.g., surfing the Internet on a tablet while watching a streaming TV show), marketing content needs to be consistent across all platforms, whether traditional or new media. Heath (2006) wrote about the extent of attention businesses should give to their social media sites. It is about finding a balance between frequently posting but not over posting. There is a lot more attention to be paid towards social media sites because people need updates to gain brand recognition. Therefore, a lot more content is need and this can often be unplanned content.Planned content begins with the creative/marketing team generating their ideas, once they have completed their ideas they send them off for approval. There is two general ways of doing so. The first is where each sector approves the plan one after another, editor, brand, followed by the legal team (Brito, 2013). Sectors may differ depending on the size and philosophy of the business. The second is where each sector is given 24 hours (or such designated time) to sign off or disapprove. If no action is given within the 24-hour period the original plan is implemented. Planned content is often noticeable to customers and is un-original or lacks excitement but is also a safer option to avoid unnecessary backlash from the public. Both routes for planned content are time-consuming as in the above; the first way to approval takes 72 hours to be approved. Although the second route can be significantly shorter it also holds more risk particularly in the legal department.\nUnplanned content is an 'in the moment' idea, \"a spontaneous, tactical reaction\". The content could be trending and not have the time to take the planned content route. The unplanned content is posted sporadically and is not calendar/date/time arranged (Deshpande, 2014). Issues with unplanned content revolve around legal issues and whether the message being sent out represents the business/brand accordingly. If a company sends out a Tweet or Facebook message too hurriedly, the company may unintentionally use insensitive language or messaging that could alienate some consumers. For example, celebrity chef Paula Deen was criticized after she made a social media post commenting about HIV-AIDS and South Africa; her message was deemed to be offensive by many observers. The main difference between planned and unplanned is the time to approve the content. Unplanned content must still be approved by marketing managers, but in a much more rapid manner e.g. 1\u20132 hours or less. Sectors may miss errors because of being hurried. When using unplanned content Brito (2013) says, \"be prepared to be reactive and respond to issues when they arise\". Brito (2013) writes about having a, \"crisis escalation plan\", because, \"It will happen\". The plan involves breaking down the issue into topics and classifying the issue into groups. Colour coding the potential risk \"identify and flag potential risks\" also helps to organise an issue. The problem can then be handled by the correct team and dissolved more effectively rather than any person at hand trying to solve the situation.\n\n\n== Implications on traditional advertising ==\n\n\n=== Minimizing use ===\nTraditional advertising techniques include print and television advertising. The Internet has already overtaken television as the largest advertising market.Web sites often include the banner or pop-up ads. Social networking sites don't always have ads. In exchange, products have entire pages and are able to interact with users. Television commercials often end with a spokesperson asking viewers to check out the product website for more information. While briefly popular, print ads included QR codes on them. These QR codes can be scanned by cell phones and computers, sending viewers to the product website. Advertising is beginning to move viewers from the traditional outlets to the electronic ones.\nWhile traditional media, like newspapers and television advertising, are largely overshadowed by the rise of social media marketing, there is still a place for traditional marketing. For example, with newspapers, readership over the years has shown a decline. However, readership with newspapers is still fiercely loyal to print-only media. 51% of newspaper readers only read the newspaper in its print form, making well-placed ads valuable.\n\n\n=== Leaks ===\nThe Internet and social networking leaks are one of the issues facing traditional advertising. Video and print ads are often leaked to the world via the Internet earlier than they are scheduled to premiere. Social networking sites allow those leaks to go viral, and be seen by many users more quickly.\nThe time difference is also a problem facing traditional advertisers. When social events occur and are broadcast on television, there is often a time delay between airings on the east coast and west coast of the United States. Social networking sites have become a hub of comment and interaction concerning the event. This allows individuals watching the event on the west coast (time-delayed) to know the outcome before it airs. The 2011 Grammy Awards highlighted this problem. Viewers on the west coast learned who won different awards based on comments made on social networking sites by individuals watching live on the east coast. Since viewers knew who won already, many tuned out and ratings were lower. All the advertisement and promotion put into the event was lost because viewers didn't have a reason to watch.\n\n\n=== Mishaps ===\nSocial media marketing provides organizations with a way to connect with their customers.  However, organizations must protect their information as well as closely watch comments and concerns on the social media they use. A flash poll done on 1225 IT executives from 33 countries revealed that social media mishaps caused organizations a combined $4.3 million in damages in 2010. The top three social media incidents an organization faced during the previous year included employees sharing too much information in public forums, loss or exposure of confidential information, and increased exposure to litigation. Due to the viral nature of the Internet, a mistake by a single employee has in some cases shown to result in devastating consequences for organizations. An example of a social media mishap includes designer Kenneth Cole's Twitter mishap in 2011. When Kenneth Cole tweeted, \"Millions are in uproar in #Cairo. Rumor has they heard our new spring collection is now available online at [Kenneth Cole's website]\".  This reference to the 2011 Egyptian revolution drew an objection from the public; it was widely objected to on the Internet. Kenneth Cole realized his mistake shortly after and responded with a statement apologizing for the tweet.In 2012 during Hurricane Sandy, Gap sent out a tweet to its followers telling them to stay safe but encouraged them to shop online and offered free shipping. The tweet was deemed insensitive, and Gap eventually took it down and apologized. Numerous additional online marketing mishap examples exist. Examples include a YouTube video of a Domino's Pizza employee violating health code standards, which went viral on the Internet and later resulted in felony charges against two employees. A Twitter hashtag posted by McDonald's in 2012 attracting attention due to numerous complaints and negative events customers experienced at the chain store; and a 2011 tweet posted by a Chrysler Group employee that no one in Detroit knows how to drive. When the Link REIT opened a Facebook page to recommend old-style restaurants, the page was flooded by furious comments criticizing the REIT for having forced a lot of restaurants and stores to shut down; it had to terminate its campaign early amid further deterioration of its corporate image.In 2018, Max Factor, MAC and other beauty brands were forced to rush to disassociate themselves from Kuwaiti beauty blogger and Instagram 'influencer' Sondos Alqattan after she criticised government moves to improve conditions for domestic workers.\n\n\n=== Ethics ===\nThe code of ethics that is affiliated with traditional marketing can also be applied to social media. However, with social media being so personal and international, there is another list of complications and challenges that come along with being ethical online. A sensitive topic about social media professionals is the subject of ethics in social media marketing practices, specifically: the proper uses of, often, very personal data. With the invention of social media, the marketer no longer has to focus solely on the basic demographics and psychographics given from television and magazines, but now they can see what consumers like to hear from advertisers, how they engage online, and what their needs and wants are. The general concept of being ethical while marking on social network sites is to be honest with the intentions of the campaign, avoid false advertising, be aware of user privacy conditions (which means not using consumers' private information for gain), respect the dignity of persons in the shared online community, and claim responsibility for any mistakes or mishaps that are results of your marketing campaign. Most social network marketers use websites like Facebook and MySpace to try to drive traffic to another website. While it is ethical to use social networking websites to spread a message to people who are genuinely interested, many people game the system with auto-friend adding programs and spam messages and bulletins. Social networking websites are becoming wise to these practices, however, and are effectively weeding out and banning offenders.\nIn addition, social media platforms have become extremely aware of their users and collect information about their viewers to connect with them in various ways.  Social-networking website Facebook Inc. is quietly working on a new advertising system that would let marketers target users with ads based on the massive amounts of information people reveal on the site about themselves.  This may be an unethical or ethical feature to some individuals.  Some people may react negatively because they believe it is an invasion of privacy.  On the other hand, some individuals may enjoy this feature because their social network recognizes their interests and sends them particular advertisements pertaining to those interests.  Consumers like to network with people who share their interests and desires.  Individuals who agree to have their social media profile public, should be aware that advertisers have the ability to take information that interests them to be able to send them information and advertisements to boost their sales.  Managers invest in social media to foster relationships and interact with customers.  This is an ethical way for managers to send messages about their advertisements and products to their consumers.\nSince social media marketing first came into being, strategists and marketers have been getting smarter and more careful with the way they collect information and distributing advertisements. With the presence of data collecting companies, there is no longer a need to target specific audiences. This can be seen as a large ethically gray area. For many users, this is a breach of privacy, but there are no laws that prevent these companies from using the information provided on their websites. Companies like Equifax, Inc., TransUnion Corp, and LexisNexis Group thrive on collecting and sharing personal information of social media users. In 2012, Facebook purchased information from 70 million households from a third-party company called Datalogix. Facebook later revealed that they purchased the information in order to create a more efficient advertising service.Facebook had an estimated 144.27 million views in 2016, approximately 12.9 million per month. Despite this high volume of traffic, very little has been done to protect the millions of users who log on to Facebook and other social media platforms each month. President Barack Obama tried to work with the Federal Trade Commission (FTC) to attempt to regulate data mining. He proposed the Privacy Bill of Rights, which would protect the average user from having their private information downloaded and shared with third-party companies. The proposed laws would give the consumer more control over what information companies can collect. President Obama was unable to pass most of these laws through congress, and it is unsure what President Trump will do with regards to social media marketing ethics.\n\n\n== Metrics ==\n\n\n=== Web site reports ===\nThis involves tracking the volume of visits, leads, and customers to a website from the individual social channel. Google Analytics is a free tool that shows the behavior and other information, such as demographics and device type used, of website visitors from social networks. This and other commercial offers can aid marketers in choosing the most effective social networks and social media marketing activities.\n\n\n=== Return on investment data ===\n\nThe end goal of any marketing effort is to generate sales. Although social media is a useful marketing tool, it is often difficult to quantify to what extent it is contributing to profit. ROI can be measured by comparing marketing analytic value to contact database or CRM and connect marketing efforts directly to sales activity.\n\n\n=== Customer response rates ===\nSeveral customers are turning towards social media to express their appreciation or frustration with brands, product or services.  Therefore, marketers can measure the frequency of which customers are discussing their brand and judge how effective their SMM strategies are. In recent studies, 72% of people surveyed expressed that they expected a response to their complaints on Twitter within an hour.\n\n\n== Social media marketing in sport ==\nThere has been an increase in social media marketing in sport, as sports teams and clubs recognise the importance of keeping a rapport with their fans and other audiences through social media. Sports personalities such as Cristiano Ronaldo have 40.7 million followers on Twitter and 49.6 million on Instagram, creating opportunities for endorsements.\n\n\n== See also ==\nIntegrated marketing communications\nInternet marketing\nSocial media in the fashion industry\nSocial media optimization\nSocial media spam\nSocial video marketing\nVisual marketing\nWeb 2.0\ninternet celebrity\n\n\n== References ==\n\n\n== External links ==\nBria, Francesca (2014). Social media and their impact on organisations: building Firm Celebrity and organisational legitimacy through social media Archived 2020-02-23 at the Wayback Machine (dissertation). Retrieved 13 September 2018\nKang, Juhee (2015). Social media marketing (dissertation). Journal of Marketing. Retrieved 8 February 2015."}, {"id": 15, "title": "Credit rating", "content": "A credit rating is an evaluation of the credit risk of a prospective debtor (an individual, a business, company or a government), predicting their ability to pay back the debt, and  an implicit forecast of the likelihood of the debtor defaulting.\nThe credit rating represents an evaluation from a credit rating agency of the qualitative and quantitative information for the prospective debtor, including information provided by the prospective debtor and other non-public information obtained by the credit rating agency's analysts.\nCredit reporting (or  credit score) \u2013 is a subset of credit rating \u2013 it is a numeric evaluation of an individual's credit worthiness, which is done by a credit bureau or consumer credit reporting agency.\n\n\n== Sovereign credit ratings ==\nA sovereign credit rating is the credit rating of a sovereign entity, such as a national government. The sovereign credit rating indicates the risk level of the investing environment of a country and is used by investors when looking to invest in particular jurisdictions, and also takes into account political risk.\nThe \"country risk rankings\" table shows the ten least-risky countries for investment as of January 2018. Ratings are further broken down into components including political risk, economic risk. Euromoney's bi-annual country risk index monitors the political and economic stability of 185 sovereign countries, with Singapore emerging as the least risky country since 2017 \u2013 it is also one of the only few countries in the world as well as the only in Asia to achieve a AAA sovereign credit rankings from all major credit agencies.Results focus foremost on economics, specifically sovereign default risk or payment default risk for exporters (also known as a trade credit risk). A. M. Best defines \"country risk\" as the risk that country-specific factors could adversely affect an insurer's ability to meet its financial obligations.\n\n\n== Short and long-term ratings ==\nA rating expresses the likelihood that the rated party will go into default within a given time horizon. In general, a time horizon of one year or under is considered short term, and anything above that is considered long term. In the past institutional investors preferred to consider long-term ratings. Nowadays, short-term ratings are commonly used.\n\n\n== Corporate credit ratings ==\n\nCredit ratings can address a corporation's financial instruments i.e. debt security such as a bond, but also the corporations itself. Ratings are assigned by credit rating agencies, the largest of which are Standard & Poor's, Moody's and Fitch Ratings. They use letter designations such as A, B, C. Higher grades are intended to represent a lower probability of default.\nAgencies do not attach a hard number of probability of default to each grade, preferring descriptive definitions such as: \"the obligor's capacity to meet its financial commitment on the obligation is extremely strong,\" or \"less vulnerable to non-payment than other speculative issues\u2026\" (Standard and Poors' definition of an AAA-rated and a BB-rated bond respectively).  However, some studies have estimated the average risk and reward of bonds by rating. One study by Moody's claimed that over a \"5-year time horizon\" bonds it gave its highest rating (Aaa) to had a \"cumulative default rate\" of 0.18%, the next highest (Aa2) 0.28%, the next (Baa2) 2.11%, 8.82% for the next (Ba2), and 31.24% for the lowest it studied (B2). (See \"Default rate\" in \"Estimated spreads and default rates by rating grade\" table to right.) Over a longer period, it stated \"the order is by and large, but not exactly, preserved\".Another study in Journal of Finance calculated the additional interest rate or \"spread\" corporate bonds pay over that of \"riskless\" US Treasury bonds, according to the bonds' rating. (See \"Basis point spread\" in table to right.) Looking at rated bonds for 1973\u201389, the authors found a AAA-rated bond paid 43 \"basis points\" (or 43/100 of a percentage point) over a US Treasury bond (so that it would yield 3.43% if the Treasury yielded 3.00%). A CCC-rated \"junk\" (or speculative) bond, on the other hand, paid over 7% (724 basis points) more than a Treasury bond on average over that period.Different rating agencies may use variations of an alphabetical combination of lowercase and uppercase letters, with either plus or minus signs or numbers added to further fine-tune the rating (see colored chart). The Standard & Poor's rating scale uses uppercase letters and pluses and minuses.\nThe Moody's rating system uses numbers and lowercase letters as well as uppercase.\nWhile Moody's, S&P and Fitch Ratings control approximately 95% of the credit ratings business, they are not the only rating agencies. DBRS's long-term ratings scale is somewhat similar to Standard & Poor's and Fitch Ratings with the words high and low replacing the + and \u2212. It goes as follows, from excellent to poor: AAA, AA (high), AA, AA (low), A (high), A, A (low), BBB (high), BBB, BBB (low), BB (high), BB, BB (low), B (high), B, B (low), CCC (high), CCC, CCC (low), CC (high), CC, CC (low), C (high), C, C (low) and D. The short-term ratings often map to long-term ratings though there is room for exceptions at the high or low side of each equivalent.S&P, Moody's, Fitch and DBRS are the only four ratings agencies that are recognized by the European Central Bank (ECB) for determining collateral requirements for banks to borrow from the central bank. The ECB uses a first, best rule among the four agencies that have the designated ECAI status, which means that it takes the highest rating among the four agencies \u2013 S&P, Moody's, Fitch and DBRS \u2013 to determine haircuts and collateral requirements for borrowing. Ratings in Europe have been under close scrutiny, particularly the highest ratings given to countries like Spain, Ireland and Italy, because they affect how much banks can borrow against sovereign debt they hold.A. M. Best rates from excellent to poor in the following manner: A++, A+, A, A\u2212, B++, B+, B, B\u2212, C++, C+, C, C\u2212, D, E, F, and S. The CTRISKS rating system is as follows: CT3A, CT2A, CT1A, CT3B, CT2B, CT1B, CT3C, CT2C and CT1C. All these CTRISKS grades are mapped to one-year probability of default.\nUnder the EU Credit Rating Agency Regulation (CRAR), the European Banking Authority has developed a series of mapping tables that map ratings to the \"Credit Quality Steps\" (CQS) as set out in regulatory capital rules and map the CQS to short run and long run benchmark default rates. These are provided in the table below:\n\n\n== See also ==\nList of countries by government budget\nList of countries by credit rating\nList of countries by tax revenue to GDP ratio\nList of countries by public debtIndividuals:\n\nCredit history\nCredit score\n\n\n== References ==\n\n\n== External links ==\n Media related to Credit rating at Wikimedia Commons\nSingapore Credit Score Guide"}, {"id": 16, "title": "Thermal energy storage", "content": "Thermal energy storage (TES) is achieved with widely different technologies. Depending on the specific technology, it allows excess thermal energy to be stored and used hours, days, months later, at scales ranging from the individual process, building, multiuser-building, district, town, or region. Usage examples are the balancing of energy demand between daytime and nighttime, storing summer heat for winter heating, or winter cold for summer air conditioning (Seasonal thermal energy storage). Storage media include water or ice-slush tanks, masses of native earth or bedrock accessed with heat exchangers by means of boreholes, deep aquifers contained between impermeable strata; shallow, lined pits filled with gravel and water and insulated at the top, as well as eutectic solutions and phase-change materials.Other sources of thermal energy for storage include heat or cold produced with heat pumps from off-peak, lower cost electric power, a practice called peak shaving; heat from combined heat and power (CHP) power plants; heat produced by renewable electrical energy that exceeds grid demand and waste heat from industrial processes. Heat storage, both seasonal and short term, is considered an important means for cheaply balancing high shares of variable renewable electricity production and integration of electricity and heating sectors in energy systems almost or completely fed by renewable energy.\n\n\n== Categories ==\nThe different kinds of thermal energy storage can be divided into three separate categories: sensible heat, latent heat, and thermo-chemical heat storage. Each of these has different advantages and disadvantages that determine their applications.\n\n\n=== Sensible heat storage ===\nSensible heat storage (SHS) is the most straightforward method. It simply means the temperature of some medium is either increased or decreased. This type of storage is the most commercially available out of the three; other techniques are less developed.\nThe materials are generally inexpensive and safe. One of the cheapest, most commonly used options is a water tank, but materials such as molten salts or metals can be heated to higher temperatures and therefore offer a higher storage capacity. Energy can also be stored underground (UTES), either in an underground tank or in some kind of heat-transfer fluid (HTF) flowing through a system of pipes, either placed vertically in U-shapes (boreholes) or horizontally in trenches. Yet another system is known as a packed-bed (or pebble-bed) storage unit, in which some fluid, usually air, flows through a bed of loosely packed material (usually rock, pebbles or ceramic brick) to add or extract heat.\nA disadvantage of SHS is its dependence on the properties of the storage medium. Storage capacities are limited by the specific heat capacity of the storage material, and the system needs to be properly designed to ensure energy extraction at a constant temperature.\n\n\n==== Molten salt technology ====\nThe sensible heat of molten salt is also used for storing solar energy at a high temperature, termed molten-salt technology or molten salt energy storage (MSES). Molten salts can be employed as a thermal energy storage method to retain thermal energy. Presently, this is a commercially used technology to store the heat collected by concentrated solar power (e.g., from a solar tower or solar trough). The heat can later be converted into superheated steam to power conventional steam turbines and generate electricity at a later time. It was demonstrated in the Solar Two project from 1995\u20131999. Estimates in 2006 predicted an annual efficiency of 99%, a reference to the energy retained by storing heat before turning it into electricity, versus converting heat directly into electricity. Various eutectic mixtures of different salts are used (e.g., sodium nitrate, potassium nitrate and calcium nitrate). Experience with such systems exists in non-solar applications in the chemical and metals industries as a heat-transport fluid.\n\nThe salt melts at 131 \u00b0C (268 \u00b0F). It is kept liquid at 288 \u00b0C (550 \u00b0F) in an insulated \"cold\" storage tank. The liquid salt is pumped through panels in a solar collector where the focused sun heats it to 566 \u00b0C (1,051 \u00b0F). It is then sent to a hot storage tank. With proper insulation of the tank the thermal energy can be usefully stored for up to a week. When electricity is needed, the hot molten salt is pumped to a conventional steam-generator to produce superheated steam for driving a conventional turbine/generator set as used in any coal, oil, or nuclear power plant. A 100-megawatt turbine would need a tank of about 9.1 metres (30 ft) tall and 24 metres (79 ft) in diameter to drive it for four hours by this design.A single tank with a divider plate to separate cold and hot molten salt is under development. It is more economical by achieving 100% more heat storage per unit volume over the dual tanks system as the molten-salt storage tank is costly due to its complicated construction. Phase Change Material (PCMs) are also used in molten-salt energy storage, while research on obtaining shape-stabilized PCMs using high porosity matrices is ongoing.Most solar thermal power plants use this thermal energy storage concept. The Solana Generating Station in the U.S. can store 6 hours worth of generating capacity in molten salt. During the summer of 2013 the Gemasolar Thermosolar solar power-tower/molten-salt plant in Spain achieved a first by continuously producing electricity 24 hours per day for 36 days. The Cerro Dominador Solar Thermal Plant, inaugurated in June 2021, has 17.5 hours of heat storage.\n\n\n==== Heat storage in tanks or rock caverns ====\n\nA steam accumulator consists of an insulated steel pressure tank containing hot water and steam under pressure.  As a heat storage device, it is used to mediate heat production by a variable or steady source from a variable demand for heat. Steam accumulators may take on a significance for energy storage in solar thermal energy projects.\n\nLarge stores are widely used in Nordic countries to store heat for several days, to decouple heat and power production and to help meet peak demands. Intersessional storage in caverns has been investigated and appears to be economical and plays a significant role in heating in Finland.\nHelen Oy estimates an 11.6 GWh capacity and 120 MW thermal output for its 260,000 m3 water cistern under Mustikkamaa (fully charged or discharged in 4 days at capacity), operating from 2021 to offset days of peak production/demand; while the 300,000 m3 rock caverns 50 m under sea level in Kruunuvuorenranta (near Laajasalo) were designated in 2018 to store heat in summer from warm seawater and release it in winter for district heating.\n\n\n==== Hot silicon technology ====\nSolid or molten silicon offers much higher storage temperatures than salts with consequent greater capacity and efficiency. It is being researched as a possible more energy efficient storage technology. Silicon is able to store more than 1 MWh of energy per cubic meter at 1400 \u00b0C. An additional advantage is the relative abundance of silicon when compared to the salts used for the same purpose.\n\n\n==== Molten aluminum ====\nAnother medium that can store thermal energy is molten (recycled) aluminum. This technology was developed by the Swedish company Azelio. The material is heated to 600 \u00b0C. When needed, the energy is transported to a Stirling engine using a heat-transfer fluid.\n\n\n==== Heat storage in hot rocks or concrete ====\nWater has one of the highest thermal capacities at 4.2 kJ/(kg\u22c5K) whereas concrete has about one third of that.  On the other hand, concrete can be heated to much higher temperatures (1200 \u00b0C) by for example electrical heating and therefore has a much higher overall volumetric capacity. Thus in the example below, an insulated cube of about 2.8 m3 would appear to provide sufficient storage for a single house to meet 50% of heating demand. This could, in principle, be used to store surplus wind or solar heat due to the ability of electrical heating to reach high temperatures. At the neighborhood level, the Wiggenhausen-S\u00fcd solar development at Friedrichshafen in southern Germany has received international attention. This features a 12,000 m3 (420,000 cu ft) reinforced concrete thermal store linked to 4,300 m2 (46,000 sq ft) of solar collectors, which will supply the 570 houses with around 50% of their heating and hot water. Siemens-Gamesa built a 130 MWh thermal storage near Hamburg with 750 \u00b0C in basalt and 1.5 MW electric output. A similar system is scheduled for Sor\u00f8, Denmark, with 41\u201358% of the stored 18 MWh heat returned for the town's district heating, and 30\u201341% returned as electricity.\u201cBrick toaster\u201d is a recently (august 2022) announced innovative heat reservoir operating at up to 1,500 \u00b0C (2,732 \u00b0F) that its maker, Titan Cement/Rondo claims should be able cut global CO2 output by 15% over 15 years.\n\n\n=== Latent heat storage ===\nBecause latent heat storage (LHS) is associated with a phase transition, the general term for the associated media is Phase-Change Material (PCM). During these transitions, heat can be added or extracted without affecting the material\u2019s temperature, giving it an advantage over SHS-technologies. Storage capacities are often higher as well.\nThere are a multitude of PCMs available, including but not limited to salts, polymers, gels, paraffin waxes and metal alloys, each with different properties. This allows for a more target-oriented system design. As the process is isothermal at the PCM\u2019s melting point, the material can be picked to have the desired temperature range. Desirable qualities include high latent heat and thermal conductivity. Furthermore, the storage unit can be more compact if volume changes during the phase transition are small.\nPCMs are further subdivided into organic, inorganic and eutectic materials. Compared to organic PCMs, inorganic materials are less flammable, cheaper and more widely available. They also have higher storage capacity and thermal conductivity. Organic PCMs, on the other hand, are less corrosive and not as prone to phase-separation. Eutectic materials, as they are mixtures, are more easily adjusted to obtain specific properties, but have low latent and specific heat capacities.\nAnother important factor in LHS is the encapsulation of the PCM. Some materials are more prone to erosion and leakage than others. The system must be carefully designed in order to avoid unnecessary loss of heat.\n\n\n==== Miscibility gap alloy technology ====\nMiscibility gap alloys  rely on the phase change of a metallic material (see: latent heat) to store thermal energy.Rather than pumping the liquid metal between tanks as in a molten-salt system, the metal is encapsulated in another metallic material that it cannot alloy with (immiscible).  Depending on the two materials selected (the phase changing material and the encapsulating material) storage densities can be between 0.2 and 2 MJ/L.\nA working fluid, typically water or steam, is used to transfer the heat into and out of the system.  Thermal conductivity of miscibility gap alloys is often higher (up to 400 W/(m\u22c5K)) than competing technologies which means quicker \"charge\" and \"discharge\" of the thermal storage is possible.  The technology has not yet been implemented on a large scale.\n\n\n==== Ice-based technology ====\n\nSeveral applications are being developed where ice is produced during off-peak periods and used for cooling at a later time. For example, air conditioning can be provided more economically by using low-cost electricity at night to freeze water into ice, then using the cooling capacity of ice in the afternoon to reduce the electricity needed to handle air conditioning demands. Thermal energy storage using ice makes use of the large heat of fusion of water. Historically, ice was transported from mountains to cities for use as a coolant. One metric ton of water (= one cubic meter) can store 334 million joules (MJ) or 317,000 BTUs (93 kWh). A relatively small storage facility can hold enough ice to cool a large building for a day or a week.\nIn addition to using ice in direct cooling applications, it is also being used in heat pump-based heating systems.  In these applications, the phase change energy provides a very significant layer of thermal capacity that is near the bottom range of temperature that water source heat pumps can operate in.  This allows the system to ride out the heaviest heating load conditions and extends the timeframe by which the source energy elements can contribute heat back into the system.\n\n\n==== Cryogenic energy storage ====\n\nCryogenic energy storage uses liquification of air or nitrogen as an energy store.\nA pilot cryogenic energy system that uses liquid air as the energy store, and low-grade waste heat to drive the thermal re-expansion of the air, operated at a power station in Slough, UK in 2010.\n\n\n=== Thermo-chemical heat storage ===\nThermo-chemical heat storage (TCS) involves some kind of reversible exotherm/endotherm chemical reaction with thermo-chemical materials (TCM). Depending on the reactants, this method can allow for an even higher storage capacity than LHS.\nIn one type of TCS, heat is applied to decompose certain molecules. The reaction products are then separated, and mixed again when required, resulting in a release of energy. Some examples are the decomposition of potassium oxide (over a range of 300\u2013800 \u00b0C, with a heat decomposition of 2.1 MJ/kg), lead oxide (300\u2013350 \u00b0C, 0.26 MJ/kg) and calcium hydroxide (above 450 \u00b0C, where the reaction rates can be increased by adding zinc or aluminum). The photochemical decomposition of nitrosyl chloride can also be used and, since it needs photons to occur, works especially well when paired with solar energy.\n\n\n==== Adsorption (or Sorption) solar heating and storage ====\nAdsorption processes also fall into this category. It can be used to not only store thermal energy, but also control air humidity. Zeolites (microporous crystalline alumina-silicates) and silica gels are well suited for this purpose. In hot, humid environments, this technology is often used in combination with lithium chloride to cool water.\nThe low cost ($200/ton) and high cycle rate (2,000\u00d7) of synthetic zeolites such as Linde 13X with water adsorbate has garnered much academic and commercial interest recently for use for thermal energy storage (TES), specifically of low-grade solar and waste heat. Several pilot projects have been funded in the EU from 2000 to the present (2020). The basic concept is to store solar thermal energy as chemical latent energy in the zeolite. Typically, hot dry air from flat plate solar collectors is made to flow through a bed of zeolite such that any water adsorbate present is driven off. Storage can be diurnal, weekly, monthly, or even seasonal depending on the volume of the zeolite and the area of the solar thermal panels. When heat is called for during the night, or sunless hours, or winter, humidified air flows through the zeolite. As the humidity is adsorbed by the zeolite, heat is released to the air and subsequently to the building space. This form of TES, with specific use of zeolites, was first taught by Guerra in 1978. Advantages over molten salts and other high temperature TES include that (1) the temperature required is only the stagnation temperature typical of a solar flat plate thermal collector, and (2) as long as the zeolite is kept dry, the energy is stored indefinitely. Because of the low temperature, and because the energy is stored as latent heat of adsorption, thus eliminating the insulation requirements of a molten salt storage system, costs are significantly lower.\n\n\n==== Salt hydrate technology ====\nOne example of an experimental storage system based on chemical reaction energy is the salt hydrate technology. The system uses the reaction energy created when salts are hydrated or dehydrated. It works by storing heat in a container containing 50% sodium hydroxide (NaOH) solution. Heat (e.g. from using a solar collector) is stored by evaporating the water in an endothermic reaction. When water is added again, heat is released in an exothermic reaction at 50 \u00b0C (120 \u00b0F). Current systems operate at 60% efficiency. The system is especially advantageous for seasonal thermal energy storage, because the dried salt can be stored at room temperature for prolonged times, without energy loss. The containers with the dehydrated salt can even be transported to a different location. The system has a higher energy density than heat stored in water and the capacity of the system can be designed to store energy from a few months to years.In 2013 the Dutch technology developer TNO presented the results of the MERITS project to store heat in a salt container. The heat, which can be derived from a solar collector on a rooftop, expels the water contained in the salt. When the water is added again, the heat is released, with almost no energy losses. A container with a few cubic meters of salt could store enough of this thermochemical energy to heat a house throughout the winter. In a temperate climate like that of the Netherlands, an average low-energy household requires about 6.7 GJ/winter. To store this energy in water (at a temperature difference of 70 \u00b0C), 23 m3 insulated water storage would be needed, exceeding the storage abilities of most households. Using salt hydrate technology with a storage density of about 1 GJ/m3, 4\u20138 m3 could be sufficient.As of 2016, researchers in several countries are conducting experiments to determine the best type of salt, or salt mixture. Low pressure within the container seems favorable for the energy transport. Especially promising are organic salts, so called ionic liquids. Compared to lithium halide-based sorbents they are less problematic in terms of limited global resources and compared to most other halides and sodium hydroxide (NaOH) they are less corrosive and not negatively affected by CO2 contaminations.\n\n\n==== Molecular bonds ====\nStoring energy in molecular bonds is being investigated. Energy densities equivalent to lithium-ion batteries have been achieved. This has been done by a DSPEC (dys-sensitized photoelectrosythesis cell). This is a cell that can store energy that has been acquired by solar panels during the day for night-time (or even later) use. It is designed by taking an indication from, well known, natural photosynthesis.\nThe DSPEC generates hydrogen fuel by making use of the acquired solar energy to split water molecules into its elements. As the result of this split, the hydrogen is isolated and the oxygen is released into the air. This sounds easier than it actually is. Four electrons of the water molecules need to be separated and transported elsewhere. Another difficult part is the process of merging the two separate hydrogen molecules.\nThe DSPEC consists of two components: a molecule and a nanoparticle. The molecule is called a chromophore-catalyst assembly which absorbs sunlight and kick starts the catalyst. This catalyst separates the electrons and the water molecules. The nanoparticles are assembled into a thin layer and a single nanoparticle has many chromophore-catalyst on it. The function of this thin layer of nanoparticles is to transfer away the electrons which are separated from the water. This thin layer of nanoparticles is coated by a layer of titanium dioxide. With this coating, the electrons that come free can be transferred more quickly so that hydrogen could be made. This coating is, again, coated with a protective coating that strengthens the connection between the chromophore-catalyst and the nanoparticle.\nUsing this method, the solar energy acquired from the solar panels is converted into fuel (hydrogen) without releasing the so-called greenhouse gasses. This fuel can be stored into a fuel cell and, at a later time, used to generate electricity.\n\n\n==== MOST ====\nAnother promising way to store solar energy for electricity and heat production is a so called molecular solar thermal system (MOST). With this approach a molecule is converted by photoisomerization into a higher-energy isomer. Photoisomerization is a process in which one (cis trans) isomer is converted into another by light (solar energy). This isomer is capable of storing the solar energy until the energy is released by a heat trigger or catalyst (then, the isomer is converted into its original isomer). A promising candidate for such a MOST is Norbornadiene (NBD). This is because there is a high energy difference between the NBD and the quadricyclane (QC) photoisomer. This energy difference is approximately 96 kJ/mol. It is also known that for such systems, the donor-acceptor substitutions provide an effective means for red shifting the longest-wavelength absorption. This improves the solar spectrum match.\nA crucial challenge for a useful MOST system is to acquire a satisfactory high energy storage density (if possible, higher than 300 kJ/kg). Another challenge of a MOST system is that light can be harvested in the visible region. The functionalization of the NBD with the donor and acceptor units is used to adjust this absorption maxima. However, this positive effect on the solar absorption is compensated by a higher molecular weight. This implies a lower energy density. This positive effect on the solar absorption has another downside. Namely, that the energy storage time is lowered when the absorption is redshifted. A possible solution to overcome this anti-correlation between the energy density and the red shifting is to couple one chromophore unit to several photo switches. In this case, it is advantageous to form so called dimers or trimers. The NBD share a common donor and/or acceptor.\nKasper Moth-Poulsen and his team tried to engineer the stability of the high energy photo isomer by having two electronically coupled photo switches with separate barriers for thermal conversion. By doing so, a blue shift occurred after the first isomerization (NBD-NBD to QC-NBD). This led to a higher energy of isomerization of the second switching event (QC-NBD to QC-QC). Another advantage of this system, by sharing a donor, is that the molecular weight per norbornadiene unit is reduced. This leads to an increase of the energy density.\nEventually, this system could reach a quantum yield of photoconversion up 94% per NBD unit. A quantum yield is a measure of the efficiency of photon emission. With this system the measured energy densities reached up to 559 kJ/kg (exceeding the target of 300 kJ/kg). So, the potential of the molecular photo switches is enormous\u2014not only for solar thermal energy storage but for other applications as well.In 2022, researchers reported combining the MOST with a chip-sized thermoelectric generator to generate electricity from it. The system can reportedly store solar energy for up to 18 years and may be an option for renewable energy storage.\n\n\n== Electric thermal storage ==\n \nStorage heaters are commonplace in European homes with time-of-use metering (traditionally using cheaper electricity at nighttime). They consist of high-density ceramic bricks or feolite blocks heated to a high temperature with electricity and may or may not have good insulation and controls to release heat over a number of hours. Some advice not to use them in areas with young children or where there is an increased risk of fires due to poor housekeeping, both due to the high temperatures involved.With the rise of wind and solar power (and other renewable energies) providing an ever increasing share of energy input into the elctricity grids in some countries, the use of larger scale electric energy storage is being explored by several commercial companies. Ideally, the utilisation of surplus renewable energy is transformed into high temperature high grade heat in highly insulated heat stores, for release later when needed. An emerging technology is the use of vacuum super insulated (VSI) heat stores. The use of electricty to generate heat, and not say direct heat from solar thermal collectors, means that very high temperatures can be realised, potentially allowing for inter seasonal heat transfer\u2014storing high grade heat in summer from surplus photovoltaics generation into heat stored for the following winter with relatively minimal standing losses.\n\n\n== Solar energy storage ==\n\nSolar energy is an application of thermal energy storage. Most practical solar thermal storage systems provide storage from a few hours to a day's worth of energy. However, a growing number of facilities use seasonal thermal energy storage (STES), enabling solar energy to be stored in summer to heat space during winter. In 2017 Drake Landing Solar Community in Alberta, Canada, achieved a year-round 97% solar heating fraction, a world record made possible by incorporating STES.The combined use of latent heat and sensible heat are possible with high temperature solar thermal input. Various eutectic metal mixtures, such as aluminum and silicon (AlSi12) offer a high melting point suited to efficient steam generation, while high alumina cement-based materials offer good storage capabilities.\n\n\n== Pumped-heat electricity storage ==\nIn pumped-heat electricity storage (PHES), a reversible heat-pump system is used to store energy as a temperature difference between two heat stores.\n\n\n=== Isentropic ===\nIsentropic systems involve two insulated containers filled, for example, with crushed rock or gravel: a hot vessel storing thermal energy at high temperature/pressure, and a cold vessel storing thermal energy at low temperature/pressure. The vessels are connected at top and bottom by pipes and the whole system is filled with an inert gas such as argon.While charging, the system can use off-peak electricity to work as a heat pump. One prototype used argon at ambient temperature and pressure from the top of the cold store is compressed adiabatically, to a pressure of, for example, 12 bar, heating it to around 500 \u00b0C (900 \u00b0F). The compressed gas is transferred to the top of the hot vessel where it percolates down through the gravel, transferring heat to the rock and cooling to ambient temperature. The cooled, but still pressurized, gas emerging at the bottom of the vessel is then adiabatically expanded to 1 bar, which lowers its temperature to \u2212150 \u00b0C. The cold gas is then passed up through the cold vessel where it cools the rock while warming to its initial condition.\nThe energy is recovered as electricity by reversing the cycle. The hot gas from the hot vessel is expanded to drive a generator and then supplied to the cold store. The cooled gas retrieved from the bottom of the cold store is compressed which heats the gas to ambient temperature. The gas is then transferred to the bottom of the hot vessel to be reheated.\nThe compression and expansion processes are provided by a specially designed reciprocating machine using sliding valves. Surplus heat generated by inefficiencies in the process is shed to the environment through heat exchangers during the discharging cycle.The developer claimed that a round trip efficiency of 72\u201380% was achievable. This compares to >80% achievable with pumped hydro energy storage.Another proposed system uses turbomachinery and is capable of operating at much higher power levels. Use of phase change material as heat storage material could enhance performance.\n\n\n== See also ==\n\n Renewable energy portal\n\n\n== References ==\n\n\n== External links ==\nASHRAE white paper on the economies of load shifting\nMSN article on Ice Storage Air Conditioning at archive.today (archived 19 January 2013)\nICE TES Thermal Energy Storage \u2013 IDE-Tech\nLaramie, Wyoming\n\"Prepared for the Thermal Energy-Storage Systems Collaborative of the California Energy Commission\" Report titled \"Source Energy and Environmental Impacts of Thermal Energy Storage.\" Tabors Caramanis & Assoc energy.ca.gov Archived 23 August 2014 at the Wayback Machine\nCompetence Center Thermal Energy Storage at Lucerne School of Engineering and Architecture\nO-Hx White Paper on the use of Thermal Energy Storage with low carbon energy\n\n\n== Further reading ==\nHyman, Lucas B. Sustainable Thermal Storage Systems: Planning, Design, and Operations. New York: McGraw-Hill, 2011. Print.\nHenrik Lund, Renewable Energy Systems: A Smart Energy Systems Approach to the Choice and Modeling of 100% Renewable Solutions, Academic Press 2014, ISBN 978-0-124-10423-5."}, {"id": 17, "title": "History of the FIFA World Cup", "content": "The FIFA World Cup was first held in 1930, when FIFA, the world's football governing body, decided to stage an international men's football tournament under the era of FIFA president Jules Rimet who put this idea into place. Jules Rimet was the president of FIFA from 1921 to 1954. Rimet was appreciated so much for bringing the idea of FIFA to life that 1946 the trophy was named the Jules Rimet Cup instead of the World Cup Trophy. The inaugural edition, held in 1930, was contested as a final tournament of only thirteen teams invited by the organization. Since then, the World Cup has experienced successive expansions and format remodeling, with its current 48-team final tournament preceded by a two-year qualifying process, involving over 200 teams from around the world.\n\n\n== International football before 1930 ==\nThe first official international football match was played in 1872 in Glasgow between Scotland and England, although at this stage the sport was rarely played outside Great Britain.\nAt the end of the 19th century, games that were considered the \"football world championship\" were meetings between leading English and Scottish clubs, like the 1895 game between Sunderland A.F.C. and the Heart of Midlothian F.C., which Sunderland won.By the twentieth century, football had gained ground all around the world and national football associations were being founded. The first official international match outside the British Isles was played between Uruguay and Argentina in Montevideo in July 1902. The F\u00e9d\u00e9ration Internationale de Football Association (FIFA) was founded in Paris on 22 May 1904 \u2013 comprising football associations from France, Belgium (the preceding two teams having played their first national against each other earlier in the month), Denmark, the Netherlands, Spain, Sweden, and Switzerland, with Germany pledging to join.As football began to increase in popularity, it was contested as an IOC-recognized Olympic sport at the 1900 and 1904 Summer Olympics, as well as at the 1906 Intercalated Games, before becoming an official FIFA-supervised Olympic competition at the 1908 Summer Olympics. Organised by England's Football Association, the event was for amateur players only and was regarded suspiciously as a show rather than a competition. The England national amateur football team won the event in both 1908 and 1912.\nThere was an attempt made by FIFA to organize an international football tournament between nations outside of the Olympic framework in 1906 and this took place in Switzerland. These were very early days for international football and the official history of FIFA describes the competition as having been a failure.With the Olympic event continuing to be contested only between amateur teams, competitions involving professional teams also started to appear. The Torneo Internazionale Stampa Sportiva, held in Turin in 1908, was one of the first, and the following year; Sir Thomas Lipton organized the Sir Thomas Lipton Trophy, also held in Turin. Both tournaments were contested between individual clubs (not national teams), each one of which represented an entire nation. For this reason, neither was really a direct forerunner of the World Cup, but notwithstanding that, the Thomas Lipton Trophy is sometimes described as The First World Cup, at the expense of its less well-known Italian predecessor.\nIn 1914, FIFA agreed to recognize the Olympic tournament as a \"world football championship for amateurs\", and took responsibility for organizing the event. This led the way for the world's first intercontinental football competition, at the 1920 Summer Olympics, won by Belgium. Uruguay won the tournaments in 1924 and 1928.\n\n\n== Beginning of the World Cup ==\nIn 1930, FIFA made the decision to stage their own international tournament. The 1932 Summer Olympics, held in Los Angeles, did not plan to include football as part of the programme because the sport was not popular in the United States. FIFA and the IOC also disagreed over the status of amateur players, and so football was dropped from the Games. FIFA president Jules Rimet thus set about organizing the inaugural World Cup tournament. With Uruguay now a two-time official world champion and due to celebrate its centenary of independence in 1930, FIFA named Uruguay as the host country. The national associations of selected nations were invited to send a team, but the choice of Uruguay as a venue for the competition meant a long and costly trip across the Atlantic Ocean for the European sides at the time of the Great Depression. No European country pledged to send a team until two months before the start of the competition. Rimet eventually persuaded teams from Belgium, France, Romania, Hungary and Yugoslavia to make the trip. In total, 13 nations took part \u2013 seven from South America, four from Europe, and two from North America.\nThe first two World Cup matches took place simultaneously and were won by France and the United States, who beat Mexico 4\u20131 and Belgium 3\u20130, respectively. The first goal in World Cup history was scored by Lucien Laurent of France. Four days later, the first World Cup hat-trick was achieved by Bert Patenaude of the U.S. in the Americans' 3\u20130 win against Paraguay. In the final, Uruguay defeated Argentina 4\u20132 in front of a crowd of 93,000 people in Montevideo to become the first nation to win a World Cup.The 1934 World Cup was hosted by Italy and was the first World Cup to include a qualification stage. Sixteen teams qualified for the tournament, a number which would be retained until the expansion of the finals tournament in 1982. Uruguay, the titleholders from 1930, still upset about the poor European attendance at their World Cup in 1930, boycotted the 1934 World Cup. Bolivia and Paraguay were also absent, allowing Argentina and Brazil to progress to the finals in Italy without having to play any qualifying matches. Egypt became the first African team to compete, but lost to Hungary in the first round. Italy won the tournament, becoming the first European team to do so.\nThe 1938 World Cup competition was also held in Europe (in France), much to the consternation of many South Americans, with Uruguay and Argentina boycotting. For the first time, the title holders and the host country were given automatic qualifications.  Following a play-off match against Latvia, Austria had officially qualified for the final round, but because of the Anschluss in April 1938 with Germany, the Austrian national team withdrew, with some Austrian players being added to the German squad (which was eliminated in the first round). Austria's place was offered to England, but they declined. This left the finals with 15 nations competing. France hosted, but for the first time the hosts did not win the competition, as Italy retained their title, beating Hungary in the final. Polish striker Ernest Willimowski became the first player to score four goals in a World Cup game during Poland's 6\u20135 loss against Brazil; his record was later equalled by other players, but was not bettered until 56 years later in the 1994 World Cup.\n\n\n== Hiatus due to World War II ==\nThe FIFA World Cup was planned to take place in 1942. Germany officially applied to host the 1942 FIFA World Cup at the 23rd FIFA Congress on 13 August 1936 in Berlin. In June 1939, Brazil also applied to host the tournament. The beginning of European hostilities in September 1939 prompted further plans for the 1942 World Cup to be cancelled, before a host country was selected. The FIFA tournament did not take place.\nDuring World War II, FIFA struggled to keep itself afloat, and it had no financial or personnel resources with which to plan a peacetime tournament for when hostilities ended. When the war ended in 1945, it was clear that FIFA would have no hope in a single year of planning and scheduling a 1946 World Cup. In fact, FIFA's first meeting was on 1 July 1946 \u2013 around the time the 1946 World Cup would ordinarily have been played \u2013 and when it planned the next World Cup for 1949 no country would host it. The only major international tournament in 1946 was the 1946 South American Championship in which Argentina beat Brazil 2\u20130 on 10 February 1946.\n\n\n== Post-war years ==\n\n\n=== 1950s ===\nCompetitions resumed with the 1950 World Cup in Brazil, which was the first to include British participants. British teams withdrew from FIFA in 1920, partly out of unwillingness to play against the countries they had been at war with, and partly as a protest against a foreign influence to football, but rejoined in 1946 following FIFA's invitation. England's involvement, however, was not to be a success. The English failed to make the final group round in a campaign that included a surprise 1\u20130 loss to the United States.The tournament also saw the return of 1930 champions Uruguay, who had boycotted the previous two World Cups. For political reasons, Eastern European countries (such as Hungary, the Soviet Union and Czechoslovakia) did not enter. Title-holder Italy did take part, despite the Superga air disaster of 1949 in which the entire Grande Torino team (many of whom were national team players) were killed. The 1950 World Cup was the only tournament not to stage a final tie, replacing knockout rounds with two group phases. The last match of the second group phase, however, is sometimes referred to as a \"final\", as the group standings meant the winners would be the overall winners. Uruguay were surprise victors over hosts Brazil with a final score of 2\u20131 (the game would later be known as Maracanazo), and became champions for the second time.  This game also held the record for the highest attendance at any sporting match, at roughly 200,000.\nThe 1954 World Cup, held in Switzerland, was the first to be televised. The Soviet Union did not participate because of their dismal performance at the 1952 Summer Olympics. Scotland made their first appearance in the tournament, but were unable to register a win, going out after the group stage. This tournament set a number of all-time goal-scoring records, including highest average goals per game and highest-scoring team (Hungary), and most goals in a single match (Austria's 7\u20135 quarter-final victory over Switzerland). West Germany were the tournament winners, defeating Olympic champions Hungary 3\u20132 in the final, overturning a 2\u20130 deficit in the process, with Helmut Rahn scoring the winner. The match is known as the Miracle of Bern in Germany.\nBrazil won the 1958 World Cup, held in Sweden, and became the first team to win a World Cup outside their home continent (only 4 teams have done this to date \u2013 Brazil in 1958, 1970, 1994 and 2002, Argentina in 1986 and 2022, Spain in 2010 and Germany in 2014). The Soviet Union participated this time, most likely due to their win at Melbourne 1956. For the first (and so far only) time, all four British teams qualified for the final round. Wales was able to take advantage of a situation in the Africa/Asia zone, where the number of withdrawals would give Israel qualification without having played a single qualifying match. This prompted FIFA to rule that qualification without playing was not allowed (despite allowing this to happen in earlier years of the Cup), and so Israel were ordered to play against one of the teams finishing second in the other groups. A tie was created, and Wales defeated Israel 2\u20130 twice in 1958. It was the first (and so far the only) time that a country played a World Cup final round after having been eliminated in the regular qualifiers. The tournament also saw the emergence of Pel\u00e9, who scored two goals in the final. French striker Just Fontaine became the top scorer of the tournament.\n\n\n=== 1960s ===\nChile hosted the 1962 World Cup. Two years before the tournament, an earthquake struck, the largest ever recorded at 9.5 magnitude, prompting officials to rebuild due to major damage to infrastructure. When the competition began, two of the best players were in poor form as Pel\u00e9 was injured in Brazil's second group match against Czechoslovakia. Also, the Soviet Union saw their goalkeeper Lev Yashin show poor form including a 2\u20131 loss to hosts Chile as the hosts captured third place.\nThe competition was also marred by overly defensive and often violent tactics. This poisonous atmosphere culminated in what was known as the Battle of Santiago first round match between Italy and Chile in which Chile won 2\u20130. Prior to the match, two Italian journalists wrote unflattering articles about the host country. In the match, players on both sides made deliberate attempts to harm opponents though only two players from Italy were sent off by English referee Ken Aston. In the end, the Italian team needed police protection to leave the field in safety.\nWhen the final whistle blew, Brazil beat Czechoslovakia for the second World Cup in a row by a final of 3\u20131 led by Garrincha and Amarildo, in Pel\u00e9's absence, and retained the Jules Rimet trophy.\nColombia's Marcos Coll made World Cup history when he scored a goal direct from a corner kick (called an Olympic Goal in Latin America), the only one ever made in a World Cup, past legendary Soviet goalkeeper Lev Yashin.\nThe 1966 World Cup, hosted by England, was the first to embrace marketing, featuring a mascot and official logo for the first time. The trophy was stolen in the run-up to the tournament but was found a week later by a dog named \"Pickles\". South Africa was banned for violating the anti-discrimination charter (apartheid). The ban remained in effect until 1992 when the South Africa Football Association was finally accepted by FIFA. The qualifying rounds of the tournament saw a controversy when the African nations decided to withdraw in protest of only one qualifying place allocated by FIFA to the regions of Asia, Oceania and Africa. The eventual qualifiers from the zone, North Korea, became the first Asian team to reach the quarter-finals, eliminating Italy in the process. England won the tournament, although Jo\u00e3o Havelange (former FIFA president from 1974 to 1998) claimed that the 1966 and 1974 World Cups were fixed so that England and Germany would win respectively. Geoff Hurst became the first player to score a hat-trick in a World Cup Final and Eus\u00e9bio, whose team Portugal were taking part in their first World Cup, was the tournament top-scorer, with nine goals to his name.\n\n\n=== 1970s ===\nThe qualification stages of the 1970 World Cup were coincidental with the Football War between Honduras and El Salvador. The finals were held in Mexico. Israel had been with Europe, but due to political issues, it was becoming harder to place them adequately in the qualifying rounds. They were grouped in Asia/Oceania. Korea DPR then refused to meet them, even though this meant automatic disqualification. The group stage clash between defending champions England and Brazil lived up to its billing, and is still remembered for England goalkeeper Gordon Banks' save from a Pel\u00e9 header on the six-yard line. The tournament is also remembered for the semi-final match between Italy and West Germany, in which five goals were scored in extra time, and Franz Beckenbauer played with a broken arm, since Germany had used up all their allowed substitutions. Italy were the eventual 4\u20133 winners, but were defeated 1\u20134 in the final by Brazil, who became the first nation to win three World Cups, and were awarded the Jules Rimet trophy permanently for their achievement.\n\nA new trophy was created for the 1974 edition, held in West Germany. After a draw in their first UEFA/CONMEBOL Intercontinental play-off match against Chile in the qualifiers, the Soviet Union refused to travel to the Chilean capital for the return fixture for political reasons, and in accordance with the regulations, Chile were awarded a victory. East Germany, Haiti, Australia and Zaire made their first finals. The tournament also saw a new format, where the two top teams from each of the earlier four groups were divided into two groups of four each again, the winner of either group playing each other in the final. The West German hosts won the competition by beating the Netherlands 2\u20131 in the final, but it was also the revolutionary Total Football system of the Dutch that captured the footballing world's imagination. The very well-playing Poland finished third, after defeating Brazil 1\u20130 (and after defeating Argentina 3\u20132 and eliminating Italy 2\u20131 in the initial group play), having barely lost in terrible rain in the semi-finals to West Germany 0\u20131.\nThe 1978 World Cup was held in Argentina, causing controversy as a military coup had taken place in the country two years earlier. Allegations that Dutch star Johan Cruyff refused to participate because of political convictions were refuted by him 30 years later. and none of the teams decided to stay away.\nThis was the hardest ever World Cup to qualify for. With 95 countries vying for 14 places (the holders and hosts qualified automatically), there were nearly seven teams competing for each place in Argentina. Hungary won their European group but still had to win a play-off against Bolivia to qualify, while England and Italy - the only former champions involved in European qualifying - were placed in the same group, with the result that England were eliminated on goal difference despite winning five of their six qualifying matches.\nIran and Tunisia were first-time participants. Tunisia won their first match against Mexico 3\u20131 and became the first African team to ever win a World Cup game. There was some on-field controversy as well. During the second round, Argentina had an advantage in their match against Peru since the kick off was several hours after Brazil's match with Poland. Brazil won their match 3\u20131, so Argentina knew that they had to beat Peru by four goals to advance to the final. Trailing 2\u20130 at half-time, Peru simply collapsed in the second half, and Argentina eventually won 6\u20130. Rumors suggested that Peru might have been bribed into allowing Argentina to win the match by such a large margin. Argentina went on to win the final 3\u20131, with the Dutch being runners-up for the second time in a row.\n\n\n== Late 20th century ==\n\n\n=== 1980s ===\nSpain hosted an expanded 1982 World Cup which featured 24 teams, the first expansion since 1934. The teams were divided into six groups of four, with the top two teams in each group advancing to the second round, where they split into four groups of three. The winners of each group advanced to the semi-finals. Cameroon, Algeria, Honduras, New Zealand and Kuwait were the debutants. The group match between Kuwait and France was stage of a farcical incident. As the French were leading 3\u20131, the Kuwaiti team stopped playing after hearing a whistle from the stands which they thought had come from referee, as French defender Maxime Bossis scored. As the Kuwaiti team were protesting the goal, Sheikh Fahid Al-Ahmad Al-Sabah, president of the Kuwait Football Association, rushed onto the pitch and gave the referee a piece of his mind, who proceeded to disallow the goal. Bossis scored another valid goal a few minutes later and France won 4\u20131.\nAlso during the group stages, Hungary beat El Salvador 10\u20131, which has been the only occasion to this day that a team scored ten goals in a World Cup match. The group match between West Germany and Austria later resulted in a change of World Cup rules, after both teams visibly aimed to keep the qualification ensuring 1\u20130 scoreline over 80 minutes. The semi-final between West Germany and France saw another controversy when German keeper Harald Schumacher's challenge took out Patrick Battiston, with the score at 1\u20131. Schumacher escaped a red card, and Germany won in a penalty shoot-out, after coming back to level from having gone 1\u20133 down. The final was won by Italy, making Italian captain Dino Zoff the oldest player to win the World Cup. Italian striker Paolo Rossi, who was making his comeback after a match-fixing scandal and the ensuing ban, was the tournament top-scorer with six goals including a classic hat-trick against Brazil.\n\nBecause of Colombia's withdrawal to host the tournament, Mexico became the first nation to hold two World Cups by hosting the 1986 World Cup. The format changed again, with the second round being replaced by a pre-quarterfinal, knockout competition, for which 16 teams would qualify. It was also decided that the final two matches in all groups would kick off simultaneously, to ensure complete fairness. Canada, Denmark and Iraq made their first finals. Jos\u00e9 Batista of Uruguay set a World Cup record being sent off after a mere 56 seconds into the game against Scotland. The quarterfinal match between England and Argentina is remembered for two remarkable Diego Maradona goals, later regarded as player of the tournament, the first, the controversial handball goal, and the second, considered to be the Goal of the Century, in which he dribbled half the length of the field past five English players before scoring. In the final, Argentina beat West Germany 3\u20132, inspired by Diego Maradona, who set up Jorge Burruchaga for the winner.\n\n\n=== 1990s ===\nThe 1990 World Cup was held in Italy. Cameroon, participating in their second World Cup, made it to the quarter-finals after beating Argentina in the opening game. No African country had ever reached the quarter-finals before. Mexico was unable to compete in the 1990 World Cup preliminary competition as a result of a two-year ban for age fraud at a youth championship, an incident known as Los Cachirules. the United States qualified for the first time since 1950. An unpleasant episode marred the South American qualifiers: during the match between Brazil and Chile, a firework landed close to the Chilean goalkeeper Roberto Rojas, who then feigned injury by cutting his own face with a razor blade he had hidden in his glove. His team refused to continue the match (as they were down a goal at the time). The plot was discovered and resulted in a 12-year suspension for Rojas and to Chile being banned from the 1994 World Cup. The final featured the same teams as in 1986. After finishing runners-up in the two previous tournaments, West Germany beat Argentina 1\u20130 in the final to record their third title. The Republic of Ireland also made their first appearance in the tournament, reaching the quarter-finals without winning a single game (four draws, with a penalty shoot-out win over Romania in the second round). This is the furthest a team has ever advanced in the World Cup without winning a game.\nThe 1994 World Cup, held in the United States, saw the first World Cup final to be decided on penalties, with Brazil edging out Italy. FR Yugoslavia was excluded due to UN sanctions in connection with the war in Bosnia-Herzegovina. Colombia qualified unexpectedly defeating Argentina 5-0. Japan narrowly missed a ticket to the World Cup after drawing with Iraq in the final match of the qualification round, remembered by fans as the \"Agony of Doha\". As a result, South Korea qualified to the tournament. Russia (taking the place of the Soviet Union which had disintegrated over 1990 and 1991) played their first World Cup competition as a new country, with Greece, Nigeria and Saudi Arabia as the other first-timers. Diego Maradona was banned mid-tournament after testing positive for cocaine: without him, Argentina were eliminated in the last 16 by Romania. Despite soccer's relative lack of popularity in the host nation, the tournament was the most financially successful in World Cup history; it broke tournament records with overall attendance of 3,587,538 and an average of 68,991 per match, marks that stood unbroken as of 2018 despite the expansion of the competition from 24 to 32 teams starting with the 1998 World Cup.The total attendance for the tournament of nearly 3.6 million remains the biggest in World Cup history. Oleg Salenko of Russia became the first player to score five goals in a single World Cup finals game in his country's 6\u20131 group stage win over Cameroon. In the same match, 42-year-old Roger Milla scored the only goal for Cameroon, becoming the oldest player ever to score in a World Cup match. Hristo Stoichkov shared the Golden Boot as the joint top goal scorer in the tournament with Oleg Salenko (six goals), as well as earning the Bronze Ball award. He led Bulgaria to a shock 2\u20131 win over defending champions Germany in the quarter-finals, before losing 2\u20131 to Italy and losing the third place play-off to Sweden, 4\u20130.\nThe 1998 World Cup was held in France, and had an expanded format featuring 32 teams. Iran beat the Maldives in qualification by the widest margin in World Cup history \u2013 17\u20130. In the finals, the second round match between France and Paraguay witnessed the first golden goal in World Cup history, as Laurent Blanc scored to give the hosts a 1\u20130 victory. Hosts France won the tournament by beating Brazil 3\u20130 in the final, with Brazilian star player Ronaldo being controversially capped for the match after having had a seizure hours before kickoff. Debutants Croatia finished a commendable third.\n\n\n== 21st century ==\n\n\n=== 2000s ===\nThe 2002 World Cup was the first to be held in Asia, and was hosted jointly by South Korea and Japan. Togolese Souleymane Mamam became the youngest player ever to take to a World Cup preliminary game field at 13 years, 310 days in Lom\u00e9 in May 2001. Australia defeated American Samoa 31\u20130 in a preliminary match \u2013 a new record for the margin of victory, and the highest-scoring match ever. The tournament was a successful one for teams traditionally regarded as minnows, with South Korea, Senegal and the United States all reaching the last eight. Brazil beat Germany 2\u20130 in the final for their fifth title. The Turkish Hakan Sukur made history by scoring the earliest World Cup goal of all time against South Korea at only 11 seconds.\nThe 2006 World Cup was held in Germany. It was the first World Cup for which the previous winner had to qualify; the host nation(s) continue to receive an automatic berth. Four African teams also made their debut in the world cup finals: Togo, Ivory Coast, Angola and Ghana who impressively made it to last 16 by beating the Czech Republic, third ranked in the world, 2\u20131, along with the United States 2\u20130, before losing to the defending champions Brazil 0\u20133.\nFirst seed and holders Brazil and second seeded England were initially English bookmakers' favourites. A strong performance by Germany brought them as far as the semi-finals. However, the final match-up was between Italy and France, in which French captain Zinedine Zidane was sent off in the last ten minutes of extra time for a headbutt to the chest of Italian central defender Marco Materazzi. Italy went on to win 5\u20133 in a penalty shootout, the score having been 1\u20131 after 90 minutes and extra time.\n\n\n=== 2010s ===\nThe 2010 World Cup was held in South Africa. It was the first cup hosted on African soil, and the cup was won by Spain. The tournament was noted for its highly defensive opening matches, controversies surrounding goal-line technology, and the introduction of vuvuzelas. Though considered as one of the tournament favorites, the Spaniards won the cup despite scoring only eight goals in seven games and losing their opening match to Switzerland. David Villa led the squad in scoring with five goals. In a final which saw a record number of yellow cards distributed and what some considered violent play from the Dutch side, the ten-man Netherlands squad were defeated 1\u20130 in the 116th minute of extra time by an Andr\u00e9s Iniesta goal.\n\nThe 2014 World Cup was held in Brazil, marking the second time that Brazil hosted the competition. The cup was won by Germany, who beat Argentina 1\u20130 in the final. The Netherlands defeated Brazil (who lost to the eventual winners, Germany, 7\u20131 in the semifinals) 3\u20130 in the bronze medal game.\nBecause of the relatively high ambient temperatures in Brazil, particularly at the northern venues, cooling breaks for the players were first introduced during these games. In this World Cup there was the debut of sensors to avoid phantom goals with the Goal-line technology, used to determine, in doubtful situations, whether the ball crossed the goal line.The 2018 World Cup was held in Russia. It was the first cup to be held in Eastern Europe. The cup was won by France, who beat Croatia 4\u20132 in the final. Belgium defeated England 2\u20130 in the bronze medal game. It was also the first cup to use the video assistant referee (VAR) system.\n\n\n=== 2020s ===\nThe 2022 World Cup, hosted by Qatar, was the first tournament to not be held in summer time in which it is usually held, and the first to be held in the Middle East. The cup was won by Argentina, who prevailed over defending champions France 4\u20132 in the penalty shootout, after the final was drawn 3\u20133 after extra time, despite France's Kylian Mbapp\u00e9 netting a hat-trick, becoming only the second player to do so in a World Cup Final. Previous tournament runners-up Croatia won the bronze medal match, beating Morocco 2\u20131, whose fourth-place finish was the furthest of any African nation at the World Cup. It is also the last to feature 32 teams, as the next edition is scheduled to expand to 48 teams.\nThe 2026 World Cup is scheduled to be jointly hosted by the United States, Canada and Mexico, making it the first to be hosted by three nations.\n\n\n=== 2030s ===\nThe host for the 2030 will be Argentina Uruguay and Paraguay in South America for the opening matches to honor the centennial of the first FIFA World Cup, Morocco in Africa, and Spain and Portugal in Europe.\nThe 2034 world cup will be hosted by Saudi Arabia, and the host for the 2038 World Cup is yet to be decided.\n\n\n== Evolution of the format ==\nThe number of teams and the format of each final tournament have varied considerably over the years. In most tournaments, the tournament consists of a round-robin group stage followed by a single-elimination knockout stage.\n\nIn 1934 and 1938 draws in knockout matches were resolved via a replay. Later, drawing of lots was provided for, though never invoked. Since 1974, penalty shootouts are used.\nIn 1954 each group had two seeded and two unseeded teams; the seeded teams played only unseeded teams and vice versa.\nUp to 1958, ranking ties in groups were to be broken via a playoff; this only happened in 1954 and 1958.\nUntil the 1990 FIFA World Cup, 2 points were conceded for a win and 1 point was conceded for a draw. Since the 1994 FIFA World Cup, 3 points are conceded for a win and 1 point is conceded for a draw.\nEach group of four teams plays a round-robin schedule. As of the 1986 World Cup, all final group games must be held simultaneously, a rule instituted by FIFA to minimize collusion amongst teams requiring a certain result to advance. FIFA instituted a policy to award three points for a win in the 1994 World Cup. Although goals for was already a tiebreaker, FIFA hoped to create an additional incentive for teams to pursue victory. The first team affected by the rule was Paraguay in 1998, which would have won its group on goal differential over Nigeria under prior FIFA rules. Paraguay advanced to the knockout phase as group runner-up and was defeated by host nation and eventual champion France in the round of 16. It is not possible under the new point system to be eliminated from the group stage with a second place or higher winning percentage, however it is possible to finish behind a team with the same winning percentage yet a lower goal difference. This took place in the 2010 World Cup when New Zealand finished with three draws and Slovakia finished with one win, one draw, and one loss. Slovakia advanced in Group F by finishing second with four points, eliminating New Zealand with three points. Under the previous FIFA point allotment system, New Zealand would have advanced with a zero goal difference, while Slovakia would have been eliminated with a goal difference of \u22121.\nThe criteria for advancement to knockout phase is as follows:\n\nGreatest number of points in group matches\nGreatest total goal difference in the three group matches\nGreatest number of goals scored in the three group matches\nIf teams remained level after those criteria, a mini-group would be formed from those teams, who would be ranked on:\nMost points earned in matches against other teams in the tie\nGreatest goal difference in matches against other teams in the tie\nGreatest number of goals scored in matches against other teams in the tie\nIf teams remained level after all these criteria, FIFA would hold a drawing of lots\nThe drawing of lots for tied teams takes place one hour after the final game in the group at the stadium where the championship match is held. The drawing of lots is similar to the World Cup draw in terms of style and format; a ball is drawn from a pot, which contains balls with the names of each tied team.As of the 2022 World Cup, lots have only been drawn once in tournament history. However, they were used to separate second and third place in a group (Republic of Ireland and the Netherlands in 1990) where both were already assured of qualification. Thus, a team has never been eliminated based upon drawn lots.\n\n\n== World Cup\u2013winning teams, captains, and managers ==\n\n\n== See also ==\nHistory of FIFA\nFIFA World Cup hosts\n\n\n== References =="}, {"id": 18, "title": "Twenty20", "content": "Twenty20 (T20) is a shortened game format of cricket. At the professional level, it was introduced by the England and Wales Cricket Board (ECB) in 2003 for the inter-county competition. In a Twenty20 game, the two teams have a single innings each, which is restricted to a maximum of twenty overs. Together with first-class and List A cricket, Twenty20 is one of the three current forms of cricket recognised by the International Cricket Council (ICC) as being at the highest international or domestic level.\nA typical Twenty20 game is completed in about two and a half hours, with each innings lasting around 70 minutes and an official 10-minute break between the innings. This is much shorter than previous forms of the game, and is closer to the timespan of other popular team sports. It was introduced to create a fast-paced game that would be attractive to spectators at the ground and viewers on television.\nThe game has succeeded in spreading around the cricket world. On most international tours there is at least one Twenty20 match and all Test-playing nations have a domestic cup competition.\n\n\n== History ==\n\n\n=== Origins ===\nWhen the Benson & Hedges Cup ended in 2002, the ECB needed another one-day competition to fill its place. Cricketing authorities were looking to boost the game's popularity with the younger generation in response to dwindling crowds and reduced sponsorship. It was intended to deliver fast-paced, exciting cricket accessible to thousands of fans who were put off by the longer versions of the game. Stuart Robertson, the marketing manager of the ECB, proposed a 20-over-per-innings game, invented by New Zealand cricketer Martin Crowe, to county chairmen in 2001 and they voted 11\u20137 in favour of adopting the new format.The first official Twenty20 matches were played on 13 June 2003 between the English counties in the Twenty20 Cup. The first season of Twenty20 in England was a relative success, with the Surrey Lions defeating the Warwickshire Bears by nine wickets in the final to claim the title. The first Twenty20 match held at Lord's, on 15 July 2004 between Middlesex and Surrey, attracted a crowd of 27,509, the highest attendance for any county cricket game at the ground \u2013 other than a one-day final \u2013 since 1953.\n\n\n=== Spread worldwide ===\nThirteen teams from different parts of the country participated in Pakistan's inaugural competition in 2004, with the Faisalabad Wolves the first winners. On 12 January 2005 Australia's first Twenty20 game was played at the WACA Ground between the Western Warriors and the Victorian Bushrangers. It drew a sell-out crowd of 20,000, which was the first one in nearly 25 years.Starting on 11 July 2006, 19 West Indies regional teams competed in what was named the Stanford 20/20 tournament. The event was financially backed by billionaire Allen Stanford, who gave at least US$28,000,000 in funding money. It was intended that the tournament would be an annual event. Guyana won the inaugural event, defeating Trinidad and Tobago by five wickets, securing US$1,000,000 in prize money.On 5 January 2007 the Queensland Bulls played the New South Wales Blues at The Gabba, Brisbane. An unexpected 16,000 fans turned up on the day to buy tickets, causing Gabba staff to throw open gates and grant many fans free entry. Attendance reached 27,653. For the February 2008 Twenty20 match between Australia and India, 85,824 people attended the match at the Melbourne Cricket Ground, involving the Twenty20 World Champions against the ODI World Champions.The Stanford Super Series was held in October 2008 between the three teams.  The respective winners of the English and Caribbean Twenty20 competitions, Middlesex and Trinidad and Tobago, and a Stanford Superstars team formed from West Indies domestic players. Trinidad and Tobago won the competition, securing US$280,000 prize money. On 1 November, the Stanford Superstars played England in what was expected to be the first of five fixtures in as many years with the winner claiming US$20,000,000 in each match. The Stanford Superstars won the first match, but no further fixtures were held as Allen Stanford was charged with fraud in 2009.\n\n\n=== T20 leagues ===\n\nSeveral T20 leagues started after the popularity of the 2007 ICC World Twenty20. The Board of Control for Cricket in India started the Indian Premier League popularly known as IPL, which is now the largest cricket league, in 2008, which utilizes the North American sports franchise system with ten teams in major Indian cities. In September 2017, the broadcasting and digital rights for the next five years (2018\u20132022) of the IPL were sold to Star India for US$2.55 billion, making it one of the world's most lucrative sports league per match. The IPL has seen a spike in its brand valuation to US$5.3 billion after the 10th edition, according to global valuation and corporate finance advisor Duff & Phelps.The Big Bash League, Bangladesh Premier League, Pakistan Super League, Caribbean Premier League, and Afghanistan Premier League started thereafter, following similar formulae, and remained popular with the fans. The Women's Big Bash League was started in 2015 by Cricket Australia, while the Kia Super League was started in England and Wales in 2016. The Mzansi Super League in South Africa was started in 2018.\nSeveral T20 leagues follow the general format of having a group stage followed by a Page playoff system among the top four teams where:\n\nThe first- and second-highest placed teams in the group stage face off, with the winner going to the final.\nThe third- and fourth-place teams face off, with the loser being eliminated.\nThe two teams who have not yet made it to the final after the above two matches have been played face off to fill the second berth in the final.In the Big Bash League, there is an additional match to determine which of the fourth- or fifth-placed teams will qualify to be in the top four.\n\n\n=== Twenty20 Internationals ===\n\nThe first Twenty20 International match was held on 5 August 2004 between the England and New Zealand women's teams, with New Zealand winning by nine runs.On 17 February 2005 Australia defeated New Zealand in the first men's international Twenty20 match, played at Eden Park in Auckland. The game was played in a light-hearted manner \u2013 both sides turned out in kit similar to that worn in the 1980s, the New Zealand team's a direct copy of that worn by the Beige Brigade. Some of the players also sported moustaches or beards and hairstyles popular in the 1980s, taking part in a competition amongst themselves for \"best retro look\", at the request of the Beige Brigade. Australia won the game comprehensively, and as the result became obvious towards the end of the NZ innings, the players and umpires took things less seriously: Glenn McGrath jokingly replayed the Trevor Chappell underarm incident from a 1981 ODI between the two sides, and Billy Bowden showed him a mock red card (red cards are not normally used in cricket) in response.\nThe first Twenty20 international in England was played between England and Australia at the Rose Bowl in Hampshire on 13 June 2005, which England won by a margin of 100 runs, a record victory which lasted until 2007.On 9 January 2006 Australia and South Africa met in the first international Twenty20 game in Australia. In a first, each player's nickname appeared on the back of his uniform, rather than his surname. The international match drew a crowd of 38,894 people at The Gabba.\nOn 16 February 2006 New Zealand defeated West Indies in a tie-breaking bowl-out 3\u20130; 126 runs were scored apiece in the game proper. The game was the last international match played by Chris Cairns.\nThe ICC has declared that it sees T20 as the optimal format for globalizing the game, and in 2018, announced that it will give international status to all T20 cricket matches played between its member nations. This resulted in a significant leap in the number of T20I matches played across the world.\n\n\n==== Twenty20 World Cup ====\n\nEvery two years an ICC World Twenty20 tournament is to take place, except in the event of an ICC Cricket World Cup being scheduled in the same year, in which case it will be held the year before. The first tournament was in 2007 in South Africa where India defeated Pakistan in the final. Two Associate teams had played in the first tournament, selected through the 2007 ICC World Cricket League Division One, a 50-over competition. In December 2007 it was decided to hold a qualifying tournament with a 20-over format to better prepare the teams. With six participants, two would qualify for the 2009 World Twenty20 and would each receive $250,000 in prize money. The second tournament was won by Pakistan, who beat Sri Lanka by eight wickets in England on 21 June 2009. The 2010 ICC World Twenty20 tournament was held in the West Indies in May 2010, where England defeated Australia by seven wickets. The 2012 ICC World Twenty20 was won by the West Indies, by defeating Sri Lanka at the finals. It was the first time in cricket history when a T20 World Cup tournament took place in an Asian country. The 2014 ICC World Twenty20 was won by Sri Lanka, by defeating India at the finals, where the tournament was held in Bangladesh. The 2016 ICC World Twenty20 was won by West Indies. In July 2020, the ICC announced that both the 2020 and 2021 editions had been postponed by one year due to the COVID-19 pandemic.\nIn June 2021, the ICC expanded the Twenty20 World Cup from 16 to 20 teams starting from the 2024 edition onwards.\n\n\n=== Impact on the game ===\nTwenty20 cricket is claimed to have resulted in a more athletic and explosive form of cricket. Indian fitness coach Ramji Srinivasan declared in an interview with the Indian fitness website Takath.com that Twenty20 had \"raised the bar\" in terms of fitness levels for all players, demanding higher levels of strength, speed, agility and reaction time from all players regardless of role in the team. Matthew Hayden credited retirement from international cricket with aiding his performance in general and fitness in particular in the Indian Premier League.Several commentators have noted that the T20 format has been embraced by many Associate members of the ICC partly because it is more financially viable to play.Former Australian captain Ricky Ponting, on the other hand, has criticized Twenty20 as being detrimental to Test cricket and for hampering batsmen's scoring skills and concentration. Former Australian captain Greg Chappell made similar complaints, fearing that young players would play too much T20 and not develop their batting skills fully, while former England player Alex Tudor feared the same for bowling skills.\nFormer West Indies captains Clive Lloyd, Michael Holding and Garfield Sobers criticised Twenty20 for its role in discouraging players from representing their test cricket national side, with many West Indies players like Chris Gayle, Sunil Narine and Dwayne Bravo preferring instead to play in a Twenty20 franchise elsewhere in the world and make far more money.\n\n\n=== Inclusion in multi-sport events ===\nIn June 2009, speaking at the annual Cowdrey Lecture at Lord's, former Australian wicketkeeper Adam Gilchrist pushed for Twenty20 to be made an Olympic sport. \"It would,\" he said, \"be difficult to see a better, quicker or cheaper way of spreading the game throughout the world.\" This became a reality starting with the 2028 Summer Olympics. T20 cricket has also been accepted into the Asian Games and Commonwealth Games.\n\n\n== Match format and rules ==\n\n\n=== Format ===\nTwenty20 match format is a form of limited overs cricket in that it involves two teams, each with a single innings. The key feature is that each team bats for a maximum of 20 overs (120 legal balls). The batting team members do not arrive from and depart to traditional dressing rooms, but come and go from a bench (typically a row of chairs) visible in the playing arena, analogous to association football's technical area or a baseball dugout.\n\n\n=== General rules ===\nThe Laws of cricket apply to Twenty20, with major exceptions:\nEach bowler may bowl a maximum of only one-fifth of the total overs per innings. For a full, uninterrupted match, this is four overs.\nIf a bowler delivers a no-ball by overstepping the crease, it costs one or two runs (depending on the competition) and their next delivery is designated a \"free-hit\". In this circumstance the batter can only be dismissed through a run out, hitting the ball twice or obstructing the field.\nThe following fielding restrictions apply:\nNo more than five fielders can be on the leg side at any time.\nDuring the first six overs, a maximum of two fielders can be outside the 30-yard circle (this is known as the powerplay).\nAfter the first six overs, a maximum of five fielders can be outside the fielding circle.\nIf the fielding team does not start to bowl their 20th over within 75 minutes, the batting side is credited an extra six runs for every whole over bowled after the 75-minute mark; the umpire may add more time to this if they believe the batting team is wasting time.\n\n\n=== Tie deciders ===\n\nCurrently, if the match ends with the scores tied and there must be a winner, the tie is broken with a one-over-per-side Eliminator or Super Over:\nEach team nominates three batsmen and one bowler to play a one-over-per-side \"mini-match\". The team which bats second in the match bats first in the Super Over. In turn, each side bats one over bowled by the one nominated opposition bowler, with their innings over if they lose two wickets before the over is completed. The side with the higher score from their Super Over wins.\nIf the Super Over also ends up in a tie, it is repeated until the tie is broken.\nIn the Australian domestic competition the Big Bash League, the Super Over is played slightly differently, with no two-wicket limit, and if the Super Over is also tied then a \"countback\" is used, with scores after the fifth ball for each team being used to determine the result. If it is still tied, then the countback goes to four balls, and so on. The latest Super Over to decide a match was between the Sydney Sixers and the Brisbane Heat on 25 January 2017, in the Big Bash League at the Brisbane Cricket Ground, with the Sixers winning 0/22 to 0/15 in the Super Over after tying on 164.Tied Twenty20 matches were previously decided by a bowl-out.\n\n\n== International ==\n\nWomen's and men's Twenty20 Internationals have been played since 2004 and 2005 respectively. To date, 76 nations have played the format, including all Test-playing nations.\n\n\n=== T20 International rankings ===\n\nIn November 2011, the ICC released the first Twenty20 International rankings for the men's game, based on the same system as the Test and ODI rankings. The rankings cover a two- to three-year period, with matches since the most recent 1 August weighted fully, matches in the preceding 12 months weighted two-thirds, and matches in the 12 months preceding that weighted one-third. To qualify for the rankings, teams must have played at least eight Twenty20 Internationals in the ranking period.The ICC Women's Rankings were launched in October 2015, which aggregated performance over all three forms of the game. In October 2018, the ICC announced that the women's ranking would be split between ODIs and T20Is, and released both tables shortly thereafter.\n\n\n== Domestic professional T20 leagues ==\n\nThis is a list of the current Twenty20 domestic competitions in several of the leading cricket countries.\n\n\n== See also ==\nList of Twenty20 cricket records\nList of Twenty20 International records\n100-ball cricket\nThe Hundred (cricket)\nT10 cricket, the 10-over format of cricket\nMajor League Cricket\nInternational League T20\nGlobal T20 Canada\n\n\n== References ==\n\n\n== External links ==\n\nCricinfo \u2013 Twenty20 records\nIPL News 2021"}, {"id": 19, "title": "Potential Energy", "content": "In physics, potential energy is the energy held by an object because of its position relative to other objects, stresses within itself, its electric charge, or other factors. The term potential energy was introduced by the 19th-century Scottish engineer and physicist William Rankine, although it has links to the ancient Greek philosopher Aristotle's concept of potentiality.\nCommon types of potential energy include the gravitational potential energy of an object, the elastic potential energy of an extended spring, and the electric potential energy of an electric charge in an electric field. The unit for energy in the International System of Units (SI) is the joule (symbol J).\nPotential energy is associated with forces that act on a body in a way that the total work done by these forces on the body depends only on the initial and final positions of the body in space. These forces, whose total work is path independent, are called conservative forces. If the force acting on a body varies over space, then one has a force field; such a field is described by vectors at every point in space, which is in-turn called a vector field. A conservative vector field can be simply expressed as the gradient of a certain scalar function, called a scalar potential. The potential energy is related to, and can be obtained from, this potential function.\n\n\n== Overview ==\nThere are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of configurations of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their configuration.\nForces derivable from a potential are also called conservative forces. The work done by a conservative force is\n\nwhere \n  \n    \n      \n        \u0394\n        U\n      \n    \n    {\\displaystyle \\Delta U}\n   is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy.  Common notations for potential energy are PE, U, V, and Ep.\nPotential energy is the energy by virtue of an object's position relative to other objects. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching a spring or lifting a mass is performed by an external force that works against the force field of the potential.  This work is stored in the force field, which is said to be stored as potential energy.  If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.\nConsider a ball whose mass is m and whose height is h. The acceleration g of free fall is approximately constant, so the weight force of the ball mg is constant. The product of force and displacement gives the work done, which is equal to the gravitational potential energy, thus\n\nThe more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.\n\n\n== Work and potential energy ==\nPotential energy is closely linked with forces. If the work done by a force on a body that moves from A to B does not depend on the path between these points (if the work is done by a conservative force), then the work of this force measured from A assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.\nIf the work for an applied force is independent of the path, then the work done by the force is evaluated from the start to the end of the trajectory of the point of application.  This means that there is a function U(x), called a \"potential\", that can be evaluated at the two points xA and xB to obtain the work over any trajectory between these two points.  It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is \n\nwhere C is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, C, from A to B.\nThe function U(x) is called the potential energy associated with the applied force.  Examples of forces that have potential energies are gravity and spring forces.\n\n\n=== Derivable from a potential ===\nIn this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve C takes a special form if the force F is related to a scalar field U\u2032(x) so that\n\nThis means that the units of U\u2032 must be this case, work along the curve is given by\n\nwhich can be evaluated using the gradient theorem to obtain\n\nThis shows that when forces are derivable from a scalar field, the work of those forces along a curve C is computed by evaluating the scalar field at the start point A and the end point B of the curve.  This means the work integral does not depend on the path between A and B and is said to be independent of the path.\nPotential energy U = \u2212U\u2032(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is\n\nIn this case, the application of the del operator to the work function yields,\n\nand the force F is said to be \"derivable from a potential\". This also necessarily implies that F must be a conservative vector field. The potential U defines a force F at every point x in space, so the set of forces is called a force field.\n\n\n=== Computing potential energy ===\nGiven a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve \u03b3(t) = r(t) from \u03b3(a) = A to \u03b3(b) = B, and computing,\n\nFor the force field F, let v = dr/dt, then the gradient theorem yields,\n\nThe power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is\n\nExamples of work that can be computed from potential functions are gravity and spring forces.\n\n\n== Potential energy for near-Earth gravity ==\nFor small height changes, gravitational potential energy can be computed using\n\nwhere m is the mass in kilograms, g is the local gravitational field (9.8 metres per second squared on Earth), h is the height above a reference level in metres, and U is the energy in joules.\nIn classical physics, gravity exerts a constant downward force F = (0, 0, Fz) on the center of mass of a body moving near the surface of the Earth.  The work of gravity on a body moving along a trajectory r(t) = (x(t), y(t), z(t)), such as the track of a roller coaster is calculated using its velocity, v = (vx, vy, vz), to obtain\n\nwhere the integral of the vertical component of velocity is the vertical distance. The work of gravity depends only on the vertical movement of the curve r(t).\n\n\n== Potential energy for a linear spring ==\n\nA horizontal spring exerts a force F = (\u2212kx, 0, 0) that is proportional to its deformation in the axial or x direction.  The work of this spring on a body moving along the space curve s(t) = (x(t), y(t), z(t)), is calculated using its velocity, v = (vx, vy, vz), to obtain\n\nFor convenience, consider contact with the spring occurs at t = 0, then the integral of the product of the distance x and the x-velocity, xvx, is x2/2.\nThe function \n\nis called the potential energy of a linear spring.\nElastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.\n\n\n== Potential energy for gravitational forces between two bodies ==\nThe gravitational potential function, also known as gravitational potential energy, is:\n\nThe negative sign follows the convention that work is gained from a loss of potential energy.\n\n\n=== Derivation ===\nThe gravitational force between two bodies of mass M and m separated by a distance r is given by Newton's law of universal gravitation\n\nwhere \n  \n    \n      \n        \n          \n            \n              r\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {r}} }\n   is a vector of length 1 pointing from M to m and G is the gravitational constant.\nLet the mass m move at the velocity v then the work of gravity on this mass as it moves from position r(t1) to  r(t2) is given by\n\nThe position and velocity of the mass m are given by\n\nwhere er and et are the radial and tangential unit vectors directed relative to the vector from M to m. Use this to simplify the formula for work of gravity to,\n\nThis calculation uses the fact that\n\n\n== Potential energy for electrostatic forces between two bodies ==\nThe electrostatic force exerted by a charge Q on another charge q separated by a distance r is given by Coulomb's Law\n\nwhere \n  \n    \n      \n        \n          \n            \n              r\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {r}} }\n   is a vector of length 1 pointing from Q to q and \u03b50 is the vacuum permittivity.\nThe work W required to move q from A to any point B in the electrostatic force field is given by the potential function\n\n\n== Reference level ==\nThe potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state; it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used; therefore it can be chosen based on convenience.\nTypically the potential energy of a system depends on the relative positions of its components only, so the reference state can also be expressed in terms of relative positions.\n\n\n== Gravitational potential energy ==\n\nGravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.\n\nConsider a book placed on top of a table. As the book is raised from the floor to the table, some external force works against the gravitational force. If the book falls back to the floor, the \"falling\" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation, and sound by the impact.\nThe factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. \"Height\" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.\n\n\n=== Local approximation ===\nThe strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant g = 9.8 m/s2 (standard gravity). In this case, a simple expression for gravitational potential energy can be derived using the W = Fd equation for work, and the equation\n\nThe amount of gravitational potential energy held by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember W = Fd). The upward force required while moving at a constant velocity is equal to the weight, mg, of an object, so the work done in lifting it through a height h is the product mgh. Thus, when accounting only for mass, gravity, and altitude, the equation is:\nwhere U is the potential energy of the object relative to its being on the Earth's surface, m is the mass of the object, g is the acceleration due to gravity, and h is the altitude of the object.Hence, the potential difference is\n\n\n=== General formula ===\nHowever, over large variations in distance, the approximation that g is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy, we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance r between the two bodies. Using that definition, the gravitational potential energy of a system of masses m1 and M2 at a distance r using the Newtonian constant of gravitation G is\n\nwhere K is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that K = 0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making U negative; for why this is physically reasonable, see below.\nGiven this formula for U, the total potential energy of a system of n bodies is found by summing, for all \n  \n    \n      \n        \n          \n            \n              n\n              (\n              n\n              \u2212\n              1\n              )\n            \n            2\n          \n        \n      \n    \n    {\\textstyle {\\frac {n(n-1)}{2}}}\n   pairs of two bodies, the potential energy of the system of those two bodies.\n\nConsidering the system of bodies as the combined set of small particles the bodies consist of, and applying the previous on the particle level we get the negative gravitational binding energy. This potential energy is more strongly negative than the total potential energy of the system of bodies as such since it also includes the negative gravitational binding energy of each body. The potential energy of the system of bodies as such is the negative of the energy needed to separate the bodies from each other to infinity, while the gravitational binding energy is the energy needed to separate all particles from each other to infinity.\n\ntherefore,\n\n\n=== Negative gravitational energy ===\nAs with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite r over another, there seem to be only two reasonable choices for the distance at which U becomes zero: \n  \n    \n      \n        r\n        =\n        0\n      \n    \n    {\\displaystyle r=0}\n   and \n  \n    \n      \n        r\n        =\n        \u221e\n      \n    \n    {\\displaystyle r=\\infty }\n  . The choice of \n  \n    \n      \n        U\n        =\n        0\n      \n    \n    {\\displaystyle U=0}\n   at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.\nThe singularity at \n  \n    \n      \n        r\n        =\n        0\n      \n    \n    {\\displaystyle r=0}\n   in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with \n  \n    \n      \n        U\n        =\n        0\n      \n    \n    {\\displaystyle U=0}\n   for \n  \n    \n      \n        r\n        =\n        0\n      \n    \n    {\\displaystyle r=0}\n  , would result in potential energy being positive, but infinitely large for all nonzero values of r, and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and r is always non-zero in practice, the choice of \n  \n    \n      \n        U\n        =\n        0\n      \n    \n    {\\displaystyle U=0}\n   at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.\nThe negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.\n\n\n=== Uses ===\n\nGravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example, in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.\nGravitational potential energy is also used to power clocks in which falling weights operate the mechanism.  It is also used by counterweights for lifting up an elevator, crane, or sash window.\nRoller coasters are an entertaining way to utilize potential energy \u2013 chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.\nAnother practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline.  In some cases the kinetic energy obtained from the potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).\n\n\n== Chemical potential energy ==\n\nChemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.\nThe similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.\n\n\n== Electric potential energy ==\n\nAn object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).\n\n\n=== Electrostatic potential energy ===\nElectrostatic potential energy between two bodies in space is obtained from the force exerted by a charge Q on another charge q which is given by\n\nwhere \n  \n    \n      \n        \n          \n            \n              r\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\hat {r}} }\n   is a vector of length 1 pointing from Q to q and \u03b50 is the vacuum permittivity.\nIf the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects.  The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.\nThe work W required to move q from A to any point B in the electrostatic force field is given by\n\ntypically given in J for Joules. A related quantity called electric potential (commonly denoted with a V for voltage) is equal to the electric potential energy per unit charge.\n\n\n=== Magnetic potential energy ===\nThe energy of a magnetic moment \n  \n    \n      \n        \n          \u03bc\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}}\n   in an externally produced magnetic B-field B has potential energy\nThe magnetization M in a field is\n\nwhere the integral can be over all space or, equivalently, where M is nonzero.\nMagnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be higher the further they are apart and lower the closer they are. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.\n\n\n== Nuclear potential energy ==\nNuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.\nNuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them can have less mass than if they were individually free, in which case this mass difference can be liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.\n\n\n== Forces and potential energy ==\nPotential energy is closely linked with forces. If the work done by a force on a body that moves from A to B does not depend on the path between these points, then the work of this force measured from A assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.\nFor example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   or \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  , corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass M and m separated by a distance r is\n\nThe gravitational potential (specific energy) of the two bodies is\n\nwhere \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the reduced mass.\nThe work done against gravity by moving an infinitesimal mass from point A with \n  \n    \n      \n        U\n        =\n        a\n      \n    \n    {\\displaystyle U=a}\n   to point B with \n  \n    \n      \n        U\n        =\n        b\n      \n    \n    {\\displaystyle U=b}\n   is \n  \n    \n      \n        (\n        b\n        \u2212\n        a\n        )\n      \n    \n    {\\displaystyle (b-a)}\n   and the work done going back the other way is \n  \n    \n      \n        (\n        a\n        \u2212\n        b\n        )\n      \n    \n    {\\displaystyle (a-b)}\n   so that the total work done in moving from A to B and returning to A is\n\nIf the potential is redefined at A to be \n  \n    \n      \n        a\n        +\n        c\n      \n    \n    {\\displaystyle a+c}\n   and the potential at B to be \n  \n    \n      \n        b\n        +\n        c\n      \n    \n    {\\displaystyle b+c}\n  , where \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is a constant (i.e. \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is\n\nas before.\nIn practical terms, this means that one can set the zero of \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).\nA conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.\n\n\n== Notes ==\n\n\n== References ==\nSerway, Raymond A.; Jewett, John W. (2010). Physics for Scientists and Engineers (8th ed.). Brooks/Cole cengage. ISBN 978-1-4390-4844-3.\nTipler, Paul (2004). Physics for Scientists and Engineers: Mechanics, Oscillations and Waves, Thermodynamics (5th ed.). W. H. Freeman. ISBN 0-7167-0809-4.\n\n\n== External links ==\nWhat is potential energy?"}, {"id": 20, "title": "United States", "content": "The United States of America (USA), commonly known as the United States (U.S.) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, and nine Minor Outlying Islands. It includes 326 Indian reservations. The U.S. is the world's third-largest country by land area, and by total area. It shares land borders with Canada to its north and with Mexico to its south and has maritime borders with the Bahamas, Cuba, Russia, and other nations. With a population of over 333 million, it is the most populous country in the Americas and the third-most populous in the world. The national capital of the United States is Washington, D.C., and its most populous city and principal financial center is New York City.\nIndigenous peoples have inhabited the Americas for thousands of years. Beginning in 1607, British colonization led to the establishment of the Thirteen Colonies in what is now the Eastern United States. They clashed with the British Crown over taxation and political representation, which led to the American Revolution and the ensuing Revolutionary War. The United States declared independence on July 4, 1776, becoming the first nation-state founded on Enlightenment principles of unalienable natural rights, consent of the governed, and republicanism. The country began expanding across North America, spanning the continent by 1848. Sectional division over slavery led to the secession of the Confederate States of America, which fought the remaining states of the Union during the American Civil War (1861\u20131865). With the Union's victory and preservation, slavery was abolished nationally. By 1900, the United States had established itself as a great power, becoming the world's largest economy. After Japan's attack on Pearl Harbor in December 1941, the U.S. entered World War II on the side of the Allies. The aftermath of the war left the United States and the Soviet Union as the world's two superpowers and led to the Cold War, during which both countries engaged in a struggle for ideological dominance and international influence, avoided direct military conflict, and competed in the Space Race, which culminated with the United States landing the first humans on the Moon in 1969. Following the Soviet Union's collapse and the end of the Cold War in the early 1990s, it emerged as the world's sole superpower. The 2020s saw the United States emerge as the leader of the AI Spring, which has led to ongoing rapid and unprecedented development in artificial intelligence, and a return to space exploration with the Artemis program, with plans to establish a permanent base on the Moon to facilitate the feasibility of human missions to Mars. \nThe United States government is a federal presidential constitutional republic and liberal democracy with three separate branches of government: legislative, executive, and judicial. It has a bicameral national legislature composed of the House of Representatives, a lower house based on population; and the Senate, an upper house based on equal representation for each state. Many policy issues are decentralized at a state or local level, with widely differing laws by jurisdiction. Americans generally value liberty, individualism, and limited government. Culturally, the country is primarily Anglophonic, with other prominent regional influences.\nOne of the world's most developed countries, the United States has the highest mean income per capita of any non-microstate and possesses by far the largest amount of wealth of any country. The American economy accounts for over a quarter of global GDP and is the largest nominally. It ranks among the highest in the world in international measures of quality of life, income and wealth, economic competitiveness, productivity, innovation, human rights, and education. The United States is a founding member of the United Nations, the World Bank, the International Monetary Fund, the Organization of American States, NATO and WHO, and is a permanent member of the United Nations Security Council. It is a recognized nuclear-weapon state and wields considerable global influence as the world's foremost political, cultural, economic, military, and scientific power. \n\n\n== Etymology ==\n\nThe first documentary evidence of the phrase \"United States of America\" dates back to a letter from January 2, 1776, written by Stephen Moylan, a Continental Army aide to General George Washington, to Joseph Reed, Washington's aide-de-camp. Moylan expressed his desire to go \"with full and ample powers from the United States of America to Spain\" to seek assistance in the Revolutionary War effort. The first known publication of the phrase \"United States of America\" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, on April 6, 1776.By June 1776, the name \"United States of America\" appeared in drafts of the Articles of Confederation and Perpetual Union, authored by John Dickinson, a Founding Father from the Province of Pennsylvania, and in the Declaration of Independence, written primarily by Thomas Jefferson and adopted by the Second Continental Congress in Philadelphia, on July 4, 1776.\n\n\n== History ==\n\n\n=== Indigenous peoples ===\n\nThe first inhabitants of North America migrated from Siberia across the Bering land bridge at least 12,000 years ago; the Clovis culture, which appeared around 11,000 BC, is believed to be the first widespread culture in the Americas. Over time, indigenous North American culutures grew increasingly sophisticated, and some, such as the Mississippian culture, developed agriculture, architecture, and complex societies. Indigenous peoples and cultures such as the Algonquian peoples, Ancestral Puebloans, and the Iroquois developed across the present-day United States. Native population estimates of what is now the United States before the arrival of European immigrants range from around 500,000 to nearly 10 million.\n\n\n=== European colonization ===\n\nChristopher Columbus began exploring the Caribbean in 1492, leading to Spanish settlements in present-day Puerto Rico, Florida, and New Mexico. France established their own settlements along the Mississippi River and Gulf of Mexico. British colonization of the East Coast began with the Virginia Colony (1607) and Plymouth Colony (1620). The Mayflower Compact and the Fundamental Orders of Connecticut established precedents for representative self-governance and constitutionalism that would develop throughout the American colonies.While European settlers experienced conflicts with Native Americans, they also engaged in trade, exchanging European tools for food and animal pelts. The Columbian exchange was catastrophic for native populations. It is estimated that up to 95 percent of the indigenous populations in the Americas perished from infectious diseases during the years following European colonization; remaining populations were often displaced by European expansion. Colonial authorities pursued policies to force Native Americans to adopt European lifestyles, and European settlers trafficked African slaves into the colonial United States through the Atlantic slave trade.The original Thirteen Colonies were administered by Great Britain, and had local governments with elections open to most white male property owners. The colonial population grew rapidly, eclipsing Native American populations; by the 1770s, the natural increase of the population was such that only a small minority of Americans had been born overseas. The colonies' distance from Britain allowed for the development of self-governance, and the First Great Awakening\u2014a series of Christian revivals\u2014fueled colonial interest in religious liberty.\n\n\n=== Revolution and expansion (1776\u20131861) ===\n\nAfter winning the French and Indian War, Britain began to assert greater control over local colonial affairs, creating colonial political resistance; one of the primary colonial grievances was that Britain taxed the colonies without giving them representation in government. In 1774, the First Continental Congress met in Philadelphia, and passed a colonial boycott of British goods. The British attempt to disarm the colonists resulted in the 1775 Battles of Lexington and Concord, igniting the American Revolutionary War. At the Second Continental Congress, the colonies appointed George Washington commander-in-chief of the Continental Army and created a committee led by Thomas Jefferson to write the Declaration of Independence, adopted on July 4, 1776. The political values of the American Revolution included liberty, inalienable individual rights; and the sovereignty of the people; supporting republicanism and rejecting monarchy, aristocracy, and hereditary political power; virtue and faithfulness in the performance of civic duties; and vilification of corruption. The Founding Fathers of the United States, which included George Washington, Benjamin Franklin, Alexander Hamilton, Thomas Jefferson, John Jay, James Madison, Thomas Paine, and John Adams, took inspiration from Ancient Greco-Roman, Renaissance, and English models and ideas.After British surrender at the siege of Yorktown in 1781, Britain signed a peace treaty. American sovereignty became internationally recognized, and the U.S. gained territory stretching west to the Mississippi River, north to present-day Canada, and south to Spanish Florida. Ratified in 1781, the Articles of Confederation established a decentralized government that operated until 1789. The Northwest Ordinance (1787) established the precedent by which the nation would expand with the admission of new states, rather than the expansion of existing states. The U.S. Constitution was drafted at the 1787 Constitutional Convention; it went into effect in 1789, creating a federation administered by three branches on the principle of checks and balances. Washington was elected the nation's first president under the Constitution, and the Bill of Rights was adopted in 1791 to allay concerns by skeptics of the more centralized government; his resignations as commander-in-chief and President set a precedent followed by John Adams, establishing peaceful transfer of power between rival parties.\nIn the late 18th century, American settlers began to expand westward, with a sense of manifest destiny. The Louisiana Purchase (1803) from France nearly doubled the territory of the United States. Lingering issues with Britain remained, leading to the War of 1812, which was fought to a draw. Spain ceded Florida and their Gulf Coast territory in 1819. As Americans expanded further into land inhabited by Native Americans, the federal government often applied policies of Indian removal or assimilation. The displacement prompted a long series of American Indian Wars west of the Mississippi River. The Republic of Texas was annexed in 1845, and the 1846 Oregon Treaty led to U.S. control of the present-day American Northwest. Victory in the Mexican\u2013American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest, resulting in the U.S. stretching from the Atlantic to the Pacific oceans. Alaska was purchased from Russia in 1867.\n\n\n=== Civil War (1861\u20131865) ===\n\nDuring the colonial period, slavery was legal in the American colonies, though the practice began to be significantly questioned during the American Revolution. The North enacted abolition laws, though support for slavery strengthened in the South, as inventions such as the cotton gin made the institution increasingly profitable for Southern elites. This sectional conflict regarding slavery culminated in the American Civil War (1861\u20131865). Eleven slave states seceded and formed the Confederate States of America, while the remaining states remained in the Union. War broke out in April 1861 after the Confederacy bombarded Fort Sumter. After the January 1863 Emancipation Proclamation, many freed slaves joined the Union Army. The war began to turn in the Union's favor following the 1863 Siege of Vicksburg and Battle of Gettysburg, and the Confederacy surrendered in 1865 after the Union's victory in the Battle of Appomattox Court House. The Reconstruction era followed the war. After the assassination of President Abraham Lincoln, Reconstruction Amendments were passed to protect the rights of African Americans. National infrastructure, including transcontinental telegraph and railroads, spurred growth in the American frontier.\n\n\n=== Post-Civil War era (1865\u20131898) ===\n\nFrom 1865 through 1918 an unprecedented stream of immigrants arrived in the United States, including 24.4 million from Europe. Most came through the port of New York City, and New York and other large cities on the East Coast became home to large Jewish, Irish, and Italian populations, while many Germans and Central Europeans moved to the Midwest. At the same time, about one million French Canadians migrated from Quebec to New England. During the Great Migration, millions of African Americans left the rural South for urban areas in the North. The Compromise of 1877 effectively ended Reconstruction and white supremacists took local control of Southern politics. African Americans endured a period of heightened, overt racism following Reconstruction, a time often called the nadir of American race relations. Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists, largely by their formation of trusts and monopolies to prevent competition. Tycoons led the nation's expansion in the railroad, petroleum, and steel industries. Banking became a major part of the economy, and the United States emerged as a pioneer of the automotive industry. These changes were accompanied by significant increases in economic inequality, slum conditions, and social unrest. This period eventually ended with the advent of the Progressive Era, which was characterized by significant reforms.\n\n\n=== Rise as a superpower (1898\u20131945) ===\n\nThe early 20th century was a time of industrial expansion and social change in the United States. Pro-American elements in Hawaii overthrew the Hawaiian monarchy; the islands were annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain following the Spanish\u2013American War. American Samoa was acquired by the United States in 1900 after the Second Samoan Civil War. The U.S. Virgin Islands were purchased from Denmark in 1917. The United States entered World War I alongside the Allies of World War I, helping to turn the tide against the Central Powers. In 1920, a constitutional amendment granted nationwide women's suffrage. During the 1920s and 1930s, radio for mass communication and the invention of early television transformed communications nationwide. The Wall Street Crash of 1929 triggered the Great Depression, which President Franklin D. Roosevelt responded to with New Deal social and economic policies. At first neutral during World War II, the U.S. began supplying war materiel to the Allies of World War II in March 1941 and entered the war in December after the Empire of Japan's attack on Pearl Harbor. The U.S. developed the first nuclear weapons and used them again the Japanese cities of Hiroshima and Nagasaki in August 1945, ending the war. The United States was one of the \"Four Policemen\" who met to plan the postwar world, alongside the United Kingdom, Soviet Union, and China. The U.S. emerged relatively unscathed from the war, with even greater economic and military influence.\n\n\n=== Cold War (1945\u20131991) ===\n\nAfter World War II, the United States entered the Cold War, where geopolitical tensions between the U.S. and the Soviet Union led the two countries to dominate world affairs. The U.S. engaged in regime change against governments perceived to be aligned with the Soviet Union, and competed in the Space Race, culminating in the first crewed Moon landing in 1969. Domestically, the U.S. experienced economic growth, urbanization, and population growth following World War II. The civil rights movement emerged, with Martin Luther King Jr. becoming a prominent leader in the early 1960s. The counterculture movement in the U.S. brought significant social changes, including the liberalization of attitudes towards recreational drug use and sexuality as well as open defiance of the military draft and opposition to intervention in Vietnam. The late 1980s and early 1990s saw the collapse of the Warsaw Pact and the dissolution of the Soviet Union, which marked the end of the Cold War and solidified the U.S. as the world's sole superpower.\n\n\n=== Modernity (1991\u2013present) ===\n\nThe 1990s saw the longest recorded expansion in American history, a dramatic decline in crime, and advances in technology, with the World Wide Web, the evolution of the Pentium microprocessor in accordance with Moore's Law, rechargeable lithium-ion batteries, the first gene therapy trial, and cloning all emerging and being improved upon throughout the decade. The Human Genome Project was formally launched in 1990, building of the Large Hadron Collider commenced in 1998, and Nasdaq became the first stock market in the United States to trade online. In 1994, the country expelled an Iraqi invasion force from Kuwait in the Persian Gulf War. \nThe September 11, 2001 attacks by the pan-Islamist militant organization Al-Qaeda led to the war on terror and subsequent military interventions in Afghanistan and Iraq. The cultural impact of the attacks was profound and long-lasting. Combined with sexual abuse scandals within churches, large-scale secularization proceeded, as many Americans were not able to reconcile the events with a benevolent higher power. The U.S. housing bubble in 2006 culminated in the Great Recession, the largest economic contraction since the Great Depression. Starting in the 2010s, political polarization increased as sociopolitical debates on cultural issues dominated political discussion. The 2020s saw the United States emerge as the leader of the AI Spring, which has led to ongoing rapid and unprecedented development in the field of artificial intelligence, as advances in transformer machine learning enabled a number of generative AI systems. The nation returned to space exploration with the Artemis Program; it plans for a crewed lunar landing and launch of the world's first planned extraterrestrial space station, both in 2025, and yearly returns to the moon thereafter. Its long-term goal is to establish a permanent base on the Moon to facilitate the feasibility of human missions to Mars.\n\n\n== Geography ==\n\nThe United States is the world's third-largest nation by land and total area behind Russia and Canada. The 48 contiguous states and the District of Columbia occupy a combined area of 3,119,885 square miles (8,080,470 km2). The coastal plain of the Atlantic seaboard gives way to inland forests and rolling hills in the Piedmont plateau region.The Appalachian Mountains and the Adirondack massif separate the East Coast from the Great Lakes and the grasslands of the Midwest. The Mississippi River System\u2014the world's fourth longest river system\u2014runs mainly north\u2013south through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.The Rocky Mountains, west of the Great Plains, extend north to south across the country, peaking at over 14,000 feet (4,300 m) in Colorado. Farther west are the rocky Great Basin and Chihuahua, Sonoran, and Mojave deserts. The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast. The lowest and highest points in the contiguous United States are in the state of California, about 84 miles (135 km) apart. At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali is the highest peak in the country and continent. Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.\n\n\n=== Climate ===\n\nWith its large size and geographic variety, the United States includes most climate types. East of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south. The western Great Plains are semi-arid. Many mountainous areas of the American West have an alpine climate. The climate is arid in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon, Washington, and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as well as its territories in the Caribbean and the Pacific.States bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley. Overall, the United States receives more high-impact extreme weather incidents than any other country. Extreme weather became more frequent in the U.S. in the 21st century, with three times the number of reported heat waves as in the 1960s. In the American Southwest, droughts became more persistent and more severe.\n\n\n=== Biodiversity and conservation ===\n\nThe U.S. is one of 17 megadiverse countries containing large numbers of endemic species: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and over 1800 species of flowering plants are found in Hawaii, few of which occur on the mainland. The United States is home to 428 mammal species, 784 birds, 311 reptiles, 295 amphibians, and 91,000 insect species.There are 63 national parks, and hundreds of other federally managed parks, forests, and wilderness areas, managed by the National Park Service and other agencies. About 28% of the country's land is publicly owned and federally managed, primarily in the western states. Most of this land is protected, though some is leased for industrial use, and less than one percent is used for military purposes.Environmental issues in the United States include debates on non-renewable resources and nuclear energy, air and water pollution, biological diversity, logging and deforestation, and climate change. The U.S. Environmental Protection Agency (EPA) is the federal agency charged with addressing most environmental-related issues. The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act. The Endangered Species Act of 1973 provides a way to protect threatened and endangered species and their habitats. The United States Fish and Wildlife Service implements and enforces the Act. As of 2020, the U.S. ranked 24th among 180 nations in the Environmental Performance Index. The country joined the Paris Agreement on climate change in 2016 and has many other environmental commitments.\n\n\n== Government and politics ==\n\nThe United States was founded on the principles of the American Enlightenment. It is a federal republic of 50 states, a federal district, five territories and several uninhabited island possessions. It is the world's oldest surviving federation, and, according to the World Economic Forum, the oldest democracy as well. It is a liberal representative democracy \"in which majority rule is tempered by minority rights protected by law.\" The U.S. Constitution serves as the country's supreme legal document, establishing the structure and responsibilities of the federal government and its relationship with the individual states.The federal government comprises three branches, which are headquartered in Washington, D.C. and regulated by a system of checks and balances.\nThe U.S. Congress, a bicameral legislature, made up of the Senate and the House of Representatives, makes federal law, declares war, approves treaties, has the power of the purse, and has the power of impeachment. The Senate has 100 members (2 from each state), elected for a six-year term. The House of Representatives has 435 members from single member congressional districts allocated to each state on the basis of population, elected for a two-year term.\nThe U.S. President is the commander-in-chief of the military, can veto legislative bills before they become law (subject to congressional override), and appoints the members of the Cabinet (subject to Senate approval) and other officers, who administer and enforce federal laws and policies through their respective agencies. The president and the vice president are elected together in a presidential election. It is an indirect election, with the winner being determined by votes cast by electors of the Electoral College. The President and Vice President serve a four-year term and may be elected to the office no more than twice.\nThe U.S. federal judiciary, whose judges are all appointed for life by the President with Senate approval, consists primarily of the U.S. Supreme Court, the U.S. Courts of Appeals, and the U.S. District Courts. The U.S. Supreme Court interprets laws and overturn those they find unconstitutional. The Supreme Court is led by the chief justice of the United States. It has nine members who serve for life. The members are appointed by the sitting president when a vacancy becomes available.\n\n\n=== Political subdivisions ===\n\nIn the American federal system, sovereignty is shared between two levels of government: federal and state. Each of the 50 states has territory where it shares sovereignty with the federal government. People in the states are also represented by local elected governments, which are administrative divisions of the states. States are subdivided into counties or county equivalents, and further divided into municipalities. The District of Columbia is a federal district that contains the capital of the United States, the city of Washington. The territories and the District of Columbia are administrative divisions of the federal government.\n\n\n=== Foreign relations ===\n\nThe United States has an established structure of foreign relations, and it had the world's second-largest diplomatic corps in 2019. It is a permanent member of the United Nations Security Council, and home to the United Nations headquarters. The United States is a member of the G7, G20, and OECD intergovernmental organizations. Almost all countries have embassies and many have consulates (official representatives) in the country. Likewise, nearly all nations host formal diplomatic missions with the United States, except Iran, North Korea, and Bhutan. Though Taiwan does not have formal diplomatic relations with the U.S., it maintains close unofficial relations. The United States regularly supplies Taiwan with military equipment to deter potential Chinese aggression.The United States has a \"Special Relationship\" with the United Kingdom and strong ties with Canada, Australia, New Zealand, the Philippines, Japan, South Korea, Israel, and several European Union countries (France, Italy, Germany, Spain, and Poland). The U.S. works closely with its NATO allies on military and national security issues, and with nations in the Americas through the Organization of American States and the United States\u2013Mexico\u2013Canada Free Trade Agreement. In South America, Colombia is traditionally considered to be the closest ally of the United States. The U.S. exercises full international defense authority and responsibility for Micronesia, the Marshall Islands, and Palau through the Compact of Free Association. It has increasingly conducted strategic cooperation with India, and its ties with China have steadily deteriorated. Since 2014, the U.S. has become a key ally of Ukraine.\n\n\n=== Military ===\n\nThe President is the commander-in-chief of the United States Armed Forces and appoints its leaders, the secretary of defense and the Joint Chiefs of Staff. The Department of Defense, which is headquartered at the Pentagon near Washington, D.C., administers five of the six service branches, which are made up of the Army, Marine Corps, Navy, Air Force, and Space Force. The Coast Guard is administered by the Department of Homeland Security in peacetime and can be transferred to the Department of the Navy in wartime.The United States spent $877 billion on its military in 2022, which is by far the largest amount of any country, making up 39% of global military spending and accounting for 3.5% of the country's GDP. The U.S. has more than 40% of the world's nuclear weapons, the second-largest amount after Russia.The United States has the third-largest combined armed forces in the world, behind the Chinese People's Liberation Army and Indian Armed Forces. The military operates about 800 bases and facilities abroad, and maintains deployments greater than 100 active duty personnel in 25 foreign countries.\n\n\n=== Law enforcement and crime ===\n\nThere are about 18,000 U.S. police agencies from local to federal level in the United States. Law in the United States is mainly enforced by local police departments and sheriff departments in their municipal or county jurisdictions. The state police departments have authority in their respective state, and federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have national jurisdiction and specialized duties, such as protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws. State courts conduct most civil and criminal trials, and federal courts handle designated crimes and appeals of state court decisions.As of January 2023, the United States has the sixth highest per-capita incarceration rate in the world, at 531 people per 100,000; and the largest prison and jail population in the world with almost 2 million people incarcerated. A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States homicide rates \"were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher.\"\n\n\n== Economy ==\n\nThe U.S. has been the world's largest economy since at least 1900. The U.S. gross domestic product (GDP) of $25.5 trillion is the largest of any country in the world, constituting over 25% of the gross world product at market exchange rates and over 15% of the gross world product at purchasing power parity (PPP). From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the Group of Seven. The country ranks first in the world by disposable income per capita, nominal GDP, second by GDP (PPP), seventh by nominal GDP per capita, and eighth by GDP (PPP) per capita.Of the world's 500 largest companies, 136 are headquartered in the U.S. The U.S. dollar is the currency most used in international transactions and is the world's foremost reserve currency, backed by the country's dominant economy, its military, the petrodollar system, and its linked eurodollar and large U.S. treasuries market. Several countries use it as their official currency and in others it is the de facto currency. It has free trade agreements with several countries, including the USMCA. The U.S. ranked second in the Global Competitiveness Report in 2019, after Singapore. While its economy has reached a post-industrial level of development, the United States remains an industrial power. As of 2018, the U.S. is the second-largest manufacturing nation after China.New York City is the world's principal financial center, with the largest economic output, and the epicenter of the principal American metropolitan economy. The New York Stock Exchange and Nasdaq, both located in New York City, are the world's two largest stock exchanges by market capitalization and trade volume. The United States is at or near the forefront of technological advancement and innovation in many economic fields, especially in artificial intelligence; computers; pharmaceuticals; and medical, aerospace and military equipment. The nation's economy is fueled by abundant natural resources, a well-developed infrastructure, and high productivity. The largest U.S. trading partners are the European Union, Mexico, Canada, China, Japan, South Korea, the United Kingdom, Vietnam, India, and Taiwan. The United States is the world's largest importer and the second-largest exporter after China. It is by far the world's largest exporter of services.Americans have the highest average household and employee income among OECD member states, and the fourth-highest median household income, up from sixth-highest in 2013. Wealth in the United States is highly concentrated; the richest 10% of the adult population own 72% of the country's household wealth, while the bottom 50% own just 2%. Income inequality in the U.S. remains at record highs, with the top fifth of earners taking home more than half of all income and giving the U.S. one of the widest income distributions among OECD members. The U.S. ranks first in the number of dollar billionaires and millionaires, with 735 billionaires and nearly 22 million millionaires (as of 2023). There were about 582,500 sheltered and unsheltered homeless persons in the U.S. in 2022, with 60% staying in an emergency shelter or transitional housing program. In 2018, six million children experienced food insecurity. Feeding America estimates that around one in seven, or approximately 11 million, children experience hunger and do not know where they will get their next meal or when. As of June 2018, 40 million people, roughly 12.7% of the U.S. population, were living in poverty, including 13.3 million children.The United States has a smaller welfare state and redistributes less income through government action than most other high-income countries. It is the only advanced economy that does not guarantee its workers paid vacation nationally and is one of a few countries in the world without federal paid family leave as a legal right. The United States has a higher percentage of low-income workers than almost any other developed nation, largely because of a weak collective bargaining system and lack of government support for at-risk workers.\n\n\n=== Science, technology, and energy ===\n\nThe United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts and the establishment of a machine tool industry enabled America's large-scale manufacturing of consumer products in the late 19th century. In the early 20th century, factory electrification, the introduction of the assembly line, and other labor-saving techniques created the system of mass production. In 2022, the United States was the country with the second-highest number of published scientific papers. As of 2021, the U.S. ranked second by the number of patent applications, and third by trademark and industrial design applications. In 2023, the United States ranked 3rd in the Global Innovation Index.As of 2022, the United States receives approximately 81% of its energy from fossil fuel and the largest source of the country's energy came from petroleum (35.8%), followed by natural gas (33.4%), renewable sources (13.3%), coal (9.8%), and nuclear power (8%). The United States constitutes less than 5% of the world's population, but consumes 17% of the world's energy. The U.S. ranks as the second-highest emitter of greenhouse gases.\n\n\n=== Transportation ===\n\nPersonal transportation in the United States is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads, making it the longest network in the world. The Oldsmobile Curved Dash and the Ford Model T, both American cars, are considered the first mass-produced and mass-affordable cars, respectively. As of 2022, the United States is the second-largest manufacturer of motor vehicles and is home to Tesla, the world's most valuable car company. American automotive company General Motors held the title of the world's best-selling automaker from 1931 to 2008. Currently, the American automotive industry is the world's second-largest automobile market by sales, and the U.S. has the highest vehicle ownership per capita in the world, with 910 vehicles per 1000 people. The United States's rail transport network, the longest network in the world, handles mostly freight.The American civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned. The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways. Of the world's 50 busiest passenger airports, 16 are in the United States, including the top five and the busiest, Hartsfield\u2013Jackson Atlanta International Airport. As of 2020, there are 19,919 airports in the United States, of which 5,217 are designated as \"public use\", including for general aviation and other activities.Of the fifty busiest container ports, four are located in the United States, of which the busiest is the Port of Los Angeles. The country's inland waterways are the world's fifth-longest, and total 41,009 km (25,482 mi).\n\n\n== Demographics ==\n\n\n=== Population ===\n\nThe U.S. Census Bureau reported 331,449,281 residents as of April 1, 2020, making the United States the third-most populous nation in the world, after China and India. According to the Bureau's U.S. Population Clock, on January 28, 2021, the U.S. population had a net gain of one person every 100 seconds, or about 864 people per day. In 2018, 52% of Americans age 15 and over were married, 6% were widowed, 10% were divorced, and 32% had never been married. In 2021, the total fertility rate for the U.S. stood at 1.7 children per woman, and it had the world's highest rate of children (23%) living in single-parent households in 2019.The United States has a diverse population; 37 ancestry groups have more than one million members. White Americans with ancestry from Europe, the Middle East or North Africa, form the largest racial and ethnic group at 57.8% of the United States population. Hispanic and Latino Americans form the second-largest group and are 18.7% of the United States population. African Americans constitute the nation's third-largest ancestry group and are 12.1% of the total United States population. Asian Americans are the country's fourth-largest group, composing 5.9% of the United States population, while the country's 3.7 million Native Americans account for about 1%. In 2020, the median age of the United States population was 38.5 years.\n\n\n=== Language ===\n\nWhile many languages are spoken in the United States, English is overwhelmingly the most commonly spoken in a majority of the country. Although there is no official language at the federal level, some laws\u2014such as U.S. naturalization requirements\u2014standardize English, and most states have declared English as the official language. Three states and four U.S. territories have recognized local or indigenous languages in addition to English, including Hawaii (Hawaiian), Alaska (twenty Native languages), South Dakota (Sioux), American Samoa (Samoan), Puerto Rico (Spanish), Guam (Chamorro), and the Northern Mariana Islands (Carolinian and Chamorro). In Puerto Rico, Spanish is more widely spoken than English.According to the American Community Survey, in 2010 some 229 million people (out of the total U.S. population of 308 million) spoke only English at home. More than 37 million spoke Spanish at home, making it the second most commonly used language. Other languages spoken at home by one million people or more include Chinese (2.8 million), Tagalog (1.6 million), Vietnamese (1.4 million), French (1.3 million), Korean (1.1 million), and German (1 million).\n\n\n=== Immigration ===\n\nAmerica's immigrant population, numbering more than 50 million, is by far the world's largest in absolute terms. In 2022, there were 87.7 million immigrants and U.S.-born children of immigrants in the United States, accounting for nearly 27% of the overall U.S. population. In 2017, out of the U.S. foreign-born population, some 45% (20.7 million) were naturalized citizens, 27% (12.3 million) were lawful permanent residents, 6% (2.2 million) were temporary lawful residents, and 23% (10.5 million) were unauthorized immigrants. In 2019, the top countries of origin for immigrants were Mexico (24% of immigrants), India (6%), China (5%), the Philippines (4.5%), and El Salvador (3%). The United States has led the world in refugee resettlement for decades, admitting more refugees than the rest of the world combined.\n\n\n=== Religion ===\n\nThe First Amendment guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment.Religious practice is widespread, among the most diverse in the world, and profoundly vibrant. An overwhelming majority of Americans believe in a higher power or spiritual force, engage in spiritual practices such as prayer, and consider themselves religious or spiritual. The country has the world's largest Christian population. A majority of the global Jewish population lives in the United States, as measured by the Law of Return. Other notable faiths include Buddhism, Hinduism, Islam, many New Age movements, and Native American religions. Religious practice varies significantly by region. In the \"Bible Belt\", located within the Southern United States, evangelical Protestantism plays a significant role culturally. New England and the Western United States tend to be less religious, with Mormonism\u2014a Restorationist movement started in New York in the 19th century\u2014uniquely being the predominant religious affiliation in the state of Utah. While there was a secularization trend after 2001, most indicators of religious belief and interest have remained stable since the mid-2010s.\"Ceremonial deism\" is common in American culture.\n\n\n=== Urbanization ===\n\nAbout 82% of Americans live in urban areas, including suburbs; about half of those reside in cities with populations over 50,000. In 2022, 333 incorporated municipalities had populations over 100,000, nine cities had more than one million residents, and four cities (New York City, Los Angeles, Chicago, and Houston) had populations exceeding two million. Many U.S. metropolitan populations are growing rapidly, particularly in the South and West.\n\n\n=== Health ===\n\nIn a preliminary report, the Centers for Disease Control and Prevention (CDC) announced that U.S. life expectancy at birth was 76.4 years in 2021 (73.2 years for men and 79.1 years for women), down 0.9 years from 2020. The chief causes listed were the COVID-19 pandemic, accidents, drug overdoses, heart and liver disease, and suicides. Life expectancy was highest among Asians and Hispanics and lowest among Black and American Indian\u2013Alaskan Native (AIAN) peoples. Starting in 1998, the life expectancy in the U.S. fell behind that of other wealthy industrialized countries, and Americans' \"health disadvantage\" gap has been increasing ever since. The U.S. has one of the highest suicide rates among high-income countries. Approximately one-third of the U.S. adult population is obese and another third is overweight. The U.S. healthcare system far outspends that of any other nation, measured both in per capita spending and as a percentage of GDP, but attains worse healthcare outcomes when compared to peer nations for reasons that are debate. The United States is the only developed nation without a system of universal healthcare, and a significant proportion of the population that does not carry health insurance. Government-funded healthcare coverage for the poor (Medicaid) and for those age 65 and older (Medicare) is available to Americans who meet the programs' income or age qualifications. In 2010, former President Obama passed the Patient Protection and Affordable Care Act.\n\n\n=== Education ===\n\nAmerican public education is operated by state and local governments and regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of five or six (beginning with kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17. Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees. The basic literacy rate is near-universal. The country has the most Nobel Prize winners in history, with 411 (having won 413 awards).The United States has many private and public institutions of higher education including many of the world's top universities, as listed by various ranking organizations, are in the United States, including 19 of the top 25. There are local community colleges with generally more open admission policies, shorter academic programs, and lower tuition. The U.S. spends more on education per student than any nation in the world, spending an average of $12,794 per year on public elementary and secondary school students in the 2016\u20132017 school year.As for public expenditures on higher education, the U.S. spends more per student than the OECD average, and more than all nations in combined public and private spending. Despite some student loan forgiveness programs in place, student loan debt has increased by 102% in the last decade, and exceeded 1.7 trillion dollars as of 2022.\n\n\n== Culture and society ==\n\nAmericans have traditionally been characterized by a unifying political belief in an \"American creed\" emphasizing liberty, equality under the law, democracy, social equality, property rights, and a preference for limited government. Culturally, the country has been described as having the values of individualism and personal autonomy, having a strong work ethic, competitiveness, and voluntary altruism towards others. According to a 2016 study by the Charities Aid Foundation, Americans donated 1.44% of total GDP to charity, the highest rate in the world by a large margin. Part of both the Anglosphere and Western World, the United States is also home to a wide variety of ethnic groups, traditions, and values, and exerts immense cultural influence globally, with the phenomenon being termed Americanization. As such, the U.S. is considered a cultural superpower.Nearly all present Americans or their ancestors came from Eurafrasia (\"the Old World\") within the past five centuries. Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa. More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as a homogenizing melting pot, and a heterogeneous salad bowl, with immigrants contributing to, and often assimilating into, mainstream American culture. The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants. Whether this perception is accurate has been a topic of debate. While mainstream culture holds that the United States is a classless society, scholars identify significant differences between the country's social classes, affecting socialization, language, and values. Americans tend to greatly value socioeconomic achievement, but being ordinary or average is promoted by some as a noble condition as well.The United States is considered to have the strongest protections of free speech of any country under the First Amendment, which protects flag desecration, hate speech, blasphemy, and lese-majesty as forms of protected expression. A 2016 Pew Research Center poll found that Americans were the most supportive of free expression of any polity measured. They are the \"most supportive of freedom of the press and the right to use the Internet without government censorship.\" It is a socially progressive country with permissive attitudes surrounding human sexuality. LGBT rights in the United States are among the most advanced in the world.\n\n\n=== Literature ===\n\nColonial American authors were influenced by John Locke and various other Enlightenment philosophers.Before and shortly after the Revolutionary War, the newspaper rose to prominence, filling a demand for anti-British national literature. Led by Ralph Waldo Emerson and Margaret Fuller in New England, transcendentalism branched from Unitarianism as the nation's first major intellectual movement. During the nineteenth-century American Renaissance, writers like Walt Whitman and Harriet Beecher Stowe established a distinctive American literary tradition. As literacy rates rose, periodicals published more stories centered around industrial workers, women, and the rural poor. Naturalism, regionalism, and realism\u2014the latter associated with Mark Twain\u2014were the major literary movements of the period. While modernism generally took on an international character, modernist authors working within the United States more often rooted their work in specific regions, peoples, and cultures. Following the Great Migration to northern cities, African-American and black West Indian authors of the Harlem Renaissance developed an independent tradition of literature that rebuked a history of inequality and celebrated black culture.  An important cultural export during the Jazz Age, these writings were a key influence on the n\u00e9gritude philosophy. In the 1950s, an ideal of homogeneity led many authors to attempt to write the Great American Novel, while the Beat Generation rejected this conformity, using styles that elevated the impact of the spoken word over mechanics to describe drug use, sexuality, and the failings of society.Contemporary literature is more pluralistic than in previous eras, with the closest thing to a unifying feature being a trend toward self-conscious experiments with language.\n\n\n=== Mass media ===\n\nMedia is broadly uncensored, with the First Amendment providing significant protections, as reiterated in New York Times Co. v. United States. The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX). The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches. As of 2021, about 83% of Americans over age 12 listen to broadcast radio, while about 41% listen to podcasts. As of September 30, 2014, there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC). Much of the public radio broadcasting is supplied by NPR, incorporated in February 1970 under the Public Broadcasting Act of 1967.Globally-recognized newspapers in the United States include The Wall Street Journal, The New York Times, The Washington Post, and USA Today. More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or, in a situation that is increasingly rare, by individuals or families. Major cities often have alternative newspapers to complement the mainstream daily papers, such as The Village Voice in New York City and LA Weekly in Los Angeles. The five most popular websites used in the U.S. are Google, YouTube, Amazon, Yahoo, and Facebook, with all of them being American companies.As of 2022, the video game market of the United States is the world's largest by revenue. Major video game publishers and developers headquartered in the United States are Sony Interactive Entertainment, Take-Two, Activision Blizzard, Electronic Arts, Xbox Game Studios, Bethesda Softworks, Epic Games, Valve, Warner Bros., Riot Games, and others. There are 444 publishers, developers, and hardware companies in California alone.\n\n\n=== Theater ===\n\nThe United States is well known for its cinema and theater. Mainstream theater in the United States derives from the old European theatrical tradition and has been heavily influenced by the British theater. The central hub of the American theater scene is Manhattan, with its divisions of Broadway, off-Broadway, and off-off-Broadway. Many movie and television stars have gotten their big break working in New York productions. Outside New York City, many cities have professional regional or resident theater companies that produce their own seasons. The biggest-budget theatrical productions are musicals. U.S. theater has an active community theater culture.The Tony Awards recognizes excellence in live Broadway theatre and are presented at an annual ceremony in Manhattan. The awards are given for Broadway productions and performances. One is also given for regional theatre. Several discretionary non-competitive awards are given as well, including a Special Tony Award, the Tony Honors for Excellence in Theatre, and the Isabelle Stevenson Award.\n\n\n=== Fashion ===\n\nThe United States and China collectively account for the majority of global apparel demand. Apart from professional business attire, American fashion is eclectic and predominantly informal. While Americans' diverse cultural roots are reflected in their clothing, particularly those of recent immigrants, sneakers, jeans, T-shirts, and baseball caps are emblematic of American styles. New York City is considered to be one of the \"big four\" global fashion capitals, along with Paris, Milan, and London. A study demonstrated that general proximity to  Manhattan's Garment District has been synonymous with American fashion since it's inception in the early 20th century.The headquarters of many leading designer labels such as Ralph Lauren Corporation, Calvin Klein, J.Crew, Michael Kors, Alexander Wang, Vera Wang, Marc Jacobs, Oscar de la Renta, Diane von Furstenberg, Donna Karan, and Victoria's Secret reside in Manhattan. Labels such as Abercrombie & Fitch and Eck\u014d Unltd. cater to various niche markets, such as pre teens. There has been a trend in the United States fashion towards sustainable clothing. New York Fashion Week is one of the most influential fashion weeks in the world, and occurs twice a year.\n\n\n=== Music ===\nThe United States has the world's largest music market with a total retail value of $15.9 billion in 2022. Most of the world's major record companies are based in the U.S.; they are represented by the Recording Industry Association of America (RIAA).American folk music encompasses numerous music genres, variously known as traditional music, traditional folk music, contemporary folk music, or roots music. Many traditional songs have been sung within the same family or folk group for generations, and sometimes trace back to such origins as the British Isles, Mainland Europe, or Africa. Elements from folk idioms such as the blues and what is known as old-time music were adopted and transformed into popular genres with global audiences.\nJazz grew from blues and ragtime in the early 20th century, developing from the innovations and recordings of composers such as W.C. Handy and Jelly Roll Morton.  Louis Armstrong and Duke Ellington increased its popularity early in the 20th century. The rhythmic and lyrical styles of African-American music have also influenced American music at large.  Banjos were brought to America through the slave trade. Minstrel shows incorporating the instrument into their acts led to its increased popularity and widespread production. Country music developed in the 1920s, bluegrass and rhythm and blues in the 1940s.\nFirst invented in the 1930s, and mass-produced by the 1940s, the electric guitar had an enormous influence on popular music, in particular due to the development of rock and roll, pioneered by Elvis Presley and Chuck Berry, among others, in the mid-1950s. Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing musical acts in worldwide sales. In the 1960s, Bob Dylan emerged from the folk revival to become one of the country's most celebrated songwriters. Mid-20th-century American pop stars such as Bing Crosby, Frank Sinatra, and Elvis Presley became global celebrities, as have artists of the late 20th century, such as Prince, Michael Jackson, Madonna, Whitney Houston, and Mariah Carey, and of the early 21st century, such as Taylor Swift, Eminem, Ariana Grande, Britney Spears, Justin Timberlake, and Bruno Mars. The musical forms of punk and hip hop both originated in the United States. American professional opera singers have reached the highest level of success in that form, including Ren\u00e9e Fleming, Leontyne Price, Beverly Sills, Nelson Eddy, and many others.\n\n\n=== Visual arts ===\n\nIn the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene. Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles, which would become known as American modernism. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. Major photographers include Alfred Stieglitz, Edward Steichen, Dorothea Lange, Edward Weston, James Van Der Zee, Ansel Adams, and Gordon Parks. The Metropolitan Museum of Art is the largest art museum in the United States. The tide of modernism and then postmodernism has brought global fame to American architects, including  Frank Lloyd Wright, Philip Johnson, and Frank Gehry.\n\n\n=== Cinema ===\n\nThe U.S. film industry has a worldwide influence and following. Hollywood, a district in northern Los Angeles, the nation's second-most populous city, is the leader in motion picture production and the most recognizable movie industry in the world. The major film studios of the United States are the primary source of the most commercially successful and most ticket-selling movies in the world. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization. The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929, and the Golden Globe Awards have been held annually since January 1944.The industry enjoyed its golden years, in what is commonly referred to as the \"Golden Age of Hollywood\", from the early sound period until the early 1960s, with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures. In the 1970s, \"New Hollywood\" or the \"Hollywood Renaissance\" was defined by grittier films influenced by French and Italian realist pictures of the post-war period. The 21st century was marked by the rise of American streaming platforms, which came to rival traditional cinema.\n\n\n=== Cuisine ===\n\nEarly settlers were introduced by Native Americans to such indigenous, non-European foods as turkey, sweet potatoes, corn, squash, and maple syrup. Of the most enduring and pervasive examples are variations of the native dish called succotash. Early settlers and later immigrants combined these with foods they had known, such as wheat flour, beef, and milk to create a distinctive American cuisine. New World crops, especially corn, potatoes, and the main course turkey, are part of a shared national menu on Thanksgiving: when many Americans make or purchase traditional dishes to celebrate the occasion.Characteristic American dishes such as apple pie, fried chicken, doughnuts, french fries, macaroni and cheese, ice cream, pizza, hamburgers, and hot dogs derive from the recipes of various immigrant groups. Mexican dishes such as burritos and tacos preexisted the United States in areas later annexed from Mexico, and pasta dishes freely adapted from Italian sources are all widely consumed.The American fast food industry, the world's first and largest, pioneered the drive-through format in the 1940s and is often viewed as being a symbol of U.S. marketing dominance. American companies such as McDonald's, Burger King, Pizza Hut, Kentucky Fried Chicken, and Domino's Pizza, among many others, have numerous outlets around the world.\n\n\n=== Sports ===\n\nThe most popular spectator sports in the U.S. are American football, basketball, baseball, soccer, and ice hockey. While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, many of which have become popular worldwide. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate European contact. The market for professional sports in the United States was approximately $69 billion in July 2013, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.American football is by several measures the most popular spectator sport in the United States; the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by tens of millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball being the top league. Basketball, soccer and ice hockey are the country's next three most popular professional team sports, with the top leagues being the National Basketball Association and the National Hockey League, which are the premier leagues worldwide for these sports. The most-watched individual sports in the U.S. are golf and auto racing, particularly NASCAR and IndyCar.On the collegiate level, earnings for the member institutions exceed $1 billion annually, and college football and basketball attract large audiences, as the NCAA Final Four is one of the most watched national sporting events. In many respects, the intercollegiate sports level serves as a feeder system to the professional level, as the elite college athletes are chosen to compete at the next level. This system differs greatly from nearly all other countries in the world, which generally have government-funded sports organizations that serve as a feeder system for professional competition.Eight Olympic Games have taken place in the United States. The 1904 Summer Olympics in St. Louis, Missouri, were the first-ever Olympic Games held outside of Europe. The Olympic Games will be held in the U.S. for a ninth time when Los Angeles hosts the 2028 Summer Olympics. U.S. athletes have won a total of 2,959 medals (1,173 gold) at the Olympic Games, by far the most of any country.In international soccer, the men's national soccer team qualified for eleven World Cups, and the women's national team has won the FIFA Women's World Cup and Olympic soccer tournament four times each. The United States hosted the 1994 FIFA World Cup and will co-host, along with Canada and Mexico, the 2026 FIFA World Cup.\n\n\n== See also ==\nLists of U.S. state topics\nOutline of the United States\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== External links ==\nKey Development Forecasts for the United States from International Futures\n\n\n=== Government ===\nOfficial U.S. Government web portal \u2013 gateway to government sites\nHouse \u2013 official website of the United States House of Representatives\nSenate \u2013 official website of the United States Senate\nWhite House \u2013 official website of the President of the United States\nSupreme Court \u2013 official website of the Supreme Court of the United States\n\n\n=== History ===\n\"Historical Documents\" \u2013 website from the National Center for Public Policy Research\n\"U.S. National Mottos: History and Constitutionality\". Religious Tolerance. Analysis by the Ontario Consultants on Religious Tolerance.\n\"Historical Statistics\" \u2013 links to US historical data\n\n\n=== Maps ===\n\"National Atlas of the United States\" \u2013 official maps from the U.S. Department of the Interior\n Wikimedia Atlas of the United States\n Geographic data related to United States at OpenStreetMap\n\"Measure of America\" \u2013 a variety of mapped information relating to health, education, income, safety and demographics in the United States"}, {"id": 21, "title": "George Washington", "content": "George Washington (February 22, 1732 \u2013 December 14, 1799) was an American military officer, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797. Appointed by the Second Continental Congress as commander of the Continental Army in June 1775, Washington led Patriot forces to victory in the American Revolutionary War and then served as president of the Constitutional Convention in 1787, which drafted and ratified the Constitution of the United States and established the American federal government. Washington has thus been called the \"Father of his Country\".\nWashington's first public office, from 1749 to 1750, was as surveyor of Culpeper County in the Colony of Virginia. He subsequently received military training and was assigned command of the Virginia Regiment during the French and Indian War. He was later elected to the Virginia House of Burgesses and was named a delegate to the Continental Congress in Philadelphia, which appointed him Commander-in-Chief of the Continental Army. Washington led American forces to a decisive victory over the British in the Revolutionary War, leading the British to sign the Treaty of Paris, which acknowledged the sovereignty and independence of the United States. He resigned his commission in 1783 after the conclusion of the Revolutionary War.\nWashington played an indispensable role in adopting and ratifying the Constitution, which replaced the Articles of Confederation in 1789. He was then twice elected president by the Electoral College unanimously. As the first U.S. president, Washington implemented a strong, well-financed national government while remaining impartial in a fierce rivalry that emerged between cabinet members Thomas Jefferson and Alexander Hamilton. During the French Revolution, he proclaimed a policy of neutrality while sanctioning the Jay Treaty. He set enduring precedents for the office of president, including use of the title \"Mr. President\" and the two-term tradition. His 1796 farewell address became a preeminent statement on republicanism in which he wrote about the importance of national unity and the dangers regionalism, partisanship, and foreign influence pose to it.\nWashington has been memorialized by monuments, a federal holiday, various media depictions, geographical locations including the national capital, the State of Washington, stamps, and currency. He is ranked among the greatest U.S. presidents. In 1976, Washington was posthumously promoted to the rank of General of the Armies, the highest rank in the U.S. Army. His legacy has become increasingly controversial over time, however, as a result of his ownership of slaves and his relationship with slavery. His historical reputation has also been complicated in modern discourse by his policy of assimilating Native Americans into Anglo-American culture and waging war against indigenous peoples during the Revolutionary War and the Northwest Indian War.\n\n\n== Early life (1732\u20131752) ==\n\nGeorge Washington was born on February 22, 1732, at Popes Creek in Westmoreland County, Virginia. He was the first of six children of Augustine and Mary Ball Washington. His father was a justice of the peace and a prominent public figure who had four additional children from his first marriage to Jane Butler. The family moved to Little Hunting Creek in 1734 before eventually settling in Ferry Farm near Fredericksburg, Virginia. When Augustine died in 1743, Washington inherited Ferry Farm and ten slaves; his older half-brother Lawrence inherited Little Hunting Creek and renamed it Mount Vernon.Washington did not have the formal education his elder brothers received at Appleby Grammar School in England, but he did attend the Lower Church School in Hartfield. He learned mathematics, trigonometry, and land surveying, and became a talented draftsman and mapmaker. By early adulthood, he was writing with \"considerable force\" and \"precision\". As a teenager, to practice his penmanship, Washington compiled over a hundred rules for social interaction styled Rules of Civility and Decent Behaviour in Company and Conversation, copied from an English translation of a French book of manners.Washington often visited Mount Vernon and Belvoir, the plantation of William Fairfax, Lawrence's father-in-law. Fairfax became Washington's patron and surrogate father, and Washington spent a month in 1748 with a team surveying Fairfax's Shenandoah Valley property. The following year, he received a surveyor's license from the College of William & Mary. Even though Washington had not served the customary apprenticeship, Fairfax appointed him surveyor of Culpeper County, Virginia, where he took his oath of office July 20, 1749. He subsequently familiarized himself with the frontier region, and though he resigned from the job in 1750, he continued to do surveys west of the Blue Ridge Mountains. By 1752, he had bought almost 1,500 acres (600 ha) in the Valley and owned 2,315 acres (937 ha).In 1751, Washington made his only trip abroad when he accompanied Lawrence to Barbados, hoping the climate would cure his brother's tuberculosis. Washington contracted smallpox during that trip, which left his face slightly scarred. Lawrence died in 1752, and Washington leased Mount Vernon from his widow Anne; he inherited it outright after her death in 1761.\n\n\n== Colonial military career (1752\u20131758) ==\nLawrence Washington's service as adjutant general of the Virginia militia inspired George to seek a commission. Virginia's lieutenant governor, Robert Dinwiddie, appointed Washington as a major and commander of one of the four militia districts. The British and French were competing for control of the Ohio Valley: the British were constructing forts along the Ohio River, and the French between the Ohio River and Lake Erie.In October 1753, Dinwiddie appointed Washington as a special envoy. He had sent Washington to demand French forces to vacate land that was claimed by the British. Washington was also appointed to make peace with the Iroquois Confederacy, and to gather further intelligence about the French forces. Washington met with Half-King Tanacharison, and other Iroquois chiefs, at Logstown, and gathered information about the numbers and locations of the French forts, as well as intelligence concerning individuals taken prisoner by the French. Washington was nicknamed Conotocaurius by Tanacharison. The name, meaning \"devourer of villages\", had been given to his great-grandfather John Washington in the late 17th century by the Susquehannock.Washington's party reached the Ohio River in November 1753, and was intercepted by a French patrol. The party was escorted to Fort Le Boeuf, where Washington was received in a friendly manner. He delivered the British demand to vacate to the French commander Saint-Pierre, but the French refused to leave. Saint-Pierre gave Washington his official answer after a few days' delay, as well as food and winter clothing for his party's journey back to Virginia. Washington completed the precarious mission in 77 days, in difficult winter conditions, achieving a measure of distinction when his report was published in Virginia and London.\n\n\n=== French and Indian War ===\n\nIn February 1754, Dinwiddie promoted Washington to lieutenant colonel and second-in-command of the 300-strong Virginia Regiment, with orders to confront French forces at the Forks of the Ohio. Washington set out with half the regiment in April and soon learned a French force of 1,000 had begun construction of Fort Duquesne there. In May, having set up a defensive position at Great Meadows, he learned that the French had made camp seven miles (11 km) away; he decided to take the offensive.The French detachment proved to be only about 50 men, so Washington advanced on May 28 with a small force of Virginians and Indian allies to ambush them. During the ambush, French forces were killed outright with muskets and hatchets, including French commander Joseph Coulon de Jumonville, who had been carrying a diplomatic message for the British. The French later found their countrymen dead and scalped, blaming Washington, who had retreated to Fort Necessity.The full Virginia Regiment joined Washington at Fort Necessity the following month with news that he had been promoted to command of the regiment and colonel upon the regimental commander's death. The regiment was reinforced by an independent company of a hundred South Carolinians led by Captain James Mackay; his royal commission outranked Washington's and a conflict of command ensued. On July 3, a French force attacked with 900 men, and the ensuing battle ended in Washington's surrender. He signed a surrender document in which he unwittingly took responsibility for \"assassinating\" Jumonville, later blaming the translator for not properly translating it.In the aftermath, Colonel James Innes took command of intercolonial forces, the Virginia Regiment was divided, and Washington was offered a captaincy in one of the newly formed regiments. He refused, however, as it would have been a demotion and instead resigned his commission. The \"Jumonville affair\" became the incident which ignited the French and Indian War, later to become part of the Seven Years' War.\nIn 1755, Washington served voluntarily as an aide to General Edward Braddock, who led a British expedition to expel the French from Fort Duquesne and the Ohio Country. On Washington's recommendation, Braddock split the army into one main column and a lightly equipped \"flying column\". Suffering from severe dysentery, Washington was left behind, and when he rejoined Braddock at Monongahela the French and their Indian allies ambushed the divided army. Two-thirds of the British force became casualties, including the mortally wounded Braddock. Under the command of Lieutenant Colonel Thomas Gage, Washington, still very ill, rallied the survivors and formed a rear guard, allowing the remnants of the force to disengage and retreat.During the engagement, he had two horses shot from under him, and his hat and coat were bullet-pierced. His conduct under fire redeemed his reputation among critics of his command in the Battle of Fort Necessity, but he was not included by the succeeding commander (Colonel Thomas Dunbar) in planning subsequent operations.The Virginia Regiment was reconstituted in August 1755, and Dinwiddie appointed Washington its commander, again with the rank of colonel. Washington clashed over seniority almost immediately, this time with John Dagworthy, another captain of superior royal rank, who commanded a detachment of Marylanders at the regiment's headquarters in Fort Cumberland. Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor as Commander-in-Chief, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. Shirley ruled in Washington's favor only in the matter of Dagworthy; Loudoun humiliated Washington, refused him a royal commission and agreed only to relieve him of the responsibility of manning Fort Cumberland.In 1758, the Virginia Regiment was assigned to the British Forbes Expedition to capture Fort Duquesne. Washington disagreed with General John Forbes' tactics and chosen route. Forbes nevertheless made Washington a brevet brigadier general and gave him command of one of the three brigades that would assault the fort. The French had abandoned the fort and the valley before the assault, however, and Washington only saw a friendly fire incident which left 14 dead and 26 injured. Frustrated, he resigned his commission soon afterwards and returned to Mount Vernon.Under Washington, the Virginia Regiment had defended 300 miles (480 km) of frontier against twenty Indian attacks in ten months. He increased the professionalism of the regiment as it grew from 300 to 1,000 men, and Virginia's frontier population suffered less than other colonies. Though he failed to realize a royal commission, he gained self-confidence, leadership skills, and knowledge of British military tactics. The destructive competition Washington witnessed among colonial politicians fostered his later support of a strong central government.\n\n\n== Marriage, civilian, and political life (1755\u20131775) ==\n\nOn January 6, 1759, Washington, at age 26, married Martha Dandridge Custis, the 27-year-old widow of wealthy plantation owner Daniel Parke Custis. The marriage took place at Martha's estate; she was intelligent, gracious, and experienced in managing a planter's estate, and the couple had a happy marriage. They moved to Mount Vernon, near Alexandria, where he lived as a planter of tobacco and wheat and emerged as a political figure.Washington's 1751 bout with smallpox is thought to have rendered him sterile, though it is equally likely that \"Martha may have sustained injury during the birth of Patsy, her final child, making additional births impossible.\" The couple lamented not having any children together. Despite this, the two raised Martha's two children John Parke Custis (Jacky) and Martha Parke Custis (Patsy), and later Jacky's two youngest children Eleanor Parke Custis (Nelly) and George Washington Parke Custis (Washy), along with numerous nieces and nephews.The marriage gave Washington control over Martha's one-third dower interest in the 18,000-acre (7,300 ha) Custis estate, and he managed the remaining two-thirds for Martha's children; the estate also included 84 slaves. As a result, he became one of the wealthiest men in Virginia, which increased his social standing.At Washington's urging, Governor Lord Botetourt fulfilled Dinwiddie's 1754 promise of land bounties to all-volunteer militia during the French and Indian War. In late 1770, Washington inspected the lands in the Ohio and Great Kanawha regions, and he engaged surveyor William Crawford to subdivide it. Crawford allotted 23,200 acres (9,400 ha) to Washington; Washington told the veterans that their land was hilly and unsuitable for farming, and he agreed to purchase 20,147 acres (8,153 ha), leaving some feeling they had been duped. He also doubled the size of Mount Vernon to 6,500 acres (2,600 ha) and, by 1775, had increased its slave population by more than a hundred.As a respected military hero and large landowner, Washington held local offices and was elected to the Virginia provincial legislature, representing Frederick County in the House of Burgesses for seven years beginning in 1758. He first ran for the seat in 1755 but was soundly beaten by Hugh West. When he ran in 1758, Washington plied voters with beer, brandy, and other beverages. Despite being away serving on the Forbes Expedition, he won the election with roughly 40 percent of the vote, defeating three opponents with the help of local supporters.Early in his legislative career, Washington rarely spoke or even attended legislative sessions. He would later become a prominent critic of Britain's taxation policy and mercantilist policies towards the American colonies and became more politically active starting in the 1760s.Washington imported luxuries and other goods from England, paying for them by exporting tobacco. His profligate spending combined with low tobacco prices left him \u00a31,800 in debt by 1764, prompting him to diversify his holdings. In 1765, because of erosion and other soil problems, he changed Mount Vernon's primary cash crop from tobacco to wheat and expanded operations to include corn flour milling and fishing.Washington soon was counted among the political and social elite in Virginia. From 1768 to 1775, he invited some 2,000 guests to Mount Vernon, mostly those whom he considered people of rank, and was known to be exceptionally cordial toward guests. Washington also took time for leisure with fox hunting, fishing, dances, theater, cards, backgammon, and billiards.Washington's stepdaughter Patsy suffered from epileptic attacks from age 12, and she died at Mount Vernon in 1773. The following day, he wrote to Burwell Bassett: \"It is easier to conceive, than to describe, the distress of this Family\". He canceled all business activity and remained with Martha every night for three months.\n\n\n=== Opposition to the British Parliament and Crown ===\n\nWashington played a central role before and during the American Revolution. His distrust of the British military had begun when he was passed over for promotion into the Regular Army. Opposed to taxes imposed by the British Parliament on the Colonies without proper representation, he and other colonists were also angered by the Royal Proclamation of 1763 which banned American settlement west of the Allegheny Mountains and protected the British fur trade.Washington believed the Stamp Act 1765 was an \"Act of Oppression\" and celebrated its repeal the following year. In March 1766, Parliament passed the Declaratory Act asserting that Parliamentary law superseded colonial law. In the late 1760s, the interference of the British Crown in American lucrative western land speculation spurred the American Revolution. Washington was a prosperous land speculator, and in 1767, he encouraged \"adventures\" to acquire backcountry western lands. Washington helped lead widespread protests against the Townshend Acts passed by Parliament in 1767, and he introduced a proposal in May 1769 which urged Virginians to boycott British goods; the Acts were mostly repealed in 1770.Parliament sought to punish Massachusetts colonists for their role in the Boston Tea Party in 1774 by passing the Coercive Acts, which Washington saw as \"an invasion of our rights and privileges\". He said Americans must not submit to acts of tyranny since \"custom and use shall make us as tame and abject slaves, as the blacks we rule over with such arbitrary sway\". That July, he and George Mason drafted a list of resolutions for the Fairfax County committee, including a call to end the Atlantic slave trade, which were adopted.On August 1, Washington attended the First Virginia Convention. There, he was selected as a delegate to the First Continental Congress. As tensions rose in 1774, he helped train militias in Virginia and organized enforcement of the Continental Association boycott of British goods instituted by the Congress.The American Revolutionary War broke out on April 19, 1775, with the Battles of Lexington and Concord and the Siege of Boston. Upon hearing the news, Washington was \"sobered and dismayed\", and he hastily departed Mount Vernon on May 4, 1775, to join the Second Continental Congress in Philadelphia.\n\n\n== Commander in chief (1775\u20131783) ==\n\nOn June 14, 1775, Congress created the Continental Army and John Adams nominated Washington as its commander-in-chief, mainly because of his military experience and the belief that a Virginian would better unite the colonies. He was unanimously elected by Congress the next day. Washington appeared before Congress in uniform and gave an acceptance speech on June 16, declining a salary, though he was later reimbursed expenses.Washington was commissioned on June 19 and officially appointed by Congress as \"General & Commander in chief of the army of the United Colonies and of all the forces raised or to be raised by them\". He was instructed to take charge of the Siege of Boston on June 22, 1775.Congress chose his primary staff officers, including Major General Artemas Ward, Adjutant General Horatio Gates, Major General Charles Lee, Major General Philip Schuyler, and Major General Nathanael Greene.  Henry Knox, a young bookkeeper, impressed Adams and Washington with ordnance knowledge and was subsequently promoted to colonel and chief of artillery. Similarly, Washington was impressed by Alexander Hamilton's intelligence and bravery. He would later promote him to colonel and appoint him his aide-de-camp.Washington initially banned the enlistment of blacks, both free and enslaved, into the Continental Army. The British saw an opportunity to divide the colonies, and the colonial governor of Virginia issued a proclamation, which promised freedom to slaves if they joined the British. Desperate for manpower by late 1777, Washington relented and overturned his ban. By the end of the war, around one-tenth of Washington's army were blacks. Following the British surrender, Washington sought to enforce terms of the preliminary Treaty of Paris (1783) by reclaiming slaves freed by the British and returning them to servitude. He arranged to make this request to Sir Guy Carleton on May 6, 1783. Instead, Carleton issued 3,000 freedom certificates and all former slaves in New York City were able to leave before the city was evacuated by the British in late November 1783.\n\n\n=== Siege of Boston ===\n\nEarly in 1775, in response to the growing rebellious movement, London sent British troops to occupy Boston, led by General Thomas Gage, commander of British forces in America. They set up fortifications, making the city impervious to attack. Local militias surrounded the city and effectively trapped the British troops, resulting in a standoff.\nAs Washington headed for Boston, word of his march preceded him, and he was greeted everywhere; gradually, he became a symbol of the Patriot cause. Upon arrival on July 2, 1775, two weeks after the Battle of Bunker Hill, he set up headquarters in Cambridge. When he went to inspect the army, he found undisciplined militia. After consultation, he initiated Benjamin Franklin's suggested reforms: drilling the soldiers and imposing strict discipline. Washington ordered his officers to identify the skills of recruits to ensure military effectiveness, while removing incompetent officers. He petitioned Gage, his former superior, to release captured Patriot officers from prison and treat them humanely. In October 1775, King George III declared that the colonies were in open rebellion and relieved Gage of command for incompetence, replacing him with General William Howe.The Continental Army, reduced to only 9,600 men by January 1776 due to expiring short-term enlistments, had to be supplemented with militia. Soon, they were joined by Knox with heavy artillery captured from Fort Ticonderoga. When the Charles River froze over, Washington was eager to cross and storm Boston, but General Gates and others were opposed to untrained militia striking well-garrisoned fortifications. Instead, he agreed to secure the Dorchester Heights, 100 feet above Boston, with Knox's artillery to try to force the British out.On March 9, under cover of darkness, Washington's troops bombarded British ships in Boston harbor. On March 17, 9,000 British troops and Loyalists began a chaotic ten-day evacuation aboard 120 ships. Soon after, Washington entered the city with 500 men, with explicit orders not to plunder the city. He refrained from exerting military authority in Boston, leaving civilian matters in the hands of local authorities.\n\n\n=== New York and New Jersey ===\n\n\n==== Battle of Long Island ====\n\nAfter the victory at Boston, Washington correctly guessed that the British would return to New York City, a Loyalist stronghold, and retaliate. He arrived there on April 13, 1776, and ordered the construction of fortifications to thwart the expected British attack. He also ordered his occupying forces to treat civilians and their property with respect, to avoid the abuses Bostonians suffered at the hands of British troops.Howe transported his resupplied army, with the British fleet, from Halifax to New York City. George Germain, who ran the British war effort in England, believed it could be won with one \"decisive blow\". The British forces, including more than a hundred ships and thousands of troops, began arriving on Staten Island on July 2 to lay siege to the city. After the Declaration of Independence was unanimously adopted on July 4, Washington informed his troops on July 9 that Congress had declared the united colonies to be \"free and independent states\".Howe's troop strength totaled 32,000 regulars and Hessian auxiliaries, and Washington's consisted of 23,000, mostly raw recruits and militia. In August, Howe landed 20,000 troops at Gravesend, Brooklyn, and approached Washington's fortifications Opposing his generals, Washington chose to fight, based on inaccurate information that Howe's army had only 8,000-plus troops. In the Battle of Long Island, Howe assaulted Washington's flank and inflicted 1,500 Patriot casualties, the British suffering 400. Washington retreated, instructing General William Heath to acquire river craft. On August 30, General William Alexander held off the British and gave cover while the army crossed the East River under darkness to Manhattan without loss of life or materiel, although Alexander was captured.\nHowe was emboldened by his Long Island victory and dispatched Washington as \"George Washington, Esq.\" in futility to negotiate peace. Washington declined, demanding to be addressed with diplomatic protocol, as general and fellow belligerent, not as a \"rebel\", lest his men be hanged as such if captured. The Royal Navy bombarded the unstable earthworks on lower Manhattan Island. Despite misgivings, Washington heeded the advice of Generals Greene and Putnam to defend Fort Washington. They were unable to hold it; Washington abandoned the fort and ordered his army north to the White Plains.Howe's pursuit forced Washington to retreat across the Hudson River to Fort Lee to avoid encirclement. Howe landed his troops on Manhattan in November and captured Fort Washington, inflicting high casualties on the Americans. Washington was responsible for delaying the retreat, though he blamed Congress and General Greene. Loyalists in New York City considered Howe a liberator and spread a rumor that Washington had set fire to the city. Patriot morale reached its lowest when Lee was captured. Now reduced to 5,400 troops, Washington's army retreated through New Jersey, and Howe broke off pursuit to set up winter quarters in New York.\n\n\n==== Crossing the Delaware, Trenton, and Princeton ====\n\nWashington crossed the Delaware River into Pennsylvania, where Lee's replacement General John Sullivan joined him with 2,000 more troops. The future of the Continental Army was in doubt due to lack of supplies, a harsh winter, expiring enlistments, and desertions. Washington was disappointed that many New Jersey residents were Loyalists or skeptical about independence.Howe split up his army and posted a Hessian garrison at Trenton to hold western New Jersey and the east shore of the Delaware. Desperate for a victory, Washington and his generals devised a surprise attack on Trenton. The army was to cross the Delaware in three divisions: one led by Washington (2,400 troops), another by General James Ewing (700), and the third by Colonel John Cadwalader (1,500). The force was to then split, with Washington taking the Pennington Road and General Sullivan traveling south on the river's edge.Washington ordered a 60-mile search for Durham boats to transport his army, and the destruction of vessels that could be used by the British. He personally risked capture while staking out the Jersey shoreline alone leading up to the crossing. Washington crossed the Delaware on Christmas night, 1776. His men followed across the ice-obstructed river from McConkey's Ferry, with 40 men per vessel. The wind churned up the waters, and they were pelted with hail, but by 3:00 a.m. on December 26, they made it across with no losses. Knox was delayed, managing frightened horses and about 18 field guns on flat-bottomed ferries. Cadwalader and Ewing failed to cross due to the ice and heavy currents. Once Knox arrived, Washington proceeded to Trenton, rather than risk being spotted returning his army to Pennsylvania.The troops spotted Hessian positions a mile from Trenton, so Washington split his force into two columns, rallying his men: \"Soldiers keep by your officers. For God's sake, keep by your officers.\" The two columns were separated at the Birmingham crossroads. General Greene's column took the upper Ferry Road, led by Washington, and General Sullivan's column advanced on River Road. The Americans marched in sleet and snowfall. Many were shoeless with bloodied feet, and two died of exposure. At sunrise, Washington, aided by Colonel Knox and artillery, led his men in a surprise attack on the unsuspecting Hessians and their commander, Colonel Johann Rall. The Hessians had 22 killed, including Colonel Rall, 83 wounded, and 850 captured with supplies.Washington retreated across the Delaware to Pennsylvania and returned to New Jersey on January 3, 1777, launching an attack on British regulars at Princeton, with 40 Americans killed or wounded and 273 British killed or captured. American Generals Hugh Mercer and John Cadwalader were being driven back by the British when Mercer was mortally wounded. Washington arrived and led the men in a counterattack which advanced to within 30 yards (27 m) of the British line.Some British troops retreated after a brief stand, while others took refuge in Nassau Hall, which became the target of Colonel Alexander Hamilton's cannons. Washington's troops charged, the British surrendered in less than an hour, and 194 soldiers laid down their arms. Howe retreated to New York City where his army remained inactive until early the next year. Washington took up winter headquarters in Jacob Arnold's Tavern in Morristown, New Jersey, while he received munition from the Hibernia mines. While in Morristown, Washington's troops disrupted British supply lines and expelled them from parts of New Jersey.During his stay in Morristown, Washington ordered the inoculation of Continental troops against smallpox. This went against the wishes of the Continental Congress who had issued a proclamation prohibiting it, but Washington feared the spread of smallpox in the army. The mass inoculation proved successful, with only isolated infections occurring and no regiments incapacitated by the disease.The British still controlled New York, and many Patriot soldiers did not re-enlist or deserted after the harsh winter campaign. Congress instituted greater rewards for re-enlisting and punishments for desertion to effect greater troop numbers. Strategically, Washington's victories at Trenton and Princeton were pivotal; they revived Patriot morale and quashed the British strategy of showing overwhelming force followed by offering generous terms, changing the course of the war. In February 1777, word of the American victories reached London, and the British realized the Patriots were in a position to demand unconditional independence.\n\n\n=== Philadelphia ===\n\n\n==== Brandywine, Germantown, and Saratoga ====\n\nIn July 1777, British General John Burgoyne led the Saratoga campaign south from Quebec through Lake Champlain and recaptured Fort Ticonderoga intending to divide New England, including control of the Hudson River. However, General Howe in British-occupied New York City blundered, taking his army south to Philadelphia rather than up the Hudson River to join Burgoyne near Albany.Washington and Gilbert du Motier, Marquis de Lafayette rushed to Philadelphia to engage Howe. In the Battle of Brandywine, on September 11, 1777, Howe outmaneuvered Washington and marched unopposed into the nation's capital at Philadelphia. A Patriot attack failed against the British at Germantown in October.In Upstate New York, the Patriots were led by General Horatio Gates. Concerned about Burgoyne's movements southward, Washington sent reinforcements north with Generals Benedict Arnold, his most aggressive field commander, and Benjamin Lincoln. On October 7, 1777, Burgoyne tried to take Bemis Heights but was isolated from support by Howe. He was forced to retreat to Saratoga and ultimately surrendered after the Battles of Saratoga. As Washington suspected, Gates' victory emboldened his critics.Biographer John Alden maintains, \"It was inevitable that the defeats of Washington's forces and the concurrent victory of the forces in upper New York should be compared.\" Admiration for Washington was waning, including little credit from John Adams.\n\n\n==== Valley Forge and Monmouth ====\n\nWashington and his Continental Army of 11,000 men went into winter quarters at Valley Forge north of Philadelphia in December 1777. There they lost between 2,000 and 3,000 men as a result of disease and lack of food, clothing, and shelter. The British were comfortably quartered in Philadelphia, paying for supplies in pounds sterling, while Washington struggled with a devalued American paper currency. The woodlands were soon exhausted of game. By February, Washington was facing lowered morale and increased desertions among his troops.An internal revolt by his officers, led by Major General Thomas Conway, prompted some members of Congress to consider removing Washington from command. Washington's supporters resisted, and the matter was dropped after much deliberation. Once the plot was exposed, Conway wrote an apology to Washington, resigned, and returned to France.Washington made repeated petitions to Congress for provisions. He received a congressional delegation to check the Army's conditions and expressed the urgency of the situation, proclaiming: \"Something must be done. Important alterations must be made.\" He recommended that Congress expedite supplies, and Congress agreed to strengthen and fund the army's supply lines by reorganizing the commissary department. By late February, supplies began arriving. Meanwhile, Baron Friedrich Wilhelm von Steuben's incessant drilling transformed Washington's recruits into a disciplined fighting force by the end of winter camp. For his services, Washington promoted Von Steuben to Major General and made him chief of staff.In early 1778, the French responded to Burgoyne's defeat and entered into a Treaty of Alliance with the Americans. Congress ratified the treaty in May, which amounted to a French declaration of war against Britain. In May 1778, Howe resigned and was replaced by Sir Henry Clinton.The British evacuated Philadelphia for New York that June and Washington summoned a war council of American and French generals. He chose a partial attack on the retreating British at the Battle of Monmouth. Generals Charles Lee and Lafayette moved with 4,000 men, without Washington's knowledge, and bungled their first attack on June 28. Washington relieved Lee and achieved a draw after an expansive battle. At nightfall, the British continued their retreat to New York, and Washington moved his army outside the city. Monmouth was Washington's last battle in the North.\n\n\n=== West Point espionage ===\n\nWashington became America's first spymaster by designing an espionage system against the British. In 1778, Major Benjamin Tallmadge formed the Culper Ring at Washington's direction to covertly collect information about the British in New York. Washington had disregarded incidents of disloyalty by Benedict Arnold, who had distinguished himself in many campaigns, including the Invasion of Quebec and the Battle of Saratoga.In 1780, Arnold began supplying British spymaster John Andr\u00e9 with sensitive information intended to compromise Washington and capture West Point, a key American defensive position on the Hudson River. Historians Nathaniel Philbrick and Ron Chernow noted possible reasons for Arnold's defection to be his anger at losing promotions to junior officers, or repeated slights from Congress. He was also deeply in debt, profiteering from the war, and disappointed by Washington's lack of support during his eventual court-martial.After repeated requests, Washington agreed to give Arnold command of West Point in August. On September 21, Arnold met Andr\u00e9 and gave him plans to take over the garrison. While returning to British lines, Andr\u00e9 was captured by militia who discovered the plans; upon hearing the news of Andr\u00e9's capture on September 24, while waiting to greet and have breakfast with Washington, Arnold immediately fleed to the HMS Vulture, the ship that had brought Andr\u00e9 to West Point, and escaped to New York.Upon being told about Arnold's treason, Washington recalled the commanders positioned under Arnold at key points around the fort to prevent any complicity. He assumed personal command at West Point and reorganized its defenses. Andr\u00e9's trial for espionage ended in a death sentence, and Washington offered to return him to the British in exchange for Arnold, but Clinton refused. Andr\u00e9 was hanged on October 2, 1780, despite his request for a firing squad, to deter other spies.\n\n\n=== Southern theater and Yorktown ===\n\nIn late 1778, General Clinton shipped 3,000 troops from New York to Georgia and launched a Southern invasion against Savannah, reinforced by 2,000 British and Loyalist troops. They repelled an attack by American patriots and French naval forces, which bolstered the British war effort.In June 1778, Iroquois warriors joined with Loyalist rangers led by Walter Butler and killed more than 200 frontiersmen, laying waste to the Wyoming Valley in Northeastern Pennsylvania. In mid-1779, in response to this and other attacks on New England towns, Washington ordered General John Sullivan to lead an expedition to force the Iroquois out of New York by effecting \"the total destruction and devastation\" of their villages and taking their women and children hostage. The expedition systematically destroyed Iroquois villages and food stocks, and forced at least 5,036 Iroquois to flee to British Canada. The campaign directly killed a few hundred Iroquois, but according to historian Rhiannon Koehler, the net effect was to reduce the Iroquois by half. They became unable to survive the harsh winter of 1779\u20131780; some historians now described the campaign as a genocide.Washington's troops went into quarters at Morristown, New Jersey for their worst winter of the war, with temperatures well below freezing. New York Harbor was frozen, snow covered the ground for weeks, and the troops again lacked provisions.In January 1780, Clinton assembled 12,500 troops and attacked Charles Town, South Carolina, defeating General Benjamin Lincoln. By June, they occupied the South Carolina Piedmont. Clinton returned to New York and left 8,000 troops under the command of General Charles Cornwallis. Congress replaced Lincoln with Horatio Gates; after his defeat in the Battle of Camden, Gates was replaced by Nathanael Greene, Washington's initial choice, but the British had firm control of the South. Washington was reinvigorated, however, when Lafayette returned from France with more ships, men, and supplies, and 5,000 veteran French troops led by Marshal Rochambeau arrived at Newport, Rhode Island in July 1780. French naval forces then landed, led by Admiral de Grasse.Washington's army went into winter quarters at New Windsor, New York in December 1780; he urged Congress and state officials to expedite provisions so the army would not \"continue to struggle under the same difficulties they have hitherto endured\". On March 1, 1781, Congress ratified the Articles of Confederation, but the government that took effect on March 2 did not have the power to levy taxes, and it loosely held the states together.General Clinton sent Benedict Arnold, now a British Brigadier General with 1,700 troops, to Virginia to capture Portsmouth and conduct raids on Patriot forces; Washington responded by sending Lafayette south to counter Arnold's efforts. Washington initially hoped to bring the fight to New York, drawing off British forces from Virginia and ending the war there, but Rochambeau advised him that Cornwallis in Virginia was the better target. De Grasse's fleet arrived off the Virginia coast, cutting off British retreat. Seeing the advantage, Washington made a feint towards Clinton in New York, then headed south to Virginia.\n\n\n==== Yorktown ====\n\nThe siege of Yorktown was a decisive victory by the combined forces of the Continental Army commanded by Washington, the French Army commanded by General Comte de Rochambeau, and the French Navy commanded by Admiral de Grasse. On August 19, the march to Yorktown led by Washington and Rochambeau began, which is known now as the \"celebrated march\". Washington was in command of an army of 7,800 Frenchmen, 3,100 militia, and 8,000 Continentals. Inexperienced in siege warfare, he often deferred to the judgment of General Rochambeau and relied on his advice. Despite this, Rochambeau never challenged Washington's authority as the battle's commanding officer.By late September, Patriot-French forces surrounded Yorktown, trapped the British Army, and prevented British reinforcements from Clinton in the North, while the French navy emerged victorious at the Battle of the Chesapeake. The final American offensive began with a shot fired by Washington. The siege ended with a British surrender on October 19, 1781; over 7,000 British soldiers became prisoners of war. Washington negotiated the terms of surrender for two days, and the official signing ceremony took place on October 19; Cornwallis claimed illness and was absent, sending General Charles O'Hara as his proxy.  As a gesture of goodwill, Washington held a dinner for the American, French, and British generals, all of whom fraternized on friendly terms and identified with one another as members of the same professional military caste.Afterwards, Washington moved the army to New Windsor, New York where they remained stationed until the Treaty of Paris was signed on September 3, 1783, formally ending the war. Although the peace treaty did not happen for two years following the end of the battle, Yorktown proved to be the last significant battle or campaign of the Revolutionary War, with the British Parliament agreeing to cease hostilities in March 1782.\n\n\n=== Demobilization and resignation ===\n\nWhen peace negotiations began in April 1782, both the British and French began gradually evacuating their forces. With the American treasury empty, unpaid and mutinous soldiers forced the adjournment of Congress. In March 1783, Washington successfully calmed the Newburgh Conspiracy, a planned munity by American officers; Congress promised each a five-year bonus. Washington submitted an account of $450,000 in expenses which he had advanced to the army, equivalent to $9.15 million in 2022. The account was settled, though it was allegedly vague about large sums and included expenses his wife had incurred through visits to his headquarters.The following month, a Congressional committee led by Alexander Hamilton began adapting the army for peacetime. In August 1783, Washington gave the Army's perspective to the committee in his Sentiments on a Peace Establishment, which advised Congress to keep a standing army, create a \"national militia\" of separate state units, and establish a navy and a national military academy.The Treaty of Paris was signed on September 3, 1783, and Britain officially recognized American independence. Washington disbanded his army, giving a farewell address to his soldiers on November 2. During this time, Washington oversaw the evacuation of British forces in New York and was greeted by parades and celebrations. Along with Governor George Clinton, he took formal possession of the city on November 25.In early December 1783, Washington bade farewell to his officers at Fraunces Tavern and resigned as commander-in-chief soon thereafter. In a final appearance in uniform, he gave a statement to the Congress: \"I consider it an indispensable duty to close this last solemn act of my official life, by commending the interests of our dearest country to the protection of Almighty God, and those who have the superintendence of them, to his holy keeping.\" Washington's resignation was acclaimed at home and abroad and showed a skeptical world that the new republic would not degenerate into chaos.The same month, Washington was appointed president-general of the Society of the Cincinnati, a newly established hereditary fraternity of Revolutionary War officers. He served in this capacity for the remainder of his life.\n\n\n== Early republic (1783\u20131789) ==\n\n\n=== Return to Mount Vernon ===\n\nWashington was longing to return home after spending just ten days at Mount Vernon out of 8+1\u20442 years of war. He arrived on Christmas Eve, delighted to be \"free of the bustle of a camp and the busy scenes of public life\". He was a celebrity and was f\u00eated during a visit to his mother at Fredericksburg in February 1784, and he received a constant stream of visitors wishing to pay their respects at Mount Vernon.Washington reactivated his interests in the Great Dismal Swamp and Potomac canal projects begun before the war, though neither paid him any dividends, and he undertook a 34-day, 680-mile (1,090 km) trip to check on his land holdings in the Ohio Country. He oversaw the completion of the remodeling work at Mount Vernon, which transformed his residence into the mansion that survives to this day\u2014although his financial situation was not strong. Creditors paid him in depreciated wartime currency, and he owed significant amounts in taxes and wages. Mount Vernon had made no profit during his absence, and he saw persistently poor crop yields due to pestilence and poor weather. His estate recorded its eleventh year running at a deficit in 1787, and there was little prospect of improvement.To make his estate profitable again, Washington undertook a new landscaping plan and succeeded in cultivating a range of fast-growing trees and native shrubs. He also began breeding mules after being gifted a Spanish jack by King Charles III of Spain in 1784. There were few mules in the United States at that time, and he believed that they would revolutionize agriculture and transportation.\n\n\n=== Constitutional Convention of 1787 ===\n\nBefore returning to private life in June 1783, Washington called for a strong union. Though he was concerned that he might be criticized for meddling in civil matters, he sent a circular letter to the states, maintaining that the Articles of Confederation was no more than \"a rope of sand\". He believed the nation was on the verge of \"anarchy and confusion\", was vulnerable to foreign intervention, and that a national constitution would unify the states under a strong central government.When Shays' Rebellion erupted in Massachusetts over taxation, Washington was further convinced that a national constitution was needed. Some nationalists feared that the new republic had descended into lawlessness, and they met on September 11, 1786, at Annapolis to ask Congress to revise the Articles of Confederation. One of their biggest efforts was getting Washington to attend. Congress agreed to a Constitutional Convention to be held in Philadelphia in Spring 1787, with each state to send delegates.On December 4, 1786, Washington was chosen to lead the Virginia delegation, but he declined on December 21. He had concerns about the legality of the convention and consulted James Madison, Henry Knox, and others. They persuaded him to attend as his presence might induce reluctant states to send delegates and smooth the way for the ratification process while also giving legitimacy to the convention. On March 28, Washington told Governor Edmund Randolph that he would attend the convention but made it clear that he was urged to attend.Washington arrived in Philadelphia on May 9, 1787, though a quorum was not attained until May 25. Benjamin Franklin nominated Washington to preside over the convention, and he was unanimously elected to serve as president general. The convention's state-mandated purpose was to revise the Articles of Confederation, and the new government would be established when the resulting document was \"duly confirmed by the several states\". Randolph introduced Madison's Virginia Plan on May 27, the third day of the convention. It called for an entirely new constitution and a sovereign national government, which Washington highly recommended.On July 10, Washington wrote to Alexander Hamilton: \"I almost despair of seeing a favorable issue to the proceedings of our convention and do therefore repent having had any agency in the business.\" Nevertheless, he lent his prestige to the work of the other delegates, unsuccessfully lobbying many to support ratification of the Constitution, such as anti-federalists Edmund Randolph and George Mason. The final version was voted on and signed by 39 of 55 delegates on September 17, 1787.\n\n\n=== Chancellor of William & Mary ===\nIn 1788, the Board of Visitors of the College of William & Mary decided to re-establish the position of Chancellor, and elected Washington to the office on January 18. The College Rector Samuel Griffin wrote to Washington inviting him to the post, and in a letter dated April 30, 1788, Washington accepted the position of the 14th Chancellor of the College of William & Mary. He continued to serve through his presidency until his death on December 14, 1799.\n\n\n=== First presidential election ===\n\nThe delegates to the Convention anticipated a Washington presidency and left it to him to define the office once elected.The state electors under the Constitution voted for the president on February 4, 1789, and Washington suspected that most republicans had not voted for him. The mandated March 4 date passed without a Congressional quorum to count the votes, but a quorum was reached on April 5. The votes were tallied the next day, and Washington won the majority of every state's electoral votes. He was informed of his election as president by Congressional Secretary Charles Thomson. John Adams received the next highest number of votes and was elected vice president. Despite feeling \"anxious and painful sensations\" about leaving Mount Vernon, he departed for New York City on April 16 to be inaugurated.\n\n\n== Presidency (1789\u20131797) ==\n\nWashington was inaugurated on April 30, 1789, taking the oath of office at Federal Hall in New York City. His coach was led by militia and a marching band and followed by statesmen and foreign dignitaries in an inaugural parade, with a crowd of 10,000. Chancellor Robert R. Livingston administered the oath, using a Bible provided by the Masons, after which the militia fired a 13-gun salute. Washington read a speech in the Senate Chamber, asking \"that Almighty Being ... consecrate the liberties and happiness of the people of the United States\". Though he wished to serve without a salary, Congress insisted that he accept it, later providing Washington $25,000 per year to defray costs of the presidency, equivalent to $6.14 million today.\nWashington wrote to James Madison: \"As the first of everything in our situation will serve to establish a precedent, it is devoutly wished on my part that these precedents be fixed on true principles.\" To that end, he preferred the title \"Mr. President\" over more majestic names proposed by the Senate, including \"His Excellency\" and \"His Highness the President\". His executive precedents included the inaugural address, messages to Congress, and the cabinet form of the executive branch.Washington planned to resign after his first term, but political strife convinced him to remain in office. He was an able administrator and a judge of talent and character, and he regularly talked with department heads to get their advice. He tolerated opposing views, despite fears that a democratic system would lead to political violence, and he conducted a smooth transition of power to his successor. He remained non-partisan throughout his presidency and opposed the divisiveness of political parties, but he favored a strong central government, was sympathetic to a Federalist form of government, and leery of the Republican opposition.Washington dealt with major problems. The old Confederation lacked the powers to handle its workload and had weak leadership, no executive, a small bureaucracy of clerks, large debt, worthless paper money, and no power to establish taxes. He had the task of assembling an executive department and relied on Tobias Lear for advice selecting its officers. Britain refused to relinquish its forts in the American West, and Barbary pirates preyed on American merchant ships in the Mediterranean before the United States even had a navy.\n\n\n=== Cabinet and executive departments ===\n\nCongress created executive departments in 1789, including the State Department in July, the War Department in August, and the Treasury Department in September. Washington appointed Edmund Randolph as Attorney General, Samuel Osgood as Postmaster General, Thomas Jefferson as Secretary of State, Henry Knox as Secretary of War, and Alexander Hamilton as Secretary of the Treasury. Washington's cabinet became a consulting and advisory body, not mandated by the Constitution.Washington's cabinet members formed rival parties with sharply opposing views, most fiercely illustrated between Hamilton and Jefferson. Washington restricted cabinet discussions to topics of his choosing, without participating in the debate. He occasionally requested cabinet opinions in writing and expected department heads to agreeably carry out his decisions.\n\n\n=== Domestic issues ===\nWashington was apolitical and opposed the formation of parties, suspecting that conflict would undermine republicanism. He exercised great restraint in using his veto power, writing that \"I give my Signature to many Bills with which my Judgment is at variance...\"His closest advisors formed two factions, portending the First Party System. Secretary of the Treasury Alexander Hamilton formed the Federalist Party to promote national credit and a financially powerful nation. Secretary of State Thomas Jefferson opposed Hamilton's agenda and founded the Jeffersonian Republicans. Washington favored Hamilton's agenda, however, and it ultimately went into effect\u2014resulting in bitter controversy.Washington proclaimed November 26, 1789, as a day of Thanksgiving to encourage national unity. \"It is the duty of all nations to acknowledge the providence of Almighty God, to obey His will, to be grateful for His benefits, and humbly to implore His protection and favor.\" He spent that day fasting and visiting debtors in prison to provide them with food and beer.\n\n\n==== African Americans ====\nIn response to two antislavery petitions that were presented to Congress in 1790, slaveholders in Georgia and South Carolina threatened to \"blow the trumpet of civil war\". Washington and Congress responded with a series of racist measures: naturalization was denied to black immigrants; blacks were barred from serving in state militias; the Southwest Territory (later the state of Tennessee) was permitted to maintain slavery; and two more slave states were admitted (Kentucky in 1792 and Tennessee in 1796). On February 12, 1793, Washington signed into law the Fugitive Slave Act, which overrode state laws and courts, allowing agents to cross state lines to return escaped slaves. Many free blacks in the north decried the law believing it would allow bounty hunting and kidnapping. The Fugitive Slave Act gave effect to the Constitution's Fugitive Slave Clause, and the Act was passed overwhelmingly in Congress.At the same time, Washington signed a reenactment of the Northwest Ordinance in 1789, which had freed all slaves brought after 1787 into a vast expanse of federal territory north of the Ohio River, except for slaves escaping from slave states. The 1787 law lapsed when the new U.S. Constitution was ratified in 1789. He also signed the Slave Trade Act of 1794, which sharply limited American involvement in the Atlantic slave trade. On February 18, 1791, Congress admitted the free state of Vermont into the Union as the 14th state as of March 4, 1791.\n\n\n==== National Bank ====\nWashington's first term was largely devoted to economic concerns. Establishment of public credit became a primary challenge for the federal government. Hamilton submitted a report to a deadlocked Congress, and he, Madison, and Jefferson reached the Compromise of 1790 in which Jefferson agreed to Hamilton's debt proposals in exchange for moving the nation's capital temporarily to Philadelphia and then south near Georgetown on the Potomac River. The terms were legislated in the Funding Act of 1790 and the Residence Act, both of which Washington signed into law. Congress authorized the assumption and payment of the nation's debts, with funding provided by customs duties and excise taxes.Hamilton caused controversy in Cabinet by advocating for the establishment of the First Bank of the United States. Madison and Jefferson objected to the idea, but legislation creating the bank easily passed Congress. Jefferson and Randolph insisted the federal government was going beyond its constitutional authority. Hamilton argued the government could charter the bank under the implied powers granted by the constitution. Washington sided with Hamilton and signed the bank legislation on February 25, 1791. The rift between Hamilton and Jefferson, meanwhile, became openly hostile.The nation's first financial crisis occurred in March 1792. Hamilton's Federalists exploited large loans to gain control of U.S. debt securities, causing a run on the national bank; the markets returned to normal by mid-April. Jefferson believed Hamilton was part of the scheme, despite Hamilton's efforts to ameliorate.\n\n\n==== Jefferson\u2013Hamilton feud ====\n\nJefferson and Hamilton adopted diametrically opposed political principles. Hamilton believed in a strong national government requiring a national bank and foreign loans to function, while Jefferson believed the states and the farm element should primarily direct the government; he also resented the idea of banks and foreign loans. To Washington's dismay, the two men persistently entered into disputes and infighting. Hamilton demanded that Jefferson resign if he could not support Washington, and Jefferson told Washington that Hamilton's fiscal system would lead to the overthrow of the republic. Washington urged them to call a truce for the sake of the nation, but they ignored him.Jefferson's political actions, his support of Freneau's National Gazette, and his attempts to undermine Hamilton nearly led Washington to dismiss him from the cabinet; he ultimately resigned his position in December 1793, and Washington forsook him.The feud led to the well-defined Federalist and Republican parties, and party affiliation became necessary for election to Congress by 1794. Washington remained aloof from congressional attacks on Hamilton, but did not publicly protect him. The Hamilton\u2013Reynolds sex scandal opened Hamilton to disgrace, but Washington continued to hold him in \"very high esteem\".\n\n\n==== Whiskey Rebellion ====\n\nIn March 1791, at Hamilton's urging, with support from Madison, Congress imposed an excise tax on distilled spirits to help curtail the national debt, which took effect in July. Grain farmers strongly protested in Pennsylvania's frontier districts; they argued that they were unrepresented and were shouldering too much of the debt, comparing their situation to British taxation pre-Revolution.\nOn August 2, Washington assembled his cabinet to discuss the situation. Unlike Washington, who had reservations about using force, Hamilton was eager to suppress the rebellion with federal authority. Wanting to avoid involving the federal government, Washington first called on Pennsylvania state officials to take the initiative, but they declined. On August 7, Washington issued his first proclamation for calling up state militias. After appealing for peace, he reminded the protestors that, unlike the rule of the British crown, the Federal law was issued by state-elected representatives.Threats and violence against tax collectors, however, escalated into defiance against federal authority in 1794 and gave rise to the Whiskey Rebellion. Washington issued a final proclamation on September 25, threatening the use of military force to no avail. The federal army was not up to the task, so Washington invoked the Militia Act of 1792 to summon state militias. Governors sent troops, initially commanded by Washington, who handed over command to Henry Lee to lead them into the rebellious districts. They took 150 prisoners, and the remaining rebels dispersed. Two of the prisoners were condemned to death, but Washington exercised his Constitutional authority for the first time and pardoned them.Washington's forceful action demonstrated that the new government could protect itself and its tax collectors. This represented the first use of federal military force against the states and citizens. Washington justified his action against \"certain self-created societies\", which he regarded as \"subversive organizations\" that threatened the national union. He did not dispute their right to protest, but he insisted that their dissent must not violate federal law. Congress agreed and extended their congratulations to him; only Madison and Jefferson expressed indifference.\n\n\n=== Foreign affairs ===\nIn April 1792, the French Revolutionary Wars began between Britain and France, and Washington declared America's neutrality. The revolutionary government of France sent diplomat Edmond-Charles Gen\u00eat to America, and he was welcomed with great enthusiasm. He created a network of new Democratic-Republican Societies promoting France's interests, but Washington denounced them and demanded that the French recall Gen\u00eat. The National Assembly of France granted Washington honorary French citizenship on August 26, 1792, during the early stages of the French Revolution.Hamilton formulated the Jay Treaty to normalize trade relations with Britain while removing them from western forts, and also to resolve financial debts remaining from the Revolution. Chief Justice John Jay acted as Washington's negotiator and signed the treaty on November 19, 1794; critical Jeffersonians, however, supported France. Washington deliberated, then supported the treaty because it avoided war with Britain, but was disappointed that its provisions favored Britain. He mobilized public opinion and secured ratification in the Senate but faced frequent public criticism.The British agreed to abandon their forts around the Great Lakes, and the United States modified the boundary with Canada. The government liquidated numerous pre-Revolution debts, and the British opened the British West Indies to American trade. The treaty secured peace with Britain and a decade of prosperous trade. Jefferson claimed that it angered France and \"invited rather than avoided\" war. Relations with France deteriorated afterward and, two days before Washington's term ended, the French Directory declared the authority to seize American ships, leaving succeeding president John Adams with prospective war.\n\n\n=== Native American affairs ===\n\nDuring the fall of 1789, Washington had to contend with the British refusing to evacuate their forts in the Northwest frontier and their concerted efforts to incite Indian tribes to attack American settlers. The Northwest tribes under Miami chief Little Turtle allied with the British to resist American expansion, and killed 1,500 settlers between 1783 and 1790.\nWashington declared that \"the Government of the United States are determined that their Administration of Indian Affairs shall be directed entirely by the great principles of Justice and humanity\", and provided that treaties should negotiate their land interests. The administration regarded powerful tribes as foreign nations, and Washington even smoked a peace pipe and drank wine with them at the President's House in Philadelphia. He made numerous attempts to conciliate them; he equated killing indigenous peoples with killing whites and sought to integrate them into European American culture.In the Southwest, negotiations failed between federal commissioners and raiding Indian tribes seeking retribution. Washington invited Creek Chief Alexander McGillivray and 24 leading chiefs to New York to negotiate a treaty and treated them like foreign dignitaries. Knox and McGillivray concluded the Treaty of New York on August 7, 1790, which provided the tribes with agricultural supplies and McGillivray with the rank of Brigadier General and an annual salary of $1,200, equivalent to $28,404 in 2022.In 1790, Washington sent Brigadier General Josiah Harmar to pacify the Northwest tribes, but Little Turtle routed him twice and forced him to withdraw. The Northwestern Confederacy of tribes used guerrilla tactics and were an effective force against the sparsely manned American Army. Washington sent Major General Arthur St. Clair from Fort Washington on an expedition to restore peace in the territory in 1791. On November 4, St. Clair's forces were ambushed and soundly defeated by tribal forces with few survivors.Washington replaced the disgraced St. Clair with the Revolutionary War hero Anthony Wayne. From 1792 to 1793, Wayne instructed his troops on Native American warfare tactics and instilled discipline which was lacking under St. Clair. In August 1794, Washington sent Wayne into tribal territory with authority to drive them out by burning their villages and crops in the Maumee Valley. On August 24, the American army defeated the Northwestern Confederacy at the Battle of Fallen Timbers, and the Treaty of Greenville in August 1795 opened two-thirds of the Ohio Country for American settlement.\n\n\n=== Second term ===\nWashington initially planned to retire after his first term, weary of office and in poor health. After dealing with the infighting in his own cabinet and with partisan critics, he showed little enthusiasm for a second term, while Martha also wanted him not to run. Washington's nephew George Augustine Washington, managing Mount Vernon in his absence, was critically ill, further increasing Washington's desire to retire.Many, however, urged him to run for a second term. Madison told him that his absence would only allow the dangerous political rift in his cabinet and the House to worsen. Jefferson also pleaded with him not to retire, agreeing to drop his attacks on Hamilton, and stating that he would also retire if Washington did. Hamilton maintained that Washington's absence would be \"deplored as the greatest evil\" to the country. With the election of 1792 nearing, Washington relented and agreed to run.On February 13, 1793, the Electoral College unanimously re-elected Washington president, and John Adams as vice president by a vote of 77 to 50. He was sworn into office by Associate Justice William Cushing on March 4, 1793, in the Senate Chamber of Congress Hall in Philadelphia. Afterwards, Washington gave a brief address before immediately retiring to the President's House.On April 22, 1793, when the French Revolutionary Wars broke out, Washington issued a proclamation which declared American neutrality. He was resolved to pursue \"a conduct friendly and impartial toward the belligerent Powers\" while also warning Americans not to intervene in the conflict. Although Washington recognized France's revolutionary government, he would eventually ask French minister to the United States Edmond-Charles Gen\u00eat be recalled over the Citizen Gen\u00eat affair. Gen\u00eat was a diplomatic troublemaker who was openly hostile toward Washington's neutrality policy. He procured four American ships as privateers to strike at Spanish forces (British allies) in Florida while organizing militias to strike at other British possessions. However, his efforts failed to draw the United States into the conflict.On July 31, 1793, Jefferson submitted his resignation from cabinet. Hamilton, desiring more income for his family, resigned from office in January 1795 and was replaced by Oliver Wolcott Jr.. While his relationship with Washington would remain friendly, Washington's relationship with his Secretary of War Henry Knox deteriorated after rumors that Knox had profited from contracts for the construction of U.S. frigates which had been commissioned under the Naval Act of 1794 in order to combat Barbary pirates, forcing Knox to resign.In the final months of his presidency, Washington was assailed by his political foes and a partisan press who accused him of being ambitious and greedy. He came to regard the press as a disuniting, \"diabolical\" force of falsehoods. At the end of his second term, Washington retired for personal and political reasons, dismayed with personal attacks, and to ensure that a truly contested presidential election could be held. He did not feel bound to a two-term limit, but his retirement set a significant precedent.\n\n\n=== Farewell Address ===\n\nIn 1796, Washington declined to run for a third term of office. In May 1792, in anticipation of his retirement, Washington instructed James Madison to prepare a \"valedictory address\", an initial draft of which was entitled the \"Farewell Address\". In May 1796, Washington sent the manuscript to Alexander Hamilton who did an extensive rewrite, while Washington provided final edits. On September 19, 1796, David Claypoole's American Daily Advertiser published the final version.Washington stressed that national identity was paramount, as a united America would safeguard freedom and prosperity. He warned the nation of three eminent dangers: regionalism, partisanship, and foreign entanglements, and said the \"name of AMERICAN, which belongs to you, in your national capacity, must always exalt the just pride of patriotism\". Washington called for men to move beyond partisanship for the common good, stressing that the United States must concentrate on its own interests. He warned against foreign alliances and their influence in domestic affairs, and bitter partisanship and the dangers of political parties. He counseled friendship and commerce with all nations, but advised against involvement in European wars. He stressed the importance of religion, asserting that \"religion and morality are indispensable supports\" in a republic. Washington's address favored Hamilton's Federalist ideology and economic policies.He closed the address by reflecting on his legacy:\n\nThough in reviewing the incidents of my Administration I am unconscious of intentional error, I am nevertheless too sensible of my defects not to think it probable that I may have committed many errors. Whatever they may be, I fervently beseech the Almighty to avert or mitigate the evils to which they may tend. I shall also carry with me the hope that my country will never cease to view them with indulgence, and that, after forty-five years of my life dedicated to its service with an upright zeal, the faults of incompetent abilities will be consigned to oblivion, as myself must soon be to the mansions of rest.\nAfter initial publication, many Republicans, including Madison, criticized the Address and described it as an anti-French campaign document, with Madison believing that Washington was strongly pro-British.In 1839, Washington biographer Jared Sparks maintained that Washington's \"Farewell Address was printed and published with the laws, by order of the legislatures, as an evidence of the value they attached to its political precepts, and of their affection for its author.\" In 1972, Washington scholar James Flexner referred to the Farewell Address as receiving as much acclaim as Thomas Jefferson's Declaration of Independence and Abraham Lincoln's Gettysburg Address. In 2010, historian Ron Chernow called the Farewell Address one of the most influential statements on republicanism.\n\n\n== Post-presidency (1797\u20131799) ==\n\n\n=== Retirement ===\nWashington retired to Mount Vernon in March 1797 and devoted time to his plantations and other business interests. His plantation operations were only minimally profitable, and his lands in the west (Piedmont) were under Indian attacks and yielded little income, with squatters there refusing to pay rent. He attempted to sell these but without success. He became an even more committed Federalist. He vocally supported the Alien and Sedition Acts and convinced Federalist John Marshall to run for Congress to weaken the Jeffersonian hold on Virginia.Washington grew restless in retirement, prompted by tensions with France; in a continuation of the French Revolutionary Wars, French privateers began seizing American ships in 1798, and relations deteriorated with France and led to the \"Quasi-War\". Washington wrote to Secretary of War James McHenry offering to organize President Adams' army. Adams nominated him for a lieutenant general commission on July 4, 1798, and the position of commander-in-chief of the armies. Washington served as the commanding general from July 13, 1798, until his death 17 months later. He participated in planning for a provisional army, but avoided involvement in details. In advising McHenry of potential officers for the army, he appeared to make a complete break with Jefferson's Democratic-Republicans: \"you could as soon scrub the blackamoor white, as to change the principles of a profest Democrat; and that he will leave nothing unattempted to overturn the government of this country.\" Washington delegated the active leadership of the army to Hamilton, a major general. No army invaded the United States during this period, and Washington did not assume a field command.Washington was known to be rich because of the well-known \"glorified fa\u00e7ade of wealth and grandeur\" at Mount Vernon, but nearly all his wealth was in the form of land and slaves rather than ready cash. To supplement his income, he erected a distillery for substantial whiskey production. He bought land parcels to spur development around the new Federal City named in his honor, and he sold individual lots to middle-income investors rather than multiple lots to large investors, believing they would more likely commit to making improvements.\n\n\n=== Final days and death ===\nOn December 12, 1799, Washington inspected his farms on horseback. He returned home late and had guests for dinner. He had a sore throat the next day but was well enough to mark trees for cutting. That evening, Washington complained of chest congestion. The next morning, however, he awoke to an inflamed throat and difficulty breathing. He ordered estate overseer George Rawlins to remove nearly a pint of his blood; bloodletting was a common practice of the time. His family summoned doctors James Craik, Gustavus Richard Brown, and Elisha C. Dick. A fourth doctor, William Thornton, arrived some hours after Washington died.Brown initially believed Washington had quinsy; Dick thought the condition was a more serious \"violent inflammation of the throat\". They continued the process of bloodletting to approximately five pints, but Washington's condition deteriorated further. Dick proposed a tracheotomy, but the other physicians were not familiar with that procedure and disapproved. Washington instructed Brown and Dick to leave the room, while he assured Craik, \"Doctor, I die hard, but I am not afraid to go.\"Washington's death came more swiftly than expected. On his deathbed, out of fear of being entombed alive, he instructed his private secretary Tobias Lear to wait three days before his burial. According to Lear, Washington died between 10 p.m. and 11 p.m. on December 14, 1799, with Martha seated at the foot of his bed. His last words were \"'Tis well\", from his conversation with Lear about his burial. He was 67.Congress immediately adjourned for the day upon news of Washington's death, and the Speaker's chair was shroud in black the next morning. The funeral was held four days after his death on December 18, 1799, at Mount Vernon, where his body was interred. Cavalry and foot soldiers led the procession, and six colonels served as the pallbearers. The Mount Vernon funeral service was restricted mostly to family and friends. Reverend Thomas Davis read the funeral service by the vault with a brief address, followed by a ceremony performed by members of Washington's Masonic lodge in Alexandria, Virginia. Word of his death traveled slowly; church bells rang in the cities, and many businesses closed. Memorial processions were held in major cities of the United States. Martha wore a black mourning cape for one year, and she burned their correspondence to protect their privacy. Only five letters between the couple are known to have survived: two from Martha to George and three from him to her.The diagnosis of Washington's illness and the immediate cause of his death have been subjects of debate since his death. The published account of doctors Craik and Brown stated that his symptoms were consistent with cynanche trachealis, a term then used to describe severe inflammation of the upper windpipe, including quinsy. Accusations have persisted since Washington's death concerning medical malpractice. Modern medical authors have concluded that he likely died from severe epiglottitis complicated by the treatments, including multiple doses of calomel, a purgative, and extensive bloodletting which almost certainly caused hypovolemic shock.\n\n\n== Burial, net worth, and aftermath ==\n\nWashington was buried in the old Washington family vault at Mount Vernon. At the time of his death, his estate was worth an estimated $780,000 in 1799, equivalent to $13.72 million in 2022. Washington's peak net worth was $587 million, including 300 slaves. Washington held title to more than 65,000 acres of land in 37 different locations.In 1830, a disgruntled ex-employee of the estate attempted to steal what he thought was Washington's skull, prompting the construction of a more secure vault. In his will, Washington had left instructions for the construction of a new vault as the old family vault was crumbling and needed repair even before his death. A new vault was constructed at Mount Vernon the following year to receive the remains of George and Martha and other relatives.In 1832, a joint Congressional committee debated moving his body from Mount Vernon to a crypt in the Capitol. The crypt had been built by architect Charles Bulfinch in the 1820s during the reconstruction of the burned-out capital, after the Burning of Washington by the British during the War of 1812. Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on \"a shore foreign to his native soil\" if the country became divided, and Washington's remains stayed in Mount Vernon.On October 7, 1837, Washington's remains, still in the original lead coffin, were placed within a marble sarcophagus designed by William Strickland and constructed by John Struthers. The sarcophagus was sealed and encased with planks, and an outer vault was constructed around it. The outer vault has the sarcophagi of both George and Martha Washington; the inner vault has the remains of other Washington family members and relatives.\n\n\n== Personal life ==\nWashington was somewhat reserved in personality, but was known for having a strong presence. He made speeches and announcements when required, but he was not a noted orator or debater. He was taller than most of his contemporaries; accounts of his height vary from 6 ft (1.83 m) to 6 ft 3.5 in (1.92 m) tall, he weighed between 210\u2013220 pounds (95\u2013100 kg) as an adult, and was known for his great strength.He had grey-blue eyes and long reddish-brown hair. He did not wear a powdered wig; instead he wore his hair curled, powdered, and tied in a queue in the fashion of the day.Washington frequently suffered from severe tooth decay and ultimately lost all his teeth but one. He had several sets of false teeth during his presidency. Contrary to common lore, these were not made of wood, but of metal, ivory, bone, animal teeth, and human teeth possibly obtained from slaves. These dental problems left him in constant pain, which he treated with laudanum.Washington was a talented equestrian, with Thomas Jefferson describing him as \"the best horseman of his age\". He collected thoroughbreds at Mount Vernon, his two favorite horses being Blueskin and Nelson. He enjoyed hunting foxes, deer, ducks, and other game. He was an excellent dancer and frequently attended the theater. He drank alcohol in moderation but was morally opposed to excessive drinking, smoking tobacco, gambling, and profanity.\n\n\n=== Religion and Freemasonry ===\n\nWashington was descended from Anglican minister Lawrence Washington, whose troubles with the Church of England may have prompted his heirs to emigrate to America. He was baptized as an infant in April 1732 and became a devoted member of the Anglican Church. He served more than 20 years as a vestryman and churchwarden at Fairfax Parish and Truco Parish in Virginia. He privately prayed and read the Bible daily, and publicly encouraged people and the nation to pray. He may have taken communion on a regular basis prior to the Revolution, but he did not do so following the war.Washington believed in a \"wise, inscrutable, and irresistible\" Creator God who was active in the Universe, contrary to deistic thought. He referred to God in Enlightenment terms, including Providence, the Creator, or the Almighty, and the Divine Author or Supreme Being. He believed in a divine power who watched over battlefields, was involved in the outcome of war, protected his life, and was involved in American politics and specifically the creation of the United States. Historian Ron Chernow has argued that Washington avoided evangelistic Christianity or hellfire-and-brimstone speech along with communion or anything inclined to \"flaunt his religiosity\", saying that he \"never used his religion as a device for partisan purposes or in official undertakings\". No mention of Jesus Christ appears in his private correspondence, and such references are rare in his public writings. At the same time, Washington frequently quoted from the Bible or paraphrased it, and often referred to the Anglican Book of Common Prayer.Washington emphasized religious toleration in a nation with numerous denominations and religions. He publicly attended services of different Christian denominations and prohibited anti-Catholic celebrations in the Army. He engaged workers at Mount Vernon without regard for religious belief or affiliation. While president, he acknowledged major religious sects and gave speeches on religious toleration. He was distinctly rooted in the ideas, values, and modes of thinking of the Enlightenment, but he harbored no contempt of organized Christianity and its clergy, \"being no bigot myself to any mode of worship\". In 1793, speaking to members of the New Church in Baltimore, Washington said, \"We have abundant reason to rejoice that in this Land the light of truth and reason has triumphed over the power of bigotry and superstition.\"Freemasonry was a widely accepted institution in the late 18th century, known for advocating moral teachings. Washington was attracted to the Masons' dedication to the Enlightenment principles of rationality, reason, and brotherhood. American Masonic lodges did not share the anti-clerical views of the controversial European lodges.A Masonic lodge was established in Fredericksburg, Virginia in September 1752, and Washington was initiated two months later at the age of 20 as one of its first Entered Apprentices. Within a year, he progressed through its ranks to become a Master Mason. Washington had high regard for the Masonic Order, but his lodge attendance was sporadic. In 1777, a convention of Virginia lodges asked him to be the Grand Master of the newly established Grand Lodge of Virginia, but he declined due to his commitments leading the Continental Army. After 1782, he frequently corresponded with Masonic lodges and members, and he was listed as Master in the Virginia charter of Alexandria Lodge No. 22 in 1788.\n\n\n== Slavery ==\n\nIn Washington's lifetime, slavery was deeply ingrained in the economic and social fabric of the Colony of Virginia, which continued after the Revolution and the establishment of Virginia as a state. Slavery was legal in all of the Thirteen Colonies prior to the American Revolution.\n\n\n=== Washington's slaves ===\nWashington owned and rented enslaved African Americans, and during his lifetime over 577 slaves lived and worked at Mount Vernon. He acquired them through inheritance, gaining control of 84 dower slaves upon his marriage to Martha, and purchased at least 71 slaves between 1752 and 1773. From 1786, he rented slaves; at the time of his death he was renting 41.Prior to the Revolutionary War, Washington's view on slavery was the same as most Virginia planters of the time. Beginning in the 1760s, however, Washington gradually grew to oppose it. His first doubts were prompted by his transition from tobacco to grain crops, which left him with a costly surplus of slaves, causing him to question the system's economic efficiency. His growing disillusionment with the institution was spurred by the principles of the Revolution and revolutionary friends such as Lafayette and Hamilton. Most historians agree the Revolution was central to the evolution of Washington's attitudes on slavery; \"After 1783,\" Kenneth Morgan writes, \"... [Washington] began to express inner tensions about the problem of slavery more frequently, though always in private\". Regardless, Washington would remain dependent on slave labor to work his farms.The many contemporary reports of slave treatment at Mount Vernon are varied and conflicting. Historian Kenneth Morgan maintains that Washington was frugal on spending for clothes and bedding for his slaves, and only provided them with just enough food, and that he maintained strict control over his slaves, instructing his overseers to keep them working hard from dawn to dusk year-round. In contrast, historian Dorothy Twohig said: \"Food, clothing, and housing seem to have been at least adequate\".Washington faced growing debts involved with the costs of supporting slaves. He held an \"engrained sense of racial superiority\" towards African Americans but harbored no ill feelings toward them. Some enslaved families worked at different locations on the plantation but were allowed to visit one another on their days off. Washington's slaves received two hours off for meals during the workday and were given time off on Sundays and religious holidays.Some accounts report that Washington opposed flogging but at times sanctioned its use, generally as a last resort, on both men and women slaves. Washington used both reward and punishment to encourage discipline and productivity in his slaves. He tried appealing to an individual's sense of pride, gave better blankets and clothing to the \"most deserving\", and motivated his slaves with cash rewards. He believed \"watchfulness and admonition\" were better deterrents against transgressions but would punish those who \"will not do their duty by fair means\". Punishment ranged in severity from demotion back to fieldwork, through whipping and beatings, to permanent separation from friends and family by sale. Historian Ron Chernow maintains that overseers were required to warn slaves before resorting to the lash and required Washington's written permission before whipping, though his extended absences did not always permit this.\nDuring his presidency, Washington brought several of his slaves to the federal capital. When the capital moved from New York City to Philadelphia in 1791, the president began rotating his slave household staff periodically between the capital and Mount Vernon. This was done deliberately to circumvent Pennsylvania's Slavery Abolition Act, which stated that any slave who lived there for more than six months was automatically freed.In May 1796, Martha's personal and favorite slave Ona Judge escaped to Portsmouth, New Hampshire. At Martha's behest, Washington attempted to capture Ona, using a Treasury agent, but failed. In February 1797, around the time of his 65th birthday, Washington's personal slave Hercules Posey escaped from Mount Vernon to Philadelphia and was never found.In February 1786, Washington took a census of Mount Vernon and recorded 224 slaves. By 1799, the slave population at Mount Vernon totaled 317, including 143 children. Washington owned 124 slaves, leased 40, and held 153 for his wife's dower interest. Washington supported many slaves who were too young or too old to work, greatly increasing Mount Vernon's slave population and causing the plantation to operate at a loss.\n\n\n=== Abolition and manumission ===\n\nBased on his private papers and on accounts from his contemporaries, Washington slowly developed a cautious sympathy toward abolitionism that eventually ended with his will freeing his long-time valet Billy Lee, and then subsequently freeing the rest of his personally owned slaves outright upon Martha's death. As president, he remained publicly silent on the topic of slavery, believing it was a nationally divisive issue that could undermine the union.During the Revolutionary War, Washington's views on slavery began to change. In a 1778 letter to Lund Washington, he made clear his desire \"to get quit of Negroes\" when discussing the exchange of slaves for the land he wanted to buy. The next year, Washington stated his intention not to separate enslaved families as a result of \"a change of masters\". During the 1780s, Washington privately expressed his support for gradual emancipation. In the 1780s, he gave moral support to a plan proposed by Lafayette to purchase land and free slaves to work on it, but declined to participate in the experiment.Washington privately expressed support for emancipation to prominent Methodists Thomas Coke and Francis Asbury in 1785 but declined to sign their petition. In personal correspondence the next year, he made clear his desire to see the institution of slavery ended by a gradual legislative process, a view that correlated with the mainstream antislavery literature published in the 1780s that Washington possessed. He significantly reduced his purchases of slaves after the war but continued to acquire them in small numbers.In 1788, Washington declined a suggestion from a leading French abolitionist, Jacques Brissot, to establish an abolitionist society in Virginia, stating that although he supported the idea, the time was not yet right. Historian Philip D. Morgan wrote that Washington was determined not to risk national unity. Washington never responded to any of the antislavery petitions he received, and the subject was not mentioned in either his last address to Congress or his Farewell Address.\nThe first clear indication that Washington seriously intended to free his slaves appears in a letter written to his secretary, Tobias Lear, in 1794. Washington instructed Lear to find buyers for his land in western Virginia, explaining in a private coda that he was doing so \"to liberate a certain species of property which I possess, very repugnantly to my own feelings\". The plan, along with others Washington considered in 1795 and 1796, could not be realized because he failed to find buyers for his land, his reluctance to break up slave families, and the refusal of the Custis heirs to help prevent such separations by freeing their dower slaves at the same time.On July 9, 1799, Washington finished making his last will; the longest provision concerned slavery. All his slaves were to be freed after the death of his wife. Washington said he did not free them immediately because his slaves intermarried with his wife's dower slaves. He forbade their sale or transportation out of Virginia. The provision also provided that old and young freed people be taken care of indefinitely; younger ones were to be taught to read and write and placed in suitable occupations. Washington emancipated 123 slaves, one of the few large slave-holding Virginians during the Revolutionary Era to do so.On January 1, 1801, one year after George Washington's death, Martha Washington signed an order to free his slaves. Many of them, having never strayed far from Mount Vernon, were reluctant to leave; others refused to abandon spouses or children still held as dower slaves by the Custis estate and also stayed with or near Martha. Following Washington's instructions in his will, funds were used to feed and clothe the young, aged, and infirm slaves until the early 1830s.\n\n\n== Historical reputation and legacy ==\n\nWashington's legacy endures as one of the most influential in American history since he served as commander-in-chief of the Continental Army, a hero of the Revolution, and the first president of the United States. Various historians maintain that he also was a dominant factor in America's founding. Revolutionary War comrade Henry Lee eulogized him as \"First in war, first in peace, and first in the hearts of his countrymen\". Lee's words became the hallmark by which Washington's reputation was impressed upon the American memory, with some biographers regarding him as the great exemplar of republicanism. He set many precedents for the national government and the presidency in particular, and he was called the \"Father of His Country\" as early as 1778.Washington became an international symbol for liberation and nationalism as the leader of the first successful revolution against a colonial empire. The Federalists made him the symbol of their party, but the Jeffersonians continued to distrust his influence for years and delayed building the Washington Monument. Washington was elected a member of the American Academy of Arts and Sciences on January 31, 1781.In 1879, Congress proclaimed Washington's Birthday to be a federal holiday. Through a congressional joint resolution Public Law 94-479, passed on January 19, 1976, with an effective appointment date of July 4, 1976, he was posthumously appointed to the grade of General of the Armies of the United States during the American Bicentennial. President Gerald Ford stated that Washington would \"rank first among all officers of the Army, past and present\". On March 13, 1978, Washington was militarily promoted to the rank of General of the Armies.In 1809, Mason Locke Weems wrote a hagiographic biography to honor Washington. Historian Ron Chernow maintains that Weems attempted to humanize Washington, making him look less stern, and to inspire \"patriotism and morality\" and to foster \"enduring myths\", such as Washington's refusal to lie about damaging his father's cherry tree. Weems' accounts have never been proven or disproven. Historian John Ferling, however, maintains that Washington remains the only founder and president ever to be referred to as \"godlike\", and points out that his character has been the most scrutinized by historians. Biographer Douglas Southall Freeman concluded, \"The great big thing stamped across that man is character.\" Expanding on Freeman's assessment, historian David Hackett Fischer defined Washington's character as \"integrity, self-discipline, courage, absolute honesty, resolve, and decision, but also forbearance, decency, and respect for others\".In the 21st century, Washington's reputation has been critically scrutinized.\nRon Chernow describes Washington as always trying to be even-handed in dealing with the Natives. He states that Washington hoped they would abandon their itinerant hunting life and adapt to fixed agricultural communities in the manner of white settlers. He also maintains that Washington never advocated outright confiscation of tribal land or the forcible removal of tribes and that he berated American settlers who abused natives, admitting that he held out no hope for peaceful relations as long as \"frontier settlers entertain the opinion that there is not the same crime (or indeed no crime at all) in killing a native as in killing a white man.\"By contrast, Colin G. Calloway wrote that, \"Washington had a lifelong obsession with getting Indian land, either for himself or for his nation, and initiated policies and campaigns that had devastating effects in Indian country.\" He stated:\n\nThe growth of the nation demanded the dispossession of Indian people. Washington hoped the process could be bloodless and that Indian people would give up their lands for a \"fair\" price and move away. But if Indians refused and resisted, as they often did, he felt he had no choice but to \"extirpate\" them and that the expeditions he sent to destroy Indian towns were therefore entirely justified.\nAlong with other Founding Fathers, Washington has been condemned for holding enslaved people. Though he expressed the desire to see the abolition of slavery come through legislation, he did not initiate or support any initiatives for bringing about its end. This has led to calls from some activists to remove his name from public buildings and his statue from public spaces. Nonetheless, Washington maintains his place among the highest-ranked U.S. Presidents.\n\n\n=== Places, namesakes, and monuments ===\n\nMany places and monuments have been named in honor of Washington, most notably Washington, D.C., the capital of the United States, and the state of Washington, the only U.S. state to be named after a president.On February 21, 1885, the Washington Monument was dedicated. The 555-foot marble obelisk, which stands on the National Mall in Washington, D.C., was built between 1848\u20131854 and 1879\u20131884 and was the tallest structure in the world between 1884 and 1889.Washington appears as one of four U.S. presidents on the Shrine of Democracy, a colossal statue by Gutzon Borglum on Mount Rushmore in South Dakota.A number of secondary schools and universities are named in honor of Washington, including George Washington University and Washington University in St. Louis.\n\n\n=== Currency and postage ===\n\nWashington appears on contemporary U.S. currency, including the one-dollar bill, the Presidential one-dollar coin and the quarter-dollar coin (the Washington quarter). Washington and Benjamin Franklin appeared on the nation's first postage stamps in 1847. Washington has since appeared on many postage issues, more than any other person.\n\n\n== See also ==\n\nFounders Online\nList of American Revolutionary War battles\nList of Continental Forces in the American Revolutionary War\nTimeline of the American Revolution\nThe Washington Papers\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== Further reading ==\nCurtis, Wayne (May 1, 2006). \"The Father of the Pina Colada?\". The Atlantic.\nCohen, Andrew (November 1, 2011). \"What George Washington Thought About the Constitution\". The Atlantic.\nLewis, Danny (September 22, 2016). \"George Washington's Biracial Family Is Getting New Recognition\". Smithsonian.\nGood, Cassandra (September 28, 2018). \"Did George Washington 'Have a Couple of Things in His Past'?\". The Atlantic.\nZegart, Amy (November 25, 2018). \"George Washington Was a Master of Deception\". The Atlantic.\nWulf, Karin (April 7, 2020). \"The President's Cabinet Was an Invention of America's First President\". Smithsonian.\nCoe, Alexis (February 12, 2020). \"George Washington Saw a Future for America: Mules\". Smithsonian.\nImmerwahr, Daniel (January 31, 2023). \"Did George Washington Burn New York?\". The Atlantic.\n\n\n== External links ==\n\nGeorge Washington's Mount Vernon\nThe Papers of George Washington, subset of Founders Online from the National Archives\nWorks by George Washington at Project Gutenberg\nWorks by George Washington at Biodiversity Heritage Library \nIn Our Time: Washington & the American Revolution, BBC Radio 4 discussion with Carol Berkin, Simon Middleton, & Colin Bonwick (June 24, 2004)\nGreat Lives: George Washington, BBC Radio 4 discussion with Matthew Parris, Michael Rose, & Frank Grizzard (October 21, 2016)\nGeorge Washington on C-SPAN\nScholarly coverage of Washington at the Miller Center, University of Virginia"}, {"id": 22, "title": "Cricket (insect)", "content": "Crickets are orthopteran insects which are related to bush crickets, and, more distantly, to grasshoppers. In older literature, such as Imms, \"crickets\" were placed at the family level (i.e. Gryllidae), but contemporary authorities including Otte now place them in the superfamily Grylloidea.  The word has been used in combination to describe more distantly related taxa in the suborder Ensifera, such as king crickets and mole crickets.\nCrickets have mainly cylindrically shaped bodies, round heads, and long antennae. Behind the head is a smooth, robust pronotum. The abdomen ends in a pair of long cerci; females have a long, cylindrical ovipositor. Diagnostic features include legs with 3-segmented tarsi; as with many Orthoptera, the hind legs have enlarged femora, providing power for jumping. The front wings are adapted as tough, leathery elytra, and some crickets chirp by rubbing parts of these together. The hind wings are membranous and folded when not in use for flight; many species, however, are flightless. The largest members of the family are the bull crickets, Brachytrupes, which are up to 5 cm (2 in) long.\nCrickets are distributed all around the world except at latitudes 55\u00b0 or higher, with the greatest diversity being in the tropics. They occur in varied habitats from grassland, bushes, and forests to marshes, beaches, and caves. Crickets are mainly nocturnal, and are best known for the loud, persistent, chirping song of males trying to attract females, although some species are mute. The singing species have good hearing, via the tympana on the tibiae of the front legs.\nCrickets often appear as characters in literature. The Talking Cricket features in Carlo Collodi's 1883 children's book, The Adventures of Pinocchio, and in films based on the book. The insect is central to Charles Dickens's 1845 The Cricket on the Hearth and George Selden's 1960 The Cricket in Times Square. Crickets are celebrated in poems by William Wordsworth, John Keats, Du Fu and Vladimir Nazor. They are kept as pets in countries from China to Europe, sometimes for cricket fighting. Crickets are efficient at converting their food into body mass, making them a candidate for food production. They are used as human food in Southeast Asia, where they are sold deep-fried in markets as snacks. They are also used to feed carnivorous pets and zoo animals. In Brazilian folklore, crickets feature as omens of various events.\n\n\n== Description ==\nCrickets are small to medium-sized insects with mostly cylindrical, somewhat vertically flattened bodies. The head is spherical with long slender antennae arising from cone-shaped scapes (first segments) and just behind these are two large compound eyes. On the forehead are three ocelli (simple eyes). The pronotum (first thoracic segment) is trapezoidal in shape, robust, and well-sclerotized. It is smooth and has neither dorsal nor lateral keels (ridges).At the tip of the abdomen is a pair of long cerci (paired appendages on rearmost segment), and in females, the ovipositor is cylindrical, long and narrow, smooth and shiny. The femora (third segments) of the back pair of legs are greatly enlarged for jumping. The tibiae (fourth segments) of the hind legs are armed with a number of moveable spurs, the arrangement of which is characteristic of each species. The tibiae of the front legs bear one or more tympani which are used for the reception of sound.The wings lie flat on the body and are very variable in size between species, being reduced in size in some crickets and missing in others. The fore wings are elytra made of tough chitin, acting as a protective shield for the soft parts of the body and in males, bear the stridulatory organs for the production of sound. The hind pair is membranous, folding fan-wise under the fore wings. In many species, the wings are not adapted for flight.The largest members of the family are the 5 cm (2 in)-long bull crickets (Brachytrupes) which excavate burrows a metre or more deep. The tree crickets (Oecanthinae) are delicate white or pale green insects with transparent fore wings, while the field crickets (Gryllinae) are robust brown or black insects.\n\n\n== Distribution and habitat ==\nCrickets have a cosmopolitan distribution, being found in all parts of the world with the exception of cold regions at latitudes higher than about 55\u00b0 North and South. They have colonised many large and small islands, sometimes flying over the sea to reach these locations, or perhaps conveyed on floating timber or by human activity. The greatest diversity occurs in tropical locations, such as in Malaysia, where 88 species were heard chirping from a single location near Kuala Lumpur. A greater number than this could have been present because some species are mute.Crickets are found in many habitats. Members of several subfamilies are found in the upper tree canopy, in bushes, and among grasses and herbs. They also occur on the ground and in caves, and some are subterranean, excavating shallow or deep burrows. Some make home in rotting wood, and certain beach-dwelling species can run and jump over the surface of water.\n\n\n== Biology ==\n\n\n=== Defence ===\nCrickets are relatively defenceless, soft-bodied insects. Most species are nocturnal and spend the day hidden in cracks, under bark, inside curling leaves, under stones or fallen logs, in leaf litter, or in the cracks in the ground that develop in dry weather. Some excavate their own shallow holes in rotting wood or underground and fold in their antennae to conceal their presence. Some of these burrows are temporary shelters, used for a single day, but others serve as more permanent residences and places for mating and laying eggs. Crickets burrow by loosening the soil with the mandibles and then carrying it with the limbs, flicking it backwards with the hind legs or pushing it with the head.Other defensive strategies are the use of camouflage, fleeing, and aggression. Some species have adopted colourings, shapes, and patterns that make it difficult for predators that hunt by sight to detect them. They tend to be dull shades of brown, grey, and green that blend into their background, and desert species tend to be pale. Some species can fly, but the mode of flight tends to be clumsy, so the most usual response to danger is to scuttle away to find a hiding place. While some crickets have a weak bite, a member of the Gryllacrididae or raspy crickets from Australia were found to have the strongest bite of any insect.\n\n\n=== Chirping ===\nMost male crickets make a loud chirping sound by stridulation (scraping two specially textured body parts together). The stridulatory organ is located on the tegmen, or fore wing, which is leathery in texture. A large vein runs along the centre of each tegmen, with comb-like serrations on its edge forming a file-like structure, and at the rear edge of the tegmen is a scraper. The tegmina are held at an angle to the body and rhythmically raised and lowered which causes the scraper on one wing to rasp on the file on the other. The central part of the tegmen contains the \"harp\", an area of thick, sclerotized membrane which resonates and amplifies the volume of sound, as does the pocket of air between the tegmina and the body wall. Most female crickets lack the necessary adaptations to stridulate, so make no sound.Several types of cricket songs are in the repertoire of some species. The calling song attracts females and repels other males, and is fairly loud. The courting song is used when a female cricket is near and encourages her to mate with the caller. A triumphal song is produced for a brief period after a successful mating and may reinforce the mating bond to encourage the female to lay some eggs rather than find another male. An aggressive song is triggered by contact chemoreceptors on the antennae that detect the presence of another male cricket.Crickets chirp at different rates depending on their species and the temperature of their environment. Most species chirp at higher rates the higher the temperature is (about 62 chirps a minute at 13 \u00b0C (55 \u00b0F) in one common species; each species has its own rate). The relationship between temperature and the rate of chirping is known as Dolbear's law. According to this law, counting the number of chirps produced in 14 seconds by the snowy tree cricket, common in the United States, and adding 40 will approximate the temperature in degrees Fahrenheit.\nIn 1975, Dr. William H. Cade discovered that the parasitic tachinid fly Ormia ochracea is attracted to the song of the cricket, and uses it to locate the male to deposit her larvae on him. It was the first known example of a natural enemy that locates its host or prey using the mating signal. Since then, many species of crickets have been found to be carrying the same parasitic fly, or related species. In response to this selective pressure, a mutation leaving males unable to chirp was observed amongst a population of Teleogryllus oceanicus on the Hawaiian island of Kauai, enabling these crickets to elude their parasitoid predators. A different mutation with the same effect was also discovered on the neighboring island of Oahu (ca. 100 miles (160 km) away). Recently, new \"purring\" males of the same species in Hawaii are able to produce a novel auditory sexual signal that can be used to attract females while greatly reducing the likelihood of parasitoid attack from the fly.\n\n\n=== Flight ===\nSome species, such as the ground crickets (Nemobiinae), are wingless; others have small fore wings and no hind wings (Copholandrevus), others lack hind wings and have shortened fore wings in females only, while others are macropterous, with the hind wings longer than the fore wings. In Teleogryllus, the proportion of macropterous individuals varies from very low to 100%. Probably, most species with hind wings longer than fore wings engage in flight.Some species, such as Gryllus assimilis, take off, fly, and land efficiently and well, while other species are clumsy fliers. In some species, the hind wings are shed, leaving wing stumps, usually after dispersal of the insect by flight. In other species, they may be pulled off and consumed by the cricket itself or by another individual, probably providing a nutritional boost.Gryllus firmus exhibits wing polymorphism; some individuals have fully functional, long hind wings and others have short wings and cannot fly. The short-winged females have smaller flight muscles, greater ovarian development, and produce more eggs, so the polymorphism adapts the cricket for either dispersal or reproduction. In some long-winged individuals, the flight muscles deteriorate during adulthood and the insect's reproductive capabilities improve.\n\n\n=== Diet ===\nCaptive crickets are omnivorous; when deprived of their natural diet, they accept a wide range of organic foodstuffs. Some species are completely herbivorous, feeding on flowers, fruit, and leaves, with ground-based species consuming seedlings, grasses, pieces of leaf, and the shoots of young plants. Others are more predatory and include in their diet invertebrate eggs, larvae, pupae, moulting insects, scale insects, and aphids. Many are scavengers and consume various organic remains, decaying plants, seedlings, and fungi. In captivity, many species have been successfully raised on a diet of ground, commercial dry dog food, supplemented with lettuce and aphids.Crickets have relatively powerful jaws, and several species have been known to bite humans.\n\n\n=== Reproduction and lifecycle ===\nMale crickets establish their dominance over each other by aggression. They start by lashing each other with their antennae and flaring their mandibles. Unless one retreats at this stage, they resort to grappling, at the same time each emitting calls that are quite unlike those uttered in other circumstances. When one achieves dominance, it sings loudly, while the loser remains silent.Females are generally attracted to males by their calls, though in nonstridulatory species, some other mechanism must be involved. After the pair has made antennal contact, a courtship period may occur during which the character of the call changes. The female mounts the male and a single spermatophore is transferred to the external genitalia of the female. Sperm flows from this into the female's oviduct over a period of a few minutes or up to an hour, depending on species. After copulation, the female may remove or eat the spermatophore; males may attempt to prevent this with various ritualised behaviours. The female may mate on several occasions with different males.\nMost crickets lay their eggs in the soil or inside the stems of plants, and to do this, female crickets have a long, needle-like or sabre-like egg-laying organ called an ovipositor. Some ground-dwelling species have dispensed with this, either depositing their eggs in an underground chamber or pushing them into the wall of a burrow. The short-tailed cricket (Anurogryllus) excavates a burrow with chambers and a defecating area, lays its eggs in a pile on a chamber floor, and after the eggs have hatched, feeds the juveniles for about a month.Crickets are hemimetabolic insects, whose lifecycle consists of an egg stage, a larval or nymph stage that increasingly resembles the adult form as the nymph grows, and an adult stage. The egg hatches into a nymph about the size of a fruit fly. This passes through about 10 larval stages, and with each successive moult, it becomes more like an adult. After the final moult, the genitalia and wings are fully developed, but a period of maturation is needed before the cricket is ready to breed.\n\n\n=== Inbreeding avoidance ===\nSome species of cricket are polyandrous. In Gryllus bimaculatus, the females select and mate with multiple viable sperm donors, preferring novel mates. Female Teleogryllus oceanicus crickets from natural populations similarly mate and store sperm from multiple males. Female crickets exert a postcopulatory fertilization bias in favour of unrelated males to avoid the genetic consequences of inbreeding. Fertilization bias depends on the control of sperm transport to the sperm storage organs. The inhibition of sperm storage by female crickets can act as a form of cryptic female choice to avoid the severe negative effects of inbreeding. Controlled-breeding experiments with the cricket Gryllus firmus demonstrated inbreeding depression, as nymphal weight and early fecundity declined substantially over the generations; this was caused as expected by an increased frequency of homozygous combinations of deleterious recessive alleles.\n\n\n=== Predators, parasites, and pathogens ===\nCrickets have many natural enemies and are subject to various pathogens and parasites. They are eaten by large numbers of vertebrate and invertebrate predators and their hard parts are often found during the examination of animal intestines. Mediterranean house geckos (Hemidactylus turcicus)  have learned that although a calling decorated cricket (Gryllodes supplicans) may be safely positioned in an out-of-reach burrow, female crickets attracted to the call can be intercepted and eaten.\nThe entomopathogenic fungus Metarhizium anisopliae attacks and kills crickets and has been used as the basis of control in pest populations. The insects are also affected by the cricket paralysis virus, which has caused high levels of fatalities in cricket-rearing facilities. Other fatal diseases that have been identified in mass-rearing establishments include Rickettsia and three further viruses. The diseases may spread more rapidly if the crickets become cannibalistic and eat the corpses.Red parasitic mites sometimes attach themselves to the dorsal region of crickets and may greatly affect them. The horsehair worm Paragordius varius is an internal parasite and can control the behaviour of its cricket host and cause it to enter water, where the parasite continues its lifecycle and the cricket likely drowns. The larvae of the sarcophagid fly Sarcophaga kellyi develop inside the body cavity of field crickets. Female parasitic wasps of Rhopalosoma lay their eggs on crickets, and their developing larvae gradually devour their hosts. Other wasps in the family Scelionidae are egg parasitoids, seeking out batches of eggs laid by crickets in plant tissues in which to insert their eggs.The fly Ormia ochracea has very acute hearing and targets calling male crickets. It locates its prey by ear and then lays its eggs nearby. The developing larvae burrow inside any crickets with which they come in contact and in the course of a week or so, devour what remains of the host before pupating. In Florida, the parasitic flies were only present in the autumn, and at that time of year, the males sang less but for longer periods. A trade-off exists for the male between attracting females and being parasitized.\n\n\n== Phylogeny and taxonomy ==\nThe phylogenetic relationships of the Gryllidae, summarized by Darryl Gwynne in 1995 from his own work (using mainly anatomical characteristics) and that of earlier authors, are shown in the following cladogram, with the Orthoptera divided into two main groups, Ensifera (crickets sensu lato) and Caelifera (grasshoppers). Fossil Ensifera are found from the late Carboniferous period (300 Mya) onwards, and the true crickets, Gryllidae, from the Triassic period (250 to 200 Mya).Cladogram after Gwynne, 1995:\nA phylogenetic study by Jost & Shaw in 2006 using sequences from 18S, 28S, and 16S rRNA supported the monophyly of Ensifera. Most ensiferan families were also found to be monophyletic, and the superfamily Gryllacridoidea was found to include Stenopelmatidae, Anostostomatidae, Gryllacrididae and Lezina. Schizodactylidae and Grylloidea were shown to be sister taxa, and Rhaphidophoridae and Tettigoniidae were found to be more closely related to Grylloidea than had previously been thought. The authors stated that \"a high degree of conflict exists between the molecular and morphological data, possibly indicating that much homoplasy is present in Ensifera, particularly in acoustic structures.\" They considered that tegmen stridulation and tibial tympanae are ancestral to Ensifera and have been lost on multiple occasions, especially within the Gryllidae.\n\n\n=== \"Cricket\" families ===\n\nSeveral families and other taxa in the Ensifera may be called \"crickets\", including:\n\nWithin the Grylloidea\nGryllidae \u2013 \"true crickets\"\nMogoplistidae \u2013 scaly crickets;\nPhalangopsidae \u2013 \"spider-crickets\" and their allies;\nTrigonidiidae - sword-tail crickets and wood or ground-crickets.\nother families in the infraorder Gryllidea may be included:\nGryllotalpidae \u2013 mole crickets;\nMyrmecophilidae \u2013 ant crickets.Strictly, taxa in Infraorder Tettigoniidea and other superfamilies are excluded\nTettigoniidae \u2013  the bush crickets or katydids \u2013 which are quite distinct and unrelated, with 4-segmented tarsi (at least in the middle and hind legs) and females with flattened ovipositors.  Also note:\nwithin this family is the genus Anabrus \u2013 the \"mormon crickets\";\n\"bush crickets\" (American usage) include members of the subfamily Trigonidiinae \u2013 which are \"true crickets\".\nSuperfamily Stenopelmatoidea \u2013 includes: king crickets (w\u0113t\u0101), leaf-rolling, Jerusalem or sand crickets;\nSuperfamily Rhaphidophoroidea \u2013 cave or camel crickets;\nSuperfamily Schizodactyloidea - dune or splay-footed crickets.\n\n\n== In human culture ==\n\n\n=== Folklore and myth ===\nThe folklore and mythology surrounding crickets is extensive. The singing of crickets in the folklore of Brazil and elsewhere is sometimes taken to be a sign of impending rain, or of a financial windfall. In \u00c1lvar N\u00fa\u00f1ez Cabeza de Vaca's  chronicles of the Spanish conquest of the Americas, the sudden chirping of a cricket heralded the sighting of land for his crew, just as their water supply had run out. In Caraguatatuba, Brazil, a black cricket in a room is said to portend illness; a grey one, money; and a green one, hope. In Alagoas state, northeast Brazil, a cricket announces death, thus it is killed if it chirps in a house.  In Barbados, a loud cricket means money is coming in; hence, a cricket must not be killed or evicted if it chirps inside a house. However, another type of cricket that is less noisy forebodes illness or death.\n\n\n=== In literature ===\nCrickets feature as major characters in novels and children's books. Charles Dickens's 1845 novella The Cricket on the Hearth, divided into sections called \"Chirps\", tells the story of a cricket which chirps on the hearth and acts as a guardian angel to a family. Carlo Collodi's 1883 children's book \"Le avventure di Pinocchio\" (The Adventures of Pinocchio) featured \"Il Grillo Parlante\" (The Talking Cricket) as one of its characters. George Selden's 1960 children's book The Cricket in Times Square tells the story of Chester the cricket from Connecticut who joins a family and their other animals, and is taken to see Times Square in New York. The story, which won the Newbery Honor, came to Selden on hearing a real cricket chirp in Times Square.Souvenirs entomologiques, a book written by the French entomologist Jean-Henri Fabre, devotes a whole chapter to the cricket, discussing its construction of a burrow and its song-making. The account is mainly of the field cricket, but also mentions the Italian cricket.Crickets have from time to time appeared in poetry. William Wordsworth's 1805 poem The Cottager to Her Infant includes the couplet \"The kitten sleeps upon the hearth, The crickets long have ceased their mirth\". John Keats's 1819 poem Ode to Autumn includes the lines \"Hedge-crickets sing; and now with treble soft / The redbreast whistles from a garden-croft\". The Chinese Tang dynasty poet Du Fu (712\u2013770) wrote a poem that in the translation by J. P. Seaton begins \"House cricket ... Trifling thing. And yet how his mournful song moves us. Out in the grass his cry was a tremble, But now, he trills beneath our bed, to share his sorrow.\"\n\n\n=== As pets and fighting animals ===\n\nCrickets are kept as pets and are considered good luck in some countries; in China, they are sometimes kept in cages or in hollowed-out gourds specially created in novel shapes. The practice was common in Japan for thousands of years; it peaked in the 19th century, though crickets are still sold at pet shops.  It is also common to have them as caged pets in some European countries, particularly in the Iberian Peninsula. Cricket fighting is a traditional Chinese pastime that dates back to the Tang dynasty (618\u2013907). Originally an indulgence of emperors, cricket fighting later became popular among commoners. The dominance and fighting ability of males does not depend on strength alone; it has been found that they become more aggressive after certain pre-fight experiences such as isolation, or when defending a refuge. Crickets forced to fly for a short while will afterwards fight for two to three times longer than they otherwise would.\n\n\n=== As food ===\nIn the southern part of Asia including Cambodia, Laos, Thailand , crickets commonly are eaten as a snack, prepared by deep frying soaked and cleaned insects. In Thailand, there are 20,000 farmers rearing crickets, with an estimated production of 7,500 tons per year and United Nation's FAO has implemented a project in Laos to improve cricket farming and, consequently, food security. The food conversion efficiency of house crickets (Acheta domesticus) is 1.7, some five times higher than that for beef cattle, and if their fecundity is taken into account, 15 to 20 times higher.Cricket flour may be used as an additive to consumer foods such as pasta, bread, crackers, and cookies. The cricket flour is being used in protein bars, pet foods, livestock feed, nutraceuticals, and other industrial uses. The United Nations says the use of insect protein, such as cricket flour, could be critical in feeding the growing population of the planet while being less damaging to the environment.Crickets are also raised as food for carnivorous zoo animals, laboratory animals, and pets. They may be \"gut loaded\" with additional minerals, such as calcium, to provide a balanced diet for predators such as tree frogs (Hylidae).\n\n\n=== Common expressions ===\nBy the 19th century \"cricket\" and \"crickets\" were in use as euphemisms for using Christ as an interjection. The addition of \"Jiminy\" (a variation of \"Gemini\"), sometimes shortened to \"Jimmy\" created the expressions \"Jiminy Cricket!\" or \"Jimmy Crickets!\" as less blasphemous alternatives to exclaiming \"Jesus Christ!\"By the end of the 20th century the sound of chirping crickets came to represent quietude in literature, theatre and film. From this sentiment arose expressions equating \"crickets\" with silence altogether, particularly when a group of assembled people makes no noise. These expressions have grown from the more descriptive, \"so quiet that you can hear crickets,\" to simply saying, \"crickets\" as shorthand for \"complete silence.\"\n\n\n=== In popular culture ===\nCricket characters feature in the Walt Disney animated movies Pinocchio (1940), where Jiminy Cricket becomes the title character's conscience, and in Mulan (1998), where Cri-Kee is carried in a cage as a symbol of luck, in the Asian manner. The Crickets was the name of Buddy Holly's rock and roll band; Holly's home town baseball team in the 1990s was called the Lubbock Crickets. Cricket is the name of a US children's literary magazine founded in 1973; it uses a cast of insect characters. The sound of crickets is often used in media to emphasize silence, often for comic effect after an awkward joke, in a similar manner to tumbleweed.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nLisa Gail Ryan, Berthold Laufer, Lafcadio Hearn (1996). Insect musicians & cricket champions: a cultural history of singing insects in China and Japan. China Books. ISBN 0-8351-2576-9.\nFranz Huber, Thomas Edwin Moore, Werner Loher (1989). Cricket behavior and neurobiology. Cornell University Press. ISBN 0-8014-2272-8.\n\n\n== External links ==\n\nhouse cricket\ntropical house cricket\nfield crickets, Gryllus spp."}, {"id": 23, "title": "Laws of the Game (association football)", "content": "The Laws of the Game are the codified rules of association football. The laws mention the number of players a team should have, the game length, the size of the field and ball, the type and nature of fouls that referees may penalise, the offside law, and many other laws that define the sport. During a match, it is the task of the referee to interpret and enforce the Laws of the Game.\nThere were various attempts to codify rules among the various types of football in the mid-19th century. The extant Laws date back to 1863 where a ruleset was formally adopted by the newly formed Football Association (FA) and written by its first secretary, Ebenezer Cobb Morley. Over time, the Laws have been amended, and since 1886 they have been maintained by the International Football Association Board (IFAB).\nThe Laws are the only rules of association football FIFA permits its members to use. The Laws currently allow some minor optional variations which can be implemented by national football associations, including some for play at the lowest levels, but otherwise almost all organised football worldwide is played under the same ruleset. Within the United States, Major League Soccer used a distinct ruleset during the 1990s and the National Federation of State High School Associations and National Collegiate Athletic Association still use rulesets that are comparable to, but different from, the IFAB Laws.\n\n\n== Laws of the Game ==\nThe Laws of the Game consist of seventeen individual laws, each law containing several rules and directions:\nLaw 1: The Field of Play\nLaw 2: The Ball\nLaw 3: The Players\nLaw 4: The Players' Equipment\nLaw 5: The Referee\nLaw 6: The Other Match Officials\nLaw 7: The Duration of the Match\nLaw 8: The Start and Restart of Play\nCovers the kick-off and dropped-ball; other methods of restarting play are covered in other laws.\nLaw 9: The Ball In and Out of Play\nLaw 10: Determining the Outcome of a Match\nLaw 11: Offside\nLaw 12: Fouls and Misconduct\nLaw 13: Free Kicks\nLaw 14: The Penalty Kick\nLaw 15: The Throw-in\nLaw 16: The Goal Kick\nLaw 17: The Corner Kick\n\n\n=== Permitted variations ===\nAll high-level association football is played according to the same laws. The Laws permit some variation for youth, veterans, disability and grassroots football, such as shortening the length of the game and the use of temporary dismissals.\n\n\n=== Presentation and interpretation ===\nIn 1997, a major revision dropped whole paragraphs and clarified many sections to simplify and strengthen the principles. These laws are written in English Common Law style and are meant to be guidelines and goals of principle that are then clarified through practice, tradition, and enforcement by the referees.\nThe actual law book had long contained 50 pages more of material, organised in numerous sections, that included many diagrams but were not officially part of the main 17 laws. In 2007, many of these additional sections along with much of the material from the FIFA Questions and Answers (Q&A), were restructured and put into a new \"Additional Instructions and Guidelines for the Referee\" section. In the 2016/2017 revision of the Laws, the material from this section was folded into the Laws themselves.\nReferees are expected to use their judgement and common sense in applying the laws; this is colloquially known as \"Law 18\".\n\n\n=== Jurisdiction and change management ===\nThe laws are administered by the International Football Association Board (IFAB). They meet at least once a year to debate and decide any changes to the text as it exists at that time. The meeting in winter generally leads to an update to the laws on 1 July of each year that take effect immediately. The laws govern all international matches and national matches of member organisations. A minimum of six of the eight-seat IFAB board needs to vote to accept a rule change. Four seats are held by FIFA to represent their 200+ member Nations, with the other four going to each of the British associations (the FA representing England, the SFA representing Scotland, FAW representing Wales and the IFA representing Northern Ireland), meaning that no change can be made without FIFA's approval, but FIFA cannot change the Laws without the approval of at least two of the British governing bodies.\n\n\n== History ==\n\n\n=== Pre-1863 ===\nIn the nineteenth century, the word \"football\" could signify a wide variety of games in which players attempted to move a ball into an opponent's goal.  The first published rules of \"football\" were those of Rugby School (1845), which permitted extensive handling, quickly followed by the Eton field game (1847), which was much more restrictive of handling the ball. Between the 1830s and 1850s, a number of sets of rules were created for use at Cambridge University \u2013 but they were generally not published at the time, and many have subsequently been lost.  The first detailed sets of rules published by football clubs (rather than a school or university) were those of Sheffield F.C. (written 1858, published 1859) which codified a game played for 20 years until being discontinued in favour of the Football Association code, and those of Melbourne FC (1859) which are the origins of Australian rules football. By the time the Football Association met in late 1863, many different sets of rules had been published, varying widely on such questions as the extent to which the ball could be handled, the treatment of offside, the amount of physical contact allowed with opponents, and the height at which a goal could be scored.\n\n\n=== 1863 rules ===\n\nIn 1863, some football clubs followed the example of Rugby School by allowing the ball to be carried in the hands, with players allowed to \"hack\" (kick in the shins) opponents who were carrying the ball. Other clubs forbade both practices.  During the FA meetings to draw up the first version of the laws, there was an acrimonious division between the \"hacking\" and \"non-hacking\" clubs. An FA meeting of 17 November 1863 discussed this question, with the \"hacking\" clubs predominating. A further meeting was scheduled in order to finalise (\"settle\") the laws. At this crucial 24 November meeting, the \"hackers\" were again in a narrow majority. During the meeting, however, the FA's secretary Ebenezer Cobb Morley brought the delegates' attention to a recently published set of football laws from Cambridge University which banned carrying and hacking. Discussion of the Cambridge rules, and suggestions for possible communication with Cambridge on the subject, served to delay the final \"settlement\" of the laws to a further meeting, on 1 December. A number of representatives who supported rugby-style football did not attend this additional meeting, resulting in hacking and carrying being banned.Francis Campbell of Blackheath F.C., the most prominent \"hacking\" club, accused FA President Arthur Pember, Morley, and their allies of managing 24 November meeting improperly in order to prevent the \"pro-hacking\" laws from being adopted. Pember strongly denied such an \"accusation of ungentlemanly conduct\".  The verdicts of later historians have been mixed:  Young accuses Campbell of \"arrogance\", while Harvey supports Campbell's allegations, accusing the non-hackers of a \"coup\" against the pro-hacking clubs. Blackheath, along with the other \"hacking\" clubs, would leave the FA as a result of this dispute.\nThe final version of the FA's laws was formally adopted and published in December 1863.  Some notable differences from the modern game are listed below:\n\nThere was no crossbar. Goals could be scored at any height (as today in Australian rules football).\nWhile most forms of handling were forbidden, players were allowed to catch the ball (provided they did not run with it or throw it). A fair catch was rewarded with a free kick (a feature that today survives in various forms in Australian rules football, rugby union and American football).\nThere was a strict offside rule, under which any player ahead of the kicker was in an offside position (similar to today's offside rule in rugby union). The only exception was when the ball was kicked from behind the goal line.\nThe throw-in was awarded to the first player (on either team) to touch the ball after it went out of play. The ball had to be thrown in at right-angles to the touchline (as today in rugby union).\nThere was no corner-kick. When the ball went behind the goal-line, there was a situation somewhat similar to rugby: if an attacking player first touched the ball after it went out of play, then the attacking team had an opportunity to take a free kick at goal from a point fifteen yards behind the point where the ball was touched (somewhat similar to a conversion in rugby). If a defender first touched the ball, then the defending team kicked the ball out from on or behind the goal line (equivalent to the goal-kick).\nTeams changed ends every time a goal was scored.\nThe rules made no provision for a goal-keeper, match officials, punishments for infringements of the rules, duration of the match, half-time, number of players, or pitch-markings (other than flags to mark the boundary of the playing area).At its meeting on 8 December 1863, the FA agreed that, as reported in Bell's Life in London, John Lillywhite would publish the Laws. The first game to be played under the new rules occurred eleven days later between Barnes and Richmond.\nAdoption of the laws was not universal among English football clubs. The Sheffield Rules continued to be used by many. Additionally, in preference for hacking as well as handling of the ball, several clubs, such as Blackheath, decided against being part of the FA in its early years and would later form the Rugby Football Union in 1871.\n\n\n=== IFAB created ===\nMinor variations between the rules used in England (the jurisdiction of the Football Association) and the other Home Nations of the United Kingdom \u2013 Scotland, Wales and Ireland \u2013 led to the creation of the International Football Association Board to oversee the rules for all the home nations. Their first meeting was in 1886. Before this, teams from different countries had to agree to which country's rules were used before playing.\n\n\n=== FIFA adoption ===\nWhen the international football body on the continent FIFA was founded in Paris in 1904, it immediately declared that FIFA would adhere to the rules laid down by the IFAB. The growing popularity of the international game led to the admittance of FIFA representatives to the IFAB in 1913. Up until 1958, it was still possible for the British associations to vote together to impose changes against the wishes of FIFA. This changed with the adoption of the current voting system whereby FIFA's support is necessary, but not sufficient, for any amendment to pass.\n\n\n=== Notable amendments ===\n\nNotable amendments to the rules include:\n1866 \u2013 The strict rugby-style offside rule is relaxed: a player is onside as long as there are three opponents between the player and the opposing goal. The award of a free kick for a fair catch (still seen in other football codes) is eliminated. A tape (corresponding to the modern crossbar) is added to the goals; previously goals could be scored at any height (as today in Australian rules football).\n1867 \u2013 The situation when the ball goes behind the goal-line is simplified: all rugby-like elements are removed, with the defending team being awarded a goal-kick regardless of which team touched the ball.1870 \u2013 All handling of the ball is forbidden (previously, players had been allowed to catch the ball). Teams change ends at half-time, but only if no goals were scored in the first half.\n1871 \u2013 Introduction of the specific position of goalkeeper, who is allowed to handle the ball \"for the protection of his goal\".\n1872 \u2013 The indirect free kick is introduced as a punishment for a handball, the first mention of a punitive action for contravening the rules. The corner kick is introduced. Teams do not change ends after goals scored during the second half.\n1873 \u2013 The throw-in is awarded against the team who kicked the ball into touch (previously it was awarded to the first player from either team to touch the ball after it went out of play). The goalkeeper may not \"carry\" the ball.\n1874 \u2013 The indirect free kick, previously used only to punish handball, is extended to cover foul play and offside. The first reference to a match official (the \"umpire\").  Previously, team captains had generally been expected to enforce the laws.\n1875 \u2013 A goal may not be directly scored from a corner-kick or from the kick-off. Teams change ends at half-time only. The goal may have either a crossbar or tape.\n1877 \u2013 The throw-in may go in any direction (previously it had to be thrown in at right-angles to the touchline, as today in rugby union).  As a result of this change, the clubs of the Sheffield Football Association agreed to abandon their own distinctive \"Sheffield Rules\" and adopt the FA laws.\n1878 \u2013 A player can be offside from a throw-in.\n1881 \u2013 The referee is introduced, to decide disputes between the umpires. The caution (for \"ungentlemanly behaviour\") and the sending-off (for violent conduct) appear in the laws for the first time.\n1883 \u2013 The International Football Conference, held between the English, Scottish, Irish and Welsh football associations in December 1882, resulted in the unification of the rules across the home nations, which entailed several changes to the FA's laws the following year.  The throw-in finally reaches its modern form, with players required to throw the ball from above the head using two hands. A player cannot be offside from a corner kick. The goalkeeper may take up to two steps while holding the ball. The goal must have a crossbar (the option of using tape is removed).  The kick-off must be kicked forwards. The touch-line is introduced (previously, the boundary of the field of play had been marked by flags).\n1887 \u2013 The goalkeeper may not handle the ball in the opposition's half.\n1888 \u2013 The drop ball is introduced as a means of restarting play after it has been suspended by the referee.\n1889 \u2013 A player may be sent off for repeated cautionable behaviour.\n1890 \u2013 A goal may not be scored directly from a goal kick.1891 \u2013 The penalty kick is introduced, for handball or foul play within 12 yards of the goal line. The umpires are replaced by linesmen. Pitch markings are introduced for the goal area, penalty area, centre spot and centre circle.\n1897 \u2013 The laws specify, for the first time, the number of players on each team (11) and the duration of each match (90 minutes, unless agreed otherwise).  The half-way line is introduced.  The maximum length of the ground is reduced from 200 yards to 130 yards.\n1901 \u2013 Goalkeepers may handle the ball for any purpose (previously the goalkeeper was permitted to handle the ball only \"in defence of his goal\").\n1902 \u2013 The goal area and penalty area assume their modern dimensions, extending six yards and eighteen yards respectively from the goal posts. The penalty spot is introduced.\n1903 \u2013 A goal may be scored directly from a free kick awarded for handball or foul play (previously all free-kicks awarded for infringements of the laws, other than penalty kicks, had been indirect). A referee may refrain from awarding a free kick or penalty in order to give advantage to the attacking team. A player may be sent off for \"bad or violent language to a Referee\".\n1907 \u2013 Players cannot be offside when in their own half.\n1912 \u2013 The goalkeeper may handle the ball only in the penalty area.\n1920 \u2013 A player cannot be offside from a throw-in.\n1924 \u2013 A goal may be scored directly from a corner kick.\n1925 \u2013 The offside rule is relaxed further: a player is onside as long as there are two opponents between the player and the opponents' goal-line (previously, three opponents had been required).\n1931 \u2013 The goalkeeper may take four steps (rather than two) while carrying the ball.\n1937 \u2013 The \"D\" is added to the pitch markings, to ensure that players do not encroach within 10 yards of the player taking a penalty kick.\n1938 \u2013 The laws are completely rewritten and reorganised by a committee under the leadership of Stanley Rous.  The rewriting introduces the schema of seventeen laws that still exists today.  A player may be sent off for \"serious foul play\".\n1958 \u2013 Substitutions of injured players is allowed in competitive matches for the first time, subject to national association approval.\n1970 \u2013 Introduction of red and yellow cards.\n1990 \u2013 A further relaxation of the offside law: a player level with the second-last opponent is considered onside (previously, such a player would have been considered offside).  A player may be sent off for an offence that denies opponents a \"clear goalscoring opportunity\".\n1992 \u2013 Introduction of the back-pass rule: the goalkeeper may not handle the ball after it has been deliberately kicked to him/her by a teammate.\n1993 - Introduction of the golden goal: if either team scored a goal during extra time in a competitive match, the game ends immediately and the scoring team becomes the winner. This rule remained in place until being removed from most competitions in 2004.\n1997 \u2013 The rules are completely rewritten, for the first time since 1938. A goal may be scored directly from the kick-off or from the goal kick.  The goalkeeper may not handle the ball after receiving it directly from a team-mate's throw-in.\n2000 \u2013 The four-step restriction on the goalkeeper handling the ball is repealed and replaced by the \"six-second rule\": the goalkeeper may not handle the ball for more than six seconds.  The goalkeeper may no longer be charged while holding the ball.\n2004 \u2013 The golden goal rule is eliminated.\n2012 \u2013 Goal-line technology permitted (but not required).\n2016 \u2013 The kick-off may be kicked in any direction.\n2018 \u2013 Video assistant referees permitted (but not required). A fourth substitution is permitted in extra time.\n2019 \u2013 Goals scored by hand, whether accidental or not, are disallowed. Attacking players can no longer interfere in defensive walls during free kicks. Substituted players have to leave the field at the nearest goal line or touchline instead of walking to their technical area. Goal kicks put the ball into play immediately (instead of having to leave the penalty area). Team officials can also be cautioned or dismissed. During penalties, goalkeepers are only required to keep one foot on the line. The dropped ball is no longer competitive, instead being dropped for the defensive goalkeeper if in the penalty area, otherwise for the team which last touched the ball.\n\n\n=== Titles of the laws ===\nThe 1938 rewriting of the laws introduced the scheme of 17 named laws that has lasted until today, with only minor alterations.  The history of the numbering and titles of the laws since 1938 is shown in the table below:\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\nThe Rules of Association Football, 1863: The First FA Rule Book Bodleian Library (2006)\n\n\n== External links ==\n\nhttps://digitalhub.fifa.com/m/43dac9099a20723/original/FIFA-Legal-Handbook.pdf\nhttps://digitalhub.fifa.com/m/75c63731a2f58da4/original/pywuivvlfl5aqvhsw2i7-pdf.pdf\nhttps://www.fas.org.sg/wp-content/uploads/2019/04/AFC-Regulations-Governing-International-Matches-2016.pdf\nLaws of the Game on the IFAB\nLaws of the Game on the FA\nThe formation of the Football Association at the Wayback Machine (archived 21 June 2006)\nHistorical documents, hosted by IFAB\nDocuments from historical IFAB meetings\nPrevious editions of the laws of the game available online:\nNotes"}, {"id": 24, "title": "Visual arts", "content": "The visual arts are art forms such as painting, drawing, printmaking, sculpture, ceramics, photography, video, filmmaking, design, crafts, and architecture. Many artistic disciplines, such as performing arts, conceptual art, and textile arts, also involve aspects of the visual arts as well as arts of other types. Also included within the visual arts are the applied arts, such as industrial design, graphic design, fashion design, interior design, and decorative art.Current usage of the term \"visual arts\" includes fine art as well as applied or decorative arts and crafts, but this was not always the case. Before the Arts and Crafts Movement in Britain and elsewhere at the turn of the 20th century, the term 'artist' had for some centuries often been restricted to a person working in the fine arts (such as painting, sculpture, or printmaking) and not the decorative arts, crafts, or applied visual arts media. The distinction was emphasized by artists of the Arts and Crafts Movement, who valued vernacular art forms as much as high forms. Art schools made a distinction between the fine arts and the crafts, maintaining that a craftsperson could not be considered a practitioner of the arts.\n\nThe increasing tendency to privilege painting, and to a lesser degree sculpture, above other arts has been a feature of Western art as well as East Asian art. In both regions, painting has been seen as relying to the highest degree on the imagination of the artist and being the furthest removed from manual labour \u2013 in Chinese painting, the most highly valued styles were those of \"scholar-painting\", at least in theory practiced by gentleman amateurs. The Western hierarchy of genres reflected similar attitudes.\n\n\n== Education and training ==\n\nTraining in the visual arts has generally been through variations of the apprentice and workshop systems. In Europe, the Renaissance movement to increase the prestige of the artist led to the academy system for training artists, and today most of the people who are pursuing a career in the arts train in art schools at tertiary levels. Visual arts have now become an elective subject in most education systems.In East Asia, arts education for nonprofessional artists typically focused on brushwork; calligraphy was numbered among the Six Arts of gentlemen in the Chinese Zhou Dynasty, and  calligraphy and Chinese painting were numbered among the four arts of scholar-officials in imperial China.Leading country in the development of the arts in Latin America, in 1875 created the National Society of the Stimulus of the Arts, founded by painters Eduardo Schiaffino, Eduardo S\u00edvori, and other artists. Their guild was rechartered as the National Academy of Fine Arts in 1905 and, in 1923, on the initiative of painter and academic Ernesto de la C\u00e1rcova, as a department in the University of Buenos Aires, the Superior Art School of the Nation. Currently, the leading educational organization for the arts in the country is the UNA Universidad Nacional de las Artes.\n\n\n== Drawing ==\n\nDrawing is a means of making an image, illustration or graphic using any of a wide variety of tools and techniques available online and offline. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface using dry media such as graphite pencils, pen and ink, inked brushes, wax color pencils, crayons, charcoals, pastels, and markers. Digital tools, including pens, stylus, that simulate the effects of these are also used. The main techniques used in drawing are: line drawing, hatching, crosshatching, random hatching, shading, scribbling, stippling, and blending. An artist who excels at drawing is referred to as a draftsman or draughtsman.Drawing and painting go back tens of thousands of years. Art of the Upper Paleolithic includes figurative art beginning between about 40,000 to 35,000 years ago. Non-figurative cave paintings consisting of hand stencils and simple geometric shapes are even older. Paleolithic cave representations of animals are found in areas such as Lascaux, France and Altamira, Spain in Europe, Maros, Sulawesi in Asia, and Gabarnmung, Australia.\nIn ancient Egypt, ink drawings on papyrus, often depicting people, were used as models for painting or sculpture. Drawings on Greek vases, initially geometric, later developed into the human form with black-figure pottery during the 7th century BC.With paper becoming common in Europe by the 15th century, drawing was adopted by masters such as Sandro Botticelli, Raphael, Michelangelo, and Leonardo da Vinci who sometimes treated drawing as an art in its own right rather than a preparatory stage for painting or sculpture.\n\n\n== Painting ==\n\nPainting taken literally is the practice of applying pigment suspended in a carrier (or medium) and a binding agent (a glue) to a surface (support) such as paper, canvas or a wall. However, when used in an artistic sense it means the use of this activity in combination with drawing, composition, or other aesthetic considerations in order to manifest the expressive and conceptual intention of the practitioner. Painting is also used to express spiritual motifs and ideas; sites of this kind of painting range from artwork depicting mythological figures on pottery to The Sistine Chapel to the human body itself.\n\n\n=== History ===\n\n\n==== Origins and early history ====\nLike drawing, painting has its documented origins in caves and on rock faces. The finest examples, believed by some to be 32,000 years old, are in the Chauvet and Lascaux caves in southern France. In shades of red, brown, yellow and black, the paintings on the walls and ceilings are of bison, cattle, horses and deer.\n\nPaintings of human figures can be found in the tombs of ancient Egypt. In the great temple of Ramses II, Nefertari, his queen, is depicted being led by Isis. The Greeks contributed to painting but much of their work has been lost. One of the best remaining representations are the Hellenistic Fayum mummy portraits. Another example is mosaic of the Battle of Issus at Pompeii, which was probably based on a Greek painting. Greek and Roman art contributed to Byzantine art in the 4th century BC, which initiated a tradition in icon painting.\n\n\n==== The Renaissance ====\n\nApart from the illuminated manuscripts produced by monks during the Middle Ages, the next significant contribution to European art was from Italy's renaissance painters. From Giotto in the 13th century to Leonardo da Vinci and Raphael at the beginning of the 16th century, this was the richest period in Italian art as the chiaroscuro techniques were used to create the illusion of 3-D space.\nPainters in northern Europe too were influenced by the Italian school. Jan van Eyck from Belgium, Pieter Bruegel the Elder from the Netherlands and Hans Holbein the Younger from Germany are among the most successful painters of the times. They used the glazing technique with oils to achieve depth and luminosity.\n\n\n==== Dutch masters ====\n\nThe 17th century witnessed the emergence of the great Dutch masters such as the versatile Rembrandt who was especially remembered for his portraits and Bible scenes, and Vermeer who specialized in interior scenes of Dutch life.\n\n\n==== Baroque ====\n\nThe Baroque started after the Renaissance, from the late 16th century to the late 17th century. Main artists of the Baroque included Caravaggio, who made heavy use of tenebrism. Peter Paul Rubens, a Flemish painter who studied in Italy, worked for local churches in Antwerp and also painted a series for Marie de' Medici. Annibale Carracci took influences from the Sistine Chapel and created the genre of illusionistic ceiling painting. Much of the development that happened in the Baroque was because of the Protestant Reformation and the resulting Counter Reformation. Much of what defines the Baroque is dramatic lighting and overall visuals.\n\n\n==== Impressionism ====\n\nImpressionism began in France in the 19th century with a loose association of artists including Claude Monet, Pierre-Auguste Renoir and Paul C\u00e9zanne who brought a new freely brushed style to painting, often choosing to paint realistic scenes of modern life outside rather than in the studio. This was achieved through a new expression of aesthetic features demonstrated by brush strokes and the impression of reality. They achieved intense color vibration by using pure, unmixed colors and short brush strokes. The movement influenced art as a dynamic, moving through time and adjusting to newfound techniques and perception of art. Attention to detail became less of a priority in achieving, whilst exploring a biased view of landscapes and nature to the artist's eye.\n\n\n==== Post-impressionism ====\n\nTowards the end of the 19th century, several young painters took impressionism a stage further, using geometric forms and unnatural color to depict emotions while striving for deeper symbolism. Of particular note are Paul Gauguin, who was strongly influenced by Asian, African and Japanese art, Vincent van Gogh, a Dutchman who moved to France where he drew on the strong sunlight of the south, and Toulouse-Lautrec, remembered for his vivid paintings of night life in the Paris district of Montmartre.\n\n\n==== Symbolism, expressionism and cubism ====\n\nEdvard Munch, a Norwegian artist, developed his symbolistic approach at the end of the 19th century, inspired by the French impressionist Manet. The Scream (1893), his most famous work, is widely interpreted as representing the universal anxiety of modern man. Partly as a result of Munch's influence, the German expressionist movement originated in Germany at the beginning of the 20th century as artists such as Ernst Kirschner and Erich Heckel began to distort reality for an emotional effect.\nIn parallel, the style known as cubism developed in France as artists focused on the volume and space of sharp structures within a composition. Pablo Picasso and Georges Braque were the leading proponents of the movement. Objects are broken up, analyzed, and re-assembled in an abstracted form. By the 1920s, the style had developed into surrealism with Dali and Magritte.\n\n\n== Printmaking ==\n\nPrintmaking is creating, for artistic purposes, an image on a matrix that is then transferred to a two-dimensional (flat) surface by means of ink (or another form of pigmentation). Except in the case of a monotype, the same matrix can be used to produce many examples of the print.\n\nHistorically, the major techniques (also called media) involved are woodcut, line engraving, etching, lithography, and screen printing (serigraphy, silk screening) but there are many others, including modern digital techniques. Normally, the print is printed on paper, but other mediums range from cloth and vellum to more modern materials.\n\n\n=== European history ===\n\nPrints in the Western tradition produced before about 1830 are known as old master prints. In Europe, from around 1400 AD woodcut, was used for master prints on paper by using printing techniques developed in the Byzantine and Islamic worlds. Michael Wolgemut improved German woodcut from about 1475, and Erhard Reuwich, a Dutchman, was the first to use cross-hatching.  At the end of the century Albrecht D\u00fcrer brought the Western woodcut to a stage that has never been surpassed, increasing the status of the single-leaf woodcut.\n\n\n=== Chinese origin and practice ===\n\nIn China, the art of printmaking developed some 1,100 years ago as illustrations alongside text cut in woodblocks for printing on paper. Initially images were mainly religious but in the Song Dynasty, artists began to cut landscapes. During the Ming (1368\u20131644) and Qing (1616\u20131911) dynasties, the technique was perfected for both religious and artistic engravings.\n\n\n=== Development in Japan 1603\u20131867 ===\n\nWoodblock printing in Japan (Japanese: \u6728\u7248\u753b, moku hanga) is a technique best known for its use in the ukiyo-e artistic genre; however, it was also used very widely for printing illustrated books in the same period. Woodblock printing had been used in China for centuries to print books, long before the advent of movable type, but was only widely adopted in Japan during the Edo period (1603\u20131867). Although similar to woodcut in western printmaking in some regards, moku hanga differs greatly in that water-based inks are used (as opposed to western woodcut, which uses oil-based inks), allowing for a wide range of vivid color, glazes and color transparency.\nAfter the decline of ukiyo-e and introduction of modern printing technologies, woodblock printing continued as a method for printing texts as well as for producing art, both within traditional modes such as ukiyo-e and in a variety of more radical or Western forms that might be construed as modern art.  In the early 20th century, shin-hanga that fused the tradition of ukiyo-e with the techniques of Western paintings became popular, and the works of Hasui Kawase and Hiroshi Yoshida gained international popularity. Institutes such as the \"Adachi Institute of Woodblock Prints\" and \"Takezasado\" continue to produce ukiyo-e prints with the same materials and methods as used in the past.\n\n\n== Photography ==\n\nPhotography is the process of making pictures by means of the action of light. The light patterns reflected or emitted from objects are recorded onto a sensitive medium or storage chip through a timed exposure. The process is done through mechanical shutters or electronically timed exposure of photons into chemical processing or digitizing devices known as cameras.\nThe word comes from the Greek \u03c6\u03c9\u03c2 phos (\"light\"), and \u03b3\u03c1\u03b1\u03c6\u03b9\u03c2 graphis (\"stylus\", \"paintbrush\") or \u03b3\u03c1\u03b1\u03c6\u03b7 graph\u00ea, together meaning \"drawing with light\" or \"representation by means of lines\" or \"drawing.\" Traditionally, the product of photography has been called a photograph. The term photo is an abbreviation; many people also call them pictures. In digital photography, the term image has begun to replace photograph. (The term image is traditional in geometric optics.)\n\n\n== Architecture ==\nArchitecture is the process and the product of planning, designing, and constructing buildings or any other structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements.\nThe earliest surviving written work on the subject of architecture is De architectura, by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation \u2013 firmness, commodity and delight. An equivalent in modern English would be:\n\nDurability \u2013 a building should stand up robustly and remain in good condition.\nUtility \u2013 it should be suitable for the purposes for which it is used.\nBeauty \u2013 it should be aesthetically pleasing.Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and \"architecture\" is the name given to the most highly formalized and respected versions of that craft.\n\n\n== Filmmaking ==\n\nFilmmaking is the process of making a motion-picture, from an initial conception and research, through scriptwriting, shooting and recording, animation or other special effects, editing, sound and music work and finally distribution to an audience; it refers broadly to the creation of all types of films, embracing documentary, strains of theatre and literature in film, and poetic or experimental practices, and is often used to refer to video-based processes as well.\n\n\n== Computer art ==\n\nVisual artists are no longer limited to traditional Visual arts media. Computers have been used as an ever more common tool in the visual arts since the 1960s. Uses include the capturing or creating of images and forms, the editing of those images and forms (including exploring multiple compositions) and the final rendering or printing (including 3D printing).\nComputer art is any in which computers played a role in production or display. Such art can be an image, sound, animation, video, CD-ROM, DVD, video game, website, algorithm, performance or gallery installation.\nMany traditional disciplines are now integrating digital technologies and, as a result, the lines between traditional works of art and new media works created using computers have been blurred. For instance, an artist may combine traditional painting with algorithmic art and other digital techniques. As a result, defining computer art by its end product can be difficult. Nevertheless, this type of art is beginning to appear in art museum exhibits, though it has yet to prove its legitimacy as a form unto itself and this technology is widely seen in contemporary art more as a tool rather than a form as with painting. On the other hand, there are computer-based artworks which belong to a new conceptual and postdigital strand, assuming the same technologies, and their social impact, as an object of inquiry.\nComputer usage has blurred the distinctions between illustrators, photographers, photo editors, 3-D modelers, and handicraft artists. Sophisticated rendering and editing software has led to multi-skilled image developers. Photographers may become digital artists. Illustrators may become animators. Handicraft may be computer-aided or use computer-generated imagery as a template. Computer clip art usage has also made the clear distinction between visual arts and page layout less obvious due to the easy access and editing of clip art in the process of paginating a document, especially to the unskilled observer.\n\n\n== Plastic arts ==\n\nPlastic arts is a term for art forms that involve physical manipulation of a plastic medium by moulding or modeling such as sculpture or ceramics. The term has also been applied to all the visual (non-literary, non-musical) arts.Materials that can be carved or shaped, such as stone or wood, concrete or steel, have also been included in the narrower definition, since, with appropriate tools, such materials are also capable of modulation. This use of the term \"plastic\" in the arts should not be confused with Piet Mondrian's use, nor with the movement he termed, in French and English, \"Neoplasticism.\"\n\n\n=== Sculpture ===\n\nSculpture is three-dimensional artwork created by shaping or combining hard or plastic material, sound, or text and or light, commonly stone (either rock or marble), clay, metal, glass, or wood. Some sculptures are created directly by finding or carving; others are assembled, built together and fired, welded, molded, or cast. Sculptures are often painted. A person who creates sculptures is called a sculptor.\nThe earliest undisputed examples of sculpture belong to the Aurignacian culture, which was located in Europe and southwest Asia and active at the beginning of the Upper Paleolithic. As well as producing some of the earliest known cave art, the people of this culture developed finely-crafted stone tools, manufacturing pendants, bracelets, ivory beads, and bone-flutes, as well as three-dimensional figurines.Because sculpture involves the use of materials that can be moulded or modulated, it is considered one of the plastic arts. The majority of public art is sculpture. Many sculptures together in a garden setting may be referred to as a sculpture garden. Sculptors do not always make sculptures by hand. With increasing technology in the 20th century and the popularity of conceptual art over technical mastery, more sculptors turned to art fabricators to produce their artworks. With fabrication, the artist creates a design and pays a fabricator to produce it. This allows sculptors to create larger and more complex sculptures out of materials like cement, metal and plastic, that they would not be able to create by hand. Sculptures can also be made with 3-d printing technology.\n\n\n== US copyright definition of visual art ==\nIn the United States, the law protecting the copyright over a piece of visual art gives a more restrictive definition of \"visual art\".\n\nA \"work of visual art\" is \u2014\n(1) a painting, drawing, print or sculpture, existing in a single copy, in a limited edition of 200 copies or fewer that are signed and consecutively numbered by the author, or, in the case of a sculpture, in multiple cast, carved, or fabricated sculptures of 200 or fewer that are consecutively numbered by the author and bear the signature or other identifying mark of the author; or\n(2) a still photographic image produced for exhibition purposes only, existing in a single copy that is signed by the author, or in a limited edition of 200 copies or fewer that are signed and consecutively numbered by the author.\nA work of visual art does not include \u2014\n(A)(i) any poster, map, globe, chart, technical drawing, diagram, model, applied art, motion picture or other audiovisual work, book, magazine, newspaper, periodical, data base, electronic information service, electronic publication, or similar publication;\n  (ii) any merchandising item or advertising, promotional, descriptive, covering, or packaging material or container;\n  (iii) any portion or part of any item described in clause (i) or (ii);\n(B) any work made for hire; or\n(C) any work not subject to copyright protection under this title.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nBarnes, A. C., The Art in Painting, 3rd ed., 1937, Harcourt, Brace & World, Inc., NY.\nBukumirovic, D. (1998). Maga Magazinovic. Biblioteka Fatalne srpkinje knj. br. 4. Beograd: Narodna knj.\nFazenda, M. J. (1997). Between the pictorial and the expression of ideas: the plastic arts and literature in the dance of Paula Massano. n.p.\nGer\u00f3n, C. (2000). Enciclopedia de las artes pl\u00e1sticas dominicanas: 1844\u20132000. 4th ed. Dominican Republic s.n.\nOliver Grau (Ed.): MediaArtHistories. MIT-Press, Cambridge 2007. with Rudolf Arnheim, Barbara Stafford, Sean Cubitt, W. J. T. Mitchell, Lev Manovich, Christiane Paul, Peter Weibel a.o. Rezensionen Archived 28 September 2011 at the Wayback Machine\nLaban, R. V. (1976). The language of movement: a guidebook to choreutics. Boston: Plays.\nLa Farge, O. (1930). Plastic prayers: dances of the Southwestern Indians. n.p.\nRestany, P. (1974). Plastics in arts. Paris, New York: n.p.\nUniversity of Pennsylvania. (1969). Plastics and new art. Philadelphia: The Falcon Pr.\n\n\n== External links ==\n\nArtLex \u2013 online dictionary of visual art terms (archived 24 April 2005)\nCalendar for Artists \u2013 calendar listing of visual art festivals.\nArt History Timeline by the Metropolitan Museum of Art."}, {"id": 25, "title": "Social network", "content": "A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\nSocial networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and \"web of group affiliations\". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.\n\n\n== Overview ==\nThe social network is a theoretical construct useful in the social sciences to study relationships between individuals, groups, organizations, or even entire societies (social units, see differentiation). The term is used to describe a social structure determined by such interactions. The ties through which any given social unit connects represent the convergence of the various social contacts of that unit. This theoretical approach is, necessarily, relational.  An axiom of the social network approach to understanding social interaction is that social phenomena should be primarily conceived and investigated through the properties of relations between and within units, instead of the properties of these units themselves. Thus, one common criticism of social network theory is that individual agency is often ignored although this may not be the case in practice (see agent-based modeling). Precisely because many different types of relations, singular or in combination, form these network configurations, network analytics are useful to a broad range of research enterprises. In social science, these fields of study include, but are not limited to anthropology, biology, communication studies, economics, geography, information science, organizational studies, social psychology, sociology, and sociolinguistics.\n\n\n== History ==\nIn the late 1890s, both \u00c9mile Durkheim and Ferdinand T\u00f6nnies foreshadowed the idea of social networks in their theories and research of social groups. T\u00f6nnies argued that social groups can exist as personal and direct social ties that either link individuals who share values and belief (Gemeinschaft, German, commonly translated as \"community\") or impersonal, formal, and instrumental social links (Gesellschaft, German, commonly translated as \"society\"). Durkheim gave a non-individualistic explanation of social facts, arguing that social phenomena arise when interacting individuals constitute a reality that can no longer be accounted for in terms of the properties of individual actors. Georg Simmel, writing at the turn of the twentieth century, pointed to the nature of networks and the effect of network size on interaction and examined the likelihood of interaction in loosely knit networks rather than groups.\nMajor developments in the field can be seen in the 1930s by several groups in psychology, anthropology, and mathematics working independently. In psychology, in the 1930s, Jacob L. Moreno began systematic recording and analysis of social interaction in small groups, especially classrooms and work groups (see sociometry). In anthropology, the foundation for social network theory is the theoretical and ethnographic work of Bronislaw Malinowski, Alfred Radcliffe-Brown, and Claude L\u00e9vi-Strauss. A group of social anthropologists associated with Max Gluckman and the Manchester School, including John A. Barnes, J. Clyde Mitchell and Elizabeth Bott Spillius, often are credited with performing some of the first fieldwork from which network analyses were performed, investigating community networks in southern Africa, India and the United Kingdom. Concomitantly, British anthropologist S. F. Nadel codified a theory of social structure that was influential in later network analysis. In sociology, the early (1930s) work of Talcott Parsons set the stage for taking a relational approach to understanding social structure. Later, drawing upon Parsons' theory, the work of sociologist Peter Blau provides a strong impetus for analyzing the relational ties of social units with his work on social exchange theory.By the 1970s, a growing number of scholars worked to combine the different tracks and traditions. One group consisted of sociologist Harrison White and his students at the Harvard University Department of Social Relations. Also independently active in the Harvard Social Relations department at the time were Charles Tilly, who focused on networks in political and community sociology and social movements, and Stanley Milgram, who developed the \"six degrees of separation\" thesis. Mark Granovetter and Barry Wellman are among the former students of White who elaborated and championed the analysis of social networks.Beginning in the late 1990s, social network analysis experienced work by sociologists, political scientists, and physicists such as Duncan J. Watts, Albert-L\u00e1szl\u00f3 Barab\u00e1si, Peter Bearman, Nicholas A. Christakis, James H. Fowler, and others, developing and applying new models and methods to emerging data available about online social networks, as well as \"digital traces\" regarding face-to-face networks.\n\n\n== Levels of analysis ==\nIn general, social networks are self-organizing, emergent, and complex, such that a globally coherent pattern appears from the local interaction of the elements that make up the system. These patterns become more apparent as network size increases. However, a global network analysis of, for example, all interpersonal relationships in the world is not feasible and is likely to contain so much information as to be uninformative. Practical limitations of computing power, ethics and participant recruitment and payment also limit the scope of a social network analysis. The nuances of a local system may be lost in a large network analysis, hence the quality of information may be more important than its scale for understanding network properties. Thus, social networks are analyzed at the scale relevant to the researcher's theoretical question. Although levels of analysis are not necessarily mutually exclusive, there are three general levels into which networks may fall: micro-level, meso-level, and macro-level.\n\n\n=== Micro level ===\nAt the micro-level, social network research typically begins with an individual, snowballing as social relationships are traced, or may begin with a small group of individuals in a particular social context.\nDyadic level: A dyad is a social relationship between two individuals. Network research on dyads may concentrate on structure of the relationship (e.g. multiplexity, strength), social equality, and tendencies toward reciprocity/mutuality.\nTriadic level: Add one individual to a dyad, and you have a triad. Research at this level may concentrate on factors such as balance and transitivity, as well as social equality and tendencies toward reciprocity/mutuality. In the balance theory of Fritz Heider the triad is the key to social dynamics. The discord in a rivalrous love triangle is an example of an unbalanced triad, likely to change to a balanced triad by a change in one of the relations. The dynamics of social friendships in society has been modeled by balancing triads. The study is carried forward with the theory of signed graphs.\nActor level: The smallest unit of analysis in a social network is an individual in their social setting, i.e., an \"actor\" or \"ego.\" Egonetwork analysis focuses on network characteristics, such as size, relationship strength, density, centrality, prestige and roles such as isolates, liaisons, and bridges. Such analyses, are most commonly used in the fields of psychology or social psychology, ethnographic kinship analysis or other genealogical studies of relationships between individuals.\nSubset level: Subset levels of network research problems begin at the micro-level, but may cross over into the meso-level of analysis. Subset level research may focus on distance and reachability, cliques, cohesive subgroups, or other group actions or behavior.\n\n\n=== Meso level ===\nIn general, meso-level theories begin with a population size that falls between the micro- and macro-levels. However, meso-level may also refer to analyses that are specifically designed to reveal connections between micro- and macro-levels. Meso-level networks are low density and may exhibit causal processes distinct from interpersonal micro-level networks.\nOrganizations: Formal organizations are social groups that distribute tasks for a collective goal. Network research on organizations may focus on either intra-organizational or inter-organizational ties in terms of formal or informal relationships. Intra-organizational networks themselves often contain multiple levels of analysis, especially in larger organizations with multiple branches, franchises or semi-autonomous departments. In these cases, research is often conducted at a work group level and organization level, focusing on the interplay between the two structures. Experiments with networked groups online have documented ways to optimize group-level coordination through diverse interventions, including the addition of autonomous agents to the groups.Randomly distributed networks: Exponential random graph models of social networks became state-of-the-art methods of social network analysis in the 1980s. This framework has the capacity to represent social-structural effects commonly observed in many human social networks, including general degree-based structural effects commonly observed in many human social networks as well as reciprocity and transitivity, and at the node-level, homophily and attribute-based activity and popularity effects, as derived from explicit hypotheses about dependencies among network ties. Parameters are given in terms of the prevalence of small subgraph configurations in the network and can be interpreted as describing the combinations of local social processes from which a given network emerges. These probability models for networks on a given set of actors allow generalization beyond the restrictive dyadic independence assumption of micro-networks, allowing models to be built from theoretical structural foundations of social behavior.\nScale-free networks: A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. In network theory a scale-free ideal network is a random network with a degree distribution that unravels the size distribution of social groups. Specific characteristics of scale-free networks vary with the theories and analytical tools used to create them, however, in general, scale-free networks have some common characteristics. One notable characteristic in a scale-free network is the relative commonness of vertices with a degree that greatly exceeds the average. The highest-degree nodes are often called \"hubs\", and may serve specific purposes in their networks, although this depends greatly on the social context. Another general characteristic of scale-free networks is the clustering coefficient distribution, which decreases as the node degree increases. This distribution also follows a power law. The Barab\u00e1si model of network evolution shown above is an example of a scale-free network.\n\n\n=== Macro level ===\nRather than tracing interpersonal interactions, macro-level analyses generally trace the outcomes of interactions, such as economic or other resource transfer interactions over a large population.\n\nLarge-scale networks: Large-scale network is a term somewhat synonymous with \"macro-level.\" It is primarily used in social and behavioral sciences, and in economics. Originally, the term was used extensively in the computer sciences (see large-scale network mapping).\nComplex networks: Most larger social networks display features of social complexity, which involves substantial non-trivial features of network topology, with patterns of complex connections between elements that are neither purely regular nor purely random (see, complexity science, dynamical system and chaos theory), as do biological, and technological networks. Such complex network features include a heavy tail in the degree distribution, a high clustering coefficient, assortativity or disassortativity among vertices, community structure (see stochastic block model), and hierarchical structure. In the case of agency-directed networks these features also include reciprocity, triad significance profile (TSP, see network motif), and other features. In contrast, many of the mathematical models of networks that have been studied in the past, such as lattices and random graphs, do not show these features.\n\n\n== Theoretical links ==\n\n\n=== Imported theories ===\nVarious theoretical frameworks have been imported for the use of social network analysis. The most prominent of these are Graph theory, Balance theory, Social comparison theory, and more recently, the Social identity approach.\n\n\n=== Indigenous theories ===\nFew complete theories have been produced from social network analysis. Two that have are structural role theory and heterophily theory.\nThe basis of Heterophily Theory was the finding in one study that more numerous weak ties can be important in seeking information and innovation, as cliques have a tendency to have more homogeneous opinions as well as share many common traits. This homophilic tendency was the reason for the members of the cliques to be attracted together in the first place. However, being similar, each member of the clique would also know more or less what the other members knew. To find new information or insights, members of the clique will have to look beyond the clique to its other friends and acquaintances. This is what Granovetter called \"the strength of weak ties\".\n\n\n== Structural holes ==\nIn the context of networks, social capital exists where people have an advantage because of their location in a network. Contacts in a network provide information, opportunities and perspectives that can be beneficial to the central player in the network. Most social structures tend to be characterized by dense clusters of strong connections. Information within these clusters tends to be rather homogeneous and redundant. Non-redundant information is most often obtained through contacts in different clusters. When two separate clusters possess non-redundant information, there is said to be a structural hole between them. Thus, a network that bridges structural holes will provide network benefits that are in some degree additive, rather than overlapping. An ideal network structure has a vine and cluster structure, providing access to many different clusters and structural holes.Networks rich in structural holes are a form of social capital in that they offer information benefits. The main player in a network that bridges structural holes is able to access information from diverse sources and clusters. For example, in business networks, this is beneficial to an individual's career because he is more likely to hear of job openings and opportunities if his network spans a wide range of contacts in different industries/sectors. This concept is similar to Mark Granovetter's theory of weak ties, which rests on the basis that having a broad range of contacts is most effective for job attainment.\n\n\n== Research clusters ==\n\n\n=== Art Networks ===\nResearch has used network analysis to examine networks created when artists are exhibited together in museum exhibition. Such networks have been shown to affect an artist's recognition in history and historical narratives, even when controlling for individual accomplishments of the artist. Other work examines how network grouping of artists can affect an individual artist's auction performance. An artist's status has been shown to increase when associated with higher status networks, though this association has diminishing returns over an artist's career.\n\n\n=== Communication ===\nCommunication Studies are often considered a part of both the social sciences and the humanities, drawing heavily on fields such as sociology, psychology, anthropology, information science, biology, political science, and economics as well as rhetoric, literary studies, and semiotics. Many communication concepts describe the transfer of information from one source to another, and can thus be conceived of in terms of a network. Social network analysis has thus been successfully applied to phenomena ranging from the social diffusion of linguistic innovation to the influence of peer learner communication on study abroad second language acquisition.\n\n\n=== Community ===\nIn J.A. Barnes' day, a \"community\" referred to a specific geographic location and studies of community ties had to do with who talked, associated, traded, and attended church with whom. Today, however, there are extended \"online\" communities developed through telecommunications devices and social network services. Such devices and services require extensive and ongoing maintenance and analysis, often using network science methods. Community development studies, today, also make extensive use of such methods.\n\n\n=== Complex networks ===\nComplex networks require methods specific to modelling and interpreting social complexity and complex adaptive systems, including techniques of dynamic network analysis.\nMechanisms such as Dual-phase evolution explain how temporal changes in connectivity contribute to the formation of structure in social networks.\n\n\n=== Conflict and Cooperation ===\nThe study of  social networks is being used to examine the nature of interdependencies between actors and the ways in which these are related to outcomes of conflict and cooperation. Areas of study include \ncooperative behavior among participants in collective actions such as protests; \npromotion of peaceful behavior, social norms, and public goods within communities through networks of informal governance;\nthe role of social networks in both intrastate conflict and interstate conflict;\nand social networking among politicians, constituents, and bureaucrats.\n\n\n=== Criminal networks ===\nIn criminology and urban sociology, much attention has been paid to the social networks among criminal actors. For example, murders can be seen as a series of exchanges between gangs. Murders can be seen to diffuse outwards from a single source, because weaker gangs cannot afford to kill members of stronger gangs in retaliation, but must commit other violent acts to maintain their reputation for strength.\n\n\n=== Diffusion of innovations ===\nDiffusion of ideas and innovations studies focus on the spread and use of ideas from one actor to another or one culture and another. This line of research seeks to explain why some become \"early adopters\" of ideas and innovations, and links social network structure with facilitating or impeding the spread of an innovation. A case in point is the social diffusion of linguistic innovation such as neologisms.\n\n\n=== Demography ===\nIn demography, the study of social networks has led to new sampling methods for estimating and reaching populations that are hard to enumerate (for example, homeless people or intravenous drug users.) For example, respondent driven sampling is a network-based sampling technique that relies on respondents to a survey recommending further respondents.\n\n\n=== Economic sociology ===\nThe field of sociology focuses almost entirely on networks of outcomes of social interactions. More narrowly, economic sociology considers behavioral interactions of individuals and groups through social capital and social \"markets\". Sociologists, such as Mark Granovetter, have developed core principles about the interactions of social structure, information, ability to punish or reward, and trust that frequently recur in their analyses of political, economic and other institutions. Granovetter examines how social structures and social networks can affect economic outcomes like hiring, price, productivity and innovation and describes sociologists' contributions to analyzing the impact of social structure and networks on the economy.\n\n\n=== Health care ===\nAnalysis of social networks is increasingly incorporated into health care analytics, not only in epidemiological studies but also in models of patient communication and education, disease prevention, mental health diagnosis and treatment, and in the study of health care organizations and systems.\n\n\n=== Human ecology ===\nHuman ecology is an interdisciplinary and transdisciplinary study of the relationship between humans and their natural, social, and built environments. The scientific philosophy of human ecology has a diffuse history with connections to geography, sociology, psychology, anthropology, zoology, and natural ecology.\n\n\n=== Language and linguistics ===\nStudies of language and linguistics, particularly evolutionary linguistics, focus on the development of linguistic forms and transfer of changes, sounds or words, from one language system to another through networks of social interaction. Social networks are also important in language shift, as groups of people add and/or abandon languages to their repertoire. This may happen through the social diffusion of linguistic innovation, and through second language acquisition via communication with peers.\n\n\n=== Literary networks ===\nIn the study of literary systems, network analysis has been applied by Anheier, Gerhards and Romo, De Nooy, Senekal, and Lotker, to study various aspects of how literature functions. The basic premise is that polysystem theory, which has been around since the writings of Even-Zohar, can be integrated with network theory and the relationships between different actors in the literary network, e.g. writers, critics, publishers, literary histories, etc., can be mapped using visualization from SNA.\n\n\n=== Organizational studies ===\nResearch studies of formal or informal organization relationships, organizational communication, economics, economic sociology, and other resource transfers. Social networks have also been used to examine how organizations interact with each other, characterizing the many informal connections that link executives together, as well as associations and connections between individual employees at different organizations. Many organizational social network studies focus on teams. Within team network studies, research assesses, for example, the predictors and outcomes of centrality and power, density and centralization of team instrumental and expressive ties, and the role of between-team networks. Intra-organizational networks have been found to affect organizational commitment, organizational identification, interpersonal citizenship behaviour.\n\n\n=== Social capital ===\nSocial capital is a form of economic and cultural capital in which social networks are central, transactions are marked by reciprocity, trust, and cooperation, and market agents produce goods and services not mainly for themselves, but for a common good. Social capital is split into three dimensions: the structural, the relational and the cognitive dimension. The structural dimension describes how partners interact with each other and which specific partners meet in a social network. Also, the structural dimension of social capital indicates the level of ties among organizations. This dimension is highly connected to the relational dimension which refers to trustworthiness, norms, expectations and identifications of the bonds between partners. The relational dimension explains the nature of these ties which is mainly illustrated by the level of trust accorded to the network of organizations. The cognitive dimension analyses the extent to which organizations share common goals and objectives as a result of their ties and interactions.Social capital is a sociological concept about the value of social relations and the role of cooperation and confidence to achieve positive outcomes. The term refers to the value one can get from their social ties. For example, newly arrived immigrants can make use of their social ties to established migrants to acquire jobs they may otherwise have trouble getting (e.g., because of unfamiliarity with the local language). A positive relationship exists between social capital and the intensity of social network use. In a dynamic framework, higher activity in a network feeds into higher social capital which itself encourages more activity.\n\n\n=== Advertising ===\nThis particular cluster focuses on brand-image and promotional strategy effectiveness, taking into account the impact of customer participation on sales and brand-image. This is gauged through techniques such as sentiment analysis which rely on mathematical areas of study such as data mining and analytics. This area of research produces vast numbers of commercial applications as the main goal of any study is to understand consumer behaviour and drive sales.\n\n\n=== Network position and benefits ===\nIn many organizations, members tend to focus their activities inside their own groups, which stifles creativity and restricts opportunities. A player whose network bridges structural holes has an advantage in detecting and developing rewarding opportunities. Such a player can mobilize social capital by acting as a \"broker\" of information between two clusters that otherwise would not have been in contact, thus providing access to new ideas, opinions and opportunities. British philosopher and political economist John Stuart Mill, writes, \"it is hardly possible to overrate the value of placing human beings in contact with persons dissimilar to themselves.... Such communication [is] one of the primary sources of progress.\" Thus, a player with a network rich in structural holes can add value to an organization through new ideas and opportunities. This in turn, helps an individual's career development and advancement.\nA social capital broker also reaps control benefits of being the facilitator of information flow between contacts. Full communication with exploratory mindsets and information exchange generated by dynamically alternating positions in a social network promotes creative and deep thinking. In the case of consulting firm Eden McCallum, the founders were able to advance their careers by bridging their connections with former big three consulting firm consultants and mid-size industry firms. By bridging structural holes and mobilizing social capital, players can advance their careers by executing new opportunities between contacts. \nThere has been research that both substantiates and refutes the benefits of information brokerage. A study of high tech Chinese firms by Zhixing Xiao found that the control benefits of structural holes are \"dissonant to the dominant firm-wide spirit of cooperation and the information benefits cannot materialize due to the communal sharing values\" of such organizations. However, this study only analyzed Chinese firms, which tend to have strong communal sharing values. Information and control benefits of structural holes are still valuable in firms that are not quite as inclusive and cooperative on the firm-wide level. In 2004, Ronald Burt studied 673 managers who ran the supply chain for one of America's largest electronics companies. He found that managers who often discussed issues with other groups were better paid, received more positive job evaluations and were more likely to be promoted. Thus, bridging structural holes can be beneficial to an organization, and in turn, to an individual's career.\n\n\n=== Social media ===\n\nComputer networks combined with social networking software produce a new medium for social interaction. A relationship over a computerized social networking service can be characterized by context, direction, and strength. The content of a relation refers to the resource that is exchanged. In a computer mediated communication context, social pairs exchange different kinds of information, including sending a data file or a computer program as well as providing emotional support or arranging a meeting. With the rise of electronic commerce, information exchanged may also correspond to exchanges of money, goods or services in the \"real\" world. Social network analysis methods have become essential to examining these types of computer mediated communication.\nIn addition, the sheer size and the volatile nature of social media has given rise to new network metrics. A key concern with networks extracted from social media is the lack of robustness of network metrics given missing data.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nAneja, Nagender; Gambhir, Sapna (August 2013). \"Ad-hoc Social Network: A Comprehensive Survey\" (PDF). International Journal of Scientific & Engineering Research. 4 (8): 156\u2013160. ISSN 2229-5518.\nBarab\u00e1si, Albert-L\u00e1szl\u00f3 (2003). Linked: How Everything Is Connected to Everything Else and What It Means for Business, Science, and Everyday Life. Plum. ISBN 978-0-452-28439-5.\nBarnett, George A. (2011). Encyclopedia of Social Networks. Sage. ISBN 978-1-4129-7911-5.\nEstrada, E (2011). The Structure of Complex Networks: Theory and Applications. Oxford University Press. ISBN 978-0-199-59175-6.\nFerguson, Niall (2018). The Square and the Tower: Networks and Power, from the Freemasons to Facebook. Penguin Press. ISBN 978-0735222915.\nFreeman, Linton C. (2004). The Development of Social Network Analysis: A Study in the Sociology of Science. Empirical Press. ISBN 978-1-59457-714-7.\nKadushin, Charles (2012). Understanding Social Networks: Theories, Concepts, and Findings. Oxford University Press. ISBN 978-0-19-537946-4.\nMauro, Rios; Petrella, Carlos (2014). La Quimera de las Redes Sociales [The Chimera of Social Networks] (in Spanish). Bubok Espa\u00f1a. ISBN 978-9974-99-637-3.\nRainie, Lee; Wellman, Barry (2012). Networked: The New Social Operating System. Cambridge, Mass.: MIT Press. ISBN 978-0262017190.\nScott, John (1991). Social Network Analysis: A Handbook. Sage. ISBN 978-0-7619-6338-7.\nWasserman, Stanley; Faust, Katherine (1994). Social Network Analysis: Methods and Applications. Structural Analysis in the Social Sciences. Cambridge University Press. ISBN 978-0-521-38269-4.\nWellman, Barry; Berkowitz, S. D. (1988). Social Structures: A Network Approach. Structural Analysis in the Social Sciences. Cambridge University Press. ISBN 978-0-521-24441-1.\n\n\n== External links ==\n\n\n=== Organizations ===\nInternational Network for Social Network Analysis\n\n\n=== Peer-reviewed journals ===\nSocial Networks\nNetwork Science\nJournal of Social Structure\nJournal of Mathematical Sociology\nSocial Network Analysis and Mining (SNAM)\n\"INSNA \u2013 Connections Journal\". Connections Bulletin of the International Network for Social Network Analysis. Toronto: International Network for Social Network Analysis. ISSN 0226-1766. Archived from the original on 2013-07-18.\n\n\n=== Textbooks and educational resources ===\nNetworks, Crowds, and Markets (2010) by D. Easley & J. Kleinberg\nIntroduction to Social Networks Methods (2005) by R. Hanneman & M. Riddle\nSocial Network Analysis Instructional Web Site by S. Borgatti\nGuide for virtual social networks for public administrations (2015) by Mauro D. R\u00edos (in Spanish)\n\n\n=== Data sets ===\n\nPajek's list of lists of datasets Archived 2014-10-10 at the Wayback Machine\nUC Irvine Network Data Repository\nStanford Large Network Dataset Collection\nM.E.J. Newman datasets\nPajek datasets\nGephi datasets\nKONECT \u2013 Koblenz network collection\nRSiena datasets"}, {"id": 26, "title": "Filmmaking", "content": "Filmmaking or film production is the process by which a motion picture is produced. Filmmaking involves a number of complex and discrete stages, beginning with an initial story, idea, or commission. Production then continues through screenwriting, casting, pre-production, shooting, sound recording, post-production, and screening the finished product before an audience, which may result in a film release and exhibition. The process is nonlinear, as the director typically shoots the script out of sequence, repeats shots as needed, and puts them together through editing later. Filmmaking occurs in a variety of economic, social, and political contexts around the world, and uses a variety of technologies and cinematic techniques to make theatrical films, episodic films for television and streaming platforms, music videos, and promotional and educations films. \nAlthough filmmaking originally involved the use of film, most film productions are now digital. Today, filmmaking refers to the process of crafting an audio-visual story commercially for distribution or broadcast.\n\n\n== Production stages ==\nFilm production consists of five major stages:\nDevelopment: Ideas for the film are created, rights to existing intellectual properties are purchased, etc., and the screenplay is written. Financing for the project is sought and obtained.\nPre-production: Arrangements and preparations are made for the shoot, such as hiring cast and film crew, selecting locations, and constructing sets.\nProduction: The raw footage and other elements of the film are recorded during the film shoot, including principal photography.\nPost-production: The images, sound, and visual effects of the recorded film are edited and combined into a finished product.\nDistribution: The completed film is distributed, marketed, and screened in cinemas or released to home video to be viewed.\n\n\n=== Development ===\nThe development stage contains both general and specific components. Each film studio has a yearly retreat where their top creative executives meet and interact on a variety of areas and topics they wish to explore through collaborations with producers and screenwriters, and then ultimately, directors, actors, and actresses. They choose trending topics from the media and real life, as well as many other sources, to determine their yearly agenda. For example, in a year when action is popular, they may wish to explore that topic in one or more movies. Sometimes, they purchase the rights to articles, bestselling novels, plays, the remaking of older films, stories with some basis in real life through a person or event, a video game, fairy tale, comic book, graphic novel. Likewise, research through surveys may inform their decisions. They may have had blockbusters from their previous year and wish to explore a sequel. They will additionally acquire a completed and independently financed and produced film. Such notable examples are Little Miss Sunshine and The English Patient as well as Roma.\nStudios hold general meetings with producers and screenwriters about original story ideas. \"In my decade working as a writer, I knew of only a few that were sold and fewer that made it to the screen,\" relays writer Wayne Powers. Alan Watt, writer-director and Founder of The LA Writer's Lab confirmed that completed original screenplays, referred to as \"specs\", make big news when they sell, but these make up a very small portion of movies that are ultimately given the green light to be produced by the president of a studio. \nThe executives return from the retreat with fairly well-established instructions. They spread these concepts through the industry community, especially to producers they have deals with (traditional studios will have those producers in offices on their lots).  Also, agents for screenwriters are made aware. This results in a pairing of producers with writers, where they develop a \"take\", a basic story idea that utilizes the concept given by studio executives. Often it is a competition with several pairings meeting with studio executives and \"pitching\" their \"take\". Very few writing jobs are from original ideas brought to studios by producers or writers.  Perhaps one movie a year will be a \"spec\" script that was purchased.  \nOnce the producer and writer have sold their approach to the desired subject matter, they begin to work.  However, many writers and producers usually pass before a particular concept is realized in a way that is awarded a green light to production. Production of Unforgiven, which earned Oscars for its Director/Star Clint Eastwood, as well as its screenwriter, David Webb Peoples, required fifteen years. Powers related that The Italian Job took approximately eight years from concept to screen, which, as Powers added, \"is average.\" And most concepts turned into paid screenplays wind up gathering dust on some executive's shelf, never to see production. \nWriters have different styles and creative processes; some have stronger track records than others. Because of this, how the development process proceeds from there and how much detail a writer returns to the studio to divulge before beginning writing can vary greatly. Screenwriters are often protected by the union, the Writers Guild of America, or WGA. The WGA allows a screenwriter to contract for One Draft, One Revision, and One Polish. Bob Eisle, Writer and Member of the Guild Board, states, \"Additional writing requires an extension of contracts and payment for additional work\". They are paid 80% of their fee after the First Draft.  Preliminary discussions are minimal with studio executives but might be quite detailed with the producer.\nNext, a screenwriter writes a screenplay over a period of several months, or however long it takes. Deadlines are in their contracts but there is no pressure to adhere to them.  Again, every writer's process and speed vary. The screenwriter may rewrite the script several times to improve dramatization, clarity, structure, characters, dialogue, and overall style. \nScript Coverage, a freelance job held by recent university graduates, does not feed scripts into the system that are ready for production nor already produced. \"Coverage\" is a way for young screenwriters to be read and their ideas might make their way up to an executive or famous producer and result in \"meet and greets\" where relations with up-and-comers can be formed. But it has not historically yielded ideas studios pursue into production. \nThe studio is the film distributor who at an early stage attempts to choose a slate of concepts that are likely to have market appeal and find potential financial success.  Hollywood distributors consider factors such as the film genre, the target audience and assumed audience, the historical success of similar films, the actors who might appear in the film, and potential directors. All these factors imply a certain appeal of the film to a possible audience. Not all films make a profit from the theatrical release alone, however, the studio mainly targets the opening weekend and the second weekend to make most domestic profits.  Occasionally, a film called a \"word of mouth film\" does not market strongly but its success spreads by word of mouth. It slowly gains its audience.  These are special circumstances and these films may remain in theaters for 5 months while a typical film run is closer to 5 weekends. Further earnings result from pay television purchases, foreign market purchases and DVD sales to establish worldwide distribution Gross of a Film.\nOnce a screenplay is \"green-lit\", directors and actors are attached and the film proceeds into the pre-production stage, although sometimes development and pre-production stages will overlap. Projects which fail to obtain a green light may have protracted difficulties in making the transition to pre-production and enter a phase referred to as developmental hell for extended period of time or until developmental turnaround.\nAnalogous to almost any business venture, financing of a film project deals with the study of filmmaking as the management and procurement of investments. It includes the dynamics of assets that are required to fund the filmmaking and liabilities incurred during the filmmaking over the time period from early development through the management of profits and losses after distribution under conditions of different degrees of uncertainty and risk. The practical aspects of filmmaking finance can also be defined as the science of the money management of all phases involved in filmmaking. Film finance aims to price assets based on their risk level and their expected rate of return based upon anticipated profits and protection against losses.\n\n\n=== Pre-production ===\n\nIn pre-production, every step of actually creating the film is carefully designed and planned. This is the phase where one would narrow down all the options of the production. It is where all the planning takes place before the camera rolls and sets the overall vision of the project. The production company is created and a production office established. The film is pre-visualized by the director and may be storyboarded with the help of illustrators and concept artists. A production budget is drawn up to plan expenditures for the film. For major productions, insurance is procured to protect against accidents. Pre-production also includes working out the shoot location and casting process. The Producer hires a Line Manager or a Production Manager to create the schedule and budget for the film.\nThe nature of the film, and the budget, determine the size and type of crew used during filmmaking. Many Hollywood blockbusters employ a cast and crew of hundreds, while a low-budget, independent film may be made by a \"skeleton crew\" of eight or nine (or fewer). These are typical crew positions:\n\nStoryboard artist: creates visual images to help the director and production designer communicate their ideas to the production team.\nDirector: is primarily responsible for the storytelling, creative decisions and acting of the film.\nAssistant director (AD): manages the shooting schedule and logistics of the production, among other tasks.  There are several types of AD, each with different responsibilities.\nFilm producer: hires the film's crew.\nUnit production manager: manages the production budget and production schedule. They also report, on behalf of the production office, to the studio executives or financiers of the film.\nLocation manager: finds and manages film locations. Nearly all pictures feature segments that are shot in the controllable environment of a studio sound stage, while outdoor sequences call for filming on location.\nProduction designer: the one who creates the visual conception of the film, working with the art director, who manages the art department which makes production sets.Costume designer: creates the clothing for the characters in the film working closely with the actors, as well as other departments.\nMakeup and hair designer: works closely with the costume designer in order to create a certain look for a character.\nCasting director: finds actors to fill the parts in the script. This normally requires that actors partake in an audition, either live in front of the casting director or in front of one or more cameras.\nChoreographer: creates and coordinates the movement and dance \u2013 typically for musicals.  Some films also credit a fight choreographer.\nDirector of photography (DOP): the head of the photography of the entire film, supervises all cinematographers and camera operators.\nProduction sound mixer: the head of the sound department during the production stage of filmmaking. They record and mix the audio on set \u2013 dialogue, presence and sound effects in monaural and ambience in stereo. They work with the boom operator, Director, DA, DP, and First AD.\nSound designer: creates the aural conception of the film, working with the supervising sound editor. On Bollywood-style Indian productions the sound designer plays the role of a director of audiography.\nComposer: creates new music for the film. (usually not until post-production)\n\n\n=== Production ===\n\nIn production, the film is created and shot. In this phase, it is key to keep planning ahead of the daily shoot. The primary aim is to stick to the budget and schedule, which requires constant vigilance. More crew will be recruited at this stage, such as the property master, script supervisor, assistant directors, stills photographer, picture editor, and sound editors. These are the most common roles in filmmaking; the production office will be free to create any unique blend of roles to suit the various responsibilities needed during the production of a film. Communication is key between the location, set, office, production company, distributors and all other parties involved. \nA typical day shooting begins with the crew arriving on the set/location by their call time. Actors usually have their own separate call times. Since set construction, dressing and lighting can take many hours or even days, they are often set up in advance.\nThe grip, electric and production design crews are typically a step ahead of the camera and sound departments: for efficiency's sake, while a scene is being filmed, they are already preparing the next one.\nWhile the crew prepares their equipment, the actors do their costumes and attend the hair and make-up departments. The actors rehearse the script and blocking with the director, and the camera and sound crews rehearse with them and make final tweaks. Finally, the action is shot in as many takes as the director wishes. Most American productions follow a specific procedure:\nThe assistant director (AD) calls \"picture is up!\" to inform everyone that a take is about to be recorded, and then \"quiet, everyone!\" Once everyone is ready to shoot, the AD calls \"roll sound\" (if the take involves sound), and the production sound mixer will start their equipment, record a verbal slate of the take's information, and announce \"sound speed\", or just \"speed\", when they are ready. The AD follows with \"roll camera\", answered by \"speed!\" by the camera operator once the camera is recording. The clapper loader, who is already in front of the camera with the clapperboard, calls \"marker!\" and slaps it shut. If the take involves extras or background action, the AD will cue them (\"action background!\"), and last is the director, telling the actors \"action!\". The AD may echo \"action\" louder on large sets.\nA take is over when the director calls \"Cut!\" and the camera and sound stop recording. The script supervisor will note any continuity issues, and the sound and camera teams log technical notes for the take on their respective report sheets. If the director decides additional takes are required, the whole process repeats. Once satisfied, the crew moves on to the next camera angle or \"setup\", until the whole scene is \"covered.\" When shooting is finished for the scene, the assistant director declares a \"wrap\" or \"moving on\", and the crew will \"strike\", or dismantle, the set for that scene.\nAt the end of the day, the director approves the next day's shooting schedule and a daily progress report is sent to the production office. This includes the report sheets from continuity, sound, and camera teams. Call sheets are distributed to the cast and crew to tell them when and where to turn up the next shooting day. Later on, the director, producer, other department heads, and, sometimes, the cast, may gather to watch that day or yesterday's footage, called dailies, and review their work.\nWith workdays often lasting fourteen or eighteen hours in remote locations, film production tends to create a team spirit. When the entire film is \"in the can\", or in the completion of the production phase, it is customary for the production office to arrange a wrap party, to thank all the cast and crew for their efforts.\nFor the production phase on live-action films, synchronizing work schedules of key cast and crew members is very important.  For many scenes, several cast members and many crew members must be physically present at the same place at the same time (and bankable stars may need to rush from one project to another).  Animated films have different workflow at the production phase, in that voice actors can record their takes in the recording studio at different times and may not see one another until the film's premiere.  Animated films also have different crew, since most physical live-action tasks are either unnecessary or are simulated by various types of animators.\n\n\n=== Post-production ===\n\nThis stage is usually thought of as starting when principal photography ends, but they may overlap. The bulk of post-production consists of the film editor reviewing the footage with the director and assembling the film out of selected takes. The production sound (dialogue) is also edited; music tracks and songs are composed and recorded if a film is intended to have a score; sound effects are designed and recorded. Any computer-generated visual effects are digitally added by an artist. Finally, all sound elements are mixed down into \"stems\", which are synchronized to the images on the screen, and the film is fully completed (\"locked\").\n\n\n=== Distribution ===\n\nDistribution is the last stage, where the film is released to movie theaters or, occasionally, directly to consumer media (VHS, VCD, DVD, Blu-ray) or direct download from a digital media provider. The film is duplicated as required (either onto film or hard disk drives) and distributed to cinemas for exhibition (screening).  Press kits, posters, and other advertising materials are published, and the film is advertised and promoted.  A B-roll clip may be released to the press based on raw footage shot for a \"making of\" documentary, which may include making-of clips as well as on-set interviews separate from those of the production company or distributor. For major films, key personnel are often contractually required to participate in promotional tours in which they appear at premieres and festivals and sit for interviews with many TV, print, and online journalists.  The largest productions may require more than one promotional tour, in order to rejuvenate audience demand at each release window.\nSince the advent of home video in the late 1970s, most major films have followed a pattern of having several distinct release windows.  A film may first be released to a few select cinemas, or if it tests well enough, may go directly into wide release.  Next, it is released, normally at different times several weeks (or months) apart, into different market segments like rental, retail, pay-per-view, in-flight entertainment, cable television, satellite television, or free-to-air broadcast television. The distribution rights for the film are also usually sold for worldwide distribution. The distributor and the production company share profits and manage losses.\n\n\n== Independent filmmaking ==\n\nFilmmaking also takes place outside of the mainstream and is commonly called independent filmmaking. Since the introduction of DV technology, the means of production have become more democratized and economically viable. Filmmakers can conceivably shoot and edit a film, create and edit the sound and music, and mix the final cut on a home computer. However, while the means of production may be democratized, financing, traditional distribution, and marketing remain difficult to accomplish outside the traditional system. In the past, most independent filmmakers have relied on film festivals (such as Sundance Film Festival, Venice Film Festival, Cannes Film Festival, and Toronto International Film Festivals) to get their films noticed and sold for distribution and production. However, the internet has allowed for the relatively inexpensive distribution of independent films on websites such as YouTube. As a result, several companies have emerged to assist filmmakers in getting independent movies seen and sold via mainstream internet marketplaces, often adjacent to popular Hollywood titles. With internet movie distribution, independent filmmakers who choose to forego a traditional distribution deal now have the ability to reach global audiences.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nFilmmaking at Curlie"}, {"id": 27, "title": "Cars 3", "content": "Cars 3 is a 2017 American animated sports comedy-adventure film produced by Pixar Animation Studios for Walt Disney Pictures. The sequel to Cars 2 (2011) and the third installment of the Cars film series, the film was directed by Brian Fee (in his directorial debut) and produced by Kevin Reher, from a screenplay written by Kiel Murray, Bob Peterson, and Mike Rich, and a story by Fee, Ben Queen, and the writing team of Eyal Podell and Jonathan E. Stewart. John Lasseter, who directed the first two Cars films, served as executive producer. The returning voices of Owen Wilson, Larry the Cable Guy, Bonnie Hunt, Tony Shalhoub, Guido Quaroni, Cheech Marin, Jenifer Lewis, Paul Dooley, Lloyd Sherr, Michael Wallis, Katherine Helmond and John Ratzenberger are joined by Cristela Alonzo, Chris Cooper, Armie Hammer, Nathan Fillion, Kerry Washington, and Lea DeLaria, in addition to a dozen NASCAR personalities. In the film, Lightning McQueen (Wilson), now a veteran racecar, must prove that he is still competitive against a new generation of technologically advanced racers, with the help of young technician Cruz Ramirez (Alonzo), to prevent a forced retirement from the Piston Cup.\nDevelopment of a third Cars film began in late 2011 after the release of its predecessor, and entered production in 2014, with Lasseter stating that it would be a \"very emotional story\", and go back to the first film's themes. The production team for the film conducted research on multiple NASCAR racers, particularly older ones, as well as a sports psychoanalyst, while also focusing on McQueen's relationship with Doc Hudson and its meaning. The production utilized a new rendering system, Rix Integration Subsystem (RIS), which was previously used in Finding Dory (2016). New cast members including Hammer and Alonzo were announced in January 2017, followed by Fillion, Washington and DeLaria two months later. Randy Newman, who had worked on the first film, composed the film's score with artists such as Andra Day, James Bay, Brad Paisley and Jorge Blanco contributing tracks for the film.\nCars 3 was first screened for the NASCAR industry in Kannapolis, North Carolina on May 23, 2017, before its theatrical release in the United States on June 16, accompanied by the animated short film Lou. It grossed $383 million worldwide against its $175 million budget, becoming the lowest-grossing film of the franchise, but still a box office success. It received generally positive reviews from critics, with praise for its animation, story, and emotional depth, with many of them deeming it an improvement over its predecessor.\n\n\n== Plot ==\nFive years after competing in the World Grand Prix, Lightning McQueen, now a seven-time Piston Cup champion, finds himself overshadowed by Jackson Storm, a rookie who is part of a new generation of racecars who use the latest technology to improve their performance. As Storm's success progresses throughout the season and attracts other rookies, most of the veterans either retire or are dismissed by their sponsors. In the season's final race at Los Angeles, Lightning starts falling behind Storm after both of them pitted. He tries to keep up, but in doing so, stalls his engine and suffers a violent crash, leaving him badly damaged and ending his worst season on record prematurely, while Storm goes on to win the Piston Cup.\nFour months later, Lightning, who has since recovered from his crash, decides that he will continue racing and calls his sponsors, Rusty and Dusty Rust-eze, who reveal they have sold Rust-eze to a businesscar named Sterling. Sterling assigns Lightning to train under Cruz Ramirez, where he struggles to adapt to modern training methods. After Lightning accidentally damages a simulator, Sterling tries to force him to retire. Adamant that he can still race, Lightning instead offers that if he wins the upcoming Florida 500, he can decide if he wants to keep racing; otherwise, he will retire immediately. Sterling reluctantly accepts the deal.\nCruz's unconventional training methods and lack of racing experience frustrate Lightning as they race in Fireball Beach and the figure-8-style demolition derby Thunder Hollow. Lightning rages at her and accidentally breaks her trophy. An upset Cruz reveals that she had always wanted to be a racer but never started a race, so she resigns as Lightning's trainer and heads back to the training center. Guilty and without other options, Lightning calls his friend Mater for advice, who suggests that Lightning track down Doc Hudson's mentor Smokey in his hometown of Thomasville, Georgia, so Lightning catches up to Cruz and convinces her to rejoin him. In Thomasville, they meet up with Smokey, who reveals that even though Doc never raced again, he found new happiness in training Lightning. After Lightning accepts that he will never be as fast as Storm, Smokey and Doc's old friends, Louise \"Barnstormer\" Nash, River Scott, and Junior \"Midnight\" Moon, help him learn new tricks to overcome his speed disadvantage, using Cruz as his sparring partner. However, during the final practice race, Cruz suddenly overtakes him, and he suffers a flashback to his crash, shaking his confidence.\nAt the Florida 500, Lightning starts at the back but, with assistance from Smokey in the pits, manages to gradually push up the ranks. Sterling, who still believes Lightning cannot win, orders Cruz back to the training center to prepare a rookie for the following race despite her wanting to stay and watch the race. Overhearing the exchange on his radio and remembering Cruz's dream of racing, Lightning avoids a massive multi-car pile-up and has his crew outfit her to take his place in the race, giving her a second chance to become a racecar. While shaky at first, Cruz is able to push up the ranks, thanks to Lightning coaching her from the pits and eventually ends up right behind Storm. Feeling threatened, Storm tries to intimidate Cruz, even attempting to ram her against the wall in the final lap. Using one of Doc's old moves, Cruz flips over Storm, overtaking him and winning the race.\nAs Cruz celebrates her victory, Sterling offers her a role on his team, but she instead takes a counteroffer from Dinoco's owner Tex Dinoco. Since Lightning and Cruz were both wearing #95, both have won the race, meaning that Lightning gets to decide if he is done racing. Returning to Radiator Springs, Lightning reveals that Tex has bought Rust-eze from Sterling. Now decked in Doc's old racing colors, Lightning decides to continue racing but trains Cruz first for the rest of the season.\n\n\n== Voice cast ==\n\nOwen Wilson as Lightning McQueen, a legendary Piston Cup veteran and Sally Carrera's boyfriend.\nCristela Alonzo as Cruz Ramirez, Lightning McQueen's trainer.\nChris Cooper as Smokey, Doc Hudson's former mechanic and crew chief who helps out Lightning and Cruz.\nNathan Fillion as Sterling, a rich businesscar and the new Rust-eze team owner.\nLarry the Cable Guy as Mater, a jolly tow truck and Lightning McQueen's best friend.\nArmie Hammer as Jackson Storm, McQueen's new racing rival.\nTom and Ray Magliozzi as Rusty and Dusty Rust-eze, respectively, the owners of Rust-eze. Following Tom's death in 2014, unused archive recordings from the first film were used for Rusty's lines.\nTony Shalhoub as Luigi, a Fiat 500.\nGuido Quaroni as Guido, a forklift who is Luigi's partner.\nBonnie Hunt as Sally Carrera, a Porsche 996, and Lightning McQueen's girlfriend.\nLea DeLaria as Miss Fritter, an intimidating monster school bus at the Thunder Hollow demolition derby.\nKerry Washington as Natalie Certain, a statistical analyst.\nBob Costas as Bob Cutlass, a race commentator.\nMargo Martindale as Louise \"Barnstormer\" Nash, a white 1950 Nash Ambassador and a retired Piston Cup racer from the 1950s who was one of the three legends to live in Thomasville with Smokey.\nIsiah Whitlock Jr. as River Scott, a grey and black 1938 Dirt Track Racer and retired Piston Cup racer who is one of Smokey's friends.\nBob Peterson as Chick Hicks, a former rival of Lightning who now hosts his own talk show called \"Chick's Picks\" on Racing Sports Network. He was originally voiced by Michael Keaton in the first film.\nBob Peterson also voices Dr. Damage, a white and orange modified ambulance who partakes in the Crazy 8 demolition derby.\nJohn Ratzenberger as Mack, a 1985 Mack Super-Liner.\nLewis Hamilton as Hamilton, Cruz' personal voice command assistant.\nSebastian Vettel and Fernando Alonso voiced the foreign language versions of Hamilton, with Vettel voicing 'Vettel' and 'Sebastian' in the Italian and German adaptations respectively, and Alonso voicing 'Fernando' in the Spanish adaptation.\nLloyd Sherr as Fillmore, a Volkswagen Type 2 microbus.\nJunior Johnson as Junior \"Midnight\" Moon, a black 1940 Ford Standard Coupe and a retired Piston Cup racer who is one of Smokey's friends.\nCheech Marin as Ramone, a 1959 Chevrolet Impala coup\u00e9 Lowrider that owns the \"Ramone's House of Body Art\" store.\nKatherine Helmond as Lizzie, a 1923 Ford Model T Coupe who is the elderly owner of a roadside souvenir and accessory shop (Radiator Springs Curios). This was Helmond's final appearance in the Cars franchise before her death in 2019.\nPaul Dooley as Sarge, a 1941 Willys Jeep.\nJenifer Lewis as Flo, the owner of \"Flo's V-8 Caf\u00e9\" and Ramone's wife.\nMadeleine McGraw as Maddy McGear, a young car who is Lightning McQueen's biggest fan.\nMichael Wallis as Sheriff, a 1949 Mercury Eight Police Cruiser police car.\nJerome Ranft as Red, a 1960s closed-cab Whitney Seagrave fire engine. He was originally voiced by Joe Ranft in the first film.\nAngel Oquendo as Bobby Swift, a Piston Cup racer and one of Lightning's friends.\nDiedrich Bader as Brick Yardley, a Piston Cup racer and one of Lightning's friends.\nAndra Day as Sweet Tea, a forklift and Louise Nash's former pitty who is now a singer at the Cotter Pin bar.Additionally, several drivers and other racing-related personalities from NASCAR made cameo appearances, including Chase Elliott as Chase Racelott, Ryan Blaney as Ryan \"Inside\" Laney, Bubba Wallace as Bubba Wheelhouse, Shannon Spake as Shannon Spokes, Daniel Su\u00e1rez as Danny Swervez, Ray Evernham as Ray Reverham, and Mike Joy as Mike Joyride. Richard Petty returns in the role of Strip \"The King\" Weathers, while his son Kyle Petty voices Cal Weathers. Humpy Wheeler, Jeff Gordon, and Darrell Waltrip all return from their previous Cars appearances as Tex Dinoco, Jeff Gorvette, and Darrell Cartrip, respectively. Paul Newman appears as Doc Hudson through the use of unused audio recordings from the first film.\n\n\n== Production ==\nDevelopment on Cars 3 began in late 2011 after the release of Cars 2, and by March 2014, pre-production on the film was underway. In October 2014, Pixar's former chief creative officer John Lasseter revealed at the Tokyo International Film Festival that the film would feature a tribute to Hayao Miyazaki's film The Castle of Cagliostro, in a form of an old Citro\u00ebn 2CV.Prior to the film's release, John Lasseter, who had directed the previous Cars films, stated that the film would have a \"very emotional story\", similar in tone to the first film. Co-writer Kiel Murray, who also co-wrote the original Cars, said of the return to the series' roots, \"With these franchises you always want to know who it's about. The first movie was about McQueen, and the second movie was a sort of off-ramp to the Mater story. We wanted to get back to the McQueen story. When we looked at what would be next for him, we wondered what that would be like both as an athlete, and also for what he was dealing with in the rest of his life.\"According to director Brian Fee, the production team did a lot of research, and, while they \"looked at athletes in other sports\", the team mainly focused on NASCAR racers. Fee said that they \"even talked to a sports psychoanalyst who explained that many of these drivers can't imagine themselves doing anything else\", an idea that resonated with the team. Mike Rich said that rookies taking over the sport is a \"kind of endless story in sports\" and compared McQueen to Wayne Gretzky and Misty May-Treanor as well as many others. Fee said that \"being a parent became [his] main resource to find and understand the emotion\" in the film's storyline. Scott Morse, the film's story supervisor, said that he wanted to highlight the film's emotional core and the character's relationships, wanting the film to feel like a sports film while also focusing on McQueen realizing \"what their relationship meant to Doc\".On January 5, 2017, it was announced that Armie Hammer and Cristela Alonzo would voice Jackson Storm and Cruz Ramirez, respectively. Two months later, Nathan Fillion, Kerry Washington and Lea DeLaria joined the cast.The production utilized a new rendering mode, Rix Integration Subsystem (RIS), which made scenes like the demolition derby race possible. The system was previously used in Finding Dory (2016). In previous Pixar films, the animators had to do the animation first before the rendering, but RIS allowed animation and rendering to take place simultaneously in a process called \"hardware shading\", making it much easier for the animators to see what a completed scene would look like when finished.Fee said that the film's animation is \"art directed realism\" and stated that it causes the film's characters and sets to \"feel more real and alive than ever before\", while Bill Cone, the film's production designer, said that \"The term [they] use is believability, which is the basis for everything [Pixar does]\". Global technology supervisor Sudeep Rangaswamy said that his team used an automatic process for the film's shots, which, in his words \"allows a lot of flexibility\" and that \"It made shots that were previously impossible to render possible\". Director of photography-camera Jeremy Lasky and editor Jason Hudak researched NASCAR footage for the film's race scenes.\n\n\n== Music ==\n\nFee said that both the score and the soundtrack \"really help support the story we are telling\". Both the soundtrack and the score were released on June 16, 2017.The soundtrack features \"Run That Race\", an original song written and performed by Dan Auerbach, who stated the song is \"about never giving up and always trying your best\". Auerbach said that the filmmakers showed him the story and some dialogue, from which he pieced together a story for the song. The soundtrack also features \"Ride\", an original song performed by ZZ Ward featuring Gary Clark Jr., which was released as a single on April 14, 2017.The film's score was composed by Pixar's frequent collaborator, Randy Newman, who previously composed the first film's score.  Tom MacDougall, Disney's executive vice-president of music, said that Newman has \"a real connection to the Cars world\" and that \"His ability to capture the feelings on this film, its characters, locations, and the Americana theme throughout is extraordinary-the music is so naturally fluid and inspired. It really feels like Randy is coming home with this score.\" Newman quoted tracks from the first film in moments where Fee \"wanted to evoke an earlier time\".\n\n\n== Release ==\n\n\n=== Theatrical ===\nCars 3 was released in theaters on Friday, June 16, 2017, in the United States, in 3D, Dolby Cinema and selected IMAX theaters, accompanied by the Pixar short film Lou. The film had a special screening for the NASCAR industry in Kannapolis, North Carolina on May 23, 2017. The world premiere was held in Anaheim, California on June 10, 2017.\n\n\n=== Video game ===\n\nA tie-in video game has been announced to accompany the film's release. It was developed by Avalanche Software, which was shut down by Disney in 2016, but was acquired and revived by Warner Bros. Interactive Entertainment. It was released on Nintendo Switch, PlayStation 3, PlayStation 4, Wii U, Xbox 360, and Xbox One on June 13, 2017, in North America, in Europe and Australia on July 14, 2017, and in Japan on July 20, 2017. As Disney no longer develops and publishes video games after the release of Disney Infinity 3.0, Warner Bros. Interactive Entertainment published the tie-in game.\n\n\n=== Home media ===\nCars 3 was released on Digital HD on October 24, 2017, and was released on DVD, Blu-ray, and 4K Ultra HD Blu-ray on November 7, 2017, by Walt Disney Studios Home Entertainment.\n\n\n=== Short film ===\nThe BBC, Disney and Lego released a short film via YouTube, on April 13, 2018, that is inspired by both the Cars franchise as well as the popular TV series Top Gear. The film tells the story of Lightning McQueen's trip to the Top Gear track, where he achieves his dream of racing against the Stig.\n\n\n== Reception ==\n\n\n=== Box office ===\nCars 3 grossed $152.9 million in the United States and Canada and $231 million in other territories for a worldwide total of $383.9 million, against a production budget of $175 million.In North America, Cars 3 was released alongside Rough Night, 47 Meters Down and All Eyez on Me, and was projected to gross $55\u201360 million from 4,256 theaters in its opening weekend. It made $2.8 million from Thursday night previews and $19.5 million on its first day. It went on to open to $53.7 million, finishing first at the box office and dethroning two-time first-place finisher Wonder Woman. Cars 3 had the lowest opening of the series, but nevertheless was the 16th Pixar film to debut at number one. In its second weekend, the film grossed $24.1 million, dropping to third place, behind Transformers: The Last Knight and Wonder Woman. In its third weekend the film made $9.7 million ($14.1 million over the five-day 4 July holiday weekend), dropping to fifth.\n\n\n=== Critical response ===\nOn the review aggregator website Rotten Tomatoes, 68% of 234 critics' reviews are positive, with an average rating of 6.10/10. The website's consensus reads: \"Cars 3 has an unexpectedly poignant story to go with its dazzling animation, suggesting Pixar's most middle-of-the-road franchise may have a surprising amount of tread left.\" Metacritic, which uses a weighted average, assigned the film a score of 59 out of 100, based on 41 critics, indicating \"mixed or average\" reviews. Audiences polled by CinemaScore gave the film an average grade of \"A\" on an A+ to F scale.The movie was considered an improvement over its predecessor by critics. Owen Gleiberman of Variety wrote, \"Cars 3 is a friendly, rollicking movie made with warmth and dash, and to the extent that it taps our primal affection for this series, it more than gets the job done. Yet in many ways it's the tasteful version of a straight-to-DVD (or streaming) sequel.\" David Fear of Rolling Stone gave the film a positive review, saying: \"There's an emotional resonance to this story about growing old, chasing glory days and the joy of passing the baton that leaves the other two films choking on its digitally rendered dust. The end goal this time out isn't just to sell a few more toys and Lightning McQueen lunchboxes. It's actually tapping into something deeper than a corporate bottom line.\" Mike Ryan of Uproxx called the film \"the Rocky III of the Cars franchise\", and wrote \"There's a hint of sadness that seems to be present throughout Cars 3 that gives it a little more weight than the previous installments.\"Alonso Duralde of TheWrap gave the film a mixed review, saying: \"As a spawner of merchandise, Cars 3 fires on all pistons but, as a movie, it's a harmless but never stimulating 109 minutes.\" Vicky Roach of News.com.au gave the film 3 out of 4 stars, saying: \"Returning to the iconic, backroads nostalgia of the original film, Cars 3 puts the flashy, unpopular middle film squarely in its rear vision mirror. The route that the filmmakers take might be familiar, but after gunning it, they take the corners like pros.\"\n\n\n=== Accolades ===\n\n\n== Possible sequel ==\nRegarding a possible Cars 4, Cars 3 producers Kevin Reher and Andrea Warren stated speaking to Cinema Blend that \"If there's a good story to tell, I mean, our heads kinda break after having gotten this one done, like \"Oh my god,\" what could you do the further adventures of? But like any sequel, from Toy Story 4 to Incredibles 2, as long as there's a good story to tell it's worth investing, we do love these characters, we love them as much as the public does.\" Regarding which character would be the main protagonist in the film, Reher and Warren stated that \"if Cruz is a breakout character, kind of like Mater was, she would be involved in a 4\". Owen Wilson stated at a Cars 3 press event that possible stories have been discussed for a Cars 4, though he would personally like for a fourth Cars film to delve into aspects of the thriller genre, akin to Cars 2. In an interview with Screen Rant, Lea DeLaria expressed interest in reprising her role as Miss Fritter while promoting the release of the short film, Miss Fritter's Racing Skoool with the Cars 3 DVD and Blu-ray release.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website\nCars 3 at IMDb\nCars 3 at AllMovie"}, {"id": 28, "title": "Grover's algorithm", "content": "In quantum computing, Grover's algorithm, also known as the quantum search algorithm, is a quantum algorithm for unstructured search that finds with high probability the unique input to a black box function that produces a particular output value, using just \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   evaluations of the function, where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the size of the function's domain. It was devised by Lov Grover in 1996.The analogous problem in classical computation cannot be solved in fewer than \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n   evaluations (because, on average, one has to check half of the domain to get a 50% chance of finding the right input). Charles H. Bennett, Ethan Bernstein, Gilles Brassard, and Umesh Vazirani proved that any quantum solution to the problem needs to evaluate the function \n  \n    \n      \n        \u03a9\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Omega ({\\sqrt {N}})}\n   times, so Grover's algorithm is asymptotically optimal. Since classical algorithms for NP-complete problems require exponentially many steps, and Grover's algorithm provides at most a quadratic speedup over the classical solution for unstructured search, this suggests that Grover's algorithm by itself will not provide polynomial-time solutions for NP-complete problems (as the square root of an exponential function is an exponential, not polynomial, function).Unlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is large, and Grover's algorithm can be applied to speed up broad classes of algorithms. Grover's algorithm could brute-force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. It may not be the case that Grover's algorithm poses a significantly increased risk to encryption over existing classical algorithms, however.\n\n\n== Applications and limitations ==\nGrover's algorithm, along with variants like amplitude amplification, can be used to speed up a broad range of algorithms. In particular, algorithms for NP-complete problems generally contain exhaustive search as a subroutine, which can be sped up by Grover's algorithm. The current best algorithm for 3SAT is one such example. Generic constraint satisfaction problems also see quadratic speedups with Grover. These algorithms do not require that the input be given in the form of an oracle, since Grover's algorithm is being applied with an explicit function, e.g. the function checking that a set of bits satisfies a 3SAT instance.\nGrover's algorithm can also give provable speedups for black-box problems in quantum query complexity, including element distinctness and the collision problem (solved with the Brassard\u2013H\u00f8yer\u2013Tapp algorithm). In these types of problems, one treats the oracle function f as a database, and the goal is to use the quantum query to this function as few times as possible.\n\n\n=== Cryptography ===\nGrover's algorithm essentially solves the task of function inversion. Roughly speaking, if we have a function \n  \n    \n      \n        y\n        =\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y=f(x)}\n   that can be evaluated on a quantum computer, Grover's algorithm allows us to calculate \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   when given \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . Consequently, Grover's algorithm gives broad asymptotic speed-ups to many kinds of brute-force attacks on symmetric-key cryptography, including collision attacks and pre-image attacks. However, this may not necessarily be the most efficient algorithm since, for example, the parallel rho algorithm is able to find a collision in SHA2 more efficiently than Grover's algorithm.\n\n\n=== Limitations ===\nGrover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input. However, this database is not represented explicitly. Instead, an oracle is invoked to evaluate an item by its index. Reading a full database item by item and converting it into such a representation may take a lot longer than Grover's search. To account for such effects, Grover's algorithm can be viewed as solving an equation or satisfying a constraint. In such applications, the oracle is a way to check the constraint and is not related to the search algorithm. This separation usually prevents algorithmic optimizations, whereas conventional search algorithms often rely on such optimizations and avoid exhaustive search. Fortunately, fast Grover's oracle implementation is possible for many constraint satisfaction and optimization problems.The major barrier to instantiating a speedup from Grover's algorithm is that the quadratic speedup achieved is too modest to overcome the large overhead of near-term quantum computers. However, later generations of fault-tolerant quantum computers with better hardware performance may be able to realize these speedups for practical instances of data.\n\n\n== Problem description ==\nAs input for Grover's algorithm, suppose we have a function \n  \n    \n      \n        f\n        :\n        {\n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        N\n        \u2212\n        1\n        }\n        \u2192\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle f\\colon \\{0,1,\\ldots ,N-1\\}\\to \\{0,1\\}}\n  . In the \"unstructured database\" analogy, the domain represent indices to a database, and f(x) = 1 if and only if the data that x points to satisfies the search criterion. We additionally assume that only one index satisfies f(x) = 1, and we call this index \u03c9. Our goal is to identify \u03c9.\nWe can access f with a subroutine (sometimes called an oracle) in the form of a unitary operator U\u03c9 that acts as follows:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    U\n                    \n                      \u03c9\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  =\n                  \u2212\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  =\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  1\n                  ,\n                \n              \n              \n                \n                  \n                    U\n                    \n                      \u03c9\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  \u2260\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  0.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}U_{\\omega }|x\\rangle =-|x\\rangle &{\\text{for }}x=\\omega {\\text{, that is, }}f(x)=1,\\\\U_{\\omega }|x\\rangle =|x\\rangle &{\\text{for }}x\\neq \\omega {\\text{, that is, }}f(x)=0.\\end{cases}}}\n  This uses the \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  -dimensional state space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  , which is supplied by a register with \n  \n    \n      \n        n\n        =\n        \u2308\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        N\n        \u2309\n      \n    \n    {\\displaystyle n=\\lceil \\log _{2}N\\rceil }\n   qubits.\nThis is often written as\n\n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        =\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        .\n      \n    \n    {\\displaystyle U_{\\omega }|x\\rangle =(-1)^{f(x)}|x\\rangle .}\n  Grover's algorithm outputs \u03c9 with probability at least 1/2 using \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   applications of U\u03c9. This probability can be made arbitrarily large by running Grover's algorithm multiple times. If one runs Grover's algorithm until \u03c9 is found, the expected number of applications is still \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n  , since it will only be run twice on average.\n\n\n=== Alternative oracle definition ===\nThis section compares the above oracle \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   with an oracle \n  \n    \n      \n        \n          U\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle U_{f}}\n  .\nU\u03c9 is different from the standard quantum oracle for a function f. This standard oracle, denoted here as Uf, uses an ancillary qubit system. The operation then represents an inversion (NOT gate) on the main system conditioned by the value of f(x) from the ancillary system:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    U\n                    \n                      f\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  \u00ac\n                  y\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  =\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  1\n                  ,\n                \n              \n              \n                \n                  \n                    U\n                    \n                      f\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  \u2260\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  0\n                  ,\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}U_{f}|x\\rangle |y\\rangle =|x\\rangle |\\neg y\\rangle &{\\text{for }}x=\\omega {\\text{, that is, }}f(x)=1,\\\\U_{f}|x\\rangle |y\\rangle =|x\\rangle |y\\rangle &{\\text{for }}x\\neq \\omega {\\text{, that is, }}f(x)=0,\\end{cases}}}\n  or briefly,\n\n  \n    \n      \n        \n          U\n          \n            f\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u27e9\n        =\n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u2295\n        f\n        (\n        x\n        )\n        \u27e9\n        .\n      \n    \n    {\\displaystyle U_{f}|x\\rangle |y\\rangle =|x\\rangle |y\\oplus f(x)\\rangle .}\n  These oracles are typically realized using uncomputation.\nIf we are given Uf as our oracle, then we can also implement U\u03c9, since U\u03c9 is Uf when the ancillary qubit is in the state \n  \n    \n      \n        \n          |\n        \n        \u2212\n        \u27e9\n        =\n        \n          \n            1\n            \n              2\n            \n          \n        \n        \n          \n            (\n          \n        \n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        \n          \n            )\n          \n        \n        =\n        H\n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |-\\rangle ={\\frac {1}{\\sqrt {2}}}{\\big (}|0\\rangle -|1\\rangle {\\big )}=H|1\\rangle }\n  :\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  U\n                  \n                    f\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  |\n                \n                x\n                \u27e9\n                \u2297\n                \n                  |\n                \n                \u2212\n                \u27e9\n                \n                  \n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      U\n                      \n                        f\n                      \n                    \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    0\n                    \u27e9\n                    \u2212\n                    \n                      U\n                      \n                        f\n                      \n                    \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    1\n                    \u27e9\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    f\n                    (\n                    x\n                    )\n                    \u27e9\n                    \u2212\n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    1\n                    \u2295\n                    f\n                    (\n                    x\n                    )\n                    \u27e9\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    {\n                    \n                      \n                        \n                          \n                            \n                              1\n                              \n                                2\n                              \n                            \n                          \n                          \n                            (\n                            \n                              \u2212\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              0\n                              \u27e9\n                              +\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              1\n                              \u27e9\n                            \n                            )\n                          \n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          =\n                          1\n                          ,\n                        \n                      \n                      \n                        \n                          \n                            \n                              1\n                              \n                                2\n                              \n                            \n                          \n                          \n                            (\n                            \n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              0\n                              \u27e9\n                              \u2212\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              1\n                              \u27e9\n                            \n                            )\n                          \n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          =\n                          0\n                        \n                      \n                    \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                (\n                \n                  U\n                  \n                    \u03c9\n                  \n                \n                \n                  |\n                \n                x\n                \u27e9\n                )\n                \u2297\n                \n                  |\n                \n                \u2212\n                \u27e9\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}U_{f}{\\big (}|x\\rangle \\otimes |-\\rangle {\\big )}&={\\frac {1}{\\sqrt {2}}}\\left(U_{f}|x\\rangle |0\\rangle -U_{f}|x\\rangle |1\\rangle \\right)\\\\&={\\frac {1}{\\sqrt {2}}}\\left(|x\\rangle |f(x)\\rangle -|x\\rangle |1\\oplus f(x)\\rangle \\right)\\\\&={\\begin{cases}{\\frac {1}{\\sqrt {2}}}\\left(-|x\\rangle |0\\rangle +|x\\rangle |1\\rangle \\right)&{\\text{if }}f(x)=1,\\\\{\\frac {1}{\\sqrt {2}}}\\left(|x\\rangle |0\\rangle -|x\\rangle |1\\rangle \\right)&{\\text{if }}f(x)=0\\end{cases}}\\\\&=(U_{\\omega }|x\\rangle )\\otimes |-\\rangle \\end{aligned}}}\n  So, Grover's algorithm can be run regardless of which oracle is given. If Uf is given, then we must maintain an additional qubit in the state \n  \n    \n      \n        \n          |\n        \n        \u2212\n        \u27e9\n      \n    \n    {\\displaystyle |-\\rangle }\n   and apply Uf in place of U\u03c9.\n\n\n== Algorithm ==\nThe steps of Grover's algorithm are given as follows:\n\nInitialize the system to the uniform superposition over all states\n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n        =\n        \n          \n            1\n            \n              N\n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            N\n            \u2212\n            1\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        .\n      \n    \n    {\\displaystyle |s\\rangle ={\\frac {1}{\\sqrt {N}}}\\sum _{x=0}^{N-1}|x\\rangle .}\n  \nPerform the following \"Grover iteration\" \n  \n    \n      \n        r\n        (\n        N\n        )\n      \n    \n    {\\displaystyle r(N)}\n   times:\nApply the operator \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n  \nApply the Grover diffusion operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        =\n        2\n        \n          |\n          s\n          \u27e9\n        \n        \n          \u27e8\n          s\n          |\n        \n        \u2212\n        I\n      \n    \n    {\\displaystyle U_{s}=2\\left|s\\right\\rangle \\left\\langle s\\right|-I}\n  \nMeasure the resulting quantum state in the computational basis.For the correctly chosen value of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , the output will be \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   with probability approaching 1 for N \u226b 1. Analysis shows that this eventual value for \n  \n    \n      \n        r\n        (\n        N\n        )\n      \n    \n    {\\displaystyle r(N)}\n   satisfies \n  \n    \n      \n        r\n        (\n        N\n        )\n        \u2264\n        \n          \n            \u2308\n          \n        \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n          \n        \n        \n          \n            \u2309\n          \n        \n      \n    \n    {\\displaystyle r(N)\\leq {\\Big \\lceil }{\\frac {\\pi }{4}}{\\sqrt {N}}{\\Big \\rceil }}\n  .\nImplementing the steps for this algorithm can be done using a number of gates linear in the number of qubits. Thus, the gate complexity of this algorithm is \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        (\n        N\n        )\n        r\n        (\n        N\n        )\n        )\n      \n    \n    {\\displaystyle O(\\log(N)r(N))}\n  , or \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        (\n        N\n        )\n        )\n      \n    \n    {\\displaystyle O(\\log(N))}\n   per iteration.\n\n\n== Geometric proof of correctness ==\nThere is a geometric interpretation of Grover's algorithm, following from the observation that the quantum state of Grover's algorithm stays in a two-dimensional subspace after each step. Consider the plane spanned by \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  ; equivalently, the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   and the perpendicular ket \n  \n    \n      \n        \n          \n            |\n          \n          \n            s\n            \u2032\n          \n          \u27e9\n          =\n          \n            \n              1\n              \n                N\n                \u2212\n                1\n              \n            \n          \n          \n            \u2211\n            \n              x\n              \u2260\n              \u03c9\n            \n          \n          \n            |\n          \n          x\n          \u27e9\n        \n      \n    \n    {\\displaystyle \\textstyle |s'\\rangle ={\\frac {1}{\\sqrt {N-1}}}\\sum _{x\\neq \\omega }|x\\rangle }\n  .\nGrover's algorithm begins with the initial ket \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n  , which lies in the subspace. The operator \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   is a reflection at the hyperplane orthogonal to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   for vectors in the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  , i.e. it acts as a reflection across \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n  . This can be seen by writing \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   in the form of a Householder reflection:\n\n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        I\n        \u2212\n        2\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \u27e8\n        \u03c9\n        \n          |\n        \n        .\n      \n    \n    {\\displaystyle U_{\\omega }=I-2|\\omega \\rangle \\langle \\omega |.}\n  The operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        =\n        2\n        \n          |\n        \n        s\n        \u27e9\n        \u27e8\n        s\n        \n          |\n        \n        \u2212\n        I\n      \n    \n    {\\displaystyle U_{s}=2|s\\rangle \\langle s|-I}\n   is a reflection through \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n  . Both operators \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   and \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   take states in the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   to states in the plane. Therefore, Grover's algorithm stays in this plane for the entire algorithm.\nIt is straightforward to check that the operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n   of each Grover iteration step rotates the state vector by an angle of \n  \n    \n      \n        \u03b8\n        =\n        2\n        arcsin\n        \u2061\n        \n          \n            \n              1\n              \n                N\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\theta =2\\arcsin {\\tfrac {1}{\\sqrt {N}}}}\n  .\nSo, with enough iterations, one can rotate from the initial state \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   to the desired output state \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  . The initial ket is close to the state orthogonal to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  :\n\n  \n    \n      \n        \u27e8\n        \n          s\n          \u2032\n        \n        \n          |\n        \n        s\n        \u27e9\n        =\n        \n          \n            \n              \n                N\n                \u2212\n                1\n              \n              N\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\langle s'|s\\rangle ={\\sqrt {\\frac {N-1}{N}}}.}\n  In geometric terms, the angle \n  \n    \n      \n        \u03b8\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle \\theta /2}\n   between \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   is given by\n\n  \n    \n      \n        sin\n        \u2061\n        \n          \n            \u03b8\n            2\n          \n        \n        =\n        \n          \n            1\n            \n              N\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sin {\\frac {\\theta }{2}}={\\frac {1}{\\sqrt {N}}}.}\n  We need to stop when the state vector passes close to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  ; after this, subsequent iterations rotate the state vector away from \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  , reducing the probability of obtaining the correct answer. The exact probability of measuring the correct answer is\n\n  \n    \n      \n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        \n          (\n          \n            \n              \n                (\n              \n            \n            r\n            +\n            \n              \n                1\n                2\n              \n            \n            \n              \n                )\n              \n            \n            \u03b8\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\sin ^{2}\\left({\\Big (}r+{\\frac {1}{2}}{\\Big )}\\theta \\right),}\n  where r is the (integer) number of Grover iterations. The earliest time that we get a near-optimal measurement is therefore \n  \n    \n      \n        r\n        \u2248\n        \u03c0\n        \n          \n            N\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle r\\approx \\pi {\\sqrt {N}}/4}\n  .\n\n\n== Algebraic proof of correctness ==\nTo complete the algebraic analysis, we need to find out what happens when we repeatedly apply \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n  . A natural way to do this is by eigenvalue analysis of a matrix. Notice that during the entire computation, the state of the algorithm is a linear combination of \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and \n  \n    \n      \n        \u03c9\n      \n    \n    {\\displaystyle \\omega }\n  . We can write the action of \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   and \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   in the space spanned by \n  \n    \n      \n        {\n        \n          |\n        \n        s\n        \u27e9\n        ,\n        \n          |\n        \n        \u03c9\n        \u27e9\n        }\n      \n    \n    {\\displaystyle \\{|s\\rangle ,|\\omega \\rangle \\}}\n   as:\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        :\n        a\n        \n          |\n        \n        \u03c9\n        \u27e9\n        +\n        b\n        \n          |\n        \n        s\n        \u27e9\n        \u21a6\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  a\n                \n              \n              \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{s}:a|\\omega \\rangle +b|s\\rangle \\mapsto [|\\omega \\rangle \\,|s\\rangle ]{\\begin{bmatrix}-1&0\\\\2/{\\sqrt {N}}&1\\end{bmatrix}}{\\begin{bmatrix}a\\\\b\\end{bmatrix}}.}\n  \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        :\n        a\n        \n          |\n        \n        \u03c9\n        \u27e9\n        +\n        b\n        \n          |\n        \n        s\n        \u27e9\n        \u21a6\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  a\n                \n              \n              \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{\\omega }:a|\\omega \\rangle +b|s\\rangle \\mapsto [|\\omega \\rangle \\,|s\\rangle ]{\\begin{bmatrix}-1&-2/{\\sqrt {N}}\\\\0&1\\end{bmatrix}}{\\begin{bmatrix}a\\\\b\\end{bmatrix}}.}\n  So in the basis \n  \n    \n      \n        {\n        \n          |\n        \n        \u03c9\n        \u27e9\n        ,\n        \n          |\n        \n        s\n        \u27e9\n        }\n      \n    \n    {\\displaystyle \\{|\\omega \\rangle ,|s\\rangle \\}}\n   (which is neither orthogonal nor a basis of the whole space) the action \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n   of applying \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   followed by \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   is given by the matrix\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                  \u2212\n                  4\n                  \n                    /\n                  \n                  N\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{s}U_{\\omega }={\\begin{bmatrix}-1&0\\\\2/{\\sqrt {N}}&1\\end{bmatrix}}{\\begin{bmatrix}-1&-2/{\\sqrt {N}}\\\\0&1\\end{bmatrix}}={\\begin{bmatrix}1&2/{\\sqrt {N}}\\\\-2/{\\sqrt {N}}&1-4/N\\end{bmatrix}}.}\n  This matrix happens to have a very convenient Jordan form. If we define \n  \n    \n      \n        t\n        =\n        arcsin\n        \u2061\n        (\n        1\n        \n          /\n        \n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle t=\\arcsin(1/{\\sqrt {N}})}\n  , it is\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        M\n        \n          \n            [\n            \n              \n                \n                  \n                    e\n                    \n                      2\n                      i\n                      t\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      2\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }=M{\\begin{bmatrix}e^{2it}&0\\\\0&e^{-2it}\\end{bmatrix}}M^{-1}}\n   where \n  \n    \n      \n        M\n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  i\n                \n                \n                  i\n                \n              \n              \n                \n                  \n                    e\n                    \n                      i\n                      t\n                    \n                  \n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle M={\\begin{bmatrix}-i&i\\\\e^{it}&e^{-it}\\end{bmatrix}}.}\n  It follows that r-th power of the matrix (corresponding to r iterations) is \n\n  \n    \n      \n        (\n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          )\n          \n            r\n          \n        \n        =\n        M\n        \n          \n            [\n            \n              \n                \n                  \n                    e\n                    \n                      2\n                      r\n                      i\n                      t\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      2\n                      r\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle (U_{s}U_{\\omega })^{r}=M{\\begin{bmatrix}e^{2rit}&0\\\\0&e^{-2rit}\\end{bmatrix}}M^{-1}.}\n  Using this form, we can use trigonometric identities to compute the probability of observing \u03c9 after r iterations mentioned in the previous section, \n\n  \n    \n      \n        \n          \n            |\n            \n              \n                \n                  [\n                  \n                    \n                      \n                        \u27e8\n                        \u03c9\n                        \n                          |\n                        \n                        \u03c9\n                        \u27e9\n                      \n                      \n                        \u27e8\n                        \u03c9\n                        \n                          |\n                        \n                        s\n                        \u27e9\n                      \n                    \n                  \n                  ]\n                \n              \n              (\n              \n                U\n                \n                  s\n                \n              \n              \n                U\n                \n                  \u03c9\n                \n              \n              \n                )\n                \n                  r\n                \n              \n              \n                \n                  [\n                  \n                    \n                      \n                        0\n                      \n                    \n                    \n                      \n                        1\n                      \n                    \n                  \n                  ]\n                \n              \n            \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        \n          (\n          \n            (\n            2\n            r\n            +\n            1\n            )\n            t\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\left|{\\begin{bmatrix}\\langle \\omega |\\omega \\rangle &\\langle \\omega |s\\rangle \\end{bmatrix}}(U_{s}U_{\\omega })^{r}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}\\right|^{2}=\\sin ^{2}\\left((2r+1)t\\right).}\n  Alternatively, one might reasonably imagine that a near-optimal time to distinguish would be when the angles 2rt and \u22122rt are as far apart as possible, which corresponds to \n  \n    \n      \n        2\n        r\n        t\n        \u2248\n        \u03c0\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 2rt\\approx \\pi /2}\n  , or \n  \n    \n      \n        r\n        =\n        \u03c0\n        \n          /\n        \n        4\n        t\n        =\n        \u03c0\n        \n          /\n        \n        4\n        arcsin\n        \u2061\n        (\n        1\n        \n          /\n        \n        \n          \n            N\n          \n        \n        )\n        \u2248\n        \u03c0\n        \n          \n            N\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle r=\\pi /4t=\\pi /4\\arcsin(1/{\\sqrt {N}})\\approx \\pi {\\sqrt {N}}/4}\n  . Then the system is in state\n\n  \n    \n      \n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        (\n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          )\n          \n            r\n          \n        \n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \u2248\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        M\n        \n          \n            [\n            \n              \n                \n                  i\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \u2212\n                  i\n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n          \n            1\n            \n              cos\n              \u2061\n              (\n              t\n              )\n            \n          \n        \n        \u2212\n        \n          |\n        \n        s\n        \u27e9\n        \n          \n            \n              sin\n              \u2061\n              (\n              t\n              )\n            \n            \n              cos\n              \u2061\n              (\n              t\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle [|\\omega \\rangle \\,|s\\rangle ](U_{s}U_{\\omega })^{r}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}\\approx [|\\omega \\rangle \\,|s\\rangle ]M{\\begin{bmatrix}i&0\\\\0&-i\\end{bmatrix}}M^{-1}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}=|\\omega \\rangle {\\frac {1}{\\cos(t)}}-|s\\rangle {\\frac {\\sin(t)}{\\cos(t)}}.}\n  A short calculation now shows that the observation yields the correct answer \u03c9 with error \n  \n    \n      \n        O\n        \n          (\n          \n            \n              1\n              N\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle O\\left({\\frac {1}{N}}\\right)}\n  .\n\n\n== Extensions and variants ==\n\n\n=== Multiple matching entries ===\n\nIf, instead of 1 matching entry, there are k matching entries, the same algorithm works, but the number of iterations must be \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            \n              (\n              \n                \n                  N\n                  k\n                \n              \n              )\n            \n            \n              1\n              \n                /\n              \n              2\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{\\left({\\frac {N}{k}}\\right)^{1/2}}}\n  instead of \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n            \n              1\n              \n                /\n              \n              2\n            \n          \n        \n        .\n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{N^{1/2}}.}\n  \nThere are several ways to handle the case if k is unknown. A simple solution performs optimally up to a constant factor: run Grover's algorithm repeatedly for increasingly small values of k, e.g., taking k = N, N/2, N/4, ..., and so on, taking \n  \n    \n      \n        k\n        =\n        N\n        \n          /\n        \n        \n          2\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle k=N/2^{t}}\n   for iteration t until a matching entry is found.\nWith sufficiently high probability, a marked entry will be found by iteration \n  \n    \n      \n        t\n        =\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        N\n        \n          /\n        \n        k\n        )\n        +\n        c\n      \n    \n    {\\displaystyle t=\\log _{2}(N/k)+c}\n   for some constant c. Thus, the total number of iterations taken is at most\n\n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            (\n          \n        \n        1\n        +\n        \n          \n            2\n          \n        \n        +\n        \n          \n            4\n          \n        \n        +\n        \u22ef\n        +\n        \n          \n            \n              N\n              \n                k\n                \n                  2\n                  \n                    c\n                  \n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        =\n        O\n        \n          \n            (\n          \n        \n        \n          \n            N\n            \n              /\n            \n            k\n          \n        \n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\pi }{4}}{\\Big (}1+{\\sqrt {2}}+{\\sqrt {4}}+\\cdots +{\\sqrt {\\frac {N}{k2^{c}}}}{\\Big )}=O{\\big (}{\\sqrt {N/k}}{\\big )}.}\n  Another approach if k is unknown is to derive it via the quantum counting algorithm prior. \nIf \n  \n    \n      \n        k\n        =\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k=N/2}\n   (or the traditional one marked state Grover's Algorithm if run with \n  \n    \n      \n        N\n        =\n        2\n      \n    \n    {\\displaystyle N=2}\n  ), the algorithm will provide no amplification. If \n  \n    \n      \n        k\n        >\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k>N/2}\n  , increasing k will begin to increase the number of iterations necessary to obtain a solution. On the other hand, if \n  \n    \n      \n        k\n        \u2265\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k\\geq N/2}\n  , a classical running of the checking oracle on a single random choice of input will more likely than not give a correct solution.\nA version of this algorithm is used in order to solve the collision problem.\n\n\n=== Quantum partial search ===\nA  modification of Grover's algorithm called quantum partial search was described by Grover and Radhakrishnan in 2004. In partial search, one is not interested in finding the exact address of the target item, only the first few digits of the address. Equivalently, we can think of \"chunking\" the search space into blocks, and then asking \"in which block is the target item?\". In many applications, such a search yields enough information if the target address contains the information wanted. For instance, to use the example given by L. K. Grover, if one has a list of students organized by class rank, we may only be interested in whether a student is in the lower 25%, 25\u201350%, 50\u201375% or 75\u2013100% percentile.\nTo describe partial search, we consider a database separated into \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   blocks, each of size \n  \n    \n      \n        b\n        =\n        N\n        \n          /\n        \n        K\n      \n    \n    {\\displaystyle b=N/K}\n  . The partial search problem is easier. Consider the approach we would take classically \u2013 we pick one block at random, and then perform a normal search through the rest of the blocks (in set theory language, the complement). If we don't find the target, then we know it's in the block we didn't search. The average number of iterations drops from \n  \n    \n      \n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle N/2}\n   to \n  \n    \n      \n        (\n        N\n        \u2212\n        b\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle (N-b)/2}\n  .\nGrover's algorithm requires \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n          \n        \n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{\\sqrt {N}}}\n   iterations. Partial search will be faster by a numerical factor that depends on the number of blocks \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  . Partial search uses \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n   global iterations and \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{2}}\n   local iterations. The global Grover operator is designated \n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle G_{1}}\n   and the local Grover operator is designated \n  \n    \n      \n        \n          G\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle G_{2}}\n  .\nThe global Grover operator acts on the blocks. Essentially, it is given as follows:\n\nPerform \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle j_{1}}\n   standard Grover iterations on the entire database.\nPerform \n  \n    \n      \n        \n          j\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j_{2}}\n   local Grover iterations. A local Grover iteration is a direct sum of Grover iterations over each block.\nPerform one standard Grover iteration.The optimal values of \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle j_{1}}\n   and \n  \n    \n      \n        \n          j\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j_{2}}\n   are discussed in the paper by Grover and Radhakrishnan. One might also wonder what happens if one applies successive partial searches at different levels of \"resolution\". This idea was studied in detail by Vladimir Korepin and Xu, who called it binary quantum search. They proved that it is not in fact any faster than performing a single partial search.\n\n\n== Optimality ==\nGrover's algorithm is optimal up to sub-constant factors. That is, any algorithm that accesses the database only by using the operator U\u03c9 must apply U\u03c9 at least a \n  \n    \n      \n        1\n        \u2212\n        o\n        (\n        1\n        )\n      \n    \n    {\\displaystyle 1-o(1)}\n   fraction as many times as Grover's algorithm. The extension of Grover's algorithm to k matching entries, \u03c0(N/k)1/2/4, is also optimal. This result is important in understanding the limits of quantum computation.\nIf the Grover's search problem was solvable with logc N applications of U\u03c9, that would imply that NP is contained in BQP, by transforming problems in NP into Grover-type search problems. The optimality of Grover's algorithm suggests that quantum computers cannot solve NP-Complete problems in polynomial time, and thus NP is not contained in BQP.\nIt has been shown that a class of non-local hidden variable quantum computers could implement a search of an \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  -item database in at most \n  \n    \n      \n        O\n        (\n        \n          \n            N\n            \n              3\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt[{3}]{N}})}\n   steps. This is faster than the \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   steps taken by Grover's algorithm.\n\n\n== See also ==\nAmplitude amplification\nBrassard\u2013H\u00f8yer\u2013Tapp algorithm (for solving the collision problem)\nShor's algorithm (for factorization)\nQuantum walk search\n\n\n== Notes ==\n\n\n== References ==\nGrover L.K.: A fast quantum mechanical algorithm for database search, Proceedings, 28th Annual ACM Symposium on the Theory of Computing, (May 1996) p. 212\nGrover L.K.: From Schr\u00f6dinger's equation to quantum search algorithm, American Journal of Physics, 69(7): 769\u2013777, 2001. Pedagogical review of the algorithm and its history.\nGrover L.K.: QUANTUM COMPUTING: How the weird logic of the subatomic world could make it possible for machines to calculate millions of times faster than they do today The Sciences, July/August 1999, pp. 24\u201330.\nNielsen, M.A. and Chuang, I.L. Quantum computation and quantum information. Cambridge University Press, 2000. Chapter 6.\nWhat's a Quantum Phone Book?, Lov Grover, Lucent Technologies\n\n\n== External links ==\n\nDavy Wybiral. \"Quantum Circuit Simulator\". Archived from the original on 2017-01-16. Retrieved 2017-01-13.\nCraig Gidney (2013-03-05). \"Grover's Quantum Search Algorithm\".\nFran\u00e7ois Schwarzentruber (2013-05-18). \"Grover's algorithm\".\nAlexander Prokopenya. \"Quantum Circuit Implementing Grover's Search Algorithm\". Wolfram Alpha.\n\"Quantum computation, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nRoberto Maestre (2018-05-11). \"Grover's Algorithm implemented in R and C\". GitHub.\nBernhard \u00d6mer. \"QCL - A Programming Language for Quantum Computers\". Retrieved 2022-04-30. Implemented in /qcl-0.6.4/lib/grover.qcl"}, {"id": 29, "title": "History of women's cricket", "content": "The history of women's cricket can be traced back to a report in The Reading Mercury on 26 July 1745 and a match that took place between the villages of Bramley and Hambledon near Guildford in Surrey.The Mercury reported:\n\n\"The greatest cricket match that was played in this part of England was on Friday, the 26th of last month, on Gosden Common, near Guildford, between eleven maids of Bramley and eleven maids of Hambledon, all dressed in white. The Bramley maids had blue ribbons and the Hambledon maids red ribbons on their heads. The Bramley girls got 119 notches and the Hambledon girls 127. There was of bothe sexes the greatest number that ever was seen on such an occasion. The girls bowled, batted, ran and catches as well as most men could do in that game.\"\n\n\n== Early years in England ==\nEarly matches were not necessarily genteel affairs. A match, on 13 July 1747, held at the Artillery Ground between a team from Charlton and another from Westdean and Chilgrove in Sussex spilled over into the following day after it was interrupted by crowd trouble. Contemporary records show that women's matches were played on many occasions between villages in Sussex, Hampshire and Surrey. Other matches, often held in front of large crowds with heavy betting on the side, pitted single women against their married counterparts. Prizes ranged from barrels of ale to pairs of lace gloves. The first county match was held in 1811 between Surrey and Hampshire at Ball's Pond in Middlesex. Two noblemen underwrote the game with 1,000 guineas, and its participants ranged in age from 14 to 60.Originally, cricket deliveries were bowled underarm. Legend has it that the roundarm bowling action was pioneered in the early 19th century by Christiana Willes, sister of John Willes, to avoid becoming ensnared in her skirts. In fact, roundarm was devised by Tom Walker in the 1790s.\n\nThe first women's cricket club was formed in 1887 at Nun Appleton in Yorkshire and named the White Heather Club. In 1890, a team known as the Original English Lady Cricketers, toured England, playing in exhibition matches to large crowds. The team was highly successful until its manager absconded with the profits, forcing the ladies to disband. James Lillywhite's Cricketers' Annual for 1890 has a photograph of the team and short article on women's cricket. \"As an exercise, cricket is probably not so severe as lawn tennis, and it is certainly not so dangerous as hunting or skating; and if, therefore, the outcome of the present movement is to induce ladies more generally to play cricket, we shall consider that a good result has been attained.\"\n\nThe Women's Cricket Association was founded in 1926. The England team first played against The Rest at Leicester in 1933 and undertook the first international tour to Australia in 1934\u201335, playing the first Women's Test match between England and Australia in December 1934. After winning two tests and drawing one. England travelled on to New Zealand where Betty Snowball scored 189 in the first Test in Christchurch.\n\n\n== Early years in Australia ==\nThe founding mother of women's cricket in Australia was the young Tasmanian, Lily Poulett-Harris, who captained the Oyster Cove team in the league she created in 1894. Lily's obituary, from her death a few years later in 1897, states that her team was almost certainly the first to be formed in the colonies [1] [2]. Following this, the Victoria Women's Cricket Association was founded in 1905 and the Australian Women's Cricket Association in 1931. The current competition is run by the Women's National Cricket League.\n\n\n== The spread to other countries ==\nThe International Women's Cricket Council was formed in 1958 to coordinate women's cricket which was now being played regularly in Australia, England, New Zealand, South Africa, the West Indies, Denmark and the Netherlands. Test cricket has now been played by Australia, England, India, Ireland, Netherlands, New Zealand, Pakistan, South Africa, Sri Lanka and the West Indies. 131 women's Test matches have been played to date, the majority featuring England or Australia. Originally these were three-day matches, but since 1985 most have been played over four days. England have played 87 Test matches since their first in 1934, winning 19, losing 11 and drawing 57. Australia have played 67 in the same period, winning 18, losing nine and drawing 40.\nThe highest total is Australia's 569 for six declared against England Women in 1998, and the highest individual score is the 242 recorded by Kiran Baluch for Pakistan Women against West Indies Women at the National Stadium, Karachi in 2003/04. Five other women have scored double centuries. Neetu David of India took eight wickets in an innings against England in 1995/56 and seven wickets have fallen to the same bowler on ten occasions. The best match figures, 13 for 226 were recorded by Shaiza Khan for Pakistan Women against West Indies Women in Karachi in 2003/04. Three English batsmen, Janet Brittin with 1935 runs at 49.61, Charlotte Edwards, 1621 at 49.09 and Rachel Heyhoe-Flint with 1594 at 45.54, head the all-time run scoring lists while six other women have scored more than 1,000 Test runs. Mary Duggan of England took 77 Test wickets at 13.49 while Australia's Betty Wilson took 68 at 11.8. Seven other women have 50 or more victims to their name [3] Archived 16 October 2011 at the Wayback Machine.\nBetty Wilson was the first player, male or female, to record a century and ten wickets in a Test match, against England at the MCG in 1958. In a remarkable match Australia were bowled out for 38 but gained a first innings lead of three in dismissing England for 35 in reply, with Wilson taking seven for seven. 35 remains the lowest total ever recorded in a women's Test. Australia, thanks to Wilson's century, set England 206 to win but the visitors held on for a draw. In 1985, Australia's Under-21 National Women's Cricket Championship was renamed the Betty Wilson Shield in her honour. Another phenomenal club performance saw right-hander Jan Molyneaux make a record 298 for Olympic v Northcote in Melbourne's A grade final in 1967. Molyneaux also made 252 not out on a separate occasion in a 477 run partnership with Dawn Rae, again for Olympic.\n\n\n== Women's cricket in the modern era ==\nClub and county cricket in England has undergone constant evolution. There is currently a National Knock-Out Cup and a league structure culminating in a Northern and Southern Premier league. The major county competition is the LV Women's County Championship, while Super Fours, featuring teams named after precious stones, bridges the gap for the elite players between domestic and international competition.\nIn April 1970, MCC's traditional Easter coaching classes at Lord's were attended by Sian Davies and Sally Slowe of Cheltenham Ladies' College (see photo in Wisden at Lord's, page 129) breaking the 'gender barrier'. The first Women's Cricket World Cup was held in England in 1973, funded in part by businessman Jack Hayward, and won by the hosts at Lords in front of Princess Anne. Enid Bakewell and Lynne Thomas, making their international debuts for England, scored unbeaten hundreds against an International XI in Brighton in a stand of 246, a record which stood for a quarter of a century [4]. Lord's staged its first women's Test match in 1979, between England and Australia.\nOne-Day International cricket has been played by Australia, Denmark, England, India, Ireland, Japan, Netherlands, New Zealand. Pakistan, Scotland, South Africa, Sri Lanka and the West Indies while Jamaica, Trinidad and Tobago and International XIs have played in World Cups. 707 ODIs have been played up to the end of the 2009 World Cup. The 455 for 5 smashed by New Zealand Women against Pakistan Women at Hagley Oval, Christchurch in 1996/97 remains the highest team score while the Netherlands Women were bowled out for just 22 against West Indies Women at Sportpark Het Schootsveld in Deventer in 2008.\nThe Women's Cricket Association handed over the running of women's cricket in England to the England and Wales Cricket Board (ECB) in 1998. In 2005, after the eighth Women's World Cup, the International Women's Cricket Council was officially integrated under the umbrella of the International Cricket Council, and an ICC Women's Cricket Committee was formed to consider all matters relating to women's cricket. The 2009 World Cup, the first held under the auspices of the ICC was won by England, the first English team of either sex to win an ICC competition.\nWomen have beaten male teams to several milestones in one-day cricket. They were the first to play an international Twenty/20 match, England taking on New Zealand at Hove in 2004. The first tie in a one-day international was also between Women's teams, hosts New Zealand tying the first match of the World Cup in 1982 against England, who went on to record another tie against Australia in the same competition. Female wicket keepers were the first to record 6 dismissals in a one-day international, New Zealand's Sarah Illingworth and India's Venkatacher Kalpana both accounting for 6 batsman on the same day in the 1993 World Cup and Belinda Clark, the former Australian captain, is the only female player to have scored a double hundred in an ODI, recording an unbeaten 229 in the 1997 World Cup against Denmark. Pakistan's Sajjida Shah is the youngest player to appear in international cricket, playing against Ireland four months after her 12th birthday. She also holds the record for the best bowling figures in a one-day international, taking 7 wickets for just 4 runs against Japan Women at the Sportpark Drieburg in Amsterdam in 2003. Fast bowler Cathryn Fitzpatrick of Australia took 180 wickets in her one-day international career.\nIn 2009 England batsman Claire Taylor was named one of Wisden's five cricketers of the year [5], the first woman to be honoured with the award in its 120-year history.\nSince at least 2017 the England and Wales Cricket Board has promoted a short-form variant known as women's softball cricket, which is played in several county leagues in England. The traditional game is sometimes referred to as \"women's hardball cricket\" where a distinction needs to be made.\n\n\n== See also ==\nLily Poulett-Harris \u2013 founder of women's cricket in Australia\n\n\n== Notes ==\n\n\n== External sources ==\nCricinfo Women\nCricketwoman portal\nICC Women's Cricket\nA History of Women's cricket\n\n\n== Further reading =="}, {"id": 30, "title": "Hash function", "content": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable length output. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally and storage space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad but rare, and average-case behaviour can be nearly optimal (minimal collision).:\u200a527\u200aHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity.\n\n\n== Overview ==\nA hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length, like an integer, or variable length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions:\n\nConvert variable-length keys into fixed length (usually machine word length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR.\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace.\nMap the key values into ones less than or equal to the size of the tableA good hash function satisfies two basic properties: 1) it should be very fast to compute; 2) it should minimize duplication of output values (collisions).  Hash functions rely on generating favourable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. \nHash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot.\n\n\n== Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added to the table there.  If the hash code indexes a full slot, some kind of collision resolution is required: the new item may be omitted (not added to the table), or replace the old item, or it can be added to the table in some other location by a specified procedure.  That procedure depends on the structure of the hash table: In chained hashing, each slot is the head of a linked list or chain, and items that collide at the slot are added to the chain.  Chains may be kept in random order and searched linearly, or in serial order, or as a self-ordering list by frequency to speed up access.  In open address hashing, the table is probed starting from the occupied slot in a specified manner, usually by linear probing, quadratic probing, or double hashing until an open slot is located or the entire table is probed (overflow).  Searching for the item follows the same procedure until the item is located, an open slot is found or the entire table has been searched (item not in table).\n\n\n=== Specialized uses ===\nHash functions are also used to build caches for large data sets stored in slow media. A cache is generally simpler than a hashed search table since any collision can be resolved by discarding or writing back the older of the two colliding items.Hash functions are an essential ingredient of the Bloom filter, a space-efficient probabilistic data structure that is used to test whether an element is a member of a set.\nA special case of hashing  is known as  geometric hashing or the grid method. In these applications, the set of all inputs is some sort of metric space, and the hashing function can be interpreted as a partition of that space into a grid of cells. The table is often an array with two or more indices (called a grid file, grid index, bucket grid, and similar names), and the hash function returns an index tuple.  This principle is widely used in computer graphics, computational geometry and many other disciplines, to solve many proximity problems in the plane or in three-dimensional space, such as finding closest pairs in a set of points, similar shapes in a list of shapes, similar images in an image database, and so on.\nHash tables are also used to implement associative arrays and dynamic sets.\n\n\n== Properties ==\n\n\n=== Uniformity ===\nA good hash function should map the expected inputs as evenly as possible over its output range.  That is, every hash value in the output range should be generated with roughly the same probability. The reason for this last requirement is that the cost of hashing-based methods goes up sharply as the number of collisions\u2014pairs of inputs that are mapped to the same hash value\u2014increases.  If some hash values are more likely to occur than others, a larger fraction of the lookup operations will have to search through a larger set of colliding table entries.\nThis criterion only requires the value to be uniformly distributed, not random in any sense. A good randomizing function is (barring computational efficiency concerns) generally a good choice as a hash function, but the converse need not be true.\nHash tables often contain only a small subset of the valid inputs. For instance, a club membership list may contain only a hundred or so member names, out of the very large set of all possible names. In these cases, the uniformity criterion should hold for almost all typical subsets of entries that may be found in the table, not just for the global set of all possible entries.\nIn other words, if a typical set of m records is hashed to n table slots, the probability of a bucket receiving many more than m/n records should be vanishingly small. In particular, if m is less than n, very few buckets should have more than one or two records.  A small number of collisions is virtually inevitable, even if n is much larger than m \u2013 see the birthday problem.\nIn special cases when the keys are known in advance and the key set is static, a hash function can be found that achieves absolute (or collisionless) uniformity.  Such a hash function is said to be perfect.  There is no algorithmic way of constructing such a function - searching for one is a factorial function of the number of keys to be mapped versus the number of table slots they're tapped into.  Finding a perfect hash function over more than a very small set of keys is usually computationally infeasible; the resulting function is likely to be more computationally complex than a standard hash function and provides only a marginal advantage over a function with good statistical properties that yields a minimum number of collisions. See universal hash function.\n\n\n=== Testing and measurement ===\nWhen testing a hash function, the uniformity of the distribution of hash values can be evaluated by the chi-squared test.   This test is a goodness-of-fit measure: it's the actual distribution of items in buckets versus the expected (or uniform) distribution of items. The formula is:\n\n  \n    \n      \n        \n          \n            \n              \n                \u2211\n                \n                  j\n                  =\n                  0\n                \n                \n                  m\n                  \u2212\n                  1\n                \n              \n              (\n              \n                b\n                \n                  j\n                \n              \n              )\n              (\n              \n                b\n                \n                  j\n                \n              \n              +\n              1\n              )\n              \n                /\n              \n              2\n            \n            \n              (\n              n\n              \n                /\n              \n              2\n              m\n              )\n              (\n              n\n              +\n              2\n              m\n              \u2212\n              1\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\sum _{j=0}^{m-1}(b_{j})(b_{j}+1)/2}{(n/2m)(n+2m-1)}}}\n  \nwhere: \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the number of keys, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is the number of buckets, \n  \n    \n      \n        \n          b\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle b_{j}}\n   is the number of items in bucket \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \nA ratio within one confidence interval (0.95 - 1.05) is indicative that the hash function evaluated has an expected uniform distribution.\nHash functions can have some technical properties that make it more likely that they'll have a uniform distribution when applied.  One is the strict avalanche criterion: whenever a single input bit is complemented, each of the output bits changes with a 50% probability.  The reason for this property is that selected subsets of the keyspace may have low variability.  For the output to be uniformly distributed, a low amount of variability, even one bit, should translate into a high amount of variability (i.e. distribution over the tablespace) in the output.  Each bit should change with a probability of 50% because if some bits are reluctant to change, the keys become clustered around those values.  If the bits want to change too readily, the mapping is approaching a fixed XOR function of a single bit.  Standard tests for this property have been described in the literature.  The relevance of the criterion to a multiplicative hash function is assessed here.\n\n\n=== Efficiency ===\nIn data storage and retrieval applications, the use of a hash function is a trade-off between search time and data storage space.  If search time were unbounded, a very compact unordered linear list would be the best medium; if storage space were unbounded, a randomly accessible structure indexable by the key-value would be very large, very sparse, but very fast.  A hash function takes a finite amount of time to map a potentially large keyspace to a feasible amount of storage space searchable in a bounded amount of time regardless of the number of keys.  In most applications, the hash function should be computable with minimum latency and secondarily in a minimum number of instructions.\nComputational complexity varies with the number of instructions required and latency of individual instructions, with the simplest being the bitwise methods (folding), followed by the multiplicative methods, and the most complex (slowest) are the division-based methods.\nBecause collisions should be infrequent, and cause a marginal delay but are otherwise harmless, it's usually preferable to choose a faster hash function over one that needs more computation but saves a few collisions.\nDivision-based implementations can be of particular concern because the division is microprogrammed on nearly all chip architectures.  Divide (modulo) by a constant can be inverted to become a multiply by the word-size multiplicative-inverse of the constant.  This can be done by the programmer, or by the compiler.  Divide can also be reduced directly into a series of shift-subtracts and shift-adds, though minimizing the number of such operations required is a daunting problem; the number of assembly instructions resulting may be more than a dozen, and swamp the pipeline.  If the architecture has hardware multiply functional units, the multiply-by-inverse is likely a better approach.\nWe can allow the table size n to not be a power of 2 and still not have to perform any remainder or division operation, as these computations are sometimes costly. For example, let n be significantly less than 2b. Consider a pseudorandom number generator function P(key) that is uniform on the interval [0, 2b \u2212 1]. A hash function uniform on the interval [0, n-1] is n P(key)/2b. We can replace the division by a (possibly faster) right bit shift: nP(key) >> b.\nIf keys are being hashed repeatedly, and the hash function is costly, computing time can be saved by precomputing the hash codes and storing them with the keys.  Matching hash codes almost certainly means the keys are identical.  This technique is used for the transposition table in game-playing programs, which stores a 64-bit hashed representation of the board position.\n\n\n=== Universality ===\n\nA universal hashing scheme is a randomized algorithm that selects a hashing function h among a family of such functions, in such a way that the probability of a collision of any two distinct keys is 1/m, where m is the number of distinct hash values desired\u2014independently of the two keys. Universal hashing ensures (in a probabilistic sense) that the hash function application will behave as well as if it were using a random function, for any distribution of the input data. It will, however, have more collisions than perfect hashing and may require more operations than a special-purpose hash function.\n\n\n=== Applicability ===\nA hash function that allows only certain table sizes, strings only up to a certain length, or can't accept a seed (i.e. allow double hashing) isn't as useful as one that does.A hash function is applicable in a variety of situations. Particularly within cryptography, notable applications include:\nIntegrity check: Identical hash values for different files imply equality, providing a reliable means to detect file modifications.\nKey derivation: Minor input changes result in a random-looking output alteration, known as the diffusion property. Thus, hash functions are valuable for key derivation functions.\nMessage Authentication Codes (MACs): Through the integration of a confidential key with the input data, hash functions can generate MACs ensuring the genuineness of the data, such as in HMACs.\nPassword storage: The password's hash value doesn't expose any password details, emphasizing the importance of securely storing hashed passwords on the server.\nSignatures: Message hashes are signed rather than the whole message.\n\n\n=== Deterministic ===\nA hash procedure must be deterministic\u2014meaning that for a given input value it must always generate the same hash value. In other words, it must be a function of the data to be hashed, in the mathematical sense of the term. This requirement excludes hash functions that depend on external variable parameters, such as pseudo-random number generators or the time of day. It also excludes functions that depend on the memory address of the object being hashed in cases that the address may change during execution (as may happen on systems that use certain methods of garbage collection), although sometimes rehashing of the item is possible.\nThe determinism is in the context of the reuse of the function. For example, Python adds the feature that hash functions make use of a randomized seed that is generated once when the Python process starts in addition to the input to be hashed. The Python hash (SipHash) is still a valid hash function when used within a single run. But if the values are persisted (for example, written to disk) they can no longer be treated as valid hash values, since in the next run the random value might differ.\n\n\n=== Defined range ===\nIt is often desirable that the output of a hash function have fixed size (but see below). If, for example, the output is constrained to 32-bit integer values, the hash values can be used to index into an array. Such hashing is commonly used to accelerate data searches. \nProducing fixed-length output from variable length input can be accomplished by breaking the input data into chunks of specific size. Hash functions used for data searches use some arithmetic expression that iteratively processes chunks of the input (such as the characters in a string) to produce the hash value.\n\n\n=== Variable range ===\nIn many applications, the range of hash values may be different for each run of the program or may change along the same run (for instance, when a hash table needs to be expanded). In those situations, one needs a hash function which takes two parameters\u2014the input data z, and the number n of allowed hash values.\nA common solution is to compute a fixed hash function with a very large range (say, 0 to 232 \u2212 1), divide the result by n, and use the division's remainder. If n is itself a power of 2, this can be done by bit masking and bit shifting. When this approach is used, the hash function must be chosen so that the result has fairly uniform distribution between 0 and n \u2212 1, for any value of n that may occur in the application. Depending on the function, the remainder may be uniform only for certain values of n, e.g. odd or prime numbers.\n\n\n=== Variable range with minimal movement (dynamic hash function) ===\nWhen the hash function is used to store values in a hash table that outlives the run of the program, and the hash table needs to be expanded or shrunk, the hash table is referred to as a dynamic hash table.\nA hash function that will relocate the minimum number of records when the table is resized is desirable.\nWhat is needed is a hash function H(z,n) \u2013 where z is the key being hashed and n is the number of allowed hash values \u2013 such that H(z,n + 1) = H(z,n) with probability close to n/(n + 1).\nLinear hashing and spiral hashing are examples of dynamic hash functions that execute in constant time but relax the property of uniformity to achieve the minimal movement property. Extendible hashing uses a dynamic hash function that requires space proportional to n to compute the hash function, and it becomes a function of the previous keys that have been inserted. Several algorithms that preserve the uniformity property but require time proportional to n to compute the value of H(z,n) have been invented.A hash function with minimal movement is especially useful in distributed hash tables.\n\n\n=== Data normalization ===\nIn some applications, the input data may contain features that are irrelevant for comparison purposes. For example, when looking up a personal name, it may be desirable to ignore the distinction between upper and lower case letters. For such data, one must use a hash function that is compatible with the data equivalence criterion being used: that is, any two inputs that are considered equivalent must yield the same hash value. This can be accomplished by normalizing the input before hashing it, as by upper-casing all letters.\n\n\n== Hashing integer data types ==\nThere are several common algorithms for hashing integers.  The method giving the best distribution is data-dependent.  One of the simplest and most common methods in practice is the modulo division method.\n\n\n=== Identity hash function ===\nIf the data to be hashed is small enough, one can use the data itself (reinterpreted as an integer) as the hashed value. The cost of computing this identity hash function is effectively zero. This hash function is perfect, as it maps each input to a distinct hash value.\nThe meaning of \"small enough\" depends on the size of the type that is used as the hashed value. For example, in Java, the hash code is a 32-bit integer. Thus the 32-bit integer Integer and 32-bit floating-point Float objects can simply use the value directly; whereas the 64-bit integer Long and 64-bit floating-point Double cannot use this method.\nOther types of data can also use this hashing scheme. For example, when mapping character strings between upper and lower case, one can use the binary encoding of each character, interpreted as an integer, to index a table that gives the alternative form of that character (\"A\" for \"a\", \"8\" for \"8\", etc.).  If each character is stored in 8 bits (as in extended ASCII or ISO Latin 1), the table has only 28 = 256 entries; in the case of Unicode characters, the table would have 17\u00d7216 = 1114112 entries.\nThe same technique can be used to map two-letter country codes like \"us\" or \"za\" to country names (262 = 676 table entries), 5-digit zip codes like 13083 to city names (100000 entries), etc. Invalid data values (such as the country code \"xx\" or the zip code 00000) may be left undefined in the table or mapped to some appropriate \"null\" value.\n\n\n=== Trivial hash function ===\nIf the keys are uniformly or sufficiently uniformly distributed over the key space, so that the key values are essentially random, they may be considered to be already 'hashed'. In this case, any number of any bits in the key may be extracted and collated as an index into the hash table. For example, a simple hash function might mask off the least significant m bits and use the result as an index into a hash table of size 2m.\n\n\n=== Folding ===\nA folding hash code is produced by dividing the input into n sections of m bits, where 2m is the table size, and using a parity-preserving bitwise operation such as ADD or XOR to combine the sections, followed by a mask or shifts to trim off any excess bits at the high or low end. For example, for a table size of 15 bits and key value of 0x0123456789ABCDEF, there are five sections consisting of 0x4DEF, 0x1357, 0x159E, 0x091A and 0x8. Adding, we obtain 0x7AA4, a 15-bit value.\n\n\n=== Mid-squares ===\nA mid-squares hash code is produced by squaring the input and extracting an appropriate number of middle digits or bits.  For example, if the input is 123,456,789 and the hash table size 10,000, squaring the key produces 15,241,578,750,190,521, so the hash code is taken as the middle 4 digits of the 17-digit number (ignoring the high digit) 8750.  The mid-squares method produces a reasonable hash code if there is not a lot of leading or trailing zeros in the key.  This is a variant of multiplicative hashing, but not as good because an arbitrary key is not a good multiplier.\n\n\n=== Division hashing ===\nA standard technique is to use a modulo function on the key, by selecting a divisor \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   which is a prime number close to the table size, so \n  \n    \n      \n        h\n        (\n        K\n        )\n        =\n        K\n        \n          mod\n          \n            M\n          \n        \n      \n    \n    {\\displaystyle h(K)=K{\\bmod {M}}}\n  . The table size is usually a power of 2.  This gives a distribution from \n  \n    \n      \n        {\n        0\n        ,\n        M\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle \\{0,M-1\\}}\n  .  This gives good results over a large number of key sets.  A significant drawback of division hashing is that division is microprogrammed on most modern architectures including x86 and can be 10 times slower than multiply. A second drawback is that it won't break up clustered keys. For example, the keys 123000, 456000, 789000, etc. modulo 1000 all map to the same address.  This technique works well in practice because many key sets are sufficiently random already, and the probability that a key set will be cyclical by a large prime number is small.\n\n\n=== Algebraic coding ===\nAlgebraic coding is a variant of the division method of hashing which uses division by a polynomial modulo 2 instead of an integer to map n bits to m bits.:\u200a512\u2013513\u200a In this approach, \n  \n    \n      \n        M\n        =\n        \n          2\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle M=2^{m}}\n   and we postulate an \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  th degree polynomial \n  \n    \n      \n        \n          Z\n        \n        (\n        x\n        )\n        =\n        \n          x\n          \n            m\n          \n        \n        +\n        \n          \u03b6\n          \n            m\n            \u2212\n            1\n          \n        \n        \n          x\n          \n            m\n            \u2212\n            1\n          \n        \n        +\n        .\n        .\n        .\n        +\n        \n          \u03b6\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {Z} (x)=x^{m}+\\zeta _{m-1}x^{m-1}+...+\\zeta _{0}}\n  . A key \n  \n    \n      \n        K\n        =\n        (\n        \n          k\n          \n            n\n            \u2212\n            1\n          \n        \n        .\n        .\n        .\n        \n          k\n          \n            1\n          \n        \n        \n          k\n          \n            0\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle K=(k_{n-1}...k_{1}k_{0})_{2}}\n   can be regarded as the polynomial \n  \n    \n      \n        K\n        (\n        x\n        )\n        =\n        \n          k\n          \n            n\n            \u2212\n            1\n          \n        \n        \n          x\n          \n            n\n            \u2212\n            1\n          \n        \n        +\n        .\n        .\n        .\n        +\n        \n          k\n          \n            1\n          \n        \n        x\n        +\n        \n          k\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle K(x)=k_{n-1}x^{n-1}+...+k_{1}x+k_{0}}\n  . The remainder using polynomial arithmetic modulo 2 is \n  \n    \n      \n        K\n        (\n        x\n        )\n        \n          mod\n          \n            Z\n          \n        \n        (\n        x\n        )\n        =\n        \n          h\n          \n            m\n            \u2212\n            1\n          \n        \n        \n          x\n          \n            m\n            \u2212\n            1\n          \n        \n        +\n        .\n        .\n        .\n        +\n        \n          h\n          \n            1\n          \n        \n        x\n        +\n        \n          h\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle K(x){\\bmod {Z}}(x)=h_{m-1}x^{m-1}+...+h_{1}x+h_{0}}\n  . Then \n  \n    \n      \n        h\n        (\n        K\n        )\n        =\n        (\n        \n          h\n          \n            m\n            \u2212\n            1\n          \n        \n        .\n        .\n        .\n        \n          h\n          \n            1\n          \n        \n        \n          h\n          \n            0\n          \n        \n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle h(K)=(h_{m-1}...h_{1}h_{0})_{2}}\n  . If \n  \n    \n      \n        Z\n        (\n        x\n        )\n      \n    \n    {\\displaystyle Z(x)}\n   is constructed to have \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   or fewer non-zero coefficients, then keys which share less than \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   bits are guaranteed to not collide.\n\n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   a function of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   and \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , a divisor of \n  \n    \n      \n        \n          2\n          \n            k\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle 2^{k}-1}\n  , is constructed from the \n  \n    \n      \n        G\n        F\n        (\n        \n          2\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle GF(2^{k})}\n   field.  Knuth gives an example: for n=15, m=10 and t=7, \n  \n    \n      \n        Z\n        (\n        x\n        )\n        =\n        \n          x\n          \n            10\n          \n        \n        +\n        \n          x\n          \n            8\n          \n        \n        +\n        \n          x\n          \n            5\n          \n        \n        +\n        \n          x\n          \n            4\n          \n        \n        +\n        \n          x\n          \n            2\n          \n        \n        +\n        x\n        +\n        1\n      \n    \n    {\\displaystyle Z(x)=x^{10}+x^{8}+x^{5}+x^{4}+x^{2}+x+1}\n  . The derivation is as follows: \nLet \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   be the smallest set of integers such that \n  \n    \n      \n        {\n        1\n        ,\n        2\n        ,\n        .\n        .\n        .\n        ,\n        t\n        }\n        \u2286\n        S\n      \n    \n    {\\displaystyle \\{1,2,...,t\\}\\subseteq S}\n   and \n  \n    \n      \n        (\n        2\n        j\n        \n          mod\n          \n            n\n          \n        \n        )\n        \u2208\n        S\n        \n        \u2200\n        j\n        \u2208\n        S\n      \n    \n    {\\displaystyle (2j{\\bmod {n}})\\in S\\quad \\forall j\\in S}\n  .Define \n  \n    \n      \n        P\n        (\n        x\n        )\n        =\n        \n          \u220f\n          \n            j\n            \u2208\n            S\n          \n        \n        (\n        x\n        \u2212\n        \n          \u03b1\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle P(x)=\\prod _{j\\in S}(x-\\alpha ^{j})}\n   where \n  \n    \n      \n        \u03b1\n        \n          \u2208\n          \n            n\n          \n        \n        G\n        F\n        (\n        \n          2\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle \\alpha \\in ^{n}GF(2^{k})}\n   and where the coefficients of \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n   are computed in this field.  Then the degree of \n  \n    \n      \n        P\n        (\n        x\n        )\n        =\n        \n          |\n        \n        S\n        \n          |\n        \n      \n    \n    {\\displaystyle P(x)=|S|}\n  . Since \n  \n    \n      \n        \n          \u03b1\n          \n            2\n            j\n          \n        \n      \n    \n    {\\displaystyle \\alpha ^{2j}}\n   is a root of \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n   whenever \n  \n    \n      \n        \n          \u03b1\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\alpha ^{j}}\n   is a root, it follows that the coefficients \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p^{i}}\n   of \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n   satisfy \n  \n    \n      \n        \n          p\n          \n            i\n          \n          \n            2\n          \n        \n        =\n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}^{2}=p_{i}}\n   so they are all 0 or 1. If \n  \n    \n      \n        R\n        (\n        x\n        )\n        =\n        \n          r\n          \n            (\n            n\n            \u2212\n            1\n            )\n          \n        \n        \n          x\n          \n            n\n            \u2212\n            1\n          \n        \n        +\n        .\n        .\n        .\n        +\n        \n          r\n          \n            1\n          \n        \n        x\n        +\n        \n          r\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle R(x)=r_{(n-1)}x^{n-1}+...+r_{1}x+r_{0}}\n   is any nonzero polynomial modulo 2 with at most \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   nonzero coefficients, then \n  \n    \n      \n        R\n        (\n        x\n        )\n      \n    \n    {\\displaystyle R(x)}\n   is not a multiple of \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n   modulo 2.  If follows that the corresponding hash function will map keys with fewer than \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   bits in common to unique indices.:\u200a542\u2013543\u200aThe usual outcome is that either \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   will get large, or \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   will get large, or both, for the scheme to be computationally feasible.  Therefore, it's more suited to hardware or microcode implementation.:\u200a542\u2013543\u200a\n\n\n=== Unique permutation hashing ===\nSee also unique permutation hashing, which has a guaranteed best worst-case insertion time.\n\n\n=== Multiplicative hashing ===\nStandard multiplicative hashing uses the formula \n  \n    \n      \n        \n          h\n          \n            a\n          \n        \n        (\n        K\n        )\n        =\n        \u230a\n        (\n        a\n        K\n        \n          mod\n          \n            W\n          \n        \n        )\n        \n          /\n        \n        (\n        W\n        \n          /\n        \n        M\n        )\n        \u230b\n      \n    \n    {\\displaystyle h_{a}(K)=\\lfloor (aK{\\bmod {W}})/(W/M)\\rfloor }\n   which produces a hash value in \n  \n    \n      \n        {\n        0\n        ,\n        \u2026\n        ,\n        M\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle \\{0,\\ldots ,M-1\\}}\n  .  The value \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   is an appropriately chosen value that should be relatively prime to \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  ; it should be large and its binary representation a random mix of 1's and 0's.   An important practical special case occurs when \n  \n    \n      \n        W\n        =\n        \n          2\n          \n            w\n          \n        \n      \n    \n    {\\displaystyle W=2^{w}}\n   and \n  \n    \n      \n        M\n        =\n        \n          2\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle M=2^{m}}\n   are powers of 2 and \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   is the machine word size. In this case this formula becomes \n  \n    \n      \n        \n          h\n          \n            a\n          \n        \n        (\n        K\n        )\n        =\n        \u230a\n        (\n        a\n        K\n        \n          \n            mod\n            \n              2\n            \n          \n          \n            w\n          \n        \n        )\n        \n          /\n        \n        \n          2\n          \n            w\n            \u2212\n            m\n          \n        \n        \u230b\n      \n    \n    {\\displaystyle h_{a}(K)=\\lfloor (aK{\\bmod {2}}^{w})/2^{w-m}\\rfloor }\n  .  This is special because arithmetic modulo \n  \n    \n      \n        \n          2\n          \n            w\n          \n        \n      \n    \n    {\\displaystyle 2^{w}}\n   is done by default in low-level programming languages and integer division by a power of 2 is simply a right-shift, so, in C, for example, this function becomes\n\nunsigned hash(unsigned K)\n{ \n   return (a*K) >> (w-m);\n}\n\nand for fixed \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   and \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   this translates into a single integer multiplication and right-shift making it one of the fastest hash functions to compute.\nMultiplicative hashing is susceptible to a \"common mistake\" that leads to poor diffusion\u2014higher-value input bits do not affect lower-value output bits.  A transmutation on the input which shifts the span of retained top bits down and XORs or ADDs them to the key before the multiplication step corrects for this. So the resulting function looks like:\nunsigned hash(unsigned K)\n{\n   K ^= K >> (w-m); \n   return (a*K) >> (w-m);\n}\n\n\n=== Fibonacci hashing ===\nFibonacci hashing is a form of multiplicative hashing in which the multiplier is \n  \n    \n      \n        \n          2\n          \n            w\n          \n        \n        \n          /\n        \n        \u03d5\n      \n    \n    {\\displaystyle 2^{w}/\\phi }\n  , where \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   is the machine word length and \n\n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   (phi) is the golden ratio (approximately 5/3). A property of this multiplier is that it uniformly distributes over the table space, blocks of consecutive keys with respect to any block of bits in the key.  Consecutive keys within the high bits or low bits of the key (or some other field) are relatively common.  The multipliers for various word lengths \n  \n    \n      \n        \n          \n          \n            w\n          \n        \n      \n    \n    {\\displaystyle ^{w}}\n   are:\n\n16:  a=4050310\n32:  a=265443576910\n48:  a=17396110258977110\n64:  a=1140071481932319848510\n\n\n=== Zobrist hashing ===\n\nTabulation hashing, more generally known as Zobrist hashing after Albert Zobrist, an American computer scientist, is a method for constructing universal families of hash functions by combining table lookup with XOR operations. This algorithm has proven to be very fast and of high quality for hashing purposes (especially hashing of integer-number keys).Zobrist hashing was originally introduced as a means of compactly representing chess positions in computer game-playing programs.  A unique random number was assigned to represent each type of piece (six each for black and white) on each space of the board.  Thus a table of 64\u00d712 such numbers is initialized at the start of the program.  The random numbers could be any length, but 64 bits was natural due to the 64 squares on the board.  A position was transcribed by cycling through the pieces in a position, indexing the corresponding random numbers (vacant spaces were not included in the calculation), and XORing them together (the starting value could be 0, the identity value for XOR, or a random seed).  The resulting value was reduced by modulo, folding or some other operation to produce a hash table index.  The original Zobrist hash was stored in the table as the representation of the position.\nLater, the method was extended to hashing integers by representing each byte in each of 4 possible positions in the word by a unique 32-bit random number.  Thus, a table of 28\u00d74 of such random numbers is constructed. A 32-bit hashed integer is transcribed by successively indexing the table with the value of each byte of the plain text integer and XORing the loaded values together (again, the starting value can be the identity value or a random seed). The natural extension to 64-bit integers is by use of a table of 28\u00d78 64-bit random numbers.\nThis kind of function has some nice theoretical properties, one of which is called 3-tuple independence meaning every 3-tuple of keys is equally likely to be mapped to any 3-tuple of hash values.\n\n\n=== Customised hash function ===\nA hash function can be designed to exploit existing entropy in the keys.  If the keys have leading or trailing zeros, or particular fields that are unused, always zero or some other constant, or generally vary little, then masking out only the volatile bits and hashing on those will provide a better and possibly faster hash function.  Selected divisors or multipliers in the division and multiplicative schemes may make more uniform hash functions if the keys are cyclic or have other redundancies.\n\n\n== Hashing variable-length data ==\nWhen the data values are long (or variable-length) character strings\u2014such as personal names, web page addresses, or mail messages\u2014their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string\u2014and depends on each character in a different way.\n\n\n=== Middle and ends ===\nSimplistic hash functions may add the first and last n characters of a string along with the length, or form a word-size hash from the middle 4 characters of a string.  This saves iterating over the (potentially long) string,\nbut hash functions that do not hash on all characters of a string can readily become linear due to redundancies, clustering or other pathologies in the key set.  Such strategies may be effective as a custom hash function if the structure of the keys is such that either the middle, ends or other fields are zero or some other invariant constant that doesn't differentiate the keys; then the invariant parts of the keys can be ignored.\n\n\n=== Character folding ===\nThe paradigmatic example of folding by characters is to add up the integer values of all the characters in the string.   A better idea is to multiply the hash total by a constant, typically a sizable prime number, before adding in the next character, ignoring overflow.  Using exclusive 'or' instead of add is also a plausible alternative. The final operation would be a modulo, mask, or other function to reduce the word value to an index the size of the table. The weakness of this procedure is that information may cluster in the upper or lower bits of the bytes, which clustering will remain in the hashed result and cause more collisions than a proper randomizing hash. ASCII byte codes, for example, have an upper bit of 0 and printable strings don't use the first 32 byte codes, so the information (95-byte codes) is clustered in the remaining bits in an unobvious manner.\nThe classic approach dubbed the PJW hash based on the work of Peter. J. Weinberger at ATT Bell Labs in the 1970s, was originally designed for hashing identifiers into compiler symbol tables as given in the \"Dragon Book\". This hash function offsets the bytes 4 bits before ADDing them together.  When the quantity wraps, the high 4 bits are shifted out and if non-zero, XORed back into the low byte of the cumulative quantity.  The result is a word size hash code to which a modulo or other reducing operation can be applied to produce the final hash index.\nToday, especially with the advent of 64-bit word sizes, much more efficient variable-length string hashing by word chunks is available.\n\n\n=== Word length folding ===\n\nModern microprocessors will allow for much faster processing if 8-bit character strings are not hashed by processing one character at a time, but by interpreting the string as an array of 32 bit or 64-bit integers and hashing/accumulating these \"wide word\" integer values by means of arithmetic operations (e.g. multiplication by constant and bit-shifting). The final word, which may have unoccupied byte positions, is filled with zeros or a specified \"randomizing\" value before being folded into the hash.  The accumulated hash code is reduced by a final modulo or other operation to yield an index into the table.\n\n\n=== Radix conversion hashing ===\nAnalogous to the way an ASCII or EBCDIC character string representing a decimal number is converted to a numeric quantity for computing, a variable length string can be converted as (x0ak\u22121+x1ak\u22122+...+xk\u22122a+xk\u22121). This is simply a polynomial in a radix a > 1 that takes the components (x0,x1,...,xk\u22121) as the characters of the input string of length k. It can be used directly as the hash code, or a hash function applied to it to map the potentially large value to the hash table size. The value of a is usually a prime number at least large enough to hold the number of different characters in the character set of potential keys. Radix conversion hashing of strings minimizes the number of collisions. Available data sizes may restrict the maximum length of string that can be hashed with this method.  For example, a 128-bit double long word will hash only a 26 character alphabetic string (ignoring case) with a radix of 29; a printable ASCII string is limited to 9 characters using radix 97 and a 64-bit long word.  However, alphabetic keys are usually of modest length, because keys must be stored in the hash table. Numeric character strings are usually not a problem; 64 bits can count up to 1019, or 19 decimal digits with radix 10.\n\n\n=== Rolling hash ===\n\nIn some applications, such as substring search, one can compute a hash function h for every k-character substring of a given n-character string by advancing a window of width k characters along the string; where  k is a fixed integer, and n is greater than k. The straightforward solution, which is to extract such a substring at every character position in the text and compute h separately, requires a number of operations proportional to k\u00b7n. However, with the proper choice of h, one can use the technique of rolling hash to compute all those hashes with an effort proportional to mk + n where m is the number of occurrences of the substring.The most familiar algorithm of this type is Rabin-Karp with best and average case performance O(n+mk) and worst case O(n\u00b7k) (in all fairness, the worst case here is gravely pathological: both the text string and substring are composed of a repeated single character, such as t=\"AAAAAAAAAAA\", and s=\"AAA\").  The hash function used for the algorithm is usually the Rabin fingerprint, designed to avoid collisions in 8-bit character strings, but other suitable hash functions are also used.\n\n\n== Analysis ==\nWorst case result for a hash function can be assessed two ways: theoretical and practical. Theoretical worst case is the probability that all keys map to a single slot.  Practical worst case is expected longest probe sequence (hash function + collision resolution method).  This analysis considers uniform hashing, that is, any key will map to any particular slot with probability 1/m, characteristic of universal hash functions.\nWhile Knuth worries about adversarial attack on real time systems, Gonnet has shown that the probability of such a case is \"ridiculously small\". His representation was that the probability of k of n keys mapping to a single slot is \n  \n    \n      \n        \n          \n            \n              \n                e\n                \n                  \u2212\n                  \u03b1\n                \n              \n              \n                \u03b1\n                \n                  k\n                \n              \n            \n            \n              k\n              !\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {e^{-\\alpha }\\alpha ^{k}}{k!}}}\n   where \u03b1 is the load factor, n/m.\n\n\n== History ==\nThe term hash offers a natural analogy with its non-technical meaning (to chop up or make a mess out of something), given how hash functions scramble their input data to derive their output.:\u200a514\u200a In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, in Herbert Hellerman's Digital Computer System Principles, even though it was already widespread jargon by then.:\u200a547\u2013548\u200a\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nCalculate hash of a given value by Timo Denk\nThe Goulburn Hashing Function  (PDF) by Mayur Patel\nHash Function Construction for Textual and Geometrical Data Retrieval  (PDF) Latest Trends on Computers, Vol.2, pp. 483\u2013489, CSCC Conference, Corfu, 2010"}, {"id": 31, "title": "Cars (franchise)", "content": "Cars is an animated film series and Disney media franchise set in a world populated by anthropomorphic vehicles created by John Lasseter, Joe Ranft and Jorgen Klubien. The franchise began with the 2006 film, Cars, produced by Pixar and released by Walt Disney Pictures. The film was followed by Cars 2 in 2011. A third film, Cars 3, was released in 2017. The now-defunct Disneytoon Studios produced the two spin-off films Planes (2013) and Planes: Fire & Rescue (2014).\nThe first two Cars films were directed by Lasseter, then-chief creative officer of Pixar, Walt Disney Animation Studios, and Disneytoon Studios, while Cars 3 was directed by Brian Fee, a storyboard artist on the previous installments. Lasseter served as executive producer of Cars 3 and the Planes films. Together, all three Cars films have accrued over $1.4 billion in box office revenue worldwide while the franchise has amassed over $10 billion in merchandising sales within its first five years.\n\n\n== History ==\nThe Cars franchise began with the original 2006 film. At the time, it was Pixar's least well received film by reviewers.The short Mater and the Ghostlight was released as an extra on the Cars DVD on November 7, 2006. A series of shorts called Cars Toons were produced and aired on the Disney Channel to keep interest up. The brand had sold nearly $10 billion in merchandise by the time of the release of Cars 2 in 2011.In 2007, Cars Four Wheels Rally ride opened in Disneyland Paris.In the summer of 2012, the 12-acre Cars Land theme area opened at Disney California Adventure in Anaheim as the main component of $1-billion park renovation.A third film, Cars 3, was announced on October 8, 2015, and was released on June 16, 2017.\n\n\n== Film series ==\n\n\n=== Cars (2006) ===\n\nCars is the seventh Pixar film. The story is about rookie race car Lightning McQueen (Owen Wilson), who gets lost on the way to California for a tiebreaker race in the Piston Cup, a famous race worldwide, and ends up in a little town called Radiator Springs on Route 66, that had been bypassed and forgotten when Interstate 40 was built. He accidentally wrecks the road and is sentenced to fix it. During his time there, he meets Mater (Larry the Cable Guy), who became his best friend, and Sally (Bonnie Hunt). He also comes across Doc Hudson(Paul Newman), who used to be a famous racecar called the Hudson Hornet until a career ending crash in 1954.\nAfter Lightning fixes the road, Doc Hudson no longer wants him in town, so he secretly alerts the media about Lightning's presence, but it doesn't take long for Hudson to realize how much Lightning has helped Radiator Springs, so he goes back to being the Hudson Hornet and becomes McQueen's crew chief, while most of the Radiator Springs folks become his pit crew. Lightning is about to win the race, but helps The King (Richard Petty) cross the finish line after Chick Hicks (Michael Keaton) causes him to crash. Chick wins the Piston Cup after being in third on the last lap, but is later booed by everyone, while Lightning receives praise for his sportsmanship.\nDespite his loss, Lightning is offered to be the new face and sponsor of Dinoco, but he declines and decides to stay with Rust-eze, his current sponsor. He does, however, arrange for Mater to ride in the Dinoco helicopter just as he promised. The film ends with Lightning setting up his racing headquarters in Radiator Springs, thereby putting it back on the map.\n\n\n=== Cars 2 (2011) ===\n\nCars 2 is the twelfth Pixar film. The story starts with Lightning McQueen competing in the World Grand Prix, organized by Sir Miles Axlerod (Eddie Izzard), a three-race event taking place in Japan, Italy, and England, with his racing rival being Italian formula car Francesco Bernoulli (John Turturro).\nAlong the way, Mater is mistaken for an American spy by British spy Finn McMissile (Michael Caine), and falls in love with junior agent Holley Shiftwell (Emily Mortimer). The three uncover a plot to sabotage the World Grand Prix, which is seemingly led by Professor Z\u00fcndapp (Thomas Kretschmann) and a group of lemon cars. When the event reaches its conclusion in England, Mater deduces that Axlerod is the mastermind behind the plot, as he started the event in the first place and had intended for cars everywhere to run on oil as revenge for the lemons' reputation as \"history's biggest loser cars\", implying that Axlerod is also a lemon.\nWith the plot foiled and the villains defeated, Mater is knighted by the Queen of the United Kingdom (Vanessa Redgrave), and a new race is held in Radiator Springs. Mater is invited by McMissile and Shiftwell to go on another mission, but chooses to stay in Radiator Springs. While his weapons get confiscated, he gets to keep the rocket engines he acquired, as the two agents take off in Siddeley (Jason Isaacs), the British spy jet.\n\n\n=== Cars 3 (2017) ===\n\nCars 3 is the eighteenth Pixar film. The story focuses on Lightning McQueen, who deals with a new generation of race cars taking over the world of racing. Jackson Storm (Armie Hammer) is an arrogant high tech racer who leads the next generation. As everyone begins to ask him if he might retire, Lightning struggles with keeping up with these racers and during the final race of the season, he suffers a violent crash.\nFour months later, a recovering Lightning mourns the late Doc Hudson and travels to the new Rust-eze Racing Center, now under the management of Sterling (Nathan Fillion). Sterling assigns Cruz Ramirez (Cristela Alonzo) to train him on the simulator, which Lightning accidentally destroys after losing control. Cruz's unconventional training methods and lack of racing experience frustrate Lightning, as they race on beaches and a demolition derby. Cruz reveals that she had always wanted to be a racer but never found the confidence to do so. In Thomasville, they encounter Hudson's old crew chief Smokey (Chris Cooper), who trains McQueen and explains to him that Hudson found happiness in mentoring him. Smokey's training methods inspire Ramirez as well.\nAt the Florida 500, Lightning begins racing, but remembers Cruz's racing dreams and has her take his place in the race. Using what she's learned on the road, Cruz found the confidence to catch up to Storm. She wins the race along with McQueen and begins racing for Dinoco, whose owner, Tex, purchases Rust-eze. Under the merged Dinoco\u2013Rust-eze brand, Cruz becomes a racer, sporting #51, and McQueen decides to continue racing, with a new paint job in memory of Hudson, but trains Cruz first for the season.\n\n\n=== Possible fourth film ===\nRegarding a possible Cars 4, Cars 3 producers Kevin Reher and Andrea Warren told Cinema Blend that \"If there's a good story to tell I mean our heads kinda break after having gotten this one done, like oh my God, what could you do the further adventures of? But like any sequel, from Toy Story 4 to Incredibles 2, as long as there's a good story to tell, it's worth investing; we do love these characters, we love them as much as the public does.\" Regarding which character would be the main protagonist in the movie, Reher and Warren stated that \"if Cruz is a breakout character, kind of like Mater was\", \"she [would] be involved in a 4\". Owen Wilson stated at a Cars 3 press event that possible stories have been discussed for a Cars 4, though he would personally like for a fourth Cars movie to delve into aspects of the thriller genre, akin to Cars 2. In an interview with Screen Rant, Lea Delaria expressed interest in reprising her role as Miss Fritter while promoting the release of the short movie Miss Fritter's Racing Skoool with the Cars 3 DVD and Blu-ray release.\n\n\n== Television series ==\n\n\n=== Cars Toons ===\n\n\n==== Cars Toons: Mater's Tall Tales (2008\u201312) ====\nMater's Tall Tales is a series of short animated films featuring Mater, Lightning McQueen, and their friends. The first three shorts premiered on October 27, 2008 on Toon Disney, Disney Channel, and ABC Family. Not exclusive to television, the episodes have also been released on DVDs/Blu-rays or as in front of theatrical films. A total of 11 episodes have been released with Time Travel Mater (2012) being the most recent.All shorts in the series follow the same tall-tale formula: Mater tells a story of something he has done in the past. When Lightning questions Mater over whether the events in the story actually occurred (or in some episodes asks him what he did next), Mater always claimed that Lightning was also involved and continues the story including Lightning's sudden participation. The shorts end with Mater leaving the scene, often followed by characters or references to the story that was being told, suggesting that story might be real.\n\n\n==== Cars Toons: Tales from Radiator Springs (2013\u201314) ====\nTales from Radiator Springs is a series of short animated films featuring the various residents of the titular location. The first three two-minute episodes - Hiccups, Bugged, and Spinning - premiered on March 22, 2013, on Disney Channel, and have been available online since March 24, 2013. A fourth short in the series, titled The Radiator Springs 500 \u00bd, was released in spring 2014 on the digital movie service Disney Movies Anywhere. It premiered August 1, 2014, on Disney Channel. The short has a running time of 6 minutes rather than the usual two-minute running time.\n\n\n=== Cars on the Road (2022) ===\n\nOn December 10, 2020, during Disney's Investors Day event, Pixar announced that an animated series starring Lightning McQueen and Mater traveling the country while meeting friends, new and old, was in development. On November 12, 2021, it was announced that the show would be titled Cars on the Road, and that Owen Wilson and Larry the Cable Guy would reprise their respective roles as Lightning McQueen and Mater. It was released on Disney+ on September 8, 2022 as part of Disney+ Day.\n\n\n== Short films ==\n\n\n=== Mater and the Ghostlight (2006) ===\n\nMater and the Ghostlight is a 2006 Pixar computer-animated short created for the DVD of Cars, which was released on October 25, 2006, in Australia and the United States on November 7, 2006. The short, set in the Cars world, tells a story of Mater being haunted by a mysterious blue light.\n\n\n=== Miss Fritter's Racing Skoool (2017) ===\nMiss Fritter's Racing Skoool is a 2017 Pixar computer-animated short created for the Blu-ray, 4K Ultra HD and DVD of Cars 3, which was released in the United States on November 7, 2017. The short, set in the Cars world, follows the \"blindsided testimonials from the Crazy 8's, touting the transformative impact Miss Fritter's Racing School has had in reshaping the direction of their lives\".\n\n\n=== Pixar Popcorn shorts ===\nIn December 2020, two Cars short films were announced as part of Disney+'s Pixar Popcorn series, which was released on January 22, 2021. As with most of the shorts present in this series, none of the characters have voice roles.\n\n\n==== Unparalleled Parking (2021) ====\nIn this short, the cars have a friendly parallel parking competition.\n\n\n==== Dancing with the Cars (2021) ====\nIn this short, the residents of Radiator Springs show off their dancing skills at Flo's V8 Caf\u00e9.\n\n\n== Spin-off film series ==\nIn 2013 Disneytoon Studios, Pixar's now-defunct sister company, released a spin-off film set in the Cars world titled Planes featuring planes as the main characters. The film was followed by a sequel titled Planes: Fire & Rescue in 2014.\n\n\n=== Planes (2013) ===\n\nPlanes is a computer-animated Cars spin-off film produced by Disneytoon Studios. The first film in a planned duology where the main characters are planes, the film was released in theaters by Walt Disney Pictures on August 9, 2013. The film was directed by Klay Hall and executive produced by John Lasseter. In the film, Dusty Crophopper, a small-town cropdusting plane, follows his dreams by competing in a world air race despite his fear of heights.\n\n\n=== Planes: Fire & Rescue (2014) ===\n\nA sequel, titled Planes: Fire & Rescue, was theatrically released on July 18, 2014. This film is also produced by Disneytoon Studios. Bobs Gannaway, co-developer of Jake and the Never Land Pirates and Mickey Mouse Clubhouse, and co-director of Mickey Mouse Works and Secret of the Wings, directed the film. Lasseter again served as executive producer. In the film, Dusty is now a world-famous air racer, but learns he will never be able to race again due to an engine problem. After accidentally starting a fire, Dusty decides to become a firefighter and trains at Piston Peak Air Attack.\n\n\n== Spin-off short film ==\n\n\n=== Vitaminamulch: Air Spectacular (2014) ===\nA Planes short film titled Vitaminamulch: Air Spectacular was released on the DVD and Blu-ray of Planes: Fire & Rescue. It was directed by Dan Abraham and executive produced by John Lasseter. In the short film, Dusty Crophopper and Chug need to replace two daredevils in an airshow Leadbottom is hosting. At first unsuccessful, Dusty and Chug accidentally start a series of events that captures the audience's attention, eventually completing the stunt.\n\n\n== Cancelled projects ==\n\n\n=== To Protect and Serve ===\nA fifth Tales from Radiator Springs short film, titled To Protect and Serve, was announced to be in development for a 2015 release. However, it was never released. \nThe short would have been focused on Sheriff as he takes a mandatory vacation and leaves two rookies (voiced by Wendi McLendon-Covey and Aziz Ansari) to take over his duties as police officer. This, however, causes problems in Radiator Springs as the rookies over-do every aspect of their job in an attempt to score commendations, turning the town into a hotbed of taped-off crime scenes. Sheriff soon senses an imbalance and returns home to right the \"justice\" that has been served.\n\n\n=== Untitled Planes sequel and spin-offs ===\nAt the D23 Expo held in July 2017, John Lasseter announced a third film in the Planes series. Tentatively titled Beyond the Sky, the film would have explored the future of aviation in outer space. The film had a release date of April 12, 2019. It was removed from the release schedule in March 2018 and on June 28, shortly after the announcement of Lasseter's departure from Disney, Disneytoon Studios was shut down, ending development on the film.Prior to its closure, the studio was planning several more spin-offs of the franchise focusing on boats, trains and other vehicles. In November 2022, concept art for one of the proposed films, tentatively titled Metro, was leaked online.\n\n\n== Reception ==\n\n\n=== Box-office performance ===\nEarning over $1.7 billion, Cars, including its Planes spin-off films, is the eleventh-highest grossing animated franchise.\nIn its opening weekend, the original Cars earned $60,119,509 in 3,985 theaters in the United States, ranking number one at the box office. In the United States, the film held onto the number one spot for two weeks before being surpassed by Click, and then by Superman Returns the following weekend. It went on to gross $461,983,149 worldwide (ranking number six in 2006 films) and $244,082,982 in the United States (the third-highest-grossing film of 2006 in the country, behind Pirates of the Caribbean: Dead Man's Chest and Night at the Museum). It was the highest grossing animated film of 2006 in the United States, but lost to Ice Age: The Meltdown in worldwide totals.Cars 2 has earned $191,452,396 in the United States and Canada, and $368,400,000 in other territories, for a worldwide total of $559,852,396. Worldwide on its opening weekend, it made $109.0 million, marking the largest opening weekend for a 2011 animated title.\n\n\n=== Critical and public response ===\n\n\n=== Awards and nominations ===\n\nThe first Cars film had a highly successful run during the 2006 awards season. Many film critic associations such as the Broadcast Film Critics Association and the National Board of Review named it the best Animated Feature Film of 2006. Cars also received the title of Best Reviewed Animated Feature of 2006 from Rotten Tomatoes. Randy Newman and James Taylor received a Grammy Award for the song \"Our Town\", which later went on to be nominated for the Academy Award for Best Original Song (an award it lost to \"I Need to Wake Up\" from An Inconvenient Truth). The film also earned an Oscar nomination for Best Animated Feature, but it lost to Happy Feet. Cars was also selected as the Favorite Family Movie at the 33rd People's Choice Awards. The most prestigious award that Cars received was the inaugural Golden Globe Award for Best Animated Feature Film. Cars also won the highest award for animation in 2006, the Best Animated Feature Annie Award. The film was also nominated for AFI's 10 Top 10 in the \"Animation\" genre.\n\n\n== Cast and characters ==\n\nThis is a list of characters from the 2006 film Cars, its 2011 and 2017 sequels Cars 2 and Cars 3, its 2013 and 2014 spin-off films Planes and  Planes: Fire & Rescue, its 2006 and 2017 short films Mater and the Ghostlight and Miss Fritter's Racing Skoool, its 2014 spin-off short film Vitaminamulch: Air Spectacular, and its 2008 and 2022 television series Cars Toons and Cars on the Road.\n\nNote: A grey cell indicates the character was not in the film.\n\n\n== Crew ==\n\n\n== Other media ==\n\n\n=== Video games ===\nIn May 2007, the Cars video game was announced to be a \"Platinum Hit\" on the Xbox, \"Greatest Hit\" on the PlayStation 2 and PlayStation Portable, and Player's Choice on the GameCube. Two sequels were released, Cars Mater-National Championship and Cars Race-O-Rama. A video game based on Cars 2 was developed by Avalanche Software and published by Disney Interactive Studios for the PlayStation 3, Xbox 360, Wii, PC, and Nintendo DS on June 21, 2011. The PlayStation 3 version of the game was reported to be compatible with stereoscopic 3D gameplay. In October 2014, Gameloft released Cars: Fast as Lightning, a customizable, city-building racing game for smartphone platforms. Lightning McQueen appears as a playable character in Lego The Incredibles which was released in June 2018,  and Rocket League in November 2023.\n\n\n=== Similar films ===\nFrom the start, at least two inspired or knock off direct to video series appeared, A Cars Life and The Little Cars, that amounted to being mockbusters.It has also been noted that the plot of Cars bears a striking resemblance to that of Doc Hollywood, the 1991 romantic comedy which stars Michael J. Fox as a hotshot young doctor, who, after causing a traffic accident in a small town, is sentenced to work at the town hospital, falls in love with a local law student and eventually acquires an appreciation for small town values.\n\n\n==== The Autobots ====\nThe Autobots was released in July 2015 by Chinese companies Bluemtv and G-Point. On January 1, 2017, Disney and Pixar were awarded damages in their lawsuit against the two companies. The Shanghai Pudong New Area People's Court ruled that the Chinese film titled The Autobots was an illegal copy of Cars thus fined the film's producer and distributor the equivalent of US$194,000. In his defense, director Zhuo Jianrong claimed he had never seen Cars.\n\n\n==== A Cars Life ====\nA Car's Life was a series animated by the UAV Corporation, produced by Spark Plug Entertainment, and distributed by Allumination Filmworks. There were a total of three films issued from 2006 to 2017. The series included A Car's Life: Sparky's Big Adventure (2006), Car's Life 2 (2011), Car's Life: Junkyard Blues (2017), which was made for television, and Car's Life 3: The Royal Heist (2017).\n\n\n==== The Little Cars ====\nThe Little Cars is a big series animated by the Toyland Video (V\u00eddeo Brinquedo) and distributed by Branscome International. There were a total of eight films issued from 2006 to 2011. Toyland Video considers its \"mockbuster\" to be following a \"Bollywood\" approach of borrowing from Hollywood.Marcus Aurelius Can\u00f4nico of Folha de S.Paulo described The Little Cars series (Os Carrinhos in Portuguese), a Brazilian computer graphics film series, as a derivative of Cars. Can\u00f4nico discussed whether lawsuits from Pixar would appear. The Brazilian Ministry of Culture posted Marcus Aurelius Can\u00f4nico's article on its website.\n\n\n== Merchandising ==\nThe Mattel-produced 1/55 scale Toy Cars were some of the most popular toys of the 2006 Summer Season. Dozens of characters are represented, with some having multiple versions available. Several stores had trouble keeping the toys in stock, and some models are still difficult to find because of being shipped in lower numbers than other characters.Some online Disney enthusiasts are comparing it to the same shortage that Mattel faced with its Toy Story line in 1995. On August 14, 2007, the die-cast Sarge car, made between May and July 2007, was recalled due to \"impermissible levels of lead\" used in the paint. Another Cars product which followed the Disney-Pixar Cars Die-Cast Line were miniature versions of the characters which were painted in different colors to represent different events called Mini Adventures. Also, Lego has sets for the sequels.On June 22, 2006, Disney Consumer Products announced that Cars merchandise broke records for retail sales based on a Disney-Pixar product, recording 10-to-1 more volume than Finding Nemo. DCP reports that product expansion will take place in the fall alongside the DVD release of the film. Mattel has announced that Cars toys will continue through 2008 with the release of at least 80 new vehicles.\nA 36 car pack called \"Motor Speedway of the South\" will feature most of the race cars seen during the opening race sequence of the film. (This is also the name for the track race in the film) Estimates from the New York Daily News indicate that sales of Cars merchandise two weeks out from the release of the film amounted to US$600 million. Estimates put out in November by the Walt Disney Company peg total sales for the brand at around $1 billion.Kelley Blue Book, a resource for appraising values of vehicles, has humorously \"appraised\" four of the cars, Lightning McQueen, Mater, Sally Carrera, and Doc Hudson according to their make/model and personalities. The United States Department of Transportation has used scenes from the film in a commercial regarding the Click It or Ticket campaign.\nIn conjunction with the film's release, a chocolate ice cream on a stick resembling a car tire was released in Australia. These ice creams were called \"Burnouts\". The naming of the particular product sparked controversy as the name \"Burnouts\" was believed to have encouraged street racing and committing burnouts. These acts are illegal and heavy fines and convictions are issued to those committing these acts in Australia. It is unknown as to whether the products have been discontinued. In Norway, the candy company Nidar produced candy with the characters on the outer packaging and pictures of the characters on the packaging of the assorted candy on the inside. These bags also came with Cars themed tattoos.\nIn the U.S., an animated Walmart truck can be seen on a Walmart advertisements for Cars. In the Walmart TV commercial Mater was talking to the Walmart truck. In South Africa, Italy, and several other countries where Opel is present (or with Opel models under Chevrolet and Vauxhall brand), GM has a campaign featuring a General Motors Astra, an Opel Meriva, and a General Motors Zafira as characters in the world of Cars, including TV ads made by Pixar, with the Opel models interacting with Lightning McQueen, Mater and Ramone.The first ad involved the Opels coming to Radiator Springs as tourists. The second involved their failed attempts at auditioning for Mater. In the end the Opels lost the part to the real Mater. In July 2006, greeting card giant Hallmark Cards unveiled its line of 2006 Keepsake Christmas ornaments. Among the collection was an ornament featuring Lightning McQueen and Mater. There is also a Cars children's clothing line, which produces various T-shirts and shorts.\n\nIn Japan, Disney Japan and Toyota backed racing team Cars Racing replaced its racing car \"Toy Story apr MR-S\" and introduced the \"Lightning McQueen apr MR-S\" for the 2008 Super GT season. The car was based on the Toyota MR-S and the externals of the car were modeled on its of McQueen as much as possible. This include their number change from their original No.101 to McQueen's #95. They won in Race 3 that season.\nAs of 2011, the Cars franchise has grossed over $10 billion in merchandise sales revenue.\n\n\n== Theme park attractions ==\n\n\n=== Cars Land ===\n\nCars Land is a 12-acre land located at Disney California Adventure which contains an entire full-sized recreation of the town of Radiator Springs from the Cars franchise. The land includes restaurants, shops, and three rides: Mater's Junkyard Jamboree, Luigi's Rollickin' Roadsters, and  Radiator Springs Racers, the main \"E-Ticket\" attraction which is one of the most expensive rides Disney has ever built at a cost of over $200 million. Radiator Springs Racers lets guests race against each other around Ornament Valley while encountering several Cars characters. Cars Land opened on June 15, 2012, with the completion of Disney California Adventure's expansion along with Buena Vista Street.\nLuigi's Rollickin' Roadsters opened on March 7, 2016, and replaced Luigi's Flying Tires, an original Cars Land ride which closed in February 2015 after complaints and injuries.\n\n\n=== Cars Four Wheels Rally ===\n\nCars Four Wheels Rally (French: Cars Quatre Roues Rallye) is an attraction in Toon Studio at Walt Disney Studios Park at Disneyland Paris in 2007. The ride opened one year after Cars showed its first screening on June 9, 2007.  The attraction's setting is car service station in the small town of Radiator Springs located in the desert. The attraction is surrounded by boulders which imitates the rocky formations of the Grand Canyon. The ride system is a highly themed Zamperla Demolition Derby.\nRiders begin the attraction by sitting down in one of the generic car-shaped vehicles. The vehicles are located on one of the four spinning plateaus. The attraction's layout is similar to \"Francis' Ladybug Boogie\" attraction at Disney California Adventure and the \"Whirlpool\" at Tokyo DisneySea, as the vehicles automatically change from one spinning plateau to the next. The vehicles change plateaus while carrying out the rotation inversion of eight separate vehicles. Cars Race Rally was the first operating Disney ride themed to the Cars franchise.\n\n\n=== Lightning McQueen's Racing Academy ===\n\nLightning McQueen's Racing Academy is a living character show attraction hosted by Lightning McQueen located in Sunset Boulevard in Disney's Hollywood Studios that opened on March 31, 2019. Lightning McQueen's Racing Academy is the first of many new attractions coming to Hollywood Studios, part of its shift from studio-like attractions to attractions featuring Disney properties such as Star Wars, Pixar, and Marvel.\nMuch like the existing Monsters, Inc. Laugh Floor in Magic Kingdom's Tomorrowland, Turtle Talk with Crush in several Disney parks (including Epcot), and Stitch Encounter in the non-American Disney parks, Lightning McQueen's Racing Academy lets guests interact with the character. The other attractions have actors voice the characters, which are manipulated via digital puppetry \u2014 the manipulation of digital animated figures in real time.\nWhile the other cars of the Cars films \u2014 Mater, Sally Carrera, and Cruz Ramirez, for instance \u2014 appear on screens during the show, Lightning appears in physical car form.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website"}, {"id": 32, "title": "Chemical energy", "content": "Chemical energy is the energy of chemical substances that is released when the substances undergo a chemical reaction and transform into other substances. Some examples of storage media of chemical energy include batteries, food, and gasoline (as well as oxygen gas, which is of high chemical energy due to its relatively weak double bond  and indispensable for chemical-energy release in gasoline combustion). Breaking and re-making chemical bonds involves energy, which may be either absorbed by or evolved from a chemical system. If reactants with relatively weak electron-pair bonds convert to more strongly bonded products, energy is released. Therefore, relatively weakly bonded and unstable molecules store chemical energy.Energy that can be released or absorbed because of a reaction between chemical substances is equal to the difference between the energy content of the products and the reactants, if the initial and final temperature is the same. This change in energy can be estimated from the bond energies of the reactants and products. It can also be calculated from \n  \n    \n      \n        \u0394\n        \n          \n            \n              U\n              \n                f\n              \n              \n                \u2218\n              \n            \n          \n          \n            \n              r\n              e\n              a\n              c\n              t\n              a\n              n\n              t\n              s\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta {U_{f}^{\\circ }}_{\\mathrm {reactants} }}\n  , the internal energy of formation of the reactant molecules, and \n  \n    \n      \n        \u0394\n        \n          \n            \n              U\n              \n                f\n              \n              \n                \u2218\n              \n            \n          \n          \n            \n              p\n              r\n              o\n              d\n              u\n              c\n              t\n              s\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta {U_{f}^{\\circ }}_{\\mathrm {products} }}\n  , the internal energy of formation of the product molecules. The internal energy change of a chemical process is equal to the heat exchanged if it is measured under conditions of constant volume and equal initial and final temperature, as in a closed container such as a bomb calorimeter. However, under conditions of constant pressure, as in reactions in vessels open to the atmosphere, the measured heat change is not always equal to the internal energy change, because pressure-volume work also releases or absorbs energy. (The heat change at constant pressure is equal to the enthalpy change, in this case the enthalpy of reaction, if initial and final temperatures are equal).\nA related term is the heat of combustion, which is the energy mostly of the weak double bonds of molecular oxygen  released due to a combustion reaction and often applied in the study of fuels. Food is similar to hydrocarbon and carbohydrate fuels, and when it is oxidized to carbon dioxide and water, the energy released is analogous to the heat of combustion (though assessed differently than for a hydrocarbon fuel\u2014see food energy).\nChemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or interactions between them. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. For example, when a fuel is burned, the chemical energy of molecular oxygen and the fuel is converted to heat. Green plants transform solar energy to chemical energy (mostly of oxygen) through the process of photosynthesis, and electrical energy can be converted to chemical energy and vice versa through electrochemical reactions.\nThe similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc. It is not a form of potential energy itself, but is more closely related to free energy. The confusion in terminology arises from the fact that in other areas of physics not dominated by entropy, all potential energy is available to do useful work and drives the system to spontaneously undergo changes of configuration, and thus there is no distinction between \"free\" and \"non-free\" potential energy (hence the one word \"potential\"). However, in systems of large entropy such as chemical systems, the total amount of energy present (and conserved according to the first law of thermodynamics) of which this chemical potential energy is a part, is separated from the amount of that energy\u2014thermodynamic free energy (from which chemical potential is derived)\u2014which (appears to) drive the system forward spontaneously as the global entropy increases (in accordance with the second law).\n\n\n== References =="}, {"id": 33, "title": "List of presidents of the United States", "content": "The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.\n\n\n== Presidents ==\n\n\n== See also ==\nActing President of the United States\nFounding Fathers of the United States\nPresident of the Continental Congress\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Works cited ==\n\n\n== External links ==\n Media related to President of the United States at Wikimedia Commons\n Quotations related to List of presidents of the United States at Wikiquote"}, {"id": 34, "title": "Dividend", "content": "A dividend is a distribution of profits by a corporation to its shareholders. When a corporation earns a profit or surplus, it is able to pay a portion of the profit as a dividend to shareholders. Any amount not distributed is taken to be re-invested in the business (called retained earnings). The current year profit as well as the retained earnings of previous years are available for distribution; a corporation is usually prohibited from paying a dividend out of its capital. Distribution to shareholders may be in cash (usually by bank transfer) or, if the corporation has a dividend reinvestment plan, the amount can be paid by the issue of further shares or by share repurchase. In some cases, the distribution may be of assets.\nThe dividend received by a shareholder is income of the shareholder and may be subject to income tax (see dividend tax). The tax treatment of this income varies considerably between jurisdictions. The corporation does not receive a tax deduction for the dividends it pays.A dividend is allocated as a fixed amount per share, with shareholders receiving a dividend in proportion to their shareholding. Dividends can provide at least temporarily stable income and raise morale among shareholders, but are not guaranteed to continue. For the joint-stock company, paying dividends is not an expense; rather, it is the division of after-tax profits among shareholders. Retained earnings (profits that have not been distributed as dividends) are shown in the shareholders' equity section on the company's balance sheet \u2013 the same as its issued share capital. Public companies usually pay dividends on a fixed schedule, but may cancel a scheduled dividend, or declare an unscheduled dividend at any time, sometimes called a special dividend to distinguish it from the regular dividends. (more usually a special dividend is paid at the same time as the regular dividend, but for a one-off higher amount). Cooperatives, on the other hand, allocate dividends according to members' activity, so their dividends are often considered to be a pre-tax expense.\nThe usually fixed payments to holders of preference shares (or preferred stock in American English) are classed as dividends. The word dividend comes from the Latin word dividendum (\"thing to be divided\").\n\n\n== History ==\n\nIn the financial history of the world, the Dutch East India Company (VOC) was the first recorded (public) company ever to pay regular dividends. The VOC paid annual dividends worth around 18 percent of the value of the shares for almost 200 years of existence (1602\u20131800).In common-law jurisdictions, courts have typically refused to intervene in companies' dividend policies, giving directors wide discretion as to the declaration or payment of dividends. The principle of non-interference was established in the Canadian case of Burland v Earle (1902), the British case of Bond v Barrow Haematite Steel Co (1902), and the Australian case of Miles v Sydney Meat-Preserving Co Ltd (1912). However in Sumiseki Materials Co Ltd v Wambo Coal Pty Ltd (2013) the Supreme Court of New South Wales broke with this precedent and recognised a shareholder's contractual right to a dividend.\n\n\n== Forms of payment ==\nCash dividends are the most common form of payment and are paid out in currency, usually via electronic funds transfer or a printed paper check. Such dividends are a form of investment income of the shareholder, usually treated as earned in the year they are paid (and not necessarily in the year a dividend was declared). For each share owned, a declared amount of money is distributed. Thus, if a person owns 100 shares and the cash dividend is 50 cents per share, the holder of the stock will be paid $50. Dividends paid are not classified as an expense, but rather a deduction of retained earnings. Dividends paid does not appear on an income statement, but does appear on the balance sheet.\nDifferent classes of stocks have different priorities when it comes to dividend payments. Preferred stocks have priority claims on a company's income. A company must pay dividends on its preferred shares before distributing income to common share shareholders. \nStock or scrip dividends are those paid out in the form of additional shares of the issuing corporation, or another corporation (such as its subsidiary corporation). They are usually issued in proportion to shares owned (for example, for every 100 shares of stock owned, a 5% stock dividend will yield 5 extra shares).\nNothing tangible will be gained if the stock is split because the total number of shares increases, lowering the price of each share, without changing the market capitalization, or total value, of the shares held. (See also Stock dilution.)\nStock dividend distributions do not affect the market capitalization of a company. Stock dividends are not includable in the gross income of the shareholder for US income tax purposes. Because the shares are issued for proceeds equal to the pre-existing market price of the shares; there is no negative dilution in the amount recoverable.Property dividends or dividends in specie (Latin for \"in kind\") are those paid out in the form of assets from the issuing corporation or another corporation, such as a subsidiary corporation. They are relatively rare and most frequently are securities of other companies owned by the issuer, however, they can take other forms, such as products and services.\nInterim dividends are dividend payments made before a company's Annual General Meeting (AGM) and final financial statements. This declared dividend usually accompanies the company's interim financial statements.\nOther dividends can be used in structured finance. Financial assets with known market value can be distributed as dividends; warrants are sometimes distributed in this way. For large companies with subsidiaries, dividends can take the form of shares in a subsidiary company. A common technique for \"spinning off\" a company from its parent is to distribute shares in the new company to the old company's shareholders. The new shares can then be traded independently.\n\n\n== Dividend coverage ==\nMost often, the payout ratio is calculated based on dividends per share and earnings per share:\n\nA payout ratio greater than 100 means the company is paying out more in dividends for the year than it earned.\nDividends are paid in cash. On the other hand, earnings are an accountancy measure and do not represent the actual cash-flow of a company. Hence, a more liquidity-driven way to determine the dividend's safety is to replace earnings by free cash flow. The free cash flow represents the company's available cash based on its operating business after investments:\n\n\n== Dividend dates ==\nA dividend that is declared must be approved by a company's board of directors before it is paid. For public companies in the US, four dates are relevant regarding dividends: The position in the UK is very similar, except that the expression \"in-dividend date\" is not used.\nDeclaration date \u2013 the day the board of directors announces its intention to pay a dividend. On that day, a liability is created and the company records that liability on its books; it now owes the money to the shareholders.\nIn-dividend date \u2013 the last day, which is one trading day before the ex-dividend date, where shares are said to be cum dividend ('with [including] dividend'). That is, existing shareholders and anyone who buys the shares on this day will receive the dividend, and any shareholders who have sold the shares lose their right to the dividend. After this date the shares becomes ex dividend.\nEx-dividend date \u2013 the day on which shares bought and sold no longer come attached with the right to be paid the most recently declared dividend. In the United States and many European countries, it is typically one trading day before the record date. This is an important date for any company that has many shareholders, including those that trade on exchanges, to enable reconciliation of who is entitled to be paid the dividend. Existing shareholders will receive the dividend even if they sell the shares on or after that date, whereas anyone who bought the shares will not receive the dividend. It is relatively common for a share's price to decrease on the ex-dividend date by an amount roughly equal to the dividend being paid, which reflects the decrease in the company's assets resulting from the payment of the dividend.\nBook closure date \u2013 when a company announces a dividend, it will also announce the date on which the company will temporarily close its books for share transfers, which is also usually the record date.\nRecord date \u2013 shareholders registered in the company's record as of the record date will be paid the dividend, while shareholders who are not registered as of this date will not receive the dividend. Registration in most countries is essentially automatic for shares purchased before the ex-dividend date.\nPayment date \u2013 the day on which dividend cheques will actually be mailed to shareholders or the dividend amount credited to their bank account.\n\n\n== Dividend frequency ==\nThe dividend frequency is the number of dividend payments within a single business year. The most usual dividend frequencies are yearly, semi-annually, quarterly and monthly. Some common dividend frequencies are quarterly in the US, semi-annually in Japan, UK and Australia and annually in Germany.\n\n\n== Dividend-reinvestment ==\nSome companies have dividend reinvestment plans, or DRIPs, not to be confused with scrips. DRIPs allow shareholders to use dividends to systematically buy small amounts of stock, usually with no commission and sometimes at a slight discount. In some cases, the shareholder might not need to pay taxes on these re-invested dividends, but in most cases they do.\n\n\n== Law and government policy on dividends ==\n\nGovernments may adopt policies on divident distribution for the protection of shareholders and the preservation of company viability, as well as treating dividends as a potential source of revenue.Most countries impose a corporate tax on the profits made by a company. Many jurisdictions also impose a tax on dividends paid by a company to its shareholders (stockholders), but the tax treatment of a dividend income varies considerably between jurisdictions. The primary tax liability is that of the shareholder, although a tax obligation may also be imposed on the corporation in the form of a withholding tax. In some cases the withholding tax may be the extent of the tax liability in relation to the dividend. A dividend tax is in addition to any tax imposed directly on the corporation on its profits.A dividend paid by a company is not an expense of the company.\n\n\n=== Australia and New Zealand ===\nAustralia and New Zealand have a dividend imputation system, wherein companies can attach franking credits or imputation credits to dividends. These franking credits represent the tax paid by the company upon its pre-tax profits. One dollar of company tax paid generates one franking credit. Companies can attach any proportion of franking up to a maximum amount that is calculated from the prevailing company tax rate: for each dollar of dividend paid, the maximum level of franking is the company tax rate divided by (1 \u2212 company tax rate). At the current 30% rate, this works out at 0.30 of a credit per 70 cents of dividend, or 42.857 cents per dollar of dividend. The shareholders who are able to use them, apply these credits against their income tax bills at a rate of a dollar per credit, thereby effectively eliminating the double taxation of company profits.\n\n\n=== India ===\nIn India, a company declaring or distributing dividends is required to pay a Corporate Dividend Tax in addition to the tax levied on their income. The dividend received by the shareholders is then exempt in their hands. Dividend-paying firms in India fell from 24 percent in 2001 to almost 19 percent in 2009 before rising to 19 percent in 2010. However, dividend income over and above \u20b91,000,000 attracts 10 percent dividend tax in the hands of the shareholder with effect from April 2016. Since the Budget 2020\u20132021, DDT has been abolished. Now, the Indian government taxes dividend income in the hands of investor according to income tax slab rates.\n\n\n=== United States and Canada ===\nThe United States and Canada impose a lower tax rate on dividend income than ordinary income, on the assertion that company profits had already been taxed as corporate tax.\n\n\n=== United Kingdom ===\nThe rules in Part 23 of the Companies Act 2006 (sections 829-853) govern the payment of dividends to shareholders. The Act refers in this section to \"distribution\", covering any kind of distribution of a company's assets to its members (with some exceptions), \"whether in cash or otherwise\". A company is only able to make a distribution out of its accumulated, realised profits, \"so far as not previously utilised by distribution or capitalisation, less its accumulated, realised losses, so far as not previously written off in a reduction or reorganisation of capital duly made\".The United Kingdom government announced in 2018 that it was considering a review of the existing rules on dividend distribution following a consultation exercise on insolvency and corporate governance. The aim was to address concerns which had emerged where companies in financial distress were still able to distribute \"significant dividends\" to their shareholders. A requirement has been proposed under which the largest companies would be required to publish a distribution policy statement covering dividend distribution.The law in England and Wales regarding dividend payment was clarified in 2018 by the England and Wales Court of Appeal in the case of Global Corporate Ltd v Hale [2018] EWCA Civ 2618. Certain payments made to a director/shareholder had been treated by the High Court as quantum meruit payments to Hale in his capacity as a company director but the Appeal Court reversed this judgment and treated the payments as dividends. At the time of payment they had been treated as \"dividends\" payable from an anticipated profit. The company subsequently went into liquidation; an attempt to recharacterise the payments as payments for services rendered was held to be unlawful.\n\n\n== Effect on stock price ==\nAfter a stock goes ex-dividend (when a dividend has just been paid, so there is no anticipation of another imminent dividend payment), the stock price should drop.\nTo calculate the amount of the drop, the traditional method is to view the financial effects of the dividend from the perspective of the company. Since the company has paid say \u00a3x in dividends per share out of its cash account on the left hand side of the balance sheet, the equity account on the right side should decrease an equivalent amount. This means that a \u00a3x dividend should result in a \u00a3x drop in the share price.\nA more accurate method of calculating the fall in price is to look at the share price and dividend from the after-tax perspective of a shareholder. The after-tax drop in the share price (or capital gain/loss) should be equivalent to the after-tax dividend. For example, if the tax of capital gains Tcg is 35%, and the tax on dividends Td is 15%, then a \u00a31 dividend is equivalent to \u00a30.85 of after-tax money. To get the same financial benefit from a capital loss, the after-tax capital loss value should equal \u00a30.85. The pre-tax capital loss would be \u00a30.85/1 \u2212 Tcg = \u00a30.85/1 \u2212 0.35 = \u00a30.85/0.65 = \u00a31.31. In this case, a dividend of \u00a31 has led to a larger drop in the share price of \u00a31.31, because the tax rate on capital losses is higher than the dividend tax rate. However in many countries the stock market is dominated by institutions which pay no additional tax on dividends received (as opposed to tax on overall profits). If that is the case, then the share price should fall by the full amount of the dividend.\nFinally, security analysis that does not take dividends into account may mute the decline in share price, for example in the case of a price\u2013earnings ratio target that does not back out cash; or amplify the decline when comparing different periods.\nThe effect of a dividend payment on share price is an important reason why it can sometimes be desirable to exercise an American option early.\n\n\n=== Criticism and analysis ===\nSome believe that company profits are best re-invested in the company with actions such as research and development, capital investment or expansion. Proponents of this view (and thus critics of dividends per se) suggest that an eagerness to return profits to shareholders may indicate the management having run out of good ideas for the future of the company. Some studies, however, have demonstrated that companies that pay dividends have higher earnings growth, suggesting that dividend payments may be evidence of confidence in earnings growth and sufficient profitability to fund future expansion. Benjamin Graham and David Dodd wrote in Securities Analysis (1934): \"The prime purpose of a business corporation is to pay dividends to its owners. A successful company is one that can pay dividends regularly and presumably increase the rate as time goes on.\"Other studies indicate that divided-paying stocks tend to offer superior long-term performance due to a variety of factors, such as dividends being associated with value stocks, with profitable companies exhibiting high levels of free cashflow and with mature, unfashionable companies that are overlooked by many investors and thus an effective contrarian strategy.Shareholders in companies that pay little or no cash dividends can reap the benefit of the company's profits when they sell their shareholding, or when a company is wound down and all assets liquidated and distributed amongst shareholders.\n\n\n==== Tax implications ====\nTaxation of dividends is often used as justification for retaining earnings, or for performing a stock buyback, in which the company buys back stock, thereby increasing the value of the stock left outstanding.\nWhen dividends are paid, individual shareholders in many countries suffer from double taxation of those dividends:\n\nthe company pays income tax to the government when it earns any income, and then\nwhen the dividend is paid, the individual shareholder pays income tax on the dividend payment.In many countries, the tax rate on dividend income is lower than for other forms of income to compensate for tax paid at the corporate level.\nA capital gain should not be confused with a dividend. Generally, a capital gain occurs where a capital asset is sold for an amount greater than the amount of its cost at the time the investment was purchased. A dividend is a parsing out a share of the profits, and is taxed at the dividend tax rate. If there is an increase of value of stock, and a shareholder chooses to sell the stock, the shareholder will pay a tax on capital gains (often taxed at a lower rate than ordinary income). If a holder of the stock chooses to not participate in the buyback, the price of the holder's shares could rise (as well as it could fall), but the tax on these gains is delayed until the sale of the shares.\nCertain types of specialized investment companies (such as a REIT in the U.S.) allow the shareholder to partially or fully avoid double taxation of dividends.\n\n\n== Other corporate entities ==\n\n\n=== Cooperatives ===\nCooperative businesses may retain their earnings, or distribute part or all of them as dividends to their members. They distribute their dividends in proportion to their members' activity, instead of the value of members' shareholding. Therefore, co-op dividends are often treated as pre-tax expenses. In other words, local tax or accounting rules may treat a dividend as a form of customer rebate or a staff bonus to be deducted from turnover before profit (tax profit or operating profit) is calculated.\nConsumers' cooperatives allocate dividends according to their members' trade with the co-op. For example, a credit union will pay a dividend to represent interest on a saver's deposit. A retail co-op store chain may return a percentage of a member's purchases from the co-op, in the form of cash, store credit, or equity. This type of dividend is sometimes known as a patronage dividend or patronage refund, as well as being informally named divi or divvy.Producer cooperatives, such as worker cooperatives, allocate dividends according to their members' contribution, such as the hours they worked or their salary.\n\n\n=== Trusts ===\nIn real estate investment trusts and royalty trusts, the distributions paid often will be consistently greater than the company earnings. This can be sustainable because the accounting earnings do not recognize any increasing value of real estate holdings and resource reserves. If there is no economic increase in the value of the company's assets then the excess distribution (or dividend) will be a return of capital and the book value of the company will have shrunk by an equal amount. This may result in capital gains which may be taxed differently from dividends representing distribution of earnings.\nThe distribution of profits by other forms of mutual organization also varies from that of joint-stock companies, though may not take the form of a dividend.\nIn the case of mutual insurance, for example, in the United States, a distribution of profits to holders of participating life policies is called a dividend.\nThese profits are generated by the investment returns of the insurer's general account, in which premiums are invested and from which claims are paid. The participating dividend may be used to decrease premiums, or to increase the cash value of the policy.\nSome life policies pay nonparticipating dividends.\nAs a contrasting example, in the United Kingdom, the surrender value of a with-profits policy is increased by a bonus, which also serves the purpose of distributing profits.\nLife insurance dividends and bonuses, while typical of mutual insurance, are also paid by some joint stock insurers.\nInsurance dividend payments are not restricted to life policies. For example, general insurer State Farm Mutual Automobile Insurance Company can distribute dividends to its vehicle insurance policyholders.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nEx-Dividend Dates: When Are You Entitled to Stock and Cash Dividends \u2013 U.S. Securities and Exchange Commission\nWhy Should Companies Pay Dividends?\nDividend Policy from studyfinance.com at the University of Arizona\nThe new U.S. dividend tax cut traps from Tennessee CPA Journal, Nov. 2004\nLearn Strategy to Earn Money from Dividends"}, {"id": 35, "title": "Stock", "content": "Stocks (also capital stock, or sometimes interchangeably, shares) consist of all the shares by which ownership of a corporation or company is divided. A single share of the stock means fractional ownership of the corporation in proportion to the total number of shares. This typically entitles the shareholder (stockholder) to that fraction of the company's earnings, proceeds from liquidation of assets (after discharge of all senior claims such as secured and unsecured debt), or voting power, often dividing these up in proportion to the amount of money each stockholder has invested. Not all stock is necessarily equal, as certain classes of stock may be issued, for example, without voting rights, with enhanced voting rights, or with a certain priority to receive profits or liquidation proceeds before or after other classes of shareholders.\nStock can be bought and sold privately or on stock exchanges. Such transactions are closely overseen by governments and regulatory bodies to prevent fraud, protect investors, and benefit the larger economy. The stocks are deposited with the depositories in the electronic format also known as Demat account. As new shares are issued by a company, the ownership and rights of existing shareholders are diluted in return for cash to sustain or grow the business. Companies can also buy back stock, which often lets investors recoup the initial investment plus capital gains from subsequent rises in stock price. Stock options issued by many companies as part of employee compensation do not represent ownership, but represent the right to buy ownership at a future time at a specified price. This would represent a windfall to the employees if the option is exercised when the market price is higher than the promised price, since if they immediately sold the stock they would keep the difference (minus taxes).\nStock bought and sold in private markets fall within the private equity realm of finance.\n\n\n== Shares ==\nA person who owns a percentage of the stock has the ownership of the corporation proportional to their share. The shares form a stock. The stock of a corporation is partitioned into shares, the total of which are stated at the time of business formation. Additional shares may subsequently be authorized by the existing shareholders and issued by the company. In some jurisdictions, each share of stock has a certain declared par value, which is a nominal accounting value used to represent the equity on the balance sheet of the corporation. In other jurisdictions, however, shares of stock may be issued without associated par value.\nShares represent a fraction of ownership in a business. A business may declare different types (or classes) of shares, each having distinctive ownership rules, privileges, or share values. Ownership of shares may be documented by issuance of a stock certificate. A stock certificate is a legal document that specifies the number of shares owned by the shareholder, and other specifics of the shares, such as the par value, if any, or the class of the shares.\nIn the United Kingdom, Republic of Ireland, South Africa, and Australia, stock can also refer, less commonly, to all kinds of marketable securities.\n\n\n== Types ==\nStock typically takes the form of shares of either common stock or preferred stock. As a unit of ownership, common stock typically carries voting rights that can be exercised in corporate decisions. Preferred stock differs from common stock in that it typically does not carry voting rights but is legally entitled to receive a certain level of dividend payments before any dividends can be issued to other shareholders. Convertible preferred stock is preferred stock that includes an option for the holder to convert the preferred shares into a fixed number of common shares, usually any time after a predetermined date. Shares of such stock are called \"convertible preferred shares\" (or \"convertible preference shares\" in the UK).\nNew equity issue may have specific legal clauses attached that differentiate them from previous issues of the issuer. Some shares of common stock may be issued without the typical voting rights, for instance, or some shares may have special rights unique to them and issued only to certain parties. Often, new issues that have not been registered with a securities governing body may be restricted from resale for certain periods of time.\nPreferred stock may be hybrid by having the qualities of bonds of fixed returns and common stock voting rights. They also have preference in the payment of dividends over common stock and also have been given preference at the time of liquidation over common stock. They have other features of accumulation in dividend. In addition, preferred stock usually comes with a letter designation at the end of the security; for example, Berkshire-Hathaway Class \"B\" shares sell under stock ticker BRK.B, whereas Class \"A\" shares of ORION DHC, Inc will sell under ticker OODHA until the company drops the \"A\" creating ticker OODH for its \"Common\" shares only designation. This extra letter does not mean that any exclusive rights exist for the shareholders but it does let investors know that the shares are considered for such, however, these rights or privileges may change based on the decisions made by the underlying company.\n\n\n=== Rule 144 stock ===\n\"Rule 144 Stock\" is an American term given to shares of stock subject to SEC Rule 144: Selling Restricted and Control Securities. Under Rule 144, restricted and controlled securities are acquired in unregistered form. Investors either purchase or take ownership of these securities through private sales (or other means such as via ESOPs or in exchange for seed money) from the issuing company (as in the case with Restricted Securities) or from an affiliate of the issuer (as in the case with Control Securities). Investors wishing to sell these securities are subject to different rules than those selling traditional common or preferred stock. These individuals will only be allowed to liquidate their securities after meeting the specific conditions set forth by SEC Rule 144. Rule 144 allows public re-sale of restricted securities if a number of different conditions are met.\n\n\n== Stock derivatives ==\n\nA stock derivative is any financial instrument for which the underlying asset is the price of an equity. Futures and options are the main types of derivatives on stocks. The underlying security may be a stock index or an individual firm's stock, e.g. single-stock futures.\nStock futures are contracts where the buyer is long, i.e., takes on the obligation to buy on the contract maturity date, and the seller is short, i.e., takes on the obligation to sell. Stock index futures are generally delivered by cash settlement.\nA stock option is a class of option. Specifically, a call option is the right (not obligation) to buy stock in the future at a fixed price and a put option is the right (not obligation) to sell stock in the future at a fixed price. Thus, the value of a stock option changes in reaction to the underlying stock of which it is a derivative. The most popular method of valuing stock options is the Black\u2013Scholes model. Apart from call options granted to employees, most stock options are transferable.\n\n\n== History ==\nDuring the Roman Republic, the state contracted (leased) out many of its services to private companies. These government contractors were called publicani, or societas publicanorum as individual companies. These companies were similar to modern corporations, or joint-stock companies more specifically, in a couple of aspects. They issued shares called partes (for large cooperatives) and particulae which were small shares that acted like today's over-the-counter shares. Polybius mentions that \"almost every citizen\" participated in the government leases. There is also evidence that the price of stocks fluctuated. The Roman orator Cicero speaks of partes illo tempore carissimae, which means \"shares that had a very high price at that time\". This implies a fluctuation of price and stock market behavior in Rome.\nAround 1250 in France at Toulouse, 100 shares of the Soci\u00e9t\u00e9 des Moulins du Bazacle, or Bazacle Milling Company were traded at a value that depended on the profitability of the mills the society owned.In 1288, the Bishop of V\u00e4ster\u00e5s acquired a 12.5% interest in Great Copper Mountain (Stora Kopparberget in Swedish) which contained the Falun Mine. The Swedish mining and forestry products company Stora has documented a stock transfer, in 1288 in exchange for an estate.\nThe earliest recognized joint-stock company in modern times was the English (later British) East India Company. It was granted an English Royal Charter by Elizabeth I on 31 December 1600, with the intention of favouring trade privileges in India. The Royal Charter effectively gave the newly created Honourable East India Company (HEIC) a 15-year monopoly on all trade in the East Indies.Soon afterwards, in 1602, the Dutch East India Company issued the first shares that were made tradeable on the Amsterdam Stock Exchange. Between 1602 and 1796 it traded 2.5 million tons of cargo with Asia on 4,785 ships and sent a million Europeans to work in Asia.\n\n\n== Shareholder ==\n\nA shareholder (or stockholder) is an individual or company (including a corporation) that legally owns one or more shares of stock in a joint stock company. Both private and public traded companies have shareholders.\nShareholders are granted special privileges depending on the class of stock, including the right to vote on matters such as elections to the board of directors, the right to share in distributions of the company's income, the right to purchase new shares issued by the company, and the right to a company's assets during a liquidation of the company. However, shareholder's rights to a company's assets are subordinate to the rights of the company's creditors.\nShareholders are one type of stakeholders, who may include anyone who has a direct or indirect equity interest in the business entity or someone with a non-equity interest in a non-profit organization. Thus it might be common to call volunteer contributors to an association stakeholders, even though they are not shareholders.\nAlthough directors and officers of a company are bound by fiduciary duties to act in the best interest of the shareholders, the shareholders themselves normally do not have such duties towards each other.\nHowever, in a few unusual cases, some courts have been willing to imply such a duty between shareholders. For example, in California, United States, majority shareholders of closely held corporations have a duty not to destroy the value of the shares held by minority shareholders.The largest shareholders (in terms of percentages of companies owned) are often mutual funds, and, especially, passively managed exchange-traded funds.\n\n\n== Application ==\nThe owners of a private company may want additional capital to invest in new projects within the company. They may also simply wish to reduce their holding, freeing up capital for their own private use. They can achieve these goals by selling shares in the company to the general public, through a sale on a stock exchange. This process is called an initial public offering, or IPO.\nBy selling shares they can sell part or all of the company to many part-owners. The purchase of one share entitles the owner of that share to literally share in the ownership of the company, a fraction of the decision-making power, and potentially a fraction of the profits, which the company may issue as dividends. The owner may also inherit debt and even litigation.\nIn the common case of a publicly traded corporation, where there may be thousands of shareholders, it is impractical to have all of them making the daily decisions required to run a company. Thus, the shareholders will use their shares as votes in the election of members of the board of directors of the company.\nIn a typical case, each share constitutes one vote. Corporations may, however, issue different classes of shares, which may have different voting rights. Owning the majority of the shares allows other shareholders to be out-voted \u2013 effective control rests with the majority shareholder (or shareholders acting in concert). In this way the original owners of the company often still have control of the company.\n\n\n=== Shareholder rights ===\nAlthough ownership of 50% of shares does result in 50% ownership of a company, it does not give the shareholder the right to use a company's building, equipment, materials, or other property. This is because the company is considered a legal person, thus it owns all its assets itself. This is important in areas such as insurance, which must be in the name of the company and not the main shareholder.\nIn most countries, boards of directors and company managers have a fiduciary responsibility to run the company in the interests of its stockholders. Nonetheless, as Martin Whitman writes:\n\n...it can safely be stated that there does not exist any publicly traded company where management works exclusively in the best interests of OPMI [Outside Passive Minority Investor] stockholders. Instead, there are both \"communities of interest\" and \"conflicts of interest\" between stockholders (principal) and management (agent). This conflict is referred to as the principal\u2013agent problem. It would be naive to think that any management would forego management compensation, and management entrenchment, just because some of these management privileges might be perceived as giving rise to a conflict of interest with OPMIs.Even though the board of directors runs the company, the shareholder has some impact on the company's policy, as the shareholders elect the board of directors. Each shareholder typically has a percentage of votes equal to the percentage of shares he or she owns. So as long as the shareholders agree that the management (agent) are performing poorly they can select a new board of directors which can then hire a new management team. In practice, however, genuinely contested board elections are rare. Board candidates are usually nominated by insiders or by the board of the directors themselves, and a considerable amount of stock is held or voted by insiders.\nOwning shares does not mean responsibility for liabilities. If a company goes broke and has to default on loans, the shareholders are not liable in any way. However, all money obtained by converting assets into cash will be used to repay loans and other debts first, so that shareholders cannot receive any money unless and until creditors have been paid (often the shareholders end up with nothing).\n\n\n=== Means of financing ===\nFinancing a company through the sale of stock in a company is known as equity financing. Alternatively, debt financing (for example issuing bonds) can be done to avoid giving up shares of ownership of the company. Unofficial financing known as trade financing usually provides the major part of a company's working capital (day-to-day operational needs).\n\n\n== Trading ==\n\nIn general, the shares of a company may be transferred from shareholders to other parties by sale or other mechanisms, unless prohibited. Most jurisdictions have established laws and regulations governing such transfers, particularly if the issuer is a publicly traded entity.\nThe desire of stockholders to trade their shares has led to the establishment of stock exchanges, organizations which provide marketplaces for trading shares and other derivatives and financial products. Today, stock traders are usually represented by a stockbroker who buys and sells shares of a wide range of companies on such exchanges. A company may list its shares on an exchange by meeting and maintaining the listing requirements of a particular stock exchange.\nMany large non-U.S companies choose to list on a U.S. exchange as well as an exchange in their home country in order to broaden their investor base. These companies must maintain a block of shares at a bank in the US, typically a certain percentage of their capital. On this basis, the holding bank establishes American depositary shares and issues an American depositary receipt (ADR) for each share a trader acquires. Likewise, many large U.S. companies list their shares at foreign exchanges to raise capital abroad.\nSmall companies that do not qualify and cannot meet the listing requirements of the major exchanges may be traded over-the-counter (OTC) by an off-exchange mechanism in which trading occurs directly between parties. The major OTC markets in the United States are the electronic quotation systems OTC Bulletin Board (OTCBB) and OTC Markets Group (formerly known as Pink OTC Markets Inc.) where individual retail investors are also represented by a brokerage firm and the quotation service's requirements for a company to be listed are minimal. Shares of companies in bankruptcy proceedings are usually listed by these quotation services after the stock is delisted from an exchange.\n\n\n=== Buying ===\nThere are various methods of buying and financing stocks, the most common being through a stockbroker. Brokerage firms, whether they are a full-service or discount broker, arrange the transfer of stock from a seller to a buyer. Most trades are actually done through brokers listed with a stock exchange.\nThere are many different brokerage firms from which to choose, such as full service brokers or discount brokers. The full service brokers usually charge more per trade, but give investment advice or more personal service; the discount brokers offer little or no investment advice but charge less for trades. Another type of broker would be a bank or credit union that may have a deal set up with either a full-service or discount broker.\nThere are other ways of buying stock besides through a broker. One way is directly from the company itself. If at least one share is owned, most companies will allow the purchase of shares directly from the company through their investor relations departments. However, the initial share of stock in the company will have to be obtained through a regular stock broker. Another way to buy stock in companies is through Direct Public Offerings which are usually sold by the company itself. A direct public offering is an initial public offering in which the stock is purchased directly from the company, usually without the aid of brokers.\nWhen it comes to financing a purchase of stocks there are two ways: purchasing stock with money that is currently in the buyer's ownership, or by buying stock on margin. Buying stock on margin means buying stock with money borrowed against the value of stocks in the same account. These stocks, or collateral, guarantee that the buyer can repay the loan; otherwise, the stockbroker has the right to sell the stock (collateral) to repay the borrowed money. He can sell if the share price drops below the margin requirement, at least 50% of the value of the stocks in the account. Buying on margin works the same way as borrowing money to buy a car or a house, using a car or house as collateral. Moreover, borrowing is not free; the broker usually charges 8\u201310% interest.\n\n\n=== Selling ===\nSelling stock is procedurally similar to buying stock. Generally, the investor wants to buy low and sell high, if not in that order (short selling); although a number of reasons may induce an investor to sell at a loss, e.g., to avoid further loss.\nAs with buying a stock, there is a transaction fee for the broker's efforts in arranging the transfer of stock from a seller to a buyer. This fee can be high or low depending on which type of brokerage, full service or discount, handles the transaction.\nAfter the transaction has been made, the seller is then entitled to all of the money. An important part of selling is keeping track of the earnings. Importantly, on selling the stock, in jurisdictions that have them, capital gains taxes will have to be paid on the additional proceeds, if any, that are in excess of the cost basis.\n\n\n=== Short selling ===\nShort selling consists of an investor immediately selling borrowed shares and then buying them back when their price has gone down (called \"covering\"). Essentially, such an investor bets that the price of the shares will drop so that they can be bought back at the lower price and thus returned to the lender at a profit.\n\n\n==== Risks of short selling ====\nThe risks of short selling stock are usually higher than those of buying stock. This is because the loss can theoretically be unlimited since the stock's value can theoretically go up indefinitely.\n\n\n=== Stock price fluctuations ===\nThe price of a stock fluctuates fundamentally due to the theory of supply and demand. Like all commodities in the market, the price of a stock is sensitive to demand. However, there are many factors that influence the demand for a particular stock. The fields of fundamental analysis and technical analysis attempt to understand market conditions that lead to price changes, or even predict future price levels. A recent study shows that customer satisfaction, as measured by the American Customer Satisfaction Index (ACSI), is significantly correlated to the market value of a stock. Stock price may be influenced by analysts' business forecast for the company and outlooks for the company's general market segment. Stocks can also fluctuate greatly due to pump and dump scams (also see List of S&P 600 companies).\n\n\n=== Share price determination ===\nAt any given moment, an equity's price is strictly a result of supply and demand. The supply, commonly referred to as the float, is the number of shares offered for sale at any one moment. The demand is the number of shares investors wish to buy at exactly that same time. The price of the stock moves in order to achieve and maintain equilibrium. The product of this instantaneous price and the float at any one time is the market capitalization of the entity offering the equity at that point in time.\nWhen prospective buyers outnumber sellers, the price rises. Eventually, sellers attracted to the high selling price enter the market and/or buyers leave, achieving equilibrium between buyers and sellers. When sellers outnumber buyers, the price falls. Eventually buyers enter and/or sellers leave, again achieving equilibrium.\nThus, the value of a share of a company at any given moment is determined by all investors voting with their money. If more investors want a stock and are willing to pay more, the price will go up. If more investors are selling a stock and there are not enough buyers, the price will go down.That does not explain how people decide the maximum price at which they are willing to buy or the minimum at which they are willing to sell. In professional investment circles the efficient market hypothesis (EMH) continues to be popular, although this theory is widely discredited in academic and professional circles. Briefly, EMH says that investing is overall (weighted by the standard deviation) rational; that the price of a stock at any given moment represents a rational evaluation of the known information that might bear on the future value of the company; and that share prices of equities are priced efficiently, which is to say that they represent accurately the expected value of the stock, as best it can be known at a given moment. In other words, prices are the result of discounting expected future cash flows.\nThe EMH model, if true, has at least two interesting consequences. First, because financial risk is presumed to require at least a small premium on expected value, the return on equity can be expected to be slightly greater than that available from non-equity investments: if not, the same rational calculations would lead equity investors to shift to these safer non-equity investments that could be expected to give the same or better return at lower risk. Second, because the price of a share at every given moment is an \"efficient\" reflection of expected value, then\u2014relative to the curve of expected return\u2014prices will tend to follow a random walk, determined by the emergence of information (randomly) over time. Professional equity investors therefore immerse themselves in the flow of fundamental information, seeking to gain an advantage over their competitors (mainly other professional investors) by more intelligently interpreting the emerging flow of information (news).\nThe EMH model does not seem to give a complete description of the process of equity price determination. For example, stock markets are more volatile than EMH would imply. In recent years it has come to be accepted that the share markets are not perfectly efficient, perhaps especially in emerging markets or other markets that are not dominated by well-informed professional investors.\nAnother theory of share price determination comes from the field of Behavioral Finance. According to Behavioral Finance, humans often make irrational decisions\u2014particularly, related to the buying and selling of securities\u2014based upon fears and misperceptions of outcomes. The irrational trading of securities can often create securities prices which vary from rational, fundamental price valuations. For instance, during the technology bubble of the late 1990s (which was followed by the dot-com bust of 2000\u20132002), technology companies were often bid beyond any rational fundamental value because of what is commonly known as the \"greater fool theory\". The \"greater fool theory\" holds that, because the predominant method of realizing returns in equity is from the sale to another investor, one should select securities that they believe that someone else will value at a higher level at some point in the future, without regard to the basis for that other party's willingness to pay a higher price.Thus, even a rational investor may bank on others' irrationality.\n\n\n=== Arbitrage trading ===\nWhen companies raise capital by offering stock on more than one exchange, the potential exists for discrepancies in the valuation of shares on different exchanges. A keen investor with access to information about such discrepancies may invest in expectation of their eventual convergence, known as arbitrage trading. Electronic trading has resulted in extensive price transparency (efficient-market hypothesis) and these discrepancies, if they exist, are short-lived and quickly equilibrated.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nGraham, Benjamin; Jason Zweig (8 July 2003) [1949]. The Intelligent Investor. Warren E. Buffett (collaborator) (2003 ed.). HarperCollins. front cover. ISBN 0-06-055566-1.\nGraham, B.; Dodd, D.; Dodd, D.L.F. (1934). Security Analysis: The Classic 1934 Edition. McGraw-Hill Education. ISBN 978-0-070-24496-2. LCCN 34023635.\nRich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not!, by Robert Kiyosaki and Sharon Lechter. Warner Business Books, 2000. ISBN 0-446-67745-0\nClason, George (2015). The Richest Man in Babylon: Original 1926 Edition. CreateSpace Independent Publishing Platform. ISBN 978-1-508-52435-9.\nBogle, John Bogle (2007). The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns. John Wiley and Sons. pp. 216. ISBN 978-0-470-10210-7.\nBuffett, W.; Cunningham, L.A. (2009). The Essays of Warren Buffett: Lessons for Investors and Managers. John Wiley & Sons (Asia) Pte Limited. ISBN 978-0-470-82441-2.\nStanley, Thomas J.; Danko, W.D. (1998). The Millionaire Next Door. Gallery Books. ISBN 978-0-671-01520-6. LCCN 98046515.\nSoros, George (1988). The Alchemy of Finance: Reading the Mind of the Market. A Touchstone book. Simon & Schuster. ISBN 978-0-671-66238-7. LCCN 87004745.\nFisher, Philip Arthur (1996). Common Stocks and Uncommon Profits and Other Writings. Wiley Investment Classics. Wiley. ISBN 978-0-471-11927-2. LCCN 95051449.\n\n\n== External links =="}, {"id": 36, "title": "Energy storage as a service", "content": "Energy storage as a service (ESaaS) allows a facility to benefit from the advantages of an energy storage system by entering into a service agreement without purchasing the system. Energy storage systems provide a range of services to generate revenue, create savings, and improve electricity resiliency. The operation of the ESaaS system is a unique combination of an advanced battery storage system, an energy management system, and a service contract which can deliver value to a business by providing reliable power more economically.\n\n\n== History ==\nScott Foster, Energy Director of the United Nations Economic Commission for Europe, is one of the leading global advocates for energy as service. He coined the term 'iEnergy' to propagate a annual/monthly subscription fee for energy, rather than the present-day commodity-led pay per kilowatt of electricity system. Foster believes a service-led system would put the onus on the energy supplier to improve reliability and offer the best possible service to customers.The term ESaaS was developed and trademarked by Constant Power Inc., a Toronto-based company, in 2016. The service has been designed to work in the North American open electricity markets. Notable other companies offering Energy Storage-as-a-Service include GI Energy Archived 2017-10-20 at the Wayback Machine, AES Corporation, TROES Corp., Stem Inc, and Younicos.\n\n\n== Components ==\nESaaS is the combination of an energy storage system, a control and monitoring system, and a service contract.\nThe most common energy storage systems used for ESaaS are lithium-ion or flow batteries due to their compact size, non-invasive installation, high efficiencies, and fast reaction times but other storage mediums may be used such as compressed air, flywheels, or pumped hydro. The batteries are sized based on the facility's needs and is paired with a power inverter to convert the DC power to AC power in order to connect directly to the facility\u2019s electricity supply.\nESaaS systems are remotely monitored and controlled by the ESaaS operator using a Supervisory Control and Data Acquisition (SCADA) system. The SCADA communicates with the facility's Energy Management System (EMS), Power Conversion System (PCS), and Battery Management System (BMS). The ESaaS operator is responsible for ensuring the ESaaS system is monitoring and responding to the facility\u2019s needs as well as overriding commands to participate in regional incentive programs such as coincident peak management and demand response programs in real time.\nThe facility benefiting from the ESaaS system is linked to the ESaaS system operator through a service contract. The contract specifies the length of the service term, payment structure, and list of services the facility wishes to participate in.\n\n\n== Services ==\nESaaS is used to perform a variety of services including:\n\nCoincident Peak Management\nDuring times of high regional demand, Independent Service Operators (ISOs)/Regional Transmission Organizations (RTOs) offer incentives for facilities to reduce or curtail their load. ESaaS allows a facility to isolate or offset their load during these high regional demand periods to decrease demand from the electricity grid to benefit from the incentives. The system is designed to work in conjunction or independent of facility curtailment.\nDemand Response\nISOs/RTOs offer facilities payment for curtailing their energy demand when dispatched by the grid operator. ESaaS allows facilities to participate in these programs by off-setting all or a portion of a facility load during a demand response occurrence. A facility can benefit from the incentive without interrupting their facility operation.\nPower Factor Correction\nDuring charging and discharging, active and reactive power may be balanced prior to supplying a facility. By balancing the amount of active and reactive power to a facility, the power factor and resulting facility electrical efficiency may be improved. This improvement may reduce a facility's monthly peak demand charge.\nPower Quality\nESaaS actively monitors electricity supply to a facility. In times of intermittent power supply, ESaaS acts as an uninterruptible power supply (UPS) to ensure uninterrupted, reliable power supply to eliminate unexpected fluctuations. Fluctuating and intermittent power affects equipment operation which may cause costly delays and defects in production.\nBack-up Power\nIf the electricity grid experiences a power outage, ESaaS offers a back-up power service to continue powering all or a portion of a facility's electricity demand. Depending on the size of the ESaaS installation, ESaaS may maintain facility operation for the duration of a grid failure.\nPeak Shaving\nESaaS actively monitors a facility\u2019s energy profile to normalize the electricity draw from the electricity grid. The ESaaS system stores energy when the facility demand is lower than average and discharges the stored energy when the facility demand is higher than average. The result is a steady draw of electricity from the electricity grid and a lower monthly peak demand charge.\nEnergy Arbitrage\nESaaS actively monitors local electricity spot prices to store energy when the price is low to be utilized when electricity prices are high. This is commonly referred to as arbitrage. The net different in price results in cost savings.\nMarket Ancillary Services\nESaaS enables facilities to participate in the local ISO/RTO markets to provide services such as frequency regulation, operating reserve, and dispatchable generation. By participating in the local market, facilities can generate revenue through the ESaaS contract.\nTransmission Support\nESaaS may provide services to ease congestion and constraint on electricity transmission networks by storing energy during heavy transmission periods to be released during less congested periods. The use of this service can prolong the life of infrastructure and defers system upgrades.\n\n\n== Markets served ==\nESaaS primarily benefits large energy consumers with an average demand of over 500 kW, although, the service may benefit smaller facilities depending on regional incentives. Current early adopters of ESaaS are manufacturers (chemical, electrical, lighting, metal, petrochemical, plastics), commercial (retail, large offices, medium offices, multi-residential, supermarkets), public facilities (colleges, universities, hotels, hospitality, schools), and resources (oil & extraction, pulp & paper, metals & ore, food processing, greenhouses).\n\n\n== Benefits ==\n\n\n=== System benefactor does not require installation capital ===\nTo participate in an ESaaS service, the installation system benefactor does not require any capital outlay. Upon installing an ESaaS service, the facility sees immediate savings and/or revenue generation. Initial capital is often a hurdle for facilities to adopt an energy storage system since in most cases, the payback period of an energy storage system is 5\u201310 years.\n\n\n=== System operated by a third-party system operator ===\nSource:ESaaS is a contracted service that is automatically controlled by a third party. This eliminates responsibility for the facility to allocate resources to manage their energy profile allowing a facility to operate their core business. The system operators have knowledge of local electricity sectors that continually monitor and update system protocols as regional markets change. The information is used to optimize the value realized by the ESaaS system while still meeting facility requirements.\n\n\n=== Environmental ===\nFor most ESaaS services, energy is stored during night time, off-peak hours when energy production is created from non-carbon emitting sources. The energy is then used to offset the required carbon emitting production during peak-times. The load shifting capability provided by ESaaS displaces heavy emitting generation requirements.\n\n\n== Pricing ==\nESaaS contracts may be structured as a cost sharing model or a fixed monthly price over a contracted term. Cost sharing models share the economical benefits of ESaaS after they are realized by the customer. The fixed price is based on potential economic benefit and applicable programs in the region of deployment. The ESaaS contract price is always less than the economic value provided by the service to ensure the client retains a net positive value through the service.\n\n\n== References =="}, {"id": 37, "title": "Post-quantum cryptography", "content": "Post-quantum cryptography (PQC) (sometimes referred to as quantum-proof, quantum-safe or quantum-resistant) is the development of cryptographic algorithms (usually public-key algorithms) that are thought to be secure against a cryptanalytic attack by a quantum computer. The problem with currently popular algorithms is that their security relies on one of three hard mathematical problems: the integer factorization problem, the discrete logarithm problem or the elliptic-curve discrete logarithm problem. All of these problems could be easily solved on a sufficiently powerful quantum computer running Shor's algorithm or even faster and less demanding (in terms of number of qubits required) alternatives.While as of 2023, quantum computers lack the processing power to break widely used cryptographic algorithms, cryptographers are designing new algorithms to prepare for Q-Day, the day when current algorithms will be vulnerable to quantum computing attacks. Their work has gained attention from academics and industry through the PQCrypto conference series hosted since 2006, several workshops on Quantum Safe Cryptography hosted by the European Telecommunications Standards Institute (ETSI), and the Institute for Quantum Computing. The rumoured existence of widespread harvest now, decrypt later programs has also been seen as a motivation for the early introduction of post-quantum algorithms, as data recorded now may still remain sensitive many years into the future.In contrast to the threat quantum computing poses to current public-key algorithms, most current symmetric cryptographic algorithms and hash functions are considered to be relatively secure against attacks by quantum computers. While the quantum Grover's algorithm does speed up attacks against symmetric ciphers, doubling the key size can effectively block these attacks. Thus post-quantum symmetric cryptography does not need to differ significantly from current symmetric cryptography.\n\n\n== Algorithms ==\nPost-quantum cryptography research is mostly focused on six different approaches:\n\n\n=== Lattice-based cryptography ===\n\nThis approach includes cryptographic systems such as learning with errors, ring learning with errors (ring-LWE), the ring learning with errors key exchange and the ring learning with errors signature, the older NTRU or GGH encryption schemes, and the newer NTRU signature and BLISS signatures. Some of these schemes like NTRU encryption have been studied for many years without anyone finding a feasible attack. Others like the ring-LWE algorithms have proofs that their security reduces to a worst-case problem. The Post Quantum Cryptography Study Group sponsored by the European Commission suggested that the Stehle\u2013Steinfeld variant of NTRU be studied for standardization rather than the NTRU algorithm. At that time, NTRU was still patented. Studies have indicated that NTRU may have more secure properties than other lattice based algorithms.\n\n\n=== Multivariate cryptography ===\n\nThis includes cryptographic systems such as the Rainbow (Unbalanced Oil and Vinegar) scheme which is based on the difficulty of solving systems of multivariate equations. Various attempts to build secure multivariate equation encryption schemes have failed. However, multivariate signature schemes like Rainbow could provide the basis for a quantum secure digital signature. The Rainbow Signature Scheme is patented.\n\n\n=== Hash-based cryptography ===\n\nThis includes cryptographic systems such as Lamport signatures, the Merkle signature scheme, the XMSS, the SPHINCS, and the WOTS schemes. Hash based digital signatures were invented in the late 1970s by Ralph Merkle and have been studied ever since as an interesting alternative to number-theoretic digital signatures like RSA and DSA. Their primary drawback is that for any hash-based public key, there is a limit on the number of signatures that can be signed using the corresponding set of private keys. This fact had reduced interest in these signatures until interest was revived due to the desire for cryptography that was resistant to attack by quantum computers. There appear to be no patents on the Merkle signature scheme and there exist many non-patented hash functions that could be used with these schemes. The stateful hash-based signature scheme XMSS developed by a team of researchers under the direction of Johannes Buchmann is described in RFC 8391.Note that all the above schemes are one-time or bounded-time signatures, Moni Naor and Moti Yung invented UOWHF hashing in 1989 and designed a signature based on hashing (the Naor-Yung scheme) which can be unlimited-time in use (the first such signature that does not require trapdoor properties).\n\n\n=== Code-based cryptography ===\nThis includes cryptographic systems which rely on error-correcting codes, such as the McEliece and Niederreiter encryption algorithms and the related Courtois, Finiasz and Sendrier Signature scheme. The original McEliece signature using random Goppa codes has withstood scrutiny for over 40 years. However, many variants of the McEliece scheme, which seek to introduce more structure into the code used in order to reduce the size of the keys, have been shown to be insecure. The Post Quantum Cryptography Study Group sponsored by the European Commission has recommended the McEliece public key encryption system as a candidate for long term protection against attacks by quantum computers.\n\n\n=== Isogeny-based cryptography ===\nThese cryptographic systems rely on the properties of isogeny graphs of elliptic curves (and higher-dimensional abelian varieties) over finite fields, in particular supersingular isogeny graphs, to create cryptographic systems. Among the more well-known representatives of this field are the Diffie-Hellman-like key exchange CSIDH which can serve as a straightforward quantum-resistant replacement for the Diffie-Hellman and elliptic curve Diffie\u2013Hellman key-exchange methods that are in widespread use today, and the signature scheme SQISign which is based on the categorical equivalence between supersingular elliptic curves and maximal orders in particular types of quaternion algebras. Another widely noticed construction, SIDH/SIKE, was spectacularly broken in 2022. The attack is however specific to the SIDH/SIKE family of schemes and does not generalize to other isogeny-based constructions.\n\n\n=== Symmetric key quantum resistance ===\nProvided one uses sufficiently large key sizes, the symmetric key cryptographic systems like AES and SNOW 3G are already resistant to attack by a quantum computer. Further, key management systems and protocols that use symmetric key cryptography instead of public key cryptography like Kerberos and the 3GPP Mobile Network Authentication Structure are also inherently secure against attack by a quantum computer. Given its widespread deployment in the world already, some researchers recommend expanded use of Kerberos-like symmetric key management as an efficient way to get post quantum cryptography today.\n\n\n== Security reductions ==\nIn cryptography research, it is desirable to prove the equivalence of a cryptographic algorithm and a known hard mathematical problem. These proofs are often called \"security reductions\", and are used to demonstrate the difficulty of cracking the encryption algorithm. In other words, the security of a given cryptographic algorithm is reduced to the security of a known hard problem. Researchers are actively looking for security reductions in the prospects for post quantum cryptography. Current results are given here:\n\n\n=== Lattice-based cryptography \u2013 Ring-LWE Signature ===\n\nIn some versions of Ring-LWE there is a security reduction to the shortest-vector problem (SVP) in a lattice as a lower bound on the security. The SVP is known to be NP-hard. Specific ring-LWE systems that have provable security reductions include a variant of Lyubashevsky's ring-LWE signatures defined in a paper by G\u00fcneysu, Lyubashevsky, and P\u00f6ppelmann. The GLYPH signature scheme is a variant of the G\u00fcneysu, Lyubashevsky, and P\u00f6ppelmann (GLP) signature which takes into account research results that have come after the publication of the GLP signature in 2012. Another Ring-LWE signature is Ring-TESLA. There also exists a \"derandomized variant\" of LWE, called Learning with Rounding (LWR), which yields \" improved speedup (by eliminating sampling small errors from a Gaussian-like distribution with deterministic errors) and bandwidth.\" While LWE utilizes the addition of a small error to conceal the lower bits, LWR utilizes rounding for the same purpose.\n\n\n=== Lattice-based cryptography \u2013 NTRU, BLISS ===\nThe security of the NTRU encryption scheme and the BLISS signature is believed to be related to, but not provably reducible to, the Closest Vector Problem (CVP) in a Lattice. The CVP is known to be NP-hard. The Post Quantum Cryptography Study Group sponsored by the European Commission suggested that the Stehle\u2013Steinfeld variant of NTRU which does have a security reduction be studied for long term use instead of the original NTRU algorithm.\n\n\n=== Multivariate cryptography \u2013 Unbalanced Oil and Vinegar ===\n\nUnbalanced Oil and Vinegar signature schemes are asymmetric cryptographic primitives based on multivariate polynomials over a finite field \n  \n    \n      \n        \n          F\n        \n      \n    \n    {\\displaystyle \\mathbb {F} }\n  . Bulygin, Petzoldt and Buchmann have shown a reduction of generic multivariate quadratic UOV systems to the NP-Hard Multivariate Quadratic Equation Solving problem.\n\n\n=== Hash-based cryptography \u2013 Merkle signature scheme ===\n\nIn 2005, Luis Garcia proved that there was a security reduction of Merkle Hash Tree signatures to the security of the underlying hash function. Garcia showed in his paper that if computationally one-way hash functions exist then the Merkle Hash Tree signature is provably secure.Therefore, if one used a hash function with a provable reduction of security to a known hard problem one would have a provable security reduction of the Merkle tree signature to that known hard problem.The Post Quantum Cryptography Study Group sponsored by the European Commission has recommended use of Merkle signature scheme for long term security protection against quantum computers.\n\n\n=== Code-based cryptography \u2013 McEliece ===\n\nThe McEliece Encryption System has a security reduction to the Syndrome Decoding Problem (SDP). The SDP is known to be NP-hard The Post Quantum Cryptography Study Group sponsored by the European Commission has recommended the use of this cryptography for long term protection against attack by a quantum computer.\n\n\n=== Code-based cryptography \u2013 RLCE ===\nIn 2016, Wang proposed a random linear code encryption scheme RLCE which is based on McEliece schemes. RLCE scheme can be constructed using any linear code such as Reed-Solomon code by inserting random columns in the underlying linear code generator matrix.\n\n\n=== Supersingular elliptic curve isogeny cryptography ===\n\nSecurity is related to the problem of constructing an isogeny between two supersingular curves with the same number of points. The most recent investigation of the difficulty of this problem is by Delfs and Galbraith indicates that this problem is as hard as the inventors of the key exchange suggest that it is. There is no security reduction to a known NP-hard problem.\n\n\n== Comparison ==\nOne common characteristic of many post-quantum cryptography algorithms is that they require larger key sizes than commonly used \"pre-quantum\" public key algorithms. There are often tradeoffs to be made in key size, computational efficiency and ciphertext or signature size. The table lists some values for different schemes at a 128 bit post-quantum security level.\n\nA practical consideration on a choice among post-quantum cryptographic algorithms is the effort required to send public keys over the internet. From this point of view, the Ring-LWE, NTRU, and SIDH algorithms provide key sizes conveniently under 1KB, hash-signature public keys come in under 5KB, and MDPC-based McEliece takes about 1KB. On the other hand, Rainbow schemes require about 125KB and Goppa-based McEliece requires a nearly 1MB key.\n\n\n=== Lattice-based cryptography \u2013 LWE key exchange and Ring-LWE key exchange ===\n\nThe fundamental idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The basic idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper appeared in 2012 after a provisional patent application was filed in 2012.\nIn 2014, Peikert presented a key transport scheme following the same basic idea of Ding's, where the new idea of sending additional 1 bit signal for rounding in Ding's construction is also utilized. For somewhat greater than 128 bits of security, Singh presents a set of parameters which have 6956-bit public keys for the Peikert's scheme. The corresponding private key would be roughly 14,000 bits.\nIn 2015, an authenticated key exchange with provable forward security following the same basic idea of Ding's was presented at Eurocrypt 2015, which is an extension of the HMQV construction in Crypto2005. The parameters for different security levels from 80 bits to 350 bits, along with the corresponding key sizes are provided in the paper.\n\n\n=== Lattice-based cryptography \u2013 NTRU encryption ===\n\nFor 128 bits of security in NTRU, Hirschhorn, Hoffstein, Howgrave-Graham and Whyte, recommend using a public key represented as a degree 613 polynomial with coefficients \n  \n    \n      \n        \n          mod\n          \n            \n              (\n              \n                2\n                \n                  10\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bmod {\\left(2^{10}\\right)}}}\n  . This results in a public key of 6130 bits. The corresponding private key would be 6743 bits.\n\n\n=== Multivariate cryptography \u2013 Rainbow signature ===\n\nFor 128 bits of security and the smallest signature size in a Rainbow multivariate quadratic equation signature scheme, Petzoldt, Bulygin and Buchmann, recommend using equations in \n  \n    \n      \n        \n          \n            F\n          \n          \n            31\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {F} _{31}}\n   with a public key size of just over 991,000 bits, a private key of just over 740,000 bits and digital signatures which are 424 bits in length.\n\n\n=== Hash-based cryptography \u2013 Merkle signature scheme ===\n\nIn order to get 128 bits of security for hash based signatures to sign 1 million messages using the fractal Merkle tree method of Naor Shenhav and Wool the public and private key sizes are roughly 36,000 bits in length.\n\n\n=== Code-based cryptography \u2013 McEliece ===\n\nFor 128 bits of security in a McEliece scheme, The European Commissions Post Quantum Cryptography Study group recommends using a binary Goppa code of length at least \n  \n    \n      \n        n\n        =\n        6960\n      \n    \n    {\\displaystyle n=6960}\n   and dimension at least \n  \n    \n      \n        k\n        =\n        5413\n      \n    \n    {\\displaystyle k=5413}\n  , and capable of correcting \n  \n    \n      \n        t\n        =\n        119\n      \n    \n    {\\displaystyle t=119}\n   errors. With these parameters the public key for the McEliece system will be a systematic generator matrix whose non-identity part takes \n  \n    \n      \n        k\n        \u00d7\n        (\n        n\n        \u2212\n        k\n        )\n        =\n        8373911\n      \n    \n    {\\displaystyle k\\times (n-k)=8373911}\n   bits. The corresponding private key, which consists of the code support with \n  \n    \n      \n        n\n        =\n        6960\n      \n    \n    {\\displaystyle n=6960}\n   elements from \n  \n    \n      \n        G\n        F\n        (\n        \n          2\n          \n            13\n          \n        \n        )\n      \n    \n    {\\displaystyle GF(2^{13})}\n   and a generator polynomial of with \n  \n    \n      \n        t\n        =\n        119\n      \n    \n    {\\displaystyle t=119}\n   coefficients from \n  \n    \n      \n        G\n        F\n        (\n        \n          2\n          \n            13\n          \n        \n        )\n      \n    \n    {\\displaystyle GF(2^{13})}\n  , will be 92,027 bits in lengthThe group is also investigating the use of Quasi-cyclic MDPC codes of length at least \n  \n    \n      \n        n\n        =\n        \n          2\n          \n            16\n          \n        \n        +\n        6\n        =\n        65542\n      \n    \n    {\\displaystyle n=2^{16}+6=65542}\n   and dimension at least \n  \n    \n      \n        k\n        =\n        \n          2\n          \n            15\n          \n        \n        +\n        3\n        =\n        32771\n      \n    \n    {\\displaystyle k=2^{15}+3=32771}\n  , and capable of correcting \n  \n    \n      \n        t\n        =\n        264\n      \n    \n    {\\displaystyle t=264}\n   errors. With these parameters the public key for the McEliece system will be the first row of a systematic generator matrix whose non-identity part takes \n  \n    \n      \n        k\n        =\n        32771\n      \n    \n    {\\displaystyle k=32771}\n   bits. The private key, a quasi-cyclic parity-check matrix with \n  \n    \n      \n        d\n        =\n        274\n      \n    \n    {\\displaystyle d=274}\n   nonzero entries on a column (or twice as much on a row), takes no more than \n  \n    \n      \n        d\n        \u00d7\n        16\n        =\n        4384\n      \n    \n    {\\displaystyle d\\times 16=4384}\n   bits when represented as the coordinates of the nonzero entries on the first row.\nBarreto et al. recommend using a binary Goppa code of length at least \n  \n    \n      \n        n\n        =\n        3307\n      \n    \n    {\\displaystyle n=3307}\n   and dimension at least \n  \n    \n      \n        k\n        =\n        2515\n      \n    \n    {\\displaystyle k=2515}\n  , and capable of correcting \n  \n    \n      \n        t\n        =\n        66\n      \n    \n    {\\displaystyle t=66}\n   errors. With these parameters the public key for the McEliece system will be a systematic generator matrix whose non-identity part takes \n  \n    \n      \n        k\n        \u00d7\n        (\n        n\n        \u2212\n        k\n        )\n        =\n        1991880\n      \n    \n    {\\displaystyle k\\times (n-k)=1991880}\n   bits. The corresponding private key, which consists of the code support with \n  \n    \n      \n        n\n        =\n        3307\n      \n    \n    {\\displaystyle n=3307}\n   elements from \n  \n    \n      \n        G\n        F\n        (\n        \n          2\n          \n            12\n          \n        \n        )\n      \n    \n    {\\displaystyle GF(2^{12})}\n   and a generator polynomial of with \n  \n    \n      \n        t\n        =\n        66\n      \n    \n    {\\displaystyle t=66}\n   coefficients from \n  \n    \n      \n        G\n        F\n        (\n        \n          2\n          \n            12\n          \n        \n        )\n      \n    \n    {\\displaystyle GF(2^{12})}\n  , will be 40,476 bits in length.\n\n\n=== Supersingular elliptic curve isogeny cryptography ===\n\nFor 128 bits of security in the supersingular isogeny Diffie-Hellman (SIDH) method, De Feo, Jao and Plut recommend using a supersingular curve modulo a 768-bit prime. If one uses elliptic curve point compression the public key will need to be no more than 8x768 or 6144 bits in length. A March 2016 paper by authors Azarderakhsh, Jao, Kalach, Koziel, and Leonardi showed how to cut the number of bits transmitted in half, which was further improved by authors Costello, Jao, Longa, Naehrig, Renes and Urbanik resulting in a compressed-key version of the SIDH protocol with public keys only 2640 bits in size. This makes the number of bits transmitted roughly equivalent to the non-quantum secure RSA and Diffie-Hellman at the same classical security level.\n\n\n=== Symmetric\u2013key-based cryptography ===\nAs a general rule, for 128 bits of security in a symmetric-key-based system, one can safely use key sizes of 256 bits. The best quantum attack against generic symmetric-key systems is an application of Grover's algorithm, which requires work proportional to the square root of the size of the key space. To transmit an encrypted key to a device that possesses the symmetric key necessary to decrypt that key requires roughly 256 bits as well. It is clear that symmetric-key systems offer the smallest key sizes for post-quantum cryptography.\n\n\n== Forward secrecy ==\nA public-key system demonstrates a property referred to as perfect forward secrecy when it generates random public keys per session for the purposes of key agreement. This means that the compromise of one message cannot lead to the compromise of others, and also that there is not a single secret value which can lead to the compromise of multiple messages. Security experts recommend using cryptographic algorithms that support forward secrecy over those that do not. The reason for this is that forward secrecy can protect against the compromise of long term private keys associated with public/private key pairs. This is viewed as a means of preventing mass surveillance by intelligence agencies.\nBoth the Ring-LWE key exchange and supersingular isogeny Diffie-Hellman (SIDH) key exchange can support forward secrecy in one exchange with the other party. Both the Ring-LWE and SIDH can also be used without forward secrecy by creating a variant of the classic ElGamal encryption variant of Diffie-Hellman.\nThe other algorithms in this article, such as NTRU, do not support forward secrecy as is.\nAny authenticated public key encryption system can be used to build a key exchange with forward secrecy.\n\n\n== Open Quantum Safe project ==\nOpen Quantum Safe (OQS) project was started in late 2016 and has the goal of developing and prototyping quantum-resistant cryptography. It aims to integrate current post-quantum schemes in one library: liboqs. liboqs is an open source C library for quantum-resistant cryptographic algorithms. It initially focuses on key exchange algorithms but by now includes several signature schemes. It provides a common API suitable for post-quantum key exchange algorithms, and will collect together various implementations. liboqs will also include a test harness and benchmarking routines to compare performance of post-quantum implementations. Furthermore, OQS also provides integration of liboqs into OpenSSL.As of March 2023, the following key exchange algorithms are supported:\nOlder supported versions that have been removed because of the progression of the NIST Post-Quantum Cryptography Standardization Project are:\n\n\n== Implementation ==\nOne of the main challenges in post-quantum cryptography is considered to be the implementation of potentially quantum safe algorithms into existing systems. There are tests done, for example by Microsoft Research implementing PICNIC in a PKI using Hardware security modules. Test implementations for Google's NewHope algorithm have also been done by HSM vendors. In August 2023, Google released a FIDO2 security key implementation of an ECC/Dilithium hybrid signature schema which was done in partnership with ETH Z\u00fcrich.Other notable implementations include:\n\nbouncycastle\nliboqs\n\n\n== See also ==\nNIST Post-Quantum Cryptography Standardization\nQuantum cryptography \u2013 cryptography based on quantum mechanics\nCrypto-shredding \u2013 Deleting encryption keys\nHarvest now, decrypt later\n\n\n== References ==\n\n\n== Further reading ==\nPost-Quantum Cryptography. Springer. 2008. p. 245. ISBN 978-3-540-88701-0.\nIsogenies in a Quantum World\nOn Ideal Lattices and Learning With Errors Over Rings\nKerberos Revisited: Quantum-Safe Authentication\nThe picnic signature scheme\n\n\n== External links ==\nPQCrypto, the post-quantum cryptography conference\nETSI Quantum Secure Standards Effort\nNIST's Post-Quantum crypto Project\nPQCrypto Usage & Deployment\nISO 27001 Certification Cost"}, {"id": 38, "title": "Cricket", "content": "Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 22-yard (20-metre) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at one of the wickets with the bat and then running between the wickets, while the bowling and fielding side tries to prevent this (by preventing the ball from leaving the field, and getting the ball to either wicket) and dismiss each batter (so they are \"out\"). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side either catching the ball after it is hit by the bat, but before it hits the ground, or hitting a wicket with the ball before a batter can cross the crease in front of the wicket. When ten batters have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.\nForms of cricket range from Twenty20 (also known as T20), with each team batting for a single innings of 20 overs (each \"over\" being a set of 6 fair opportunities for the batting team to score) and the game generally lasting three to four hours, to Test matches played over five days. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core layered with tightly wound string.\nThe earliest known definite reference to cricket is to it being played in South East England in the mid-16th century. It spread globally with the expansion of the British Empire, with the first international matches in the second half of the 19th century. The game's governing body is the International Cricket Council (ICC), which has over 100 members, twelve of which are full members who play Test matches. The game's rules, the Laws of Cricket, are maintained by Marylebone Cricket Club (MCC) in London. The sport is followed primarily in South Asia, Australia, New Zealand, the United Kingdom, Southern Africa and the West Indies.Women's cricket, which is organised and played separately, has also achieved international standard. \nThe most successful side playing international cricket is Australia, which has won eight One Day International trophies, including six World Cups, more than any other country and has been the top-rated Test side more than any other country.\n\n\n== History ==\n\n\n=== Origins ===\n\nCricket is one of many games in the \"club ball\" sphere that basically involve hitting a ball with a hand-held implement; others include baseball (which shares many similarities with cricket, both belonging in the more specific bat-and-ball games category), golf, hockey, tennis, squash, badminton and table tennis. In cricket's case, a key difference is the existence of a solid target structure, the wicket (originally, it is thought, a \"wicket gate\" through which sheep were herded), that the batter must defend. The cricket historian Harry Altham identified three \"groups\" of \"club ball\" games: the \"hockey group\", in which the ball is driven to and from between two targets (the goals); the \"golf group\", in which the ball is driven towards an undefended target (the hole); and the \"cricket group\", in which \"the ball is aimed at a mark (the wicket) and driven away from it\".It is generally believed that cricket originated as a children's game in the south-eastern counties of England, sometime during the medieval period. Although there are claims for prior dates, the earliest definite reference to cricket being played comes from evidence given at a court case in Guildford in January 1597 (Old Style, equating to January 1598 in the modern calendar). The case concerned ownership of a certain plot of land and the court heard the testimony of a 59-year-old coroner, John Derrick, who gave witness that:\nBeing a scholler in the ffree schoole of Guldeford hee and diverse of his fellows did runne and play there at creckett and other plaies.\nGiven Derrick's age, it was about half a century earlier when he was at school and so it is certain that cricket was being played c.\u20091550 by boys in Surrey. The view that it was originally a children's game is reinforced by Randle Cotgrave's 1611 English-French dictionary in which he defined the noun \"crosse\" as \"the crooked staff wherewith boys play at cricket\" and the verb form \"crosser\" as \"to play at cricket\".One possible source for the sport's name is the Old English word \"cryce\" (or \"cricc\") meaning a crutch or staff. In Samuel Johnson's Dictionary, he derived cricket from \"cryce, Saxon, a stick\". In Old French, the word \"criquet\" seems to have meant a kind of club or stick. Given the strong medieval trade connections between south-east England and the County of Flanders when the latter belonged to the Duchy of Burgundy, the name may have been derived from the Middle Dutch (in use in Flanders at the time) \"krick\"(-e), meaning a stick (crook). Another possible source is the Middle Dutch word \"krickstoel\", meaning a long low stool used for kneeling in church and which resembled the long low wicket with two stumps used in early cricket. According to Heiner Gillmeister, a European language expert of Bonn University, \"cricket\" derives from the Middle Dutch phrase for hockey, met de (krik ket)sen (i.e., \"with the stick chase\"). Gillmeister has suggested that not only the name but also the sport itself may be of Flemish origin.\n\n\n=== Growth of amateur and professional cricket in England ===\nAlthough the main object of the game has always been to score the most runs, the early form of cricket differed from the modern game in certain key technical aspects; the North American variant of cricket known as wicket retained many of these aspects. The ball was bowled underarm by the bowler and along the ground towards a batter armed with a bat that in shape resembled a hockey stick; the batter defended a low, two-stump wicket; and runs were called notches because the scorers recorded them by notching tally sticks.In 1611, the year Cotgrave's dictionary was published, ecclesiastical court records at Sidlesham in Sussex state that two parishioners, Bartholomew Wyatt and Richard Latter, failed to attend church on Easter Sunday because they were playing cricket. They were fined 12d each and ordered to do penance. This is the earliest mention of adult participation in cricket and it was around the same time that the earliest known organised inter-parish or village match was played \u2013 at Chevening, Kent. In 1624, a player called Jasper Vinall died after he was accidentally struck on the head during a match between two parish teams in Sussex.Cricket remained a low-key local pursuit for much of the 17th century. It is known, through numerous references found in the records of ecclesiastical court cases, to have been proscribed at times by the Puritans before and during the Commonwealth. The problem was nearly always the issue of Sunday play as the Puritans considered cricket to be \"profane\" if played on the Sabbath, especially if large crowds or gambling were involved.According to the social historian Derek Birley, there was a \"great upsurge of sport after the Restoration\" in 1660. Several members of the court of King Charles II took a strong interest in cricket during that era. Gambling on sport became a problem significant enough for Parliament to pass the 1664 Gambling Act, limiting stakes to \u00a3100 which was, in any case, a colossal sum exceeding the annual income of 99% of the population. Along with horse racing, as well as prizefighting and other types of blood sport, cricket was perceived to be a gambling sport. Rich patrons made matches for high stakes, forming teams in which they engaged the first professional players. By the end of the century, cricket had developed into a major sport that was spreading throughout England and was already being taken abroad by English mariners and colonisers \u2013 the earliest reference to cricket overseas is dated 1676. A 1697 newspaper report survives of \"a great cricket match\" played in Sussex \"for fifty guineas apiece\" \u2013 this is the earliest known contest that is generally considered a First Class match.The patrons, and other players from the social class known as the \"gentry\", began to classify themselves as \"amateurs\" to establish a clear distinction from the professionals, who were invariably members of the working class, even to the point of having separate changing and dining facilities. The gentry, including such high-ranking nobles as the Dukes of Richmond, exerted their honour code of noblesse oblige to claim rights of leadership in any sporting contests they took part in, especially as it was necessary for them to play alongside their \"social inferiors\" if they were to win their bets. In time, a perception took hold that the typical amateur who played in first-class cricket, until 1962 when amateurism was abolished, was someone with a public school education who had then gone to one of Cambridge or Oxford University \u2013 society insisted that such people were \"officers and gentlemen\" whose destiny was to provide leadership. In a purely financial sense, the cricketing amateur would theoretically claim expenses for playing while his professional counterpart played under contract and was paid a wage or match fee; in practice, many amateurs claimed more than actual expenditure and the derisive term \"shamateur\" was coined to describe the practice.\n\n\n=== English cricket in the 18th and 19th centuries ===\nThe game underwent major development in the 18th century to become England's national sport. Its success was underwritten by the twin necessities of patronage and betting. Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury. The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season. Bowling underwent an evolution around 1760 when bowlers began to pitch the ball instead of rolling or skimming it towards the batter. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old \"hockey stick\" shape.The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point. MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century included the three stump wicket and leg before wicket (lbw).The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial. Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839. In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.\nThe most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.The last two decades before the First World War have been called the \"Golden Age of cricket\". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.\n\n\n=== Cricket becomes an international sport ===\nIn 1844, the first-ever international match took place between what were essentially club teams, from the United States and Canada, in Toronto; Canada won. In 1859, a team of English players went to North America on the first overseas tour. Meanwhile, the British Empire had been instrumental in spreading the game overseas and by the middle of the 19th century it had become well established in Australia, the Caribbean, British India (which includes present-day Pakistan and Bangladesh), New Zealand, North America and South Africa.In 1862, an English team made the first tour of Australia. The first Australian team to travel overseas consisted of Aboriginal stockmen which toured England in 1868.In 1876\u201377, an England team took part in what was retrospectively recognized as the first-ever Test match at the Melbourne Cricket Ground against Australia. The rivalry between England and Australia gave birth to The Ashes in 1882, and this has remained Test cricket's most famous contest. Test cricket began to expand in 1888\u201389 when South Africa played England.\n\n\n=== World cricket in the 20th century ===\nThe inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batter of all time. Test cricket continued to expand during the 20th century with the addition of the West Indies (1928), New Zealand (1930) and India (1932) before the Second World War and then Pakistan (1952), Sri Lanka (1982), Zimbabwe (1992), Bangladesh (2000), Ireland and Afghanistan (both 2018) in the post-war period. South Africa was banned from international cricket from 1970 to 1992 as part of the apartheid boycott.\n\n\n=== The rise of limited overs cricket ===\nCricket entered a new era in 1963 when English counties introduced the limited overs variant. As it was sure to produce a result, limited overs cricket was lucrative and the number of matches increased. The first Limited Overs International was played in 1971 and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975. In the 21st century, a new limited overs form, Twenty20, made an immediate impact. On 22 June 2017, Afghanistan and Ireland became the 11th and 12th ICC full members, enabling them to play Test cricket.\n\n\n== Laws and gameplay ==\n\nIn cricket, the rules of the game are specified in a code called The Laws of Cricket (hereinafter called \"the Laws\") which has a global remit. There are 42 Laws (always written with a capital \"L\"). The earliest known version of the code was drafted in 1744 and, since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.\n\n\n=== Playing area ===\n\nCricket is a bat-and-ball game played on a cricket field (see image of cricket pitch and creases) between two teams of eleven players each. The field is usually circular or oval in shape and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line or a combination of these; the boundary must if possible be marked along its entire length.In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart. The pitch is a flat surface 10 feet (3.0 m) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.\nAs illustrated, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a twelve-foot line (six feet either side of the wicket), it is, in fact, unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an eight-foot line, so that it extends four feet behind the bowling crease, but is also, in fact, unlimited in length.\n\n\n=== Match structure and closure ===\n\nBefore a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings. Innings is the term used for each phase of play in the match. In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batters. When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day. During an innings, all eleven members of the fielding team take the field, but usually only two members of the batting team are on the field at any given time. The exception to this is if a batter has any type of illness or injury restricting his or her ability to run, in this case the batter is allowed a runner who can run between the wickets when the batter hits a scoring run or runs, though this does not apply in international cricket. The order of batters is usually announced just before the match, but it can be varied.The main objective of each team is to score more runs than their opponents but, in some forms of cricket, it is also necessary to dismiss all of the opposition batters in their final innings in order to win the match, which would otherwise be drawn. If the team batting last is all out having scored fewer runs than their opponents, they are said to have \"lost by n runs\" (where n is the difference between the aggregate number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have \"won by n wickets\", where n is the number of wickets left to fall. For example, a team that passes its opponents' total having lost six wickets (i.e., six of their batters have been dismissed) have won the match \"by four wickets\".In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have \"won by an innings and n runs\", and does not need to bat again: n is the difference between the two teams' aggregate scores. If the team batting last is all out, and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side with only 62 happening in first-class matches from the earliest known instance in 1741 until January 2017. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.If the match has only a single innings per side, then usually a maximum number of overs applies to each innings. Such a match is called a \"limited overs\" or \"one-day\" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. In some cases, ties are broken by having each team bat for a one-over innings known as a Super Over; subsequent Super Overs may be played if the first Super Over ends in a tie. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth\u2013Lewis\u2013Stern method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a \"no-result\" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible; for example, wet weather.In all forms of cricket, the umpires can abandon the match if bad light or rain makes it impossible to continue. There have been instances of entire matches, even Test matches scheduled to be played over five days, being lost to bad weather without a ball being bowled: for example, the third Test of the 1970/71 series in Australia.\n\n\n==== Innings ====\n\nThe innings (ending with 's' in both singular and plural form) is the term used for each phase of play during a match. Depending on the type of match being played, each team has either one or two innings. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. The innings terminates if the batting team is \"all out\", a term defined by the Laws: \"at the fall of a wicket or the retirement of a batter, further balls remain to be bowled but no further batter is available to come in\". In this situation, one of the batters has not been dismissed and is termed not out; this is because he has no partners left and there must always be two active batters while the innings is in progress.\nAn innings may end early while there are still two not out batters:\nthe batting team's captain may declare the innings closed even though some of his players have not had a turn to bat: this is a tactical decision by the captain, usually because he believes his team have scored sufficient runs and need time to dismiss the opposition in their innings\nthe set number of overs (i.e., in a limited overs match) have been bowled\nthe match has ended prematurely due to bad weather or running out of time\nin the final innings of the match, the batting side has reached its target and won the game.\n\n\n===== Overs =====\n\nThe Laws state that, throughout an innings, \"the ball shall be bowled from each end alternately in overs of 6 balls\". The name \"over\" came about because the umpire calls \"Over!\" when six balls have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batters do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a \"spell\". The batters do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice versa. The umpires also change positions so that the one who was at \"square leg\" now stands behind the wicket at the non-striker's end and vice versa.\n\n\n=== Clothing and equipment ===\n\nThe wicket-keeper (a specialised fielder behind the batter) and the batters wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head and a box for male players inside the trousers (to protect the crotch area). Some batters wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batter (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woolen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white and this remains the case in Test and first-class cricket but, in limited overs cricket, team colours are worn instead.\n\n\n==== Bat and ball ====\n\nThe essence of the sport is that a bowler delivers (i.e., bowls) the ball from his or her end of the pitch towards the batter who, armed with a bat, is \"on strike\" at the other end (see next sub-section: Basic gameplay).\nThe bat is made of wood, usually Salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than 4.25 inches (10.8 cm) wide and the total length of the bat not more than 38 inches (97 cm). There is no standard for the weight, which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).The ball is a hard leather-seamed spheroid, with a circumference of 9 inches (23 cm). The ball has a \"seam\": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable;  during the course of this deterioration, its behaviour in flight will change and can influence the outcome of the match. Players will, therefore, attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva is legal, even when the polishing is deliberately done on one side only to increase the ball's swing through the air, but the acts of rubbing other substances into the ball, scratching the surface or picking at the seam are illegal ball tampering.\n\n\n=== Player roles ===\n\n\n==== Basic gameplay: bowler to batter ====\nDuring normal play, thirteen players and two umpires are on the field. Two of the players are batters and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch.\nIn the photo, the two batters (3 & 8; wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 & 11; wearing dark blue) are in shot. One of the two umpires (1; wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batter (8) at the other end who is called the \"striker\". The other batter (3) at the bowling end is called the \"non-striker\". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9) and behind him stands one of the fielders in a position called \"first slip\" (11). While the bowler and the first slip are wearing conventional kit only, the two batters and the wicket-keeper are wearing protective gear including safety helmets, padded gloves and leg guards (pads).\nWhile the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called \"square leg\", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or, at least, to prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, to hit the ball away from the pitch in order to score runs.\nSome players are skilled in both batting and bowling, or as either of these as well as wicket-keeping, so are termed all-rounders.  Bowlers are classified according to their style, generally as fast bowlers, seam bowlers or spinners. Batters are classified according to whether they are right-handed or left-handed.\n\n\n==== Fielding ====\n\nOf the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of him, but the substitute cannot bowl or act as a captain, except in the case of concussion substitutes in international cricket. The substitute leaves the field when the injured player is fit to return. The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers.\n\n\n==== Bowling and dismissal ====\n\nMost bowlers are considered specialists in that they are selected for the team because of their skill as a bowler, although some are all-rounders and even specialist batters bowl occasionally. The specialists bowl several times during an innings but may not bowl two overs consecutively. If the captain wants a bowler to \"change ends\", another bowler must temporarily fill in so that the change is not immediate.A bowler reaches his delivery stride by means of a \"run-up\" and an over is deemed to have begun when the bowler starts his run-up for the first delivery of that over, the ball then being \"in play\". Fast bowlers, needing momentum, take a lengthy run up while bowlers with a slow delivery take no more than a couple of steps before bowling. The fastest bowlers can deliver the ball at a speed of over 145 kilometres per hour (90 mph) and they sometimes rely on sheer speed to try to defeat the batter, who is forced to react very quickly. Other fast bowlers rely on a mixture of speed and guile by making the ball seam or swing (i.e. curve) in flight. This type of delivery can deceive a batter into miscuing his shot, for example, so that the ball just touches the edge of the bat and can then be \"caught behind\" by the wicket-keeper or a slip fielder. At the other end of the bowling scale is the spin bowler who bowls at a relatively slow pace and relies entirely on guile to deceive the batter. A spinner will often \"buy his wicket\" by \"tossing one up\" (in a slower, steeper parabolic path) to lure the batter into making a poor shot. The batter has to be very wary of such deliveries as they are often \"flighted\" or spun so that the ball will not behave quite as he expects and he could be \"trapped\" into getting himself out. In between the pacemen and the spinners are the medium paced seamers who rely on persistent accuracy to try to contain the rate of scoring and wear down the batter's concentration.There are nine ways in which a batter can be dismissed: five relatively common and four extremely rare. The common forms of dismissal are bowled, caught, leg before wicket (lbw), run out and stumped. Rare methods are hit wicket, hit the ball twice, obstructing the field and timed out. The Laws state that the fielding team, usually the bowler in practice, must appeal for a dismissal before the umpire can give his decision. If the batter is out, the umpire raises a forefinger and says \"Out!\"; otherwise, he will shake his head and say \"Not out\". There is, effectively, a tenth method of dismissal, retired out, which is not an on-field dismissal as such but rather a retrospective one for which no fielder is credited.\n\n\n==== Batting, runs and extras ====\n\nBatters take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team. Substitute batters are generally not allowed, except in the case of concussion substitutes in international cricket.In order to begin batting the batter first adopts a batting stance. Standardly, this involves adopting a slight crouch with the feet pointing across the front of the wicket, looking in the direction of the bowler, and holding the bat so it passes over the feet and so its tip can rest on the ground near to the toes of the back foot.A skilled batter can use a wide array of \"shots\" or \"strokes\" in both defensive and attacking mode. The idea is to hit the ball to the best effect with the flat surface of the bat's blade. If the ball touches the side of the bat it is called an \"edge\". The batter does not have to play a shot and can allow the ball to go through to the wicketkeeper. Equally, he does not have to attempt a run when he hits the ball with his bat. Batters do not always seek to hit the ball as hard as possible, and a good player can score runs just by making a deft stroke with a turn of the wrists or by simply \"blocking\" the ball but directing it away from fielders so that he has time to take a run. A wide variety of shots are played, the batter's repertoire including strokes named according to the style of swing and the direction aimed: e.g., \"cut\", \"drive\", \"hook\", \"pull\".The batter on strike (i.e. the \"striker\") must prevent the ball hitting the wicket, and try to score runs by hitting the ball with his bat so that he and his partner have time to run from one end of the pitch to the other before the fielding side can return the ball. To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batters carry their bats as they run). Each completed run increments the score of both the team and the striker.\nThe decision to attempt a run is ideally made by the batter who has the better view of the ball's progress, and this is communicated by calling: usually \"yes\", \"no\" or \"wait\". More than one run can be scored from a single hit: hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more. To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batters do not need to run. Hits for five are unusual and generally rely on the help of \"overthrows\" by a fielder returning the ball. \nIf an odd number of runs is scored by the striker, the two batters have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.Additional runs can be gained by the batting team as extras (called \"sundries\" in Australia) due to errors made by the fielding side. This is achieved in four ways: no-ball, a penalty of one extra conceded by the bowler if he breaks the rules; wide, a penalty of one extra conceded by the bowler if he bowls so that the ball is out of the batter's reach; bye, an extra awarded if the batter misses the ball and it goes past the wicket-keeper and gives the batters time to run in the conventional way; leg bye, as for a bye except that the ball has hit the batter's body, though not his bat. If the bowler has conceded a no-ball or a wide, his team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again and hence the batting side has the opportunity to score more runs from this extra ball.\n\n\n==== Specialist roles ====\n\nThe captain is often the most experienced player in the team, certainly the most tactically astute, and can possess any of the main skillsets as a batter, a bowler or a wicket-keeper. Within the Laws, the captain has certain responsibilities in terms of nominating his players to the umpires before the match and ensuring that his players conduct themselves \"within the spirit and traditions of the game as well as within the Laws\".The wicket-keeper (sometimes called simply the \"keeper\") is a specialist fielder subject to various rules within the Laws about his equipment and demeanour. He is the only member of the fielding side who can effect a stumping and is the only one permitted to wear gloves and external leg guards.Depending on their primary skills, the other ten players in the team tend to be classified as specialist batters or specialist bowlers. Generally, a team will include five or six specialist batters and four or five specialist bowlers, plus the wicket-keeper.\n\n\n=== Umpires and scorers ===\n\nThe game on the field is regulated by the two umpires, one of whom stands behind the wicket at the bowler's end, the other in a position called \"square leg\" which is about 15\u201320 metres away from the batter on strike and in line with the popping crease on which he is taking guard. The umpires have several responsibilities including adjudication on whether a ball has been correctly bowled (i.e., not a no-ball or a wide); when a run is scored; whether a batter is out (the fielding side must first appeal to the umpire, usually with the phrase \"How's that?\" or \"Owzat?\"); when intervals start and end; and the suitability of the pitch, field and weather for playing the game. The umpires are authorised to interrupt or even abandon a match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.Off the field in televised matches, there is usually a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test and Limited Overs International matches played between two ICC full member countries. These matches also have a match referee whose job is to ensure that play is within the Laws and the spirit of the game.The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire (see image, right). For example, the umpire raises a forefinger to signal that the batter is out (has been dismissed); he raises both arms above his head if the batter has hit the ball for six runs. The scorers are required by the Laws to record all runs scored, wickets taken and overs bowled; in practice, they also note significant amounts of additional data relating to the game.A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks and runs were originally called notches. According to Rowland Bowen, the earliest known scorecard templates were introduced in 1776 by T. Pratt of Sevenoaks and soon came into general use. It is believed that scorecards were printed and sold at Lord's for the first time in 1846.\n\n\n=== Spirit of the Game ===\n\nBesides observing the Laws, cricketers must respect the \"Spirit of Cricket\", a concept encompassing sportsmanship, fair play and mutual respect. This spirit has long been considered an integral part of the sport but is only nebulously defined. Amidst concern that the spirit was weakening, in 2000 a Preamble was added to the Laws instructing all participants to play within the spirit of the game. The Preamble was last updated in 2017, now opening with the line:\n\"Cricket owes much of its appeal and enjoyment to the fact that it should be played not only\naccording to the Laws, but also within the Spirit of Cricket\".\nThe Preamble is a short statement intended to emphasise the \"positive behaviours that make cricket an exciting game that encourages leadership, friendship, and teamwork.\" Its second line states that \"the major responsibility for ensuring fair play rests with the captains, but extends to all players, match officials and, especially in junior cricket, teachers, coaches and parents.\"The umpires are the sole judges of fair and unfair play. They are required under the Laws to intervene in case of dangerous or unfair play or in cases of unacceptable conduct by a player.\nPrevious versions of the Spirit identified actions that were deemed contrary (for example, appealing knowing that the batter is not out) but all specifics are now covered in the Laws of Cricket, the relevant governing playing regulations and disciplinary codes, or left to the judgement of the umpires, captains, their clubs and governing bodies. The terse expression of the Spirit of Cricket now avoids the diversity of cultural conventions that exist in the detail of sportsmanship \u2013 or its absence.\n\n\n== Women's cricket ==\n\nWomen's cricket was first recorded in Surrey in 1745. International development began at the start of the 20th century and the first Test match was played between Australia and England in December 1934. The following year, New Zealand joined them, and in 2007 Netherland became the tenth women's Test nation when they made their debut against South Africa. In 1958, the International Women's Cricket Council was founded (it merged with the ICC in 2005). In 1973, the first Cricket World Cup of any kind took place when a Women's World Cup was held in England.  In 2005, the International Women's Cricket Council was merged with the International Cricket Council (ICC) to form one unified body to help manage and develop cricket. The ICC Women's Rankings were launched on 1 October 2015 covering all three formats of women's cricket. In October 2018 following the ICC's decision to award T20 International status to all members, the Women's rankings were split into separate ODI (for Full Members) and T20I lists.\n\n\n== Governance ==\n\nThe International Cricket Council (ICC), which has its headquarters in Dubai, is the global governing body of cricket. It was founded as the Imperial Cricket Conference in 1909 by representatives from England, Australia and South Africa, renamed the International Cricket Conference in 1965 and took up its current name in 1989. The ICC in 2017 has 105 member nations, twelve of which hold full membership and can play Test cricket. The ICC is responsible for the organisation and governance of cricket's major international tournaments, notably the men's and women's versions of the Cricket World Cup. It also appoints the umpires and referees that officiate at all sanctioned Test matches, Limited Overs Internationals and Twenty20 Internationals.\nEach member nation has a national cricket board which regulates cricket matches played in its country, selects the national squad, and organises home and away tours for the national team. In the West Indies, which for cricket purposes is a federation of nations, these matters are addressed by Cricket West Indies.The table below lists the ICC full members and their national cricket boards:\n\n\n== Forms of cricket ==\n\nCricket is a multi-faceted sport with multiple formats that can effectively be divided into first-class cricket, limited overs cricket and, historically, single wicket cricket. \nThe highest standard is Test cricket (always written with a capital \"T\") which is in effect the international version of first-class cricket and is restricted to teams representing the twelve countries that are full members of the ICC (see above). Although the term \"Test match\" was not coined until much later, Test cricket is deemed to have begun with two matches between Australia and England in the 1876\u201377 Australian season; since 1882, most Test series between England and Australia have been played for a trophy known as The Ashes. The term \"first-class\", in general usage, is applied to top-level domestic cricket. Test matches are played over five days and first-class over three to four days; in all of these matches, the teams are allotted two innings each and the draw is a valid result.Limited overs cricket is always scheduled for completion in a single day, and the teams are allotted one innings each. There are two main types: List A which normally allows fifty overs per team; and Twenty20 in which the teams have twenty overs each. Both of the limited overs forms are played internationally as Limited Overs Internationals (LOI) and Twenty20 Internationals (T20I). List A was introduced in England in the 1963 season as a knockout cup contested by the first-class county clubs. In 1969, a national league competition was established. The concept was gradually introduced to the other leading cricket countries and the first limited overs international was played in 1971. In 1975, the first Cricket World Cup took place in England. Twenty20 is a new variant of limited overs itself with the purpose being to complete the match within about three to four hours, usually in an evening session. The first Twenty20 World Championship was held in 2007. In addition, a few full-member cricket boards have decided to start leagues that are played in the T10 format, in which games are intended to last approximately 90 minutes. Limited overs matches cannot be drawn, although a tie is possible and an unfinished match is a \"no result\".Single wicket was popular in the 18th and 19th centuries and its matches were generally considered top-class. In this form, although each team may have from one to six players, there is only one batter in at a time and he must face every delivery bowled while his innings lasts. Single wicket has rarely been played since limited overs cricket began. Matches tended to have two innings per team like a full first-class one and they could end in a draw.\n\n\n== Competitions ==\nCricket is played at both the international and domestic level. There is one major international championship per format, and top-level domestic competitions mirror the three main international formats. There are now a number of T20 leagues, which have spawned a \"T20 freelancer\" phenomenon.\n\n\n=== International competitions ===\n\nMost international matches are played as parts of 'tours', when one nation travels to another for a number of weeks or months, and plays a number of matches of various sorts against the host nation. Sometimes a perpetual trophy is awarded to the winner of the Test series, the most famous of which is The Ashes.\nThe ICC also organises competitions that are for several countries at once, including the Cricket World Cup, ICC T20 World Cup and ICC Champions Trophy. A league competition for Test matches played as part of normal tours, the ICC World Test Championship, had been proposed several times, and its first instance began in 2019. A league competition for ODIs, the ICC Cricket World Cup Super League, began in August 2020 and lasted only for one edition. The ICC maintains Test rankings, ODI rankings and T20 rankings systems for the countries which play these forms of cricket.\nCompetitions for member nations of the ICC with Associate status include the ICC Intercontinental Cup, for first-class cricket matches, and the World Cricket League for one-day matches, the final matches of which now also serve as the ICC World Cup Qualifier.\nThe game's only appearance in an Olympic Games was the 1900 Olympics. However, it is scheduled to make a return, with the T20 format of the game, in the 2028 Summer Olympics in Los Angeles.\n\n\n=== National competitions ===\n\n\n==== First-class ====\n\nFirst-class cricket in England is played for the most part by the 18 county clubs which contest the County Championship. The concept of a champion county has existed since the 18th century but the official competition was not established until 1890. The most successful club has been Yorkshire, who had won 32 official titles (plus one shared) as of 2019.Australia established its national first-class championship in 1892\u201393 when the Sheffield Shield was introduced. In Australia, the first-class teams represent the various states. New South Wales has the highest number of titles.\nThe other ICC full members have national championship trophies called the Ahmad Shah Abdali 4-day Tournament (Afghanistan); the National Cricket League (Bangladesh); the Ranji Trophy (India); the Inter-Provincial Championship (Ireland); the Plunket Shield (New Zealand); the Quaid-e-Azam Trophy (Pakistan); the Currie Cup (South Africa); the Premier Trophy (Sri Lanka); the Shell Shield (West Indies); and the Logan Cup (Zimbabwe).\n\n\n==== Limited overs ====\n\n\n==== Other ====\n\n\n=== Club and school cricket ===\n\nThe world's earliest known cricket match was a village cricket meeting in Kent which has been deduced from a 1640 court case recording a \"cricketing\" of \"the Weald and the Upland\" versus \"the Chalk Hill\" at Chevening \"about thirty years since\" (i.e., c.\u20091611). Inter-parish contests became popular in the first half of the 17th century and continued to develop through the 18th with the first local leagues being founded in the second half of the 19th.At the grassroots level, local club cricket is essentially an amateur pastime for those involved but still usually involves teams playing in competitions at weekends or in the evening. Schools cricket, first known in southern England in the 17th century, has a similar scenario and both are widely played in the countries where cricket is popular. Although there can be variations in game format, compared with professional cricket, the Laws are always observed and club/school matches are therefore formal and competitive events. The sport has numerous informal variants such as French cricket. On the North American side, in 2023, Monroe Township High School, in Monroe Township, Middlesex County, New Jersey, launched the first U.S. high school cricket club.\n\n\n== Culture ==\n\n\n=== Influence on everyday life ===\nCricket has had a broad impact on popular culture, both in the Commonwealth of Nations and elsewhere. It has, for example, influenced the lexicon of these nations, especially the English language, with various phrases such as \"that's not cricket\" (that's unfair), \"had a good innings\" (lived a long life) and \"sticky wicket\". \"On a sticky wicket\" (aka \"sticky dog\" or \"glue pot\") is a metaphor used to describe a difficult circumstance. It originated as a term for difficult batting conditions in cricket, caused by a damp and soft pitch.\n\n\n=== In the arts and popular culture ===\n\nCricket is the subject of works by noted English poets, including William Blake and Lord Byron. Beyond a Boundary (1963), written by Trinidadian C. L. R. James, is often named the best book on any sport ever written.\nIn the visual arts, notable cricket paintings include Albert Chevallier Tayler's Kent vs Lancashire at Canterbury (1907) and Russell Drysdale's The Cricketers (1948), which has been called \"possibly the most famous Australian painting of the 20th century.\" French impressionist Camille Pissarro painted cricket on a visit to England in the 1890s. Francis Bacon, an avid cricket fan, captured a batter in motion. Caribbean artist Wendy Nanan's cricket images are featured in a limited edition first day cover for Royal Mail's \"World of Invention\" stamp issue, which celebrated the London Cricket Conference 1\u20133 March 2007, first international workshop of its kind and part of the celebrations leading up to the 2007 Cricket World Cup.In music, many calypsos make reference to the Sport of Cricket.\n\n\n=== Influence on other sports ===\nCricket has close historical ties with Australian rules football and many players have competed at top levels in both sports. In 1858, prominent Australian cricketer Tom Wills called for the formation of a \"foot-ball club\" with \"a code of laws\" to keep cricketers fit during the off-season. The Melbourne Football Club was founded the following year, and Wills and three other members codified the first laws of the game. It is typically played on modified cricket fields.In England, a number of association football clubs owe their origins to cricketers who sought to play football as a means of keeping fit during the winter months. Derby County was founded as a branch of the Derbyshire County Cricket Club in 1884; Aston Villa (1874) and Everton (1876) were both founded by members of church cricket teams. Sheffield United's Bramall Lane ground was, from 1854, the home of the Sheffield Cricket Club, and then of Yorkshire; it was not used for football until 1862 and was shared by Yorkshire and Sheffield United from 1889 to 1973.In the late 19th century, a former cricketer, English-born Henry Chadwick of Brooklyn, New York, was credited with devising the baseball box score (which he adapted from the cricket scorecard) for reporting game events. The first box score appeared in an 1859 issue of the Clipper. The statistical record is so central to the game's \"historical essence\" that Chadwick is sometimes referred to as \"the Father of Baseball\" because he facilitated the popularity of the sport in its early days.\n\n\n== See also ==\nGlossary of cricket terms\nWillow and StumpyRelated sports\n\nStreet cricket\nBete-ombro \u2013 Brazilian version\nPlaquita \u2013 Dominican version\nBaseball\nComparison of baseball and cricket\nStoolball\n\n\n== Footnotes ==\n\n\n== Citations ==\n\n\n== Sources ==\n\n\n== Further reading ==\nGuha, Ramachandra (2002). A Corner of a Foreign Field: The Indian History of a British Sport. London: Picador. ISBN 0-330-49117-2. OCLC 255899689.\n\n\n== External links ==\n\nCricket at Curlie\nInternational Cricket Council (ICC)\nESPNcricinfo\n\"Cricket\". Encyclop\u00e6dia Britannica Online"}, {"id": 39, "title": "Virtual community", "content": "A virtual community is a social work of individuals who connect through specific social media, potentially crossing geographical and political boundaries in order to pursue mutual interests or goals. Some of the most pervasive virtual communities are online communities operating under social networking services.\nHoward Rheingold discussed virtual communities in his book, The Virtual Community, published in 1993. The book's discussion ranges from Rheingold's adventures on The WELL, computer-mediated communication, social groups and information science. Technologies cited include Usene8t, MUDs (Multi-User Dungeon) and their derivatives MUSHes and MOOs, Internet Relay Chat (IRC), chat rooms and electronic mailing lists. Rheingold also points out the potential benefits for personal psychological well-being, as well as for society at large, of belonging to a virtual community. At the same time, it showed that job engagement positively influences virtual communities of practice engagement.Virtual communities all encourage interaction, sometimes focusing around a particular interest or just to communicate. Some virtual communities do both. Community members are allowed to interact over a shared passion through various means: message boards, chat rooms, social networking World Wide Web sites, or virtual worlds. Members usually become attached to the community world, logging in and out on sites all day every day, which can certainly become an addiction.\n\n\n== Introduction ==\nThe traditional definition of a community is of geographically circumscribed entity (neighborhoods, villages, etc.).  Virtual communities are usually dispersed geographically, and therefore are not communities under the original definition. Some online communities are linked geographically, and are known as community websites. However, if one considers communities to simply possess boundaries of some sort between their members and non-members, then a virtual community is certainly a community. Virtual communities resemble real life communities in the sense that they both provide support, information, friendship and acceptance between strangers. Being in a virtual community space you may be expected to feel a sense of belonging and a mutual attachment among the members that are in your space.\nOne of the most influential part about virtual communities is the opportunity to communicate through several media platforms or networks. Now that virtual communities exists, this had leveraged out the things we once did prior to virtual communities, such as postal services, fax machines, and even speaking on the telephone. Early research into the existence of media-based communities was concerned with the nature of reality, whether communities actually could exist through the media, which could place virtual community research into the social sciences definition of ontology. In the seventeenth century, scholars associated with the Royal Society of London formed a community through the exchange of letters. \"Community without propinquity\", coined by urban planner Melvin Webber in 1963 and \"community liberated\", analyzed by Barry Wellman in 1979 began the modern era of thinking about non-local community.  As well, Benedict Anderson's Imagined Communities in 1983, described how different technologies, such as national newspapers, contributed to the development of national and regional consciousness among early nation-states. Some authors that built their theories on Anderson's Imagined communities have been critical of the concept, claiming that all communities are based on communication and that virtual/real dichotomy is disintegrating, making use of the word \"virtual\" problematic or even obsolete.\n\n\n== Purpose ==\nVirtual communities are used for a variety of social and professional groups; interaction between community members vary from personal to purely formal. For example, an email distribution list could serve as a personal means of communicating with family and friends, and also formally to coordinate with coworkers.\n\n\n=== User experience testing to determine social codes ===\nUser experience is the ultimate goal for the program or software used by an internet community, because user experience will determine the software's success. The software for social media pages or virtual communities is structured around the users\u2019 experience and designed specifically for online use.\nUser experience testing is utilized to reveal something about the personal experience of the human being using a product or system. When it comes to testing user experience in a software interface, three main characteristics are needed: a user who is engaged, a user who is interacting with a product or interface, and defining the users\u2019 experience in ways that are and observable or measurable.\nUser experience metrics are based on a reliability and repeatability, using a consistent set of measurements to result in comparable outcomes. User experience metrics are based on user retention, using a consistent set of measurements to collect data on user experience. \nThe widespread use of the Internet and virtual communities by millions of diverse users for socializing is a phenomenon that raises new issues for researchers and developers. The vast number and diversity of individuals participating in virtual communities worldwide makes it a challenge to test usability across platforms to ensure the best overall user experience.  Some well-established measures applied to the usability framework for online communities are speed of learning, productivity, user satisfaction, how much people remember using the software, and how many errors they make.\nThe human computer interactions that are measured during a usability experience test focus on the individuals rather than their social interactions in the online community. The success of online communities depend on the integration of usability and social semiotics. Usability testing metrics can be used to determine social codes by evaluating a user's habits when interacting with a program. Social codes are established and reinforced by the regular repetition of behavioral patterns. People communicate their social identities or culture code through the work they do, the way they talk, the clothes they wear, their eating habits, domestic environments and possessions, and use of leisure time. Usability testing metrics can be used to determine social codes by evaluating a user's habits when interacting with a program.The information provided during a usability test can determine demographic factors and help define the semiotic social code. Dialogue and social interactions, support information design, navigation support, and accessibility are integral components specific to online communities. As virtual communities grow, so do the diversity of their users. However, the technologies are not made to be any more or less intuitive. Usability tests can ensure users are communicating effectively using social and semiotic codes while maintaining their social identities. Efficient communication requires a common set of signs in the minds of those seeking to communicate. As technologies evolve and mature, they tend to be used by an increasingly diverse set of users. This kind of increasing complexity and evolution of technology doesn't necessarily mean that the technologies are becoming easier to use. Usability testing in virtual communities can ensure users are communicating effectively through social and semiotic codes and maintenance of social realities and identities.\n\n\n== Effects ==\n\n\n=== On health ===\nConcerns with a virtual community's tendency to promote less socializing include: verbal aggression and inhibitions, promotion of suicide and issues with privacy. However, studies regarding the health effects of these communities did not show any negative effects. There was a high drop-out rate of participants in the study.Recent studies have looked into development of health related communities and their impact on those already suffering health issues.  These forms of social networks allow for open conversation between individuals who are going through similar experiences, whether themselves or in their family. Such sites have so grown in popularity that now many health care providers form groups for their patients by providing web areas where one may direct questions to doctors. These sites prove especially useful when related to rare medical conditions. People with rare or debilitating disorders may not be able to access support groups in their physical community, thus online communities act as primary means for such support. Online health communities can serve as supportive outlets as they facilitate connecting with others who truly understand the disease, as well as offer more practical support, such as receiving help in adjusting to life with the disease. Each patient on online health communities are on there for different reasons, as some may need quick answers to questions they have, or someone to talk to.Involvement in social communities of similar health interests has created a means for patients to develop a better understanding and behavior towards treatment and health practices. Some of these users could have very serious life-threatening issues which these personal contexts could become very helpful to these users, as the issues are very complex. Patients increasingly use such outlets, as this is providing personalized and emotional support and information, that will help them and have a better experience. The extent to which these practices have effects on health are still being studied.\nStudies on health networks have mostly been conducted on groups which typically suffer the most from extreme forms of  diseases, for example cancer patients, HIV patients, or patients with other life-threatening diseases.  It is general knowledge that one participates in online communities to interact with society and develop relationships.  Individuals who suffer from rare or severe illnesses are unable to meet physically because of distance or because it could be a risk to their health to leave a secure environment. Thus, they have turned to the internet.\nSome studies have indicated that virtual communities can provide valuable benefits to their users. Online health-focused communities were shown to offer a unique form of emotional support that differed from event-based realities and informational support networks. Growing amounts of presented material show how online communities affect the health of their users.  Apparently the creation of health communities has a positive impact on those who are ill or in need of medical information.\n\n\n=== On civic participation ===\nIt was found that young individuals  are more bored with politics and history topics, and instead are more interested in celebrity dramas and topics. Young individuals claim that \u2018voicing what you feel\u2019, doesn\u2019t mean you are \u2018being heard\u2019, so they feel the need to not participate in these engagements, as they believe they aren\u2019t being listened to anyway. Over the years, things have changed, as new forms of civic engagement and citizenship have emerged from the rise of social networking sites. Networking sites act as a medium for expression and discourse about issues in specific user communities. Online content-sharing sites have made it easy for youth as well as others to not only express themselves and their ideas through digital media, but also connect with large networked communities. Within these spaces, young people are pushing the boundaries of traditional forms of engagement such as voting and joining political organizations and creating their own ways to discuss, connect, and act in their communities.Civic engagement through online volunteering has shown to have positive effects on personal satisfaction and development. Some 84 percent of online volunteers found that their online volunteering experience had contributed to their personal development and learning.\n\n\n=== On communication ===\nIn his book The Wealth of Networks from 2006, Yochai Benkler suggests that virtual communities would \"come to represent a new form of human communal existence, providing new scope for building a shared experience of human interaction\". Although Benkler's prediction has not become entirely true, clearly communications and social relations are extremely complex within a virtual community. The two main effects that can be seen according to Benkler are a \"thickening of preexisting relations with friends, family and neighbours\" and the beginnings of the \"emergence of greater scope for limited-purpose, loose relationships\". Despite being acknowledged as \"loose\" relationships, Benkler argues that they remain meaningful.\nPrevious concerns about the effects of Internet use on community and family fell into two categories: 1) sustained, intimate human relations \"are critical to well-functioning human beings as a matter of psychological need\" and 2) people with \"social capital\" are better off than those who lack it. It leads to better results in terms of political participation. However, Benkler argues that unless Internet connections actually displace direct, unmediated, human contact, there is no basis to think that using the Internet will lead to a decline in those nourishing connections we need psychologically, or in the useful connections we make socially. Benkler continues to suggest that the nature of an individual changes over time, based on social practices and expectations. There is a shift from individuals who depend upon locally embedded, unmediated and stable social relationships to networked individuals who are more dependent upon their own combination of strong and weak ties across boundaries and weave their own fluid relationships. Manuel Castells calls this the 'networked society'.\n\n\n=== On Identity ===\nIn 1997, MCI Communications released the \"Anthem\" advertisement, heralding the internet as a utopia without age, race, or gender. Lisa Nakamura argues in chapter 16 of her 2002 book After/image of identity: Gender, Technology, and Identity Politics, that technology gives us iterations of our age, race and gender in virtual spaces, as opposed to them being fully extinguished. Nakamura uses a metaphor of \"after-images\" to describe the cultural phenomenon of expressing identity on the internet. The idea is that any performance of identity on the internet is simultaneously present and past-tense, \"posthuman and projectionary\", due to its immortality.Doctor Sherry Turkle, professor of Social Studies of Science and Technology at MIT, believes the internet is a place where actions of discrimination are less likely to occur. In her 1995 book Life on the Screen: Identity in the Age of the Internet, she argues that discrimination is easier in reality as it is easier to identify as face value, what is contrary to your norm. The internet allows for a more fluid expression of identity and thus, we become more accepting of inconsistent personae within ourselves and others. For these reasons, Turkle argues users existing in online spaces are less compelled to judge or compare ourselves to our peers, allowing people in virtual settings an opportunity to gain a greater capacity for acknowledging diversity.Nakamura argues against this view, coining the term Identity Tourism in her 1999 article Race In/For Cyberspace: Identity Tourism and Racial Passing on the Internet. Identity tourism, in the context of cyberspace, is a term used to the describe the phenomenon of users donning and doffing other-race and other-gender personae. Nakamura finds that performed behavior from these identity tourists often perpetuate stereotypes.In the 1998 book Communities in Cyberspace, authors Marc A. Smith and Peter Kollock, perceives the interactions with strangers are based upon with whom we are speaking or interacting with. Everything from clothes, voice, body language, gestures, and power, we rely on these abilities to identify others, which play a role with how we will speak or interact with them. Smith and Kollock believes that online interactions breaks away of all of the face-to-face gestures and signs that us people tend to show in front of one another. Although this is difficult to do online, it also provides space to play with one\u2019s identity.\n\n\n==== Gender ====\nThe gaming community is extremely vast and accessible to a wide variety of people, However, there are negative effects on the relationships 'gamers' have with the medium when expressing identity of gender. Doctor Adrienne Shaw notes in her 2012 article Do you identify as a gamer? Gender, race, sexuality, and gamer identity, that gender, perhaps subconsciously, plays a large role in identifying oneself as a 'gamer.'  Representation in video games have become a problem as it forget the minority of players who are not just the stereotyped white teen male gamer, as there are so many players from different backgrounds who consume these games but aren't being represented.\n\n\n== Types ==\n\n\n=== Internet-based ===\nThe explosive diffusion of the Internet since the mid-1990s fostered the proliferation of virtual communities in the form of social networking services and online communities. Virtual communities may synthesize Web 2.0 technologies with the community, and therefore have been described as Community 2.0, although strong community bonds have been forged online since the early 1970s on timeshare systems like PLATO and later on Usenet. Online communities depend upon social interaction and exchange between users online. This interaction emphasizes the reciprocity element of the unwritten social contract between community members.\n\n\n=== Internet message boards ===\nAn online message board is a forum where people can discuss thoughts or ideas on various topics or simply express an idea. Users may choose which thread, or board of discussion, they would like to read or contribute to. A user will start a discussion by making a post. Other users who choose to respond can follow the discussion by adding their own posts to that thread at any time. Unlike in spoken conversations, message boards do not usually have instantaneous responses; users actively go to the website to check for responses.\nAnyone can register to participate in an online message board. People can choose to participate in the virtual community, even if or when they choose not to contribute their thoughts and ideas. Unlike chat rooms, at least in practice, message boards can accommodate an almost infinite number of users.\nInternet users' urges to talk to and reach out to strangers online is unlike those in real-life encounters where people are hesitant and often unwilling to step in to help strangers. Studies have shown that people are more likely to intervene when they are the only one in a situation. With Internet message boards, users at their computers are alone, which might contribute to their willingness to reach out. Another possible explanation is that people can withdraw from a situation much more easily online than off. They can simply click exit or log off, whereas they would have to find a physical exit and deal with the repercussions of trying to leave a situation in real life. The lack of status that is presented with an online identity also might encourage people, because, if one chooses to keep it private, there is no associated label of gender, age, ethnicity or lifestyle.\n\n\n=== Online chat rooms ===\nShortly after the rise of interest in message boards and forums, people started to want a way of communicating with their \"communities\" in real time. The downside to message boards was that people would have to wait until another user replied to their posting, which, with people all around the world in different time frames, could take a while. The development of online chat rooms allowed people to talk to whoever was online at the same time they were. This way, messages were sent and online users could immediately respond.\nThe original development by CompuServe CB hosted forty channels in which users could talk to one another in real time. The idea of forty different channels led to the idea of chat rooms that were specific to different topics. Users could choose to join an already existent chat room they found interesting, or start a new \"room\" if they found nothing to their liking. Real-time chatting was also brought into virtual games, where people could play against one another and also talk to one another through text. Now, chat rooms can be found on all sorts of topics, so that people can talk with others who share similar interests. Chat rooms are now provided by Internet Relay Chat (IRC) and other individual websites such as Yahoo, MSN, and AOL.\nChat room users communicate through text-based messaging. Most chat room providers are similar and include an input box, a message window, and a participant list. The input box is where users can type their text-based message to be sent to the providing server. The server will then transmit the message to the computers of anyone in the chat room so that it can be displayed in the message window. The message window allows the conversation to be tracked and usually places a time stamp once the message is posted. There is usually a list of the users who are currently in the room, so that people can see who is in their virtual community.\nUsers can communicate as if they are speaking to one another in real life. This \"simulated reality\" attribute makes it easy for users to form a virtual community, because chat rooms allow users to get to know one another as if they were meeting in real life. The individual \"room\" feature also makes it more likely that the people within a chat room share a similar interest; an interest that allows them to bond with one another and be willing to form a friendship.\n\n\n=== Virtual worlds ===\n\nVirtual worlds are the most interactive of all virtual community forms. In this type of virtual community, people are connected by living as an avatar in a computer-based world. Users create their own avatar character (from choosing the avatar's outfits to designing the avatar's house) and control their character's life and interactions with other characters in the 3D virtual world. It is similar to a computer game, however there is no objective for the players. A virtual world simply gives users the opportunity to build and operate a fantasy life in the virtual realm. Characters within the world can talk to one another and have almost the same interactions people would have in reality. For example, characters can socialize with one another and hold intimate relationships online.\nThis type of virtual community allows for people to not only hold conversations with others in real time, but also to engage and interact with others. The avatars that users create are like humans. Users can choose to make avatars like themselves, or take on an entirely different personality than them. When characters interact with other characters, they can get to know one another through text-based talking and virtual experience (such as having avatars go on a date in the virtual world). A virtual community chat room may give real-time conversations, but people can only talk to one another. In a virtual world, characters can do activities together, just like friends could do in reality. Communities in virtual worlds are most similar to real-life communities because the characters are physically in the same place, even if the users who are operating the characters are not. Second Life is one of the most popular virtual worlds on the Internet. Whyville offers a good alternative for younger audiences where safety and privacy are a concern. In Whyville, you use the virtual world's simulation aspect to experiment and learn about various phenomena.\nAnother use for virtual worlds has been in business communications. Benefits from virtual world technology such as photo realistic avatars and positional sound create an atmosphere for participants that provides a less fatiguing sense of presence. Enterprise controls that allow the meeting host to dictate the permissions of the attendees such as who can speak, or who can move about allow the host to control the meeting environment. Zoom, is a popular platform that has grown over the COVID-19 pandemic. Where those who host meetings on this platform, can dictate who can or cannot speak, by muting or unmuting them, along with who is able to join. Several companies are creating business based virtual worlds including Second Life. These business based worlds have stricter controls and allow functionality such as muting individual participants, desktop sharing, or access lists to provide a highly interactive and controlled virtual world to a specific business or group. Business based virtual worlds also may provide various enterprise features such as Single Sign on with third party providers, or Content Encryption.\n\n\n=== Social network services ===\nSocial networking services are the most prominent type of virtual community. They are either a website or software platform that focuses on creating and maintaining relationships. Facebook, Twitter, and Instagram are all virtual communities. With these sites, one often creates a profile or account, and adds friends or follow friends. This allows people to connect and look for support using the social networking service as a gathering place. These websites often allow for people to keep up to date with their friends and acquaintances' activities without making much of an effort. On several of these sites you may be able to video chat, with several people at once, making the connections feel more like you are together. On Facebook, for example, one can upload photos and videos, chat, make friends, reconnect with old ones, and join groups or causes.\n\n\n=== Specialized information communities ===\nParticipatory culture plays a large role in online and virtual communities. In participatory culture, users feel that their contributions are important and that by contributing, they are forming meaningful connections with other users. The differences between being a producer of content on the website and being a consumer on the website become blurred and overlap. According to Henry Jenkins, \"Members believe their contributions matter and feel some degree of social connection with one another \"(Jenkins, et al. 2005). The exchange and consumption of information requires a degree of \"digital literacy\", such that users are able to \"archive, annotate, appropriate, transform and recirculate media content\" (Jenkins). Specialized information communities centralizes a specific group of users who are all interested in the same topic. For example, TasteofHome.com, the website of the magazine Taste of Home, is a specialized information community that focuses on baking and cooking. The users contribute consumer information relating to their hobby and additionally participate in further specialized groups and forums. Specialized Information Communities are a place where people with similar interests can discuss and share their experiences and interests.\n\n\n== Howard Rheingold's study ==\nHoward Rheingold's Virtual Community could be compared with Mark Granovetter's ground-breaking \"strength of weak ties\" article published twenty years earlier in the American Journal of Sociology. Rheingold translated, practiced and published Granovetter's conjectures about strong and weak ties in the online world. His comment on the first page even illustrates the social networks in the virtual society: \"My seven year old daughter knows that her father congregates with a family of invisible friends who seem to gather in his computer. Sometimes he talks to them, even if nobody else can see them. And she knows that these invisible friends sometimes show up in the flesh, materializing from the next block or the other side of the world.\" (page 1).  Indeed, in his revised version of Virtual Community, Rheingold goes so far to say that had he read Barry Wellman's work earlier, he would have called his book \"online social networks\".\nRheingold's definition contains the terms \"social aggregation and personal relationships\" (pp3). Lipnack & Stamps (1997) and Mowshowitz (1997) point out how virtual communities can work across space, time and organizational boundaries; Lipnack & Stamps (1997) mention a common purpose; and Lee, Eom, Jung and Kim (2004) introduce \"desocialization\" which means that there is less frequent interaction with humans in traditional settings, e.g. an increase in virtual socialization. Calhoun (1991) presents a dystopia argument, asserting the impersonality of virtual networks. He argues that IT has a negative influence on offline interaction between individuals because virtual life takes over our lives. He believes that it also creates different personalities in people which can cause frictions in offline and online communities and groups and in personal contacts. (Wellman & Haythornthwaite, 2002). Recently, Mitch Parsell (2008) has suggested that virtual communities, particularly those that leverage Web 2.0 resources, can be pernicious by leading to attitude polarization, increased prejudices and enabling sick individuals to deliberately indulge in their diseases.\n\n\n== Advantages of Internet communities ==\nInternet communities offer the advantage of instant information exchange that is not possible in a real-life community. This interaction allows people to engage in many activities from their home, such as: shopping, paying bills, and searching for specific information. Users of online communities also have access to thousands of specific discussion groups where they can form specialized relationships and access information in such categories as: politics, technical assistance, social activities, health (see above) and recreational pleasures. Virtual communities provide an ideal medium for these types of relationships because information can easily be posted and response times can be very fast. Another benefit is that these types of communities can give users a feeling of membership and belonging. Users can give and receive support, and it is simple and cheap to use.Economically, virtual communities can be commercially successful, making money through membership fees, subscriptions, usage fees, and advertising commission. Consumers generally feel very comfortable making transactions online provided that the seller has a good reputation throughout the community. Virtual communities also provide the advantage of disintermediation in commercial transactions, which eliminates vendors and connects buyers directly to suppliers. Disintermediation eliminates pricey mark-ups and allows for a more direct line of contact between the consumer and the manufacturer.\n\n\n== Disadvantages of Internet communities ==\nWhile instant communication means fast access, it also means that information is posted without being reviewed for correctness. It is difficult to choose reliable sources because there is no editor who reviews each post and makes sure it is up to a certain degree of quality.In theory, online identities can be kept anonymous which enables people to use the virtual community for fantasy role playing as in the case of Second Life's use of avatars. Some professionals urge caution with users who use online communities because predators also frequent these communities looking for victims who are vulnerable to online identity theft or online predators.There are also issues still surrounding bullying on internet communities. With users not having to show their face and being behind the camera, people will use threatening and discriminating acts towards other people because they feel that they wouldn't face any consequences.There are still standing issues with gender and race on the online community as well. Where only the 'normal' people are being represented on the screen, and those of different background and genders are being more displayed, when majority are them are the one's consuming these activities.\n\n\n== See also ==\n\n\n== Notes and references ==\n\n\n== Bibliography ==\nAnderson, Benedict R. O'G. (1983). Imagined communities: reflections on the origin and spread of nationalism. London: Verso. ISBN 978-0-86091-546-1. OCLC 239999655.\nBarzilai, G. (2003). Communities and Law: Politics and Cultures of Legal Identities. Ann Arbor: The University of Michigan Press.\nElse, Liz & Turkle, Sherry. \"Living online: I'll have to ask my friends\", New Scientist, issue 2569, 20 September 2006. (interview)\nEbner, W.; Leimeister, J. M.; Krcmar, H. (2009) (2009). \"Community Engineering for Innovations \u2013 The Ideas Competition as a method to nurture a Virtual Community for Innovations\" (PDF). R&D Management. 39 (4): 342\u2013356. doi:10.1111/j.1467-9310.2009.00564.x. S2CID 16316321.{{cite journal}}:  CS1 maint: multiple names: authors list (link) CS1 maint: numeric names: authors list (link)\nFarmer, F. R. (1993). \"Social Dimensions of Habitat's Citizenry.\" Virtual Realities: An Anthology of Industry and Culture, C. Loeffler, ed., Gijutsu Hyoron Sha, Tokyo, Japan\nGouv\u00eaa, Mario de Paula Leite (18\u201321 July 2000). \"The Challenges of Building an International Virtual Community Using Internet Technologies\". Internet Society INET 2000 conference proceedings.\nHafner, K. 2001. The WELL: A Story of Love, Death and Real Life in the Seminal Online Community Carroll & Graf Publishers (ISBN 0-7867-0846-8)\nHagel, J. & Armstrong, A. (1997). Net Gain: Expanding Markets through Virtual Communities. Boston: Harvard Business School Press (ISBN 0-87584-759-5)\nJones, G. Ravid; Rafaeli, S. (2004). \"Information Overload and the Message Dynamics of Online Interaction Spaces: A Theoretical Model and Empirical Exploration\". Information Systems Research. 15 (2): 194\u2013210. CiteSeerX 10.1.1.127.6976. doi:10.1287/isre.1040.0023. S2CID 207227328.\nKim, A.J. (2000). Community Building on the Web: Secret Strategies for Successful Online Communities. London: Addison Wesley (ISBN 0-201-87484-9)\nKim, A. J. (2004) (24 January 2004). \"Emergent Purpose. Musings of a Social Architect\". Retrieved 4 April 2006.{{cite web}}:  CS1 maint: numeric names: authors list (link)\nKollock, Peter. 1999. \"The Economies of Online Cooperation: Gifts and Public Goods in Cyberspace,\" in Communities in Cyberspace. Marc Smith and Peter Kollock (editors). London: Routledge.\nThe author has made available an  \"Online working draft\".\nKosorukoff, A. & Goldberg, D. E. (2002) Genetic algorithm as a form of organization, Proceedings of Genetic and Evolutionary Computation Conference, GECCO-2002, pp 965\u2013972.\nLeimeister, J. M.; Ebner, W.; Krcmar, H. (2005). \"Design, Implementation and Evaluation of Trust-supporting Components in Virtual Communities for Patients\". Journal of Management Information Systems (JMIS). p. 21 (4), pp. 101\u2013136. Archived from the original on 23 June 2011. Retrieved 23 August 2010.{{cite web}}:  CS1 maint: multiple names: authors list (link) CS1 maint: numeric names: authors list (link)\nLeimeister, J. M.; Sidiras, P.; Krcmar, H. (2006). \"Exploring Success Factors of Virtual Communities: The Perspectives of Members and Operators\". Journal of Organizational Computing & Electronic Commerce (JoCEC): 16 (3&4), 277\u2013298.{{cite journal}}:  CS1 maint: multiple names: authors list (link) CS1 maint: numeric names: authors list (link)\nMorningstar, C. and F. R. Farmer (1990). \"The Lessons of Lucasfilm's Habitat, The First International Conference on Cyberspace, Austin, TX, USA\".{{cite web}}:  CS1 maint: numeric names: authors list (link)\nNaone, Erica, \"Who Owns Your Friends?: Social-networking sites are fighting over control of users' personal information.\", MIT Technology Review, July/August 2008\nNeus, A. (2001). \"Managing Information Quality in Virtual Communities of Practice; Lessons learned from a decade's experience with exploding internet communication\" (PDF). IQ 2001: The 6th International Conference on Information Quality at MIT. Archived from the original (PDF) on 16 August 2010. Retrieved 23 August 2010.{{cite web}}:  CS1 maint: numeric names: authors list (link)\nParsell, Mitch (2008). \"Pernicious virtual communities: Identity, polarisation and the Web 2.0\". Ethics and Information Technology. 10: 41\u201356. doi:10.1007/s10676-008-9153-y. S2CID 33207414.\nPreece, J. (2000). Online Communities: Supporting Sociability, Designing Usability. Chichester: John Wiley & Sons Ltd. (ISBN 0-471-80599-8)\nProdnik, Jernej (2012). \"Post-Fordist Communities and Cyberspace, A Critical Approach\". In H. Breslow and A. Mousoutzanis (eds.), Cybercultures: Mediations of Community, Culture, Politics. Rodopi: Amsterdam, New York. pp. 75\u2013100. Retrieved 2 February 2013.{{cite web}}:  CS1 maint: numeric names: authors list (link)\nRheingold, H. (2000). The Virtual Community: Homesteading on the Electronic Frontier. London: MIT Press. (ISBN 0-262-68121-8)\nThe author has made available an online copy\nRosenkranz, C., Feddersen, C. (2010) (14 December 2017). \"?\". Managing viable virtual communities: an exploratory case study and explanatory model. International Journal of Web Based Communities. p. Volume 6, Number 1 5\u201314. Archived from the original on 3 January 2013.{{cite web}}:  CS1 maint: multiple names: authors list (link) CS1 maint: numeric names: authors list (link)\nSeabrook, J. 1997. Deeper: My Two-Year Odyssey in Cyberspace Simon & Schuster (ISBN 0-684-80175-2)\nSmith, M. \"Voices from the WELL: The Logic of the Virtual Commons\" UCLA Department of Sociology.\nSudweeks, F., McLaughlin, M.L. & Rafaeli, S. (1998) Network and Netplay Virtual Groups on the Internet, MIT Press.\nPortions available online as: Journal of Computer Mediated Communication, 2\nVan der Crabben, Jan. \"Performed Intimacy in Virtual Worlds\". Archived from the original on 31 May 2009.\nBarry Wellman, \"An Electronic Group is Virtually a Social Network.\" Pp. 179\u2013205 in Culture of the Internet, edited by Sara Kiesler. Mahwah, NJ: Lawrence Erlbaum, 1997. Translated into German as \"Die elektronische Gruppe als soziales Netzwerk.\" Pp. 134\u201367 in Virtuelle Gruppen, edited by Udo Thiedeke. Wiesbaden: Westdeutscher Verlag, 2000.\nTrier, M. (2007) Virtual Knowledge Communities \u2013 IT-supported Visualization and Analysis. Saarbr\u00fccken, Germany: VDM Verlag Dr. M\u00fcller. (ISBN 3-8364-1540-2).\nUrstadt, Bryant, \"Social Networking Is Not a Business: Web 2.0\u2014the dream of the user-built, user-centered, user-run Internet\u2014has delivered on just about every promise except profit. Will its most prominent example, social networking, ever make any money?\", MIT Technology Review, July/August 2008"}, {"id": 40, "title": "Credit score", "content": "A credit score is a numerical expression based on a level analysis of a person's credit files, to represent the creditworthiness of an individual. A credit score is primarily based on a credit report, information typically sourced from credit bureaus.Lenders, such as banks and credit card companies, use credit scores to evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt. Lenders use credit scores to determine who qualifies for a loan, at what interest rate, and what credit limits.  Lenders also use credit scores to determine which customers are likely to bring in the most revenue.\nCredit scoring is not limited to banks. Other organizations, such as mobile phone companies, insurance companies, landlords, and government departments employ the same techniques. Digital finance companies such as online lenders also use alternative data sources to calculate the creditworthiness of borrowers.\n\n\n== By country ==\n\n\n=== Australia ===\nIn Australia, credit scoring is widely accepted as the primary method of assessing creditworthiness. Credit scoring is used not only to determine whether credit should be approved to an applicant, but for credit scoring in the setting of credit limits on credit or store cards, in behavioral modelling such as collections scoring, and also in the pre-approval of additional credit to a company's existing client base.\nAlthough logistic (or non-linear) probability modelling is still the most popular means by which to develop scorecards, various other methods offer powerful alternatives, including MARS, CART, CHAID, and random forests.\nPrior to 12 March 2014 Veda Advantage, the main provider of credit file data, provided only a negative credit reporting system containing information on applications for credit and adverse listings indicating a default under a credit contract. Veda was acquired by Equifax in Feb 2016, making Equifax the largest credit agency in Australia.With the subsequent introduction of positive reporting, lending companies have begun an uptake of its usage with some implementing risk based pricing to set lending rates.\n\n\n=== Austria ===\nIn Austria, credit scoring is done as a blacklist. Consumers who did not pay bills end up on the blacklists that are held by different credit bureaus. Having an entry on the black list may result in the denial of contracts. Certain enterprises including telecom carriers use the list on a regular basis. Banks also use these lists, but rather inquire about security and income when considering loans. Beside these lists several agencies and credit bureaus provide credit scoring of consumers.\nAccording to the Austrian Data Protection Act, consumers must opt-in for the use of their private data for any purpose. Consumers can also withhold permission to use the data later, making illegal any further distribution or use of the collected data. Consumers also have the right to receive a free copy of all data held by credit bureaus once a year. Wrong or unlawfully collected data must be deleted or corrected.\n\n\n=== Brazil ===\nCredit scoring is relatively new in Brazil. Previously, credit reporting was done as a blacklist and each lender used to assess potential borrowers on their own criteria. Nowadays, the system of credit reports and scores in Brazil is very similar to that in the United States.\nA credit score is a number based on a statistical analysis of a person's credit information, which represents the creditworthiness of that person. It is the most important tool used by financial institutions during a credit analysis that aims to assist the decision-making process of granting credit and conducting business, in order to verify the likelihood that people will pay their bills. A credit score is primarily based on credit report information, typically from one of the three major credit bureaus: Serasa Experian, Boa Vista (previously Equifax do Brasil) and SPC Brasil.There are different methods of calculating credit scores in Brazil. In general, scores range from 0 to 1000 indicating what is the chance of a certain profile of consumers paying their bills on time in the next 12 months. The score is calculated from several factors, but practically it analyzes a person's trajectory as a consumer, what includes up to date payments of bills, history of negative debts, financial relationships with companies and updated personal data on credit protection agencies, such as Serasa Experian, Boa Vista, SPC, Quod and Foregon.\n\n\n=== Canada ===\nThe system of credit reports and scores in Canada is very similar to that in the United States and India, with two of the same reporting agencies active in the country:  Equifax and TransUnion. (Experian, which entered the Canadian market with the purchase of Northern Credit Bureaus in 2008, announced the closing of its Canadian operations as of 18 April 2009).\nThere are, however, some key differences. One is that, unlike in the United States, where a consumer is allowed only one free copy of their credit report a year, in Canada, the consumer may order a free copy of their credit report any number of times in a year, as long as the request is made in writing, and as long as the consumer asks for a printed copy to be delivered by mail. Borrowell and CreditKarma offers free credit report and credit check and this request by the consumer is noted in the credit report as a 'soft inquiry', so it has no effect on their credit score. According to Equifax's ScorePower Report, Equifax Beacon scores range from 300 to 900. Trans Union Emperica scores also range from 300 to 900.\nThe Government of Canada offers a free publication called Understanding Your Credit Report.  This publication provides sample credit report and credit score documents, with explanations of the notations and codes that are used. It also contains general information on how to build or improve credit history, and how to check for signs that identity theft has occurred. The publication is available online at the Financial Consumer Agency of Canada. Paper copies can also be ordered at no charge for residents of Canada.\n\n\n=== China ===\n\nPrivate companies have developed credit score systems such as Sesame Credit (which is provided by Alibaba affiliate Ant Financial) and Tencent Credit.\n\n\n=== Denmark ===\nThe credit scoring is widely used in Denmark by the banks and a number of private companies within telco and others. The credit scoring is split in two:\n\nPrivate: The probability of defaulting\nBusinesses: The probability of bankruptcyFor privates, the credit scoring is always made by the creditor. For businesses it is either made by the creditor or by a third party. A business credit score is a direct representation of a company's creditworthiness.\nThere are a few companies who have specialized in developing credit scorecards in Denmark:\n\nExperian (generic rating for business)\nBisnode (generic rating for business)The credit scorecards in Denmark are mainly based on information provided by the applicant and publicly available data. It is very restricted by legislation compared to its neighbouring countries.\n\n\n=== Germany ===\nIn Germany, credit scoring is widely accepted as the primary method of assessing creditworthiness. Credit scoring is used not only to determine whether credit should be approved to an applicant, but for credit scoring in the setting of credit limits on credit or store cards, in behavioral modelling such as collections scoring, and also in the pre-approval of additional credit to a company's existing client base.\nConsumers have the right to receive a free copy of all data held by credit bureaus once a year. At present Schufa, the main provider of credit file data, provides scores for about three-quarters of the German population.\n\n\n=== India ===\nIn India, there are four credit information companies licensed by Reserve Bank of India. The Credit Information Bureau (India) Limited (CIBIL) has functioned as a Credit Information Company from January 2001. Subsequently, in 2010, Experian, Equifax and CRIF High Mark were given licenses by Reserve Bank of India to operate as Credit Information Companies in India. Transunion bought CIBIL.\nAlthough all the four credit information companies have developed their individual credit scores, the most popular is CIBIL credit score. The CIBIL credit score is a three  digit number that represents a summary of individuals' credit history and credit rating. This score ranges from 300 to 900, with 900 being the best score. Individuals with no credit history will have a score of \u22121. If the credit history is less than six months, the score will be 0. CIBIL credit score takes time to build up and usually it takes between 18 and 36 months or more of credit usage to obtain a satisfactory credit score.\n\n\n=== Norway ===\nIn Norway, credit scoring services are provided by three credit scoring agencies: Dun & Bradstreet, Experian and Lindorff Decision. Credit scoring is based on publicly available information such as demographic data, tax returns, taxable income and any Betalingsanmerkning (non-payment records) that might be registered on the credit-scored individual. Upon being scored, an individual will receive a notice (written or by e-mail) from the scoring agency stating who performed the credit score as well as any information provided in the score. In addition, many credit institutions use custom scorecards based on any number of parameters. Credit scores range between 300 and 999.\n\n\n=== Republic of Ireland ===\nIn the Republic of Ireland, a person's credit score is calculated by the Irish Credit Bureau (ICB), a private organisation, financed by its members (financial institutions and local authorities). A person taking out a loan must consent to their data being given to the ICB. A person may receive their own credit report on paying a \u20ac6 fee to the ICB. Credit scores run from 224 (the worst value) to 581 (the best value).There is also a separate Central Credit Register (CCR) maintained by the Central Bank of Ireland, founded in 2017 under the terms of the Credit Reporting Act 2013. The lender must check the CCR if a person is borrowing more than \u20ac2,000, and can also check it if the loan is lower; consent from the borrower is not required.Information is removed from both registers (ICB and CCR) five years after the loan is repaid.\n\n\n=== South Africa ===\nCredit scoring is used throughout the credit industry in South Africa, with the likes of banks, micro-lenders, clothing retailers, furniture retailers, specialized lenders and insurers all using credit scores.  Currently all four retail credit bureau offer credit bureau scores.  The data stored by the credit bureaus include both positive and negative data, increasing the predictive power of the individual scores.  TransUnion (formerly ITC) offer the Empirica Score which is, as of mid-2010, in its 4th generation.  The Empirica score is segmented into two suites: the account origination (AO) and account management (AM).  Experian South Africa likewise has a Delphi credit score with their fourth generation about to be released (late 2010). In 2011, Compuscan released Compuscore ABC, a scoring suite which predicts the probability of customer default throughout the credit life cycle. Six years later, Compuscan introduced Compuscore PSY, a 3-digit psychometric-based credit bureau score used by lenders to make informed lending decisions on thin files or marginal declines.\n\n\n=== Sri Lanka ===\nAccording to the provisions of Credit Information Bureau Act No 18 of 1990 (as amended by Act No 42 of 2008), CRIB has been delegated with power to issue credit reports to any subject to whom that information is related to. The Bureau commenced to issue Self Inquiry Credit Reports in December 2009.\n\n\n=== Sweden ===\nSweden has a system for credit scoring that aims to find people with a history of neglect to pay bills or, most commonly, taxes. Anyone who does not pay their debts on time, and fails to make payments after a reminder, will have their case forwarded to the Swedish Enforcement Authority which is a national authority for collecting debts. The mere appearance of a company, or government office, as a debtor to this authority will result in a record among private credit bureaus; however, this does not apply to individuals as debtors. This record is called a Betalningsanm\u00e4rkning (non-payment record) and by law can be stored for three years for an individual and five years for a company. This kind of nonpayment record will make it very difficult to get a loan, rent an apartment, get telephone subscriptions, rent a car or get a job where you handle cash. The banks also use income and asset figures in connection with loan assessments.If a person gets an injunction to pay issued by the Enforcement Authority, it is possible to dispute it. Then the party requesting the payment must show its correctness in district court. Failure to dispute is seen as admitting the debt. If the debtor loses the court trial, costs for the trial are added to the debt. Taxes and authority fees must always be paid on demand unless payment has already been made.\nEvery person with a Swedish national identification number must register a valid address, even if living abroad, since sent letters are considered to have been delivered to that person once they reach the registered address. As an example, Swedish astronaut Christer Fuglesang got a Betalningsanm\u00e4rkning since a car he had ordered, and therefore owned, passed a toll station for the Stockholm congestion tax. At the time, he was living in the USA training for his first Space Shuttle mission and had an old invalid address registered for the car. Letters with payment requests did not reach him on time. The case was appealed and retracted, but the non-payment record remained for three years since it could not be retracted according to the law.\n\n\n=== United Kingdom ===\n\nCredit scoring in the United Kingdom is very different to that of the United States and other nations. There is no such thing as a universal credit score or credit rating in the UK. Each lender will assess potential borrowers on their own criteria, and these algorithms are effectively trade secrets.\n\"Credit scores\" which are available for individuals to see and provided from Credit Reference Agencies such as Call Credit, Equifax, Experian and TransUnion are marketed to consumers and are not usually used by lenders. Most lenders instead use their own internal scoring mechanism.The most popular statistical technique used is logistic regression to predict a binary outcome: bad debt (meaning the borrower has defaulted on the loan) or not. Some banks also build regression models that predict the amount of bad debt a customer may incur. Typically this is much harder to predict, and most banks focus only on the binary outcome.\nCredit scoring is closely regulated only by the Financial Conduct Authority when used for the purposes of the Advanced approach to Capital Adequacy under Basel II regulations.\nCredit scoring is closely regulated in the UK, with the industry regulator being the Information Commissioner's Office (ICO). Consumers can also send complaints to the Financial Ombudsman Service if they experience problems with any Credit Reference Agency.It is very difficult for a consumer to know in advance whether they have a high enough credit score to be accepted for credit with a given lender.  This situation is due to the complexity and structure of credit scoring, which differs from one lender to another.\nLenders need not reveal their credit score head, nor need they reveal the minimum credit score required for the applicant to be accepted, because there may not be such a minimum score.\nIf the applicant is declined for credit, the lender is not obliged to reveal the exact reason why. However industry associations including the Finance and Leasing Association oblige their members to provide a satisfactory reason. Credit-bureau data sharing agreements also require that an applicant declined based on credit-bureau data be told that this is the reason and the address of the credit bureau must be provided.\n\n\n=== United States ===\n\nIn the United States, a credit score is a number based on a statistical analysis of a person's credit files, that in theory represents the creditworthiness of that person, which is the likelihood that people will pay their bills.  A credit score is primarily based on credit report information, typically from one of the three major credit bureaus: Experian, TransUnion, and Equifax. Income and employment history (or lack thereof) are not considered by the major credit bureaus when calculating credit scores.\nThere are different methods of calculating credit scores. FICO scores, the most widely used type of credit score, is a credit score developed by FICO, previously known as Fair Isaac Corporation.  As of 2018, there were 29 different versions of FICO scores in use in the United States. Some of these versions are \"industry specific\" scores, that is, scores produced for particular market segments, including automotive lending and bankcard (credit card) lending. Industry-specific FICO scores produced for automotive lending are formulated differently than FICO scores produced for bankcard lending. Nearly every consumer will have different FICO scores depending upon which type of FICO score is ordered by a lender; for example, a consumer with several paid-in-full car loans but no reported credit card payment history will generally score better on a FICO automotive-enhanced score than on a FICO bankcard-enhanced score. FICO also produces several \"general purpose\" scores which are not tailored to any particular industry. Industry-specific FICO scores range from 250 to 900, whereas general purpose scores range from 300 to 850.\nFICO scores are used by many mortgage lenders that use a risk-based system to determine the possibility that the borrower may default on financial obligations to the mortgage lender. For most mortgages originated in the United States, three credit scores are obtained on a consumer: a Beacon 5.0 score (Beacon is a trademark of FICO) which is calculated from the consumer's Equifax credit history, a FICO Model II score, which is calculated from the consumer's Experian credit history, and a Classic04 score, which is calculated from the consumer's Trans Union history.\nCredit bureaus also often re-sell FICO scores directly to consumers, often a general-purpose FICO 8 score. Previously, the credit bureaus also sold their own credit scores which they developed themselves, and which did not require payment to FICO to utilize: Equifax's RISK score and Experian's PLUS score. However, as of 2018, these scores are no longer sold by the credit bureaus. Trans Union offers a Vantage 3.0 score for sale to consumers, which is a version of the VantageScore credit score. In addition, many large lenders, including the major credit card issuers, have developed their own proprietary scoring models.\nStudies have shown scores to be predictive of risk in the underwriting of both credit and insurance.  Some studies even suggest that most consumers are the beneficiaries of lower credit costs and insurance premiums due to the use of credit scores.Usage of credit histories in employment screenings increased from 19% in 1996 to 42% in 2006.:\u200a1\u200a  However, credit reports for employment screening purposes do not include credit scores.:\u200a2\u200aAmericans are entitled to one free credit report in every 12-month period from each of the three credit bureaus, but are not entitled to receive a free credit score. The three credit bureaus run Annualcreditreport.com, where users can get their free credit reports. Credit scores are available as an add-on feature of the report for a fee. If the consumer disputes an item on a credit report obtained using the free system, under the Fair Credit Reporting Act (FCRA), the credit bureaus have 45 days to investigate, rather than 30 days for reports obtained otherwise.\nAlternatively, consumers wishing to obtain their credit scores can in some cases purchase them separately from the credit bureaus or can purchase their FICO score directly from FICO. Credit scores (including FICO scores) are also made available free by subscription to one of the many credit report monitoring services available from the credit bureaus or other third parties, although to actually get the scores free from most such services, one must use a credit card to sign up for a free trial subscription of the service and then cancel before the first monthly charge. Websites like WalletHub, Credit Sesame and Credit Karma provide free credit scores with no credit card required, using the TransUnion VantageScore 3.0 model. Credit.com uses the Experian VantageScore 3.0 model. Until March 2009, holders of credit cards issued by Washington Mutual were offered a free FICO score each month through the bank's Web site. (Chase, which took over Washington Mutual in 2008, discontinued this practice in March 2009.) Chase resumed the practice of offering a free FICO score in March 2010 of select card members to the exclusion of the majority of former WAMU card holders.\nUnder the Fair Credit Reporting Act, a consumer is entitled to a free credit report (but not a free credit score) within 60 days of any adverse action (e.g., being denied credit, or receiving substandard credit terms from a lender) taken as a result of their credit score. Under the Wall Street reform bill passed on 22 July 2010, a consumer is entitled to receive a free credit score if they are denied a loan or insurance due to their credit score.In the United States, the median generic FICO score was 723 in 2006 and 711 in 2011.  The performance definition of the FICO risk score (its stated design objective) is to predict the likelihood that a consumer will go 90 days past due or worse in the subsequent 24 months after the score has been calculated.  The higher the consumer's score, the less likely he or she will go 90 days past due in the subsequent 24 months after the score has been calculated.  Because different lending uses (mortgage, automobile, credit card) have different parameters, FICO algorithms are adjusted according to the predictability of that use.  For this reason, a person might have a higher credit score for a revolving credit card debt when compared to a mortgage credit score taken at the same point in time.\n\n\n== See also ==\nAlternative data\nBank statement\nCredit bureau\nCredit history\nCredit reference\nCredit scorecards\n\n\n== References ==\n\n\n== External links ==\n\"FTC Guide to Credit Scores\". 25 May 2021.\n\"Credit Scores Video \u2013 Federal Trade Commission\". YouTube. Archived from the original on 2021-12-19.\n\"Financial Consumer Agency of Canada\".\n\"How Credit Scores Work\". 16 July 2002.\nDemyanyk, Yuliya (16 November 2010). \"Your Credit Score Is a Ranking, Not a Score\". Economic Commentary (2010\u201316)."}, {"id": 41, "title": "Cryptanalysis", "content": "Cryptanalysis (from the Greek krypt\u00f3s, \"hidden\", and anal\u00fdein, \"to analyze\") refers to the process of analyzing information systems in order to understand hidden aspects of the systems.  Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.\nEven though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\n\n\n== Overview ==\nIn encryption, confidential information (called the \"plaintext\") is sent securely to a recipient by the sender first converting it into an unreadable form (\"ciphertext\") using an encryption algorithm.  The ciphertext is sent through an insecure channel to the recipient.  The recipient decrypts the ciphertext by applying an inverse decryption algorithm, recovering the plaintext.  To decrypt the ciphertext, the recipient requires a secret knowledge from the sender, usually a string of letters, numbers, or bits, called a cryptographic key.  The concept is that even if an unauthorized person gets access to the ciphertext during transmission, without the secret key they cannot convert it back to plaintext.\nEncryption has been used throughout history to send important military, diplomatic and commercial messages, and today is very widely used in computer networking to protect email and internet communication.\nThe goal of cryptanalysis is for a third party, a cryptanalyst, to gain as much information as possible about the original (\"plaintext\"), attempting to \"break\" the encryption to read the ciphertext and learning the secret key so future messages can be decrypted and read.  A mathematical technique to do this is called a cryptographic attack. Cryptographic attacks can be characterized in a number of ways:\n\n\n=== Amount of information available to the attacker ===\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim \"the enemy knows the system\" \u2013 in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice \u2013 throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been broken through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\nCiphertext-only: the cryptanalyst has access only to a collection of ciphertexts or codetexts.\nKnown-plaintext: the attacker has a set of ciphertexts to which they know the corresponding plaintext.\nChosen-plaintext (chosen-ciphertext): the attacker can obtain the ciphertexts (plaintexts) corresponding to an arbitrary set of plaintexts (ciphertexts) of their own choosing.\nAdaptive chosen-plaintext: like a chosen-plaintext attack, except the attacker can choose subsequent plaintexts based on information learned from previous encryptions, similarly to the Adaptive chosen ciphertext attack.\nRelated-key attack: Like a chosen-plaintext attack, except the attacker can obtain ciphertexts encrypted under two different keys. The keys are unknown, but the relationship between them is known; for example, two keys that differ in the one bit.\n\n\n=== Computational resources required ===\nAttacks can also be characterised by the resources they require. Those resources include:\nTime \u2013 the number of computation steps (e.g., test encryptions) which must be performed.\nMemory \u2013 the amount of storage required to perform the attack.\nData \u2013 the quantity and type of plaintexts and ciphertexts required for a particular approach.It is sometimes difficult to predict these quantities precisely, especially when the attack is not practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated order of magnitude of their attacks' difficulty, saying, for example, \"SHA-1 collisions now 252.\"Bruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\n\n\n=== Partial breaks ===\nThe results of cryptanalysis can also vary in usefulness. Cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\n\nTotal break \u2013 the attacker deduces the secret key.\nGlobal deduction \u2013 the attacker discovers a functionally equivalent algorithm for encryption and decryption, but without learning the key.\nInstance (local) deduction \u2013 the attacker discovers additional plaintexts (or ciphertexts) not previously known.\nInformation deduction \u2013 the attacker gains some Shannon information about plaintexts (or ciphertexts) not previously known.\nDistinguishing algorithm \u2013 the attacker can distinguish the cipher from a random permutation.Academic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\nIn academic cryptography, a weakness or a break in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking the full system.\n\n\n== History ==\n\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography\u2014new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\n\n\n=== Classical ciphers ===\n\nAlthough the actual word \"cryptanalysis\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. David Kahn notes in The Codebreakers that Arab scholars were the first people to systematically document cryptanalytic methods.The first known recorded explanation of cryptanalysis was given by Al-Kindi (c. 801\u2013873, also known as \"Alkindus\" in Europe), a 9th-century Arab polymath, in Risalah fi Istikhraj al-Mu'amma (A Manuscript on Deciphering Cryptographic Messages). This treatise contains the first description of the method of frequency analysis. Al-Kindi is thus regarded as the first codebreaker in history. His breakthrough work was influenced by Al-Khalil (717\u2013786), who wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.Frequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.Al-Kindi's invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers was the most significant cryptanalytic advance until World War II. Al-Kindi's Risalah fi Istikhraj al-Mu'amma described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic. An important contribution of Ibn Adlan (1187\u20131268) was on sample size for use of frequency analysis.In Europe, Italian scholar Giambattista della Porta (1535\u20131615) was the author of a seminal work on cryptanalysis, De Furtivis Literarum Notis.Successful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigen\u00e8re (1523\u201396). For some three centuries, the Vigen\u00e8re cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (le chiffre ind\u00e9chiffrable\u2014\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791\u20131871) and later, independently, Friedrich Kasiski (1805\u201381) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigen\u00e8re system.\n\n\n=== Ciphers from World War I and World War II ===\n\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers \u2013 including the Enigma machine and the Lorenz cipher \u2013 and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.Cryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.In practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers \u2013 the first electronic digital computers to be controlled by a program.\n\n\n==== Indicator ====\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the indicator, as it indicates to the receiving operator how to set his machine to decipher the message.Poorly designed and implemented indicator systems allowed first Polish cryptographers and then the British cryptographers at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify depths that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\n\n\n==== Depth ====\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"in depth.\" This may be detected by the messages having the same indicator by which the sending operator informs the receiving operator about the key generator initial settings for the message.Generally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by \u2295 ):\n\nPlaintext \u2295 Key = CiphertextDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\n\nCiphertext \u2295 Key = Plaintext(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\n\nCiphertext1 \u2295 Ciphertext2 = Plaintext1 \u2295 Plaintext2The individual plaintexts can then be worked out linguistically by trying probable words (or phrases), also known as \"cribs,\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\n\n(Plaintext1 \u2295 Plaintext2) \u2295 Plaintext1 = Plaintext2The recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\n\nPlaintext1 \u2295 Ciphertext1 = KeyKnowledge of a key then allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\n\n\n=== Development of modern cryptography ===\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today.\n\nEven though computation was used to great effect in the cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\nMany are the cryptosystems offered by the hundreds of commercial vendors today that cannot be broken by any known methods of cryptanalysis. Indeed, in such systems even a chosen plaintext attack, in which a selected plaintext is matched against its ciphertext, cannot yield the key that unlock[s] other messages. In a sense, then, cryptanalysis is dead. But that is not the end of the story. Cryptanalysis may be dead, but there is \u2013 to mix my metaphors \u2013 more than one way to skin a cat.\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"However, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\nThe block cipher Madryga, proposed in 1984 but not widely used, was found to be susceptible to ciphertext-only attacks in 1998.\nFEAL-4, proposed as a replacement for the DES standard encryption algorithm but not widely used, was demolished by a spate of attacks from the academic community, many of which are entirely practical.\nThe A5/1, A5/2, CMEA, and DECT systems used in mobile and wireless phone technology can all be broken in hours, minutes or even in real-time using widely available computing equipment.\nBrute-force keyspace search has broken some real-world ciphers and applications, including single-DES (see EFF DES cracker), 40-bit \"export-strength\" cryptography, and the DVD Content Scrambling System.\nIn 2001, Wired Equivalent Privacy (WEP), a protocol used to secure Wi-Fi wireless networks, was shown to be breakable in practice because of a weakness in the RC4 cipher and aspects of the WEP design that made related-key attacks practical. WEP was later replaced by Wi-Fi Protected Access.\nIn 2008, researchers conducted a proof-of-concept break of SSL using weaknesses in the MD5 hash function and certificate issuer practices that made it possible to exploit collision attacks on hash functions. The certificate issuers involved changed their practices to prevent the attack from being repeated.Thus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\n\n\n== Symmetric ciphers ==\nBoomerang attack\nBrute-force attack\nDavies' attack\nDifferential cryptanalysis\nImpossible differential cryptanalysis\nImprobable differential cryptanalysis\nIntegral cryptanalysis\nLinear cryptanalysis\nMeet-in-the-middle attack\nMod-n cryptanalysis\nRelated-key attack\nSandwich attack\nSlide attack\nXSL attack\n\n\n== Asymmetric ciphers ==\nAsymmetric cryptography (or public-key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.Asymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie\u2013Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization \u2013 a breakthrough in factoring would impact the security of RSA.In 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.Another distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\n\n\n== Attacking cryptographic hash systems ==\nBirthday attack\nHash function security summary\nRainbow table\n\n\n== Side-channel attacks ==\n\nBlack-bag cryptanalysis\nMan-in-the-middle attack\nPower analysis\nReplay attack\nRubber-hose cryptanalysis\nTiming analysis\n\n\n== Quantum computing applications for cryptanalysis ==\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.By using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.\n\n\n== See also ==\nEconomics of security\nGlobal surveillance\nInformation assurance, a term for information security often used in government\nInformation security, the overarching goal of most cryptography\nNational Cipher Challenge\nSecurity engineering, the design of applications and protocols\nSecurity vulnerability; vulnerabilities can include cryptographic or other flaws\nTopics in cryptography\nZendian Problem\n\n\n=== Historic cryptanalysts ===\nConel Hugh O'Donel Alexander\nCharles Babbage\nLambros D. Callimahos\nJoan Clarke\nAlastair Denniston\nAgnes Meyer Driscoll\nElizebeth Friedman\nWilliam F. Friedman\nMeredith Gardner\nFriedrich Kasiski\nAl-Kindi\nDilly Knox\nSolomon Kullback\nMarian Rejewski\nJoseph Rochefort, whose contributions affected the outcome of the Battle of Midway\nFrank Rowlett\nAbraham Sinkov\nGiovanni Soro, the Renaissance's first outstanding cryptanalyst\nJohn Tiltman\nAlan Turing\nWilliam T. Tutte\nJohn Wallis \u2013 17th-century English mathematician\nWilliam Stone Weedon \u2013 worked with Fredson Bowers in World War II\nHerbert Yardley\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== Further reading ==\nBard, Gregory V. (2009). Algebraic Cryptanalysis. Springer. ISBN 978-1-4419-1019-6.\nHinek, M. Jason (2009). Cryptanalysis of RSA and Its Variants. CRC Press. ISBN 978-1-4200-7518-2.\nJoux, Antoine (2009). Algorithmic Cryptanalysis. CRC Press. ISBN 978-1-4200-7002-6.\nJunod, Pascal; Canteaut, Anne (2011). Advanced Linear Cryptanalysis of Block and Stream Ciphers. IOS Press. ISBN 978-1-60750-844-1.\nStamp, Mark; Low, Richard (2007). Applied Cryptanalysis: Breaking Ciphers in the Real World. John Wiley & Sons. ISBN 978-0-470-11486-5.\nSwenson, Christopher (2008). Modern cryptanalysis: techniques for advanced code breaking. John Wiley & Sons. ISBN 978-0-470-13593-8.\nWagstaff, Samuel S. (2003). Cryptanalysis of number-theoretic ciphers. CRC Press. ISBN 978-1-58488-153-7.\n\n\n== External links ==\n\nBasic Cryptanalysis (files contain 5 line header, that has to be removed first)\nDistributed Computing Projects\nList of tools for cryptanalysis on modern cryptography\nSimon Singh's crypto corner\nThe National Museum of Computing\nUltraAnvil tool for attacking simple substitution ciphers\nHow Alan Turing Cracked The Enigma Code Imperial War Museums"}, {"id": 42, "title": "Streaming media", "content": "Streaming media is multimedia for playback using an offline or online media player. Technically, the stream is delivered and consumed in a continuous manner from a client, with little or no intermediate storage in network elements. Streaming refers to the delivery method of content, rather than the content itself.\nDistinguishing delivery method from the media applies specifically to telecommunications networks, as most of the traditional media delivery systems are either inherently streaming (e.g. radio, television) or inherently non-streaming (e.g. books, videotapes, audio CDs). There are challenges with streaming content on the Internet. For example, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or poor buffering of the content, and users lacking compatible hardware or software systems may be unable to stream certain content. With the use of buffering of the content for just a few seconds in advance of playback, the quality can be much improved.\nLivestreaming is the real-time delivery of content during production, much as live television broadcasts content via television channels. Livestreaming requires a form of source media (e.g. a video camera, an audio interface, screen capture software), an encoder to digitize the content, a media publisher, and a content delivery network to distribute and deliver the content.\nStreaming is an alternative to file downloading, a process in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user can use their media player to start playing digital video or digital audio content before the entire file has been transmitted. The term \"streaming media\" can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered \"streaming text\".\nStreaming is most prevalent in video on demand and streaming television services. Other services stream music or video games.\n\n\n== Etymology ==\nThe term \"streaming\" was first used for tape drives manufactured by Data Electronics Inc. that were meant to slowly ramp up and run for the entire track; slower ramp times lowered drive costs. \"Streaming\" was applied in the early 1990s, as a better description for video on demand and later live video on IP networks. It was first done by Starlight Networks for video streaming and Real Networks for audio streaming. Such video had previously been referred to by the misnomer \"store and forward video.\"\n\n\n== Precursors ==\nBeginning in 1881, Th\u00e9\u00e2trophone enabled subscribers to listen to opera and theatre performances over telephone lines. This operated until 1932. The concept of media streaming eventually came to America.In the early 1920s, George Owen Squier was granted patents for a system for the transmission and distribution of signals over electrical lines, which was the technical basis for what later became Muzak, a technology streaming continuous music to commercial customers without the use of radio.\nThe Telephone Music Service, a live jukebox service, began in 1929 and continued until 1997. The clientele eventually included 120 bars and restaurants in the Pittsburgh area. A tavern customer would deposit money in the jukebox, use a telephone on top of the jukebox, and ask the operator to play a song. The operator would find the record in the studio library of more than 100,000 records, put it on a turntable, and the music would be piped over the telephone line to play in the tavern. The music media began as 78s, 33s and 45s, played on the six turntables they monitored. CDs and tapes were incorporated in later years.\nThe business had a succession of owners, notably Bill Purse, his daughter Helen Reutzel, and finally, Dotti White. The revenue stream of each quarter was split 60% to the music service and 40% to the tavern owner. This business model eventually became unsustainable due to city permits and the cost of setting up these telephone lines.\n\n\n== History ==\n\n\n=== Early development ===\n\nAttempts to display media on computers date back to the earliest days of computing in the mid-20th century. However, little progress was made for several decades, primarily due to the high cost and limited capabilities of computer hardware. From the late 1980s through the 1990s, consumer-grade personal computers became powerful enough to display various media. The primary technical issues related to streaming were having enough CPU and bus bandwidth to support the required data rates, achieving real-time computing performance required to prevent buffer underrun and enable smooth streaming of the content. However, computer networks were still limited in the mid-1990s, and audio and video media were usually delivered over non-streaming channels, such as playback from a local hard disk drive or CD-ROMs on the end user's computer.\nIn 1990 the first commercial Ethernet switch was introduced by Kalpana, which enabled the more powerful computer networks that led to the first streaming video solutions used by schools and corporations.\nPractical streaming media was only made possible with advances in data compression, due to the impractically high bandwidth requirements of uncompressed media. Raw digital audio encoded with pulse-code modulation (PCM) requires a bandwidth of 1.4 Mbit/s for uncompressed CD audio, while raw digital video requires a bandwidth of 168 Mbit/s for SD video and over 1000 Mbit/s for FHD video.\n\n\n=== Late 1990s to early 2000s ===\n\nDuring the late 1990s and early 2000s, users had increased access to computer networks, especially the Internet. During the early 2000s, users had access to increased network bandwidth, especially in the last mile. These technological improvements facilitated the streaming of audio and video content to computer users in their homes and workplaces. There was also an increasing use of standard protocols and formats, such as TCP/IP, HTTP, HTML as the Internet became increasingly commercialized, which led to an infusion of investment into the sector.\nThe band Severe Tire Damage was the first group to perform live on the Internet. On June 24, 1993, the band was playing a gig at Xerox PARC while elsewhere in the building, scientists were discussing new technology (the Mbone) for broadcasting on the Internet using multicasting. As proof of PARC's technology, the band's performance was broadcast and could be seen live in Australia and elsewhere. In a March 2017 interview, band member Russ Haines stated that the band had used approximately \"half of the total bandwidth of the internet\" to stream the performance, which was a 152\u2009\u00d7\u200976 pixel video, updated eight to twelve times per second, with audio quality that was, \"at best, a bad telephone connection.\" In October 1994, a school music festival was webcast from the Michael Fowler Centre in Wellington, New Zealand. The technician who arranged the webcast, local council employee Richard Naylor, later commented: \"We had 16 viewers in 12 countries.\"RealNetworks pioneered the broadcast of a baseball game between the New York Yankees and the Seattle Mariners over the Internet in 1995. The first symphonic concert on the Internet\u2014a collaboration between the Seattle Symphony and guest musicians Slash, Matt Cameron, and Barrett Martin\u2014took place at the Paramount Theater in Seattle, Washington, on November 10, 1995.In 1996, Marc Scarpa produced the first large-scale, online, live broadcast, the Adam Yauch-led Tibetan Freedom Concert, an event that would define the format of social change broadcasts. Scarpa continued to pioneer in the streaming media world with projects such as Woodstock '99, Townhall with President Clinton, and more recently Covered CA's campaign \"Tell a Friend Get Covered\" which was live streamed on YouTube.\n\n\n=== Business developments ===\nXing Technology was founded in 1989, and developed a JPEG streaming product called \"StreamWorks\".  Another streaming product appeared in late 1992 and was named StarWorks. StarWorks enabled on-demand MPEG-1 full-motion videos to be randomly accessed on corporate Ethernet networks. Starworks was from Starlight Networks, who also pioneered live video streaming on Ethernet and via Internet Protocol over satellites with Hughes Network Systems. Other early companies that created streaming media technology include Progressive Networks and Protocomm prior to widespread World Wide Web usage.  After the Netscape IPO in 1995 (and the release of Windows 95, with built-in TCP/IP support), usage of the Internet expanded, and many companies \"went public\", including Progressive Networks (which was renamed \"RealNetworks\", and listed on Nasdaq as \"RNWK\"). As the web became even more popular in the late 90s, streaming video on the internet blossomed from startups such as Vivo Software (later acquired by RealNetworks), VDOnet (acquired by RealNetworks), Precept (acquired by Cisco), and Xing (acquired by RealNetworks).Microsoft developed a media player known as ActiveMovie in 1995 that supported streaming media and included a proprietary streaming format, which was the precursor to the streaming feature later in Windows Media Player 6.4 in 1999. In June 1999 Apple also introduced a streaming media format in its QuickTime 4 application. It was later also widely adopted on websites along with RealPlayer and Windows Media streaming formats. The competing formats on websites required each user to download the respective applications for streaming and resulted in many users having to have all three applications on their computer for general compatibility.\nIn 2000 Industryview.com launched its \"world's largest streaming video archive\" website to help businesses promote themselves. Webcasting became an emerging tool for business marketing and advertising that combined the immersive nature of television with the interactivity of the Web. The ability to collect data and feedback from potential customers caused this technology to gain momentum quickly.Around 2002, the interest in a single, unified, streaming format and the widespread adoption of Adobe Flash prompted the development of a video streaming format through Flash, which was the format used in Flash-based players on video hosting sites. The first popular video streaming site, YouTube, was founded by Steve Chen, Chad Hurley and Jawed Karim in 2005. It initially used a Flash-based player, which played MPEG-4 AVC video and AAC audio, but now defaults to HTML5 video. Increasing consumer demand for live streaming prompted YouTube to implement a new live streaming service to users. The company currently also offers a (secured) link returning the available connection speed of the user.The Recording Industry Association of America (RIAA) revealed through its 2015, earnings report that streaming services were responsible for 34.3 percent of the year's total music industry's revenue, growing 29 percent from the previous year and becoming the largest source of income, pulling in around $2.4 billion. US streaming revenue grew 57 percent to $1.6 billion in the first half of 2016 and accounted for almost half of industry sales.\n\n\n=== Streaming wars ===\n\nThe term streaming wars was coined to discuss the new era (starting in 2019) of competition between video streaming services such as Netflix, Amazon Prime Video, Hulu, Max, Disney+, Paramount+, Apple TV+, and Peacock.Competition among online platforms has forced them to find ways to differentiate themselves. One key way they have done this is by offering exclusive content, often self-produced and created specifically for a market. This approach to streaming competition can have disadvantages for consumers and the industry as a whole.  Once content is made available online, the corresponding piracy searches decrease. Competition or legal availability across multiple platforms effectively deters online piracy, and more exclusivity does not necessarily translate into higher average investment in content because investment decisions are also dependent on the level and type of competition in online markets.This competition was increased during the first two years of the COVID-19 pandemic as more people stayed home and watched TV. \"The COVID-19 pandemic has led to a seismic shift in the film & TV industry in terms of how films are made, distributed and screened. Many industries have been hit by the economic affect of the pandemic\" (Totaro Donato). In August 2022, a CNN headline declared \"The streaming wars are over\" as pandemic-era restrictions had largely ended and audience growth had stalled. This lead services to focus on profit over market share by cutting production budgets, cracking down on password sharing, and introducing ad-supported tiers. A December 2022 article in The Verge echoed this, declaring an end to the \"golden age of the streaming wars\".In September 2023, several streaming services formed a trade association named the Streaming Innovation Alliance (SIA), spearheaded by Charles Rivkin of the Motion Picture Association (MPA). Former U.S. representative Fred Upton and former Federal Communications Commission (FCC) acting chair Mignon Clyburn serve as senior advisors. Founding members include AfroLandTV, America Nu Network, BET+, Discovery+, Disney+, Disney+ Hotstar, ESPN+, For Us By Us Network, Hulu, Max, the MPA, MotorTrend+, Netflix, Paramount+, Peacock, Pluto TV, Star+, Telemundo, TelevisaUnivision, Vault TV, and Vix. Notably absent were Apple, Amazon, Roku, and Tubi.\n\n\n== Use by the general public ==\nAdvances in computer networking, combined with powerful home computers and operating systems made streaming media affordable and easy for the public. Stand-alone Internet radio devices emerged to offer listeners a non-technical option for listening to audio streams. These audio-streaming services became increasingly popular; streaming music reached 118.1 billion streams in 2013.\n\nIn general, multimedia content is data intensive, so media storage and transmission costs are still significant. Media is generally compressed for transport and storage. Increasing consumer demand for streaming of high-definition (HD) content has led the industry to develop technologies such as WirelessHD and G.hn, which are optimized for streaming HD content. Many developers have introduced HD streaming apps that work on smaller devices such as tablets and smartphones for everyday purposes. \nA media stream can be streamed either live or on demand. Live streams are generally provided by a means called true streaming. True streaming sends the information straight to the computer or device without saving to a local file. On-demand streaming is provided by a means called progressive download. Progressive download saves the received information to a local file and then is played from that location. On-demand streams are often saved to files for extended amounts of time; while the live streams are only available at one time only (e.g. during the football game).Streaming media is increasingly being coupled with use of social media. For example, sites such as YouTube encourage social interaction in webcasts through features such as live chat, online surveys, user posting of comments online and more. Furthermore, streaming media is increasingly being used for social business and e-learning.The Horowitz Research State of Pay TV, OTT and SVOD 2017 report said that 70 percent of those viewing content did so through a streaming service and that 40 percent of TV viewing was done this way, twice the number from five years earlier. Millennials, the report said, streamed 60 percent of content.\n\n\n=== Transition from DVD ===\nOne of the movie streaming industry's largest impacts was on the DVD industry, which drastically dropped in popularity and profitability with the mass popularization of online content. The rise of media streaming caused the downfall of many DVD rental companies such as Blockbuster. In July 2015, The New York Times published an article about Netflix's DVD services. It stated that Netflix was continuing their DVD services with 5.3 million subscribers, which was a significant drop from the previous year. On the other hand, their streaming services had 65 million members.\n\n\n=== Napster ===\nMusic streaming is one of the most popular ways in which consumers interact with streaming media. In the age of digitization, the private consumption of music transformed into a public good largely due to one player in the market: Napster.\nNapster, a peer-to-peer (P2P) file-sharing network where users could upload and download MP3 files freely, broke all music industry conventions when it launched in early 1999 in Hull, Massachusetts. The platform was developed by Shawn and John Fanning as well as Sean Parker. In an interview from 2009, Shawn Fanning explained that Napster \"was something that came to me as a result of seeing a sort of an unmet need and the passion people had for being able to find all this music, particularly a lot of the obscure stuff which wouldn't be something you go to a record store and purchase, so it felt like a problem worth solving.\"Not only did this development disrupt the music industry by making songs that previously required payment to be freely accessible to any Napster user, but it also demonstrated the power of P2P networks in turning any digital file into a public, shareable good. For the brief period of time that Napster existed, mp3 files fundamentally changed as a type of good. Songs were no longer financially excludable \u2013 barring access to a computer with internet access \u2013 and they were not rival, meaning if one person downloaded a song it did not diminish another user from doing the same. Napster, like most other providers of public goods, faced the free-rider problem. Every user benefits when an individual uploads an mp3 file, but there is no requirement or mechanism that forces all users to share their music. Generally, the platform encouraged sharing; users who downloaded files from others often had their own files available for upload as well. However, not everyone chose to share their files. There was not a built-in incentive specifically discouraging users from sharing their own files.This structure revolutionized the consumer's perception of ownership over digital goods \u2013 it made music freely replicable. Napster quickly garnered millions of users, growing faster than any other business in history. At the peak of its existence, Napster boasted about 80 million users globally. The site gained so much traffic that many college campuses had to block access to Napster because it created network congestion from so many students sharing music files.The advent of Napster sparked the creation of numerous other P2P sites including LimeWire (2000), BitTorrent (2001), and the Pirate Bay (2003). The reign of P2P networks was short-lived. The first to fall was Napster in 2001. Numerous lawsuits were filed against Napster by various record labels, all of which were subsidiaries of Universal Music Group, Sony Music Entertainment, Warner Music Group, or EMI. In addition to this, the Recording Industry Association of America (RIAA) also filed a lawsuit against Napster on the grounds of unauthorized distribution of copyrighted material, which ultimately led Napster to shut down in 2001. In an interview with the New York Times, Gary Stiffelman, who represents Eminem, Aerosmith, and TLC, explained, \"I'm not an opponent of artists' music being included in these services, I'm just an opponent of their revenue not being shared.\"\n\n\n==== The fight for intellectual property rights: A&M Records, Inc. v. Napster, Inc. ====\nThe lawsuit A&M Records, Inc. v. Napster, Inc. fundamentally changed the way consumers interact with music streaming. It was argued on 2 October 2000 and was decided on 12 February 2001. The Court of Appeals for the Ninth Circuit ruled that a P2P file-sharing service could be held liable for contributory and vicarious infringement of copyright, serving as a landmark decision for Intellectual property law.The first issue that the Court addressed was fair use, which says that otherwise infringing activities are permissible so long as it is for purposes \"such as criticism, comment, news reporting, teaching [...] scholarship, or research.\" Judge Beezer, the judge for this case, noted that Napster claimed that its services fit \"three specific alleged fair uses: sampling, where users make temporary copies of a work before purchasing; space-shifting, where users access a sound recording through the Napster system that they already own in audio CD format; and permissive distribution of recordings by both new and established artists.\" Judge Beezer found that Napster did not fit these criteria, instead enabling their users to repeatedly copy music, which would affect the market value of the copyrighted good.\nThe second claim by the plaintiffs was that Napster was actively contributing to copyright infringement since it had knowledge of widespread file sharing on its platform. Since Napster took no action to reduce infringement and financially benefited from repeated use, the court ruled against the P2P site. The court found that \"as much as eighty-seven percent of the files available on Napster may be copyrighted and more than seventy percent may be owned or administered by plaintiffs.\"The injunction ordered against Napster ended the brief period in which music streaming was a public good \u2013 non-rival and non-excludable in nature. Other P2P networks had some success at sharing MP3s, though they all met a similar fate in court. The ruling set the precedent that copyrighted digital content cannot be freely replicated and shared unless given consent by the owner, thereby strengthening the property rights of artists and record labels alike.\n\n\n=== Music streaming platforms ===\nAlthough music streaming is no longer a freely replicable public good, streaming platforms such as Spotify, Deezer, Apple Music, SoundCloud, YouTube Music, and Amazon Music have shifted music streaming to a club-type good. While some platforms, most notably Spotify, give customers access to a freemium service that enables the use of limited features for exposure to advertisements, most companies operate under a premium subscription model. Under such circumstances, music streaming is financially excludable, requiring that customers pay a monthly fee for access to a music library, but non-rival, since one customer's use does not impair another's.\nThere is competition between services similar but lesser to the streaming wars for video media. As of 2019 Spotify has over 207 million users in 78 countries, As of 2018 Apple Music has about 60 million, and SoundCloud has 175 million. All platforms provide varying degrees of accessibility. Apple Music and Prime Music only offer their services for paid subscribers, whereas Spotify and SoundCloud offer freemium and premium services. Napster, owned by Rhapsody since 2011, has resurfaced as a music streaming platform offering subscription-based services to over 4.5 million users as of January 2017.The music industry's response to music streaming was initially negative. Along with music piracy, streaming services disrupted the market and contributed to the fall in US revenue from $14.6 billion in 1999 to $6.3 billion in 2009. CDs and single-track downloads were not selling because content was freely available on the Internet. By 2018, however, music streaming revenue exceeded that of traditional revenue streams (e.g. record sales, album sales, downloads). Streaming revenue is now one of the largest driving forces behind the growth in the music industry. In an interview, Jonathan Dworkin, a senior vice president of strategy and business development at Universal, said, \"We cannot be afraid of perpetual change, because that dynamism is driving growth.\"\n\n\n=== COVID-19 pandemic ===\nBy August 2020, the COVID-19 pandemic had streaming services busier than ever. In the UK alone, twelve million people joined a new streaming service that they had not previously had.An impact analysis of 2020 data by the International Confederation of Societies of Authors and Composers (CISAC) indicated that remuneration from digital streaming of music increased with a strong rise in digital royalty collection (up 16.6% to EUR 2.4 billion), but it would not compensate the overall loss of income of authors from concerts, public performance and broadcast.  The International Federation of the Phonographic Industry (IFPI) recompiled the music industry initiatives around the world related to the COVID-19. In its State of the Industry report, it recorded that the global recorded music market grew by 7.4% in 2022, the 6th consecutive year of growth. This growth was driven by streaming, mostly from paid subscription streaming revenues which increased by 18.5%, fueled by 443 million users of subscription accounts by the end of 2020.The COVID-19 pandemic has also driven an increase in misinformation and disinformation, particularly on streaming platforms like YouTube and podcasts.\n\n\n=== Local/home streaming ===\nStreaming also refers to the offline streaming of multimedia at home. This is made possible by technologies such as DLNA, which allow devices on the same local network to connect to each other and share media.\n\n\n== Technologies ==\n\n\n=== Bandwidth ===\nA broadband speed of 2 Mbit/s or more is recommended for streaming standard-definition video, for example to a Roku, Apple TV, Google TV or a Sony TV Blu-ray Disc Player. 5 Mbit/s is recommended for high-definition content and 9 Mbit/s for ultra-high-definition content. Streaming media storage size is calculated from the streaming bandwidth and length of the media using the following formula (for a single user and file): storage size in megabytes is equal to length (in seconds) \u00d7 bit rate (in bit/s) / (8 \u00d7 1024 \u00d7 1024). For example, one hour of digital video encoded at 300 kbit/s (this was a typical broadband video in 2005 and it was usually encoded in 320\u2009\u00d7\u2009240 resolution) will be: (3,600 s \u00d7 300,000 bit/s) / (8 \u00d7 1024 \u00d7 1024) requires around 128 MB of storage.\nIf the file is stored on a server for on-demand streaming and this stream is viewed by 1,000 people at the same time using a Unicast protocol, the requirement is 300 kbit/s \u00d7 1,000 = 300,000 kbit/s = 300 Mbit/s of bandwidth. This is equivalent to around 135 GB per hour. Using a multicast protocol the server sends out only a single stream that is common to all users. Therefore, such a stream would only use 300 kbit/s of server bandwidth.\nIn 2018 video was more than 60% of data traffic worldwide and accounted for 80% of growth in data usage.\n\n\n=== Protocols ===\nVideo and audio streams are compressed to make the file size smaller. Audio coding formats include MP3, Vorbis, AAC and Opus. Video coding formats include H.264, HEVC, VP8 and VP9. Encoded audio and video streams are assembled in a container bitstream such as MP4, FLV, WebM, ASF or ISMA. The bitstream is delivered from a streaming server to a streaming client (e.g., the computer user with their Internet-connected laptop) using a transport protocol, such as Adobe's RTMP or RTP.\nIn the 2010s, technologies such as Apple's HLS, Microsoft's Smooth Streaming, Adobe's HDS and non-proprietary formats such as MPEG-DASH emerged to enable adaptive bitrate streaming over HTTP as an alternative to using proprietary transport protocols. Often, a streaming transport protocol is used to send video from an event venue to a cloud transcoding service and content delivery network, which then uses HTTP-based transport protocols to distribute the video to individual homes and users. The streaming client (the end user) may interact with the streaming server using a control protocol, such as MMS or RTSP.\nThe quality of the interaction between servers and users is based on the workload of the streaming service; as more users attempt to access a service the quality may be affected by resource constraints in the service. Deploying clusters of streaming servers is one such method where there are regional servers spread across the network, managed by a singular, central server containing copies of all the media files as well as the IP addresses of the regional servers. This central server then uses load balancing and scheduling algorithms to redirect users to nearby regional servers capable of accommodating them. This approach also allows the central server to provide streaming data to both users as well as regional servers using FFMpeg libraries if required, thus demanding the central server to have powerful data processing and immense storage capabilities. In return, workloads on the streaming backbone network are balanced and alleviated, allowing for optimal streaming quality.Designing a network protocol to support streaming media raises many problems. Datagram protocols, such as the User Datagram Protocol (UDP), send the media stream as a series of small packets. This is simple and efficient; however, there is no mechanism within the protocol to guarantee delivery. It is up to the receiving application to detect loss or corruption and recover data using error correction techniques. If data is lost, the stream may suffer a dropout. The Real Time Streaming Protocol (RTSP), Real-time Transport Protocol (RTP) and the Real-time Transport Control Protocol (RTCP) were specifically designed to stream media over networks. RTSP runs over a variety of transport protocols, while the latter two are built on top of UDP.\nHTTP adaptive bitrate streaming is based on HTTP progressive download, but contrary to the previous approach, here the files are very small, so that they can be compared to the streaming of packets, much like the case of using RTSP and RTP. Reliable protocols, such as the Transmission Control Protocol (TCP), guarantee correct delivery of each bit in the media stream. It means, however, that when there is data loss on the network, the media stream stalls while the protocol handlers detect the loss and retransmit the missing data. Clients can minimize this effect by buffering data for display. While delay due to buffering is acceptable in video-on-demand scenarios, users of interactive applications such as video conferencing will experience a loss of fidelity if the delay caused by buffering exceeds 200 ms.\nUnicast protocols send a separate copy of the media stream from the server to each recipient. Unicast is the norm for most Internet connections but does not scale well when many users want to view the same television program concurrently. Multicast protocols were developed to reduce server and network loads resulting from duplicate data streams that occur when many recipients receive unicast content streams independently. These protocols send a single stream from the source to a group of recipients. Depending on the network infrastructure and type, multicast transmission may or may not be feasible. One potential disadvantage of multicasting is the loss of video on demand functionality. Continuous streaming of radio or television material usually precludes the recipient's ability to control playback. However, this problem can be mitigated by elements such as caching servers, digital set-top boxes, and buffered media players.\nIP multicast provides a means to send a single media stream to a group of recipients on a computer network. A connection management protocol, usually Internet Group Management Protocol, is used to manage the delivery of multicast streams to the groups of recipients on a LAN. One of the challenges in deploying IP multicast is that routers and firewalls between LANs must allow the passage of packets destined to multicast groups. If the organization that is serving the content has control over the network between server and recipients (i.e., educational, government, and corporate intranets), then routing protocols such as Protocol Independent Multicast can be used to deliver stream content to multiple local area network segments. \nPeer-to-peer (P2P) protocols arrange for prerecorded streams to be sent between computers. This prevents the server and its network connections from becoming a bottleneck. However, it raises technical, performance, security, quality, and business issues.\nContent delivery networks (CDNs) use intermediate servers to distribute the load. Internet-compatible unicast delivery is used between CDN nodes and streaming destinations.\n\n\n=== Recording ===\nMedia that is livestreamed can be recorded through certain media players such as VLC player, or through the use of a screen recorder. Live-streaming platforms such as Twitch may also incorporate a video on demand system that allows automatic recording of live broadcasts so that they can be watched later. YouTube also has recordings of live broadcasts, including television shows aired on major networks. These streams have the potential to be recorded by anyone who has access to them, whether legally or otherwise.\n\n\n=== View recommendation ===\nMost streaming services feature a recommender system for viewing based on each user's view history in conjunction with all viewers' aggregated view histories. Rather than focusing on subjective categorization of content by content curators), there is an assumption that, with the immensity of data collected on viewing habits, the choices of those who are first to view content can be algorithmically extrapolated to the totality of the user base, with increasing probabilistic accuracy as to the likelihood of their choosing and enjoying the recommended content as more data is collected.\n\n\n== Applications and marketing ==\nUseful and typical applications of streaming are, for example, long video lectures performed online. An advantage of this presentation is that these lectures can be very long, although they can always be interrupted or repeated at arbitrary places. Streaming enables new content marketing concepts. For example, the Berlin Philharmonic Orchestra sells Internet live streams of whole concerts, instead of several CDs or similar fixed media, by their Digital Concert Hall using YouTube for trailers. These online concerts are also spread over a lot of different places including cinemas at various places on the globe. A similar concept is used by the Metropolitan Opera in New York. There also is a livestream from the International Space Station. In video entertainment, video streaming platforms like Netflix, Hulu, and Disney+ are mainstream elements of the media industry.Marketers have found many opportunities offered by streaming media and the platforms that offer them, especially in light of the significant increase in the use of streaming media during COVID lockdowns from 2020 onwards. While revenue and placement traditional advertising continues to decrease, digital marketing increased in 15% in 2021, with digital media and search representing 65% of the expenditures. \nA case study commissioned by the WIPO indicates that streaming services attract advertising budgets with the opportunities provided with interactivity and the use of data from users, resulting in personalization on a mass scale with content marketing. Targeted marketing is expanding with the use of artificial intelligence, in particular programmatic advertisement, a tool that helps advertisers decide their campaign parameters, and whether they are interested in buying advertising space online or not. One example of advertising space acquisition is Real-Time Bidding (RTB).\n\n\n== Challenges ==\n\n\n=== Copyright issues ===\n\nThe availability of large bandwidth internet enabled the audiovisual streaming services to attract large number of users around the world.\nFor OTT platforms, original content represents a critical variable in order to capture more subscribers. This generated a number of effects related to the copyright over the audiovisual content and its international exploitation through streaming such as contractual practices, international exploitation of rights, widespread use of standards and metadata in digital files. The WIPO has indicated the several basic copyright issues arising for those pursuing to work in the film and music industry in the era of streaming.\nStreaming copyrighted content can involve making infringing copies of the works in question. The recording and distribution of streamed content is also an issue for many companies that rely on revenue based on views or attendance.\n\n\n=== Greenhouse gas emissions ===\nThe net greenhouse gas emissions from streaming music were estimated at between 0.2 and 0.35 million metric tons CO2eq (between 200,000 and 340,000 long tons; 220,000 and 390,000 short tons) per year in the United States, by a 2019 study. This was an increase from emissions in the pre-digital music period, which were estimated at \"0.14 million metric tons (140,000 long tons; 150,000 short tons) in 1977, 0.136 million (134,000 long tons; 150,000 short tons) in 1988, and 0.157 million (155,000 long tons; 173,000 short tons) in 2000.\" However this is far less than other everyday activities such as eating, for example greenhouse gas emissions in the United States from beef cattle (burping of ruminants only - not including their manure) were 129 million metric tons (127 million long tons; 142 million short tons) in 2019.A 2021 study claimed that, based on the amount of data transmitted, one hour of streaming or videoconferencing \"emits 150\u20131,000 grams (5\u201335 oz) of carbon dioxide ... requires 2\u201312 liters (0.4\u20132.6 imp gal; 0.5\u20133.2 U.S. gal) of water and demands a land area adding up to about the size of an iPad Mini.\" The study suggests that turning the camera off during video calls can reduce the greenhouse gas and water use footprints by 96%, and that an 86% reduction is possible by using standard definition rather than high definition when streaming content with apps such as Netflix or Hulu. However another study estimated a relatively low amount of 36 grams per hour (1.3 ounces per hour), and concluded that watching a Netflix video for half an hour emitted only the same as driving a gasoline fuelled car for about 100 meters (330 ft), so not a significant amount.One way to decrease greenhouse gas emissions associated with streaming music is making data centers carbon neutral, by converting to electricity produced from renewable sources. On an individual level, purchase of a physical CD may be more environmentally friendly if it is to be played more than 27 times. Another option for reducing energy use can be downloading the music for offline listening, to reduce the need for streaming over distance. The Spotify service has a built-in local cache to reduce the necessity of repeating song streams.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nHagen, Anja Nylund (2020). Music in Streams: Communicating Music in the Streaming Paradigm, In Michael Filimowicz & Veronika Tzankova (ed.), Reimagining Communication: Mediation (1st Edition). Routledge.\nPreston, J. (11 December 2011). \"Occupy Video Showcases Live Streaming\". The New York Times.\nSherman, Alex (27 October 2019). \"AT&T, Disney and Comcast have very different plans for the streaming wars \u2013 here's what they're doing and why\". CNBC.\n\n\n== External links ==\n\"The Early History of the Streaming Media Industry and The Battle Between Microsoft & Real\". streamingmedia.com. March 2016. Archived from the original on 21 March 2016. Retrieved 25 March 2016.\n\"What is Streaming? A high-level view of streaming media technology, history\". streamingmedia.com. Retrieved 25 March 2016."}, {"id": 43, "title": "Solution (Chemistry)", "content": "In chemistry, a solution is a special type of homogeneous mixture composed of two or more substances. In such a mixture, a solute is a substance dissolved in another substance, known as a solvent. If the attractive forces between the solvent and solute particles are greater than the attractive forces holding the solute particles together, the solvent particles pull the solute particles apart and surround them. These surrounded solute particles then move away from the solid solute and out into the solution. The mixing process of a solution happens at a scale where the effects of chemical polarity are involved, resulting in interactions that are specific to solvation. The solution usually has the state of the solvent when the solvent is the larger fraction of the mixture, as is commonly the case. One important parameter of a solution is the concentration, which is a measure of the amount of solute in a given amount of solution or solvent. The term \"aqueous solution\" is used when one of the solvents is water.\n\n\n== Characteristics ==\nA solution is a homogeneous mixture of two or more substances.\nThe particles of solute in a solution cannot be seen by the naked eye. By contrast, particles may be visible in a suspension.\nA solution does not cause beams of light to scatter. By contrast, the particles in a suspension can cause Tyndall scattering or Rayleigh scattering.\nA solution is stable; solutes will not precipitate unless added in excess of the mixture's solubility, at which point the excess would remain in its solid phase, referred to as hypersaturation.\nThe solute from a solution cannot be separated by filtration (or mechanically).\nIt is composed of only one phase.\n\n\n== Types ==\nHomogeneous means that the components of the mixture form a single phase. Heterogeneous means that the components of the mixture are of different phase. The properties of the mixture (such as concentration, temperature, and density) can be uniformly distributed through the volume but only in absence of diffusion phenomena or after their completion. Usually, the substance present in the greatest amount is considered the solvent. Solvents can be gases, liquids, or solids. One or more components present in the solution other than the solvent are called solutes. The solution has the same physical state as the solvent.\n\n\n=== Gaseous mixtures ===\nIf the solvent is a gas, only gases (non-condensable) or vapors (condensable) are dissolved under a given set of conditions. An example of a gaseous solution is air (oxygen and other gases dissolved in nitrogen). Since interactions between gaseous molecules play almost no role, non-condensable gases form rather trivial solutions. In the literature, they are not even classified as solutions, but simply addressed as homogeneous mixtures of gases. The Brownian motion and the permanent molecular agitation of gas molecules guarantee the homogeneity of the gaseous systems. Non-condensable gases mixtures (e.g., air/CO2, or air/xenon) do not spontaneously demix, nor sediment, as distinctly stratified and separate gas layers as a function of their relative density. Diffusion forces efficiently counteract gravitation forces under normal conditions prevailing on Earth. The case of condensable vapors is different: once the saturation vapor pressure at a given temperature is reached, vapor excess condenses into the liquid state.\n\n\n=== Liquid solutions ===\nIf the solvent is a liquid, then almost all gases, liquids, and solids can be dissolved. Here are some examples:\n\nGas in liquid:\nOxygen in water\nCarbon dioxide in water \u2013 a less simple example, because the solution is accompanied by a chemical reaction (formation of ions). The visible bubbles in carbonated water are not the dissolved gas, but only an effervescence of carbon dioxide that has come out of solution; the dissolved gas itself is not visible since it is dissolved on a molecular level.\nLiquid in liquid:\nThe mixing of two or more substances of the same chemistry but different concentrations to form a constant. (Homogenization of solutions)\nAlcoholic beverages are basically solutions of ethanol in water.\nSolid in liquid:\nSucrose (table sugar) in water\nSodium chloride (NaCl) (table salt) or any other salt in water, which forms an electrolyte: When dissolving, salt dissociates into ions.\nSolutions in water are especially common, and are called aqueous solutions.\nNon-aqueous solutions are when the liquid solvent involved is not water.Counterexamples are provided by liquid mixtures that are not homogeneous: colloids, suspensions, emulsions are not considered solutions.\nBody fluids are examples of complex liquid solutions, containing many solutes. Many of these are electrolytes since they contain solute ions, such as potassium. Furthermore, they contain solute molecules like sugar and urea. Oxygen and carbon dioxide are also essential components of blood chemistry, where significant changes in their concentrations may be a sign of severe illness or injury.\n\n\n=== Solid solutions ===\nIf the solvent is a solid, then gases, liquids, and solids can be dissolved.\n\nGas in solids:\nHydrogen dissolves rather well in metals, especially in palladium; this is studied as a means of hydrogen storage.\nLiquid in solid:\nMercury in gold, forming an amalgam\nWater in solid salt or sugar, forming moist solids\nHexane in paraffin wax\nPolymers containing plasticizers such as phthalate (liquid) in PVC (solid)\nSolid in solid:\nSteel, basically a solution of carbon atoms in a crystalline matrix of iron atoms\nAlloys like bronze and many others\nRadium sulfate dissolved in barium sulfate: a true solid solution of Ra in BaSO4\n\n\n== Solubility ==\n\nThe ability of one compound to dissolve in another compound is called solubility. When a liquid can completely dissolve in another liquid the two liquids are miscible. Two substances that can never mix to form a solution are said to be immiscible.\nAll solutions have a positive entropy of mixing. The interactions between different molecules or ions may be energetically favored or not. If interactions are unfavorable, then the free energy decreases with increasing solute concentration. At some point, the energy loss outweighs the entropy gain, and no more solute particles can be dissolved; the solution is said to be saturated. However, the point at which a solution can become saturated can change significantly with different environmental factors, such as temperature, pressure, and contamination. For some solute-solvent combinations, a supersaturated solution can be prepared by raising the solubility (for example by increasing the temperature) to dissolve more solute and then lowering it (for example by cooling).\nUsually, the greater the temperature of the solvent, the more of a given solid solute it can dissolve. However, most gases and some compounds exhibit solubilities that decrease with increased temperature. Such behavior is a result of an exothermic enthalpy of solution. Some surfactants exhibit this behaviour. The solubility of liquids in liquids is generally less temperature-sensitive than that of solids or gases.\n\n\n== Properties ==\nThe physical properties of compounds such as melting point and boiling point change when other compounds are added. Together they are called colligative properties. There are several ways to quantify the amount of one compound dissolved in the other compounds collectively called concentration. Examples include molarity, volume fraction, and mole fraction.\nThe properties of ideal solutions can be calculated by the linear combination of the properties of its components. If both solute and solvent exist in equal quantities (such as in a 50% ethanol, 50% water solution), the concepts of \"solute\" and \"solvent\" become less relevant, but the substance that is more often used as a solvent is normally designated as the solvent (in this example, water).\n\n\n== Liquid solution characteristics ==\n\nIn principle, all types of liquids can behave as solvents: liquid noble gases, molten metals, molten salts, molten covalent networks, and molecular liquids. In the practice of chemistry and biochemistry, most solvents are molecular liquids. They can be classified into polar and non-polar, according to whether their molecules possess a permanent electric dipole moment. Another distinction is whether their molecules can form hydrogen bonds (protic and aprotic solvents). Water, the most commonly used solvent, is both polar and sustains hydrogen bonds.\n\nSalts dissolve in polar solvents, forming positive and negative ions that are attracted to the negative and positive ends of the solvent molecule, respectively. If the solvent is water, hydration occurs when the charged solute ions become surrounded by water molecules. A standard example is aqueous saltwater. Such solutions are called electrolytes. Whenever salt dissolves in water ion association has to be taken into account.\nPolar solutes dissolve in polar solvents, forming polar bonds or hydrogen bonds. As an example, all alcoholic beverages are aqueous solutions of ethanol. On the other hand, non-polar solutes dissolve better in non-polar solvents. Examples are hydrocarbons such as oil and grease that easily mix, while being incompatible with water.\nAn example of the immiscibility of oil and water is a leak of petroleum from a damaged tanker, that does not dissolve in the ocean water but rather floats on the surface.\n\n\n=== Preparation from constituent ingredients ===\nIt is common practice in laboratories to make a solution directly from its constituent ingredients.  There are three cases in practical calculation:\n\nCase 1: amount of solvent volume is given.\nCase 2: amount of solute mass is given.\nCase 3: amount of final solution volume is given.In the following equations, A is solvent, B is solute, and C is concentration. Solute volume contribution is considered through the ideal solution model.\n\nCase 1: amount (mL) of solvent volume VA is given. Solute mass mB = C VA dA /(100-C/dB)\nCase 2: amount of solute mass mB is given. Solvent volume VA = mB (100/C-1/ dB )\nCase 3: amount (mL) of final solution volume Vt is given. Solute mass mB = C Vt /100; Solvent volume VA=(100/C-1/ dB) mB\nCase 2: solute mass is known, VA = mB 100/C\nCase 3: total solution volume is known, same equation as case 1. VA=Vt; mB = C VA /100Example: Make 2 g/100mL of NaCl solution with 1 L water. The density of the resulting solution is considered to be equal to that of water, statement holding especially for dilute solutions, so the density information is not required.\n\nmB = C VA = ( 2 / 100 ) g/mL \u00d7 1000 mL = 20 gChemists often make concentrated stock solutions that may then be diluted as needed for laboratory applications. Standard solutions are those where concentrations of solutes are accurately and precisely known.\n\n\n== See also ==\n\nMolar solution \u2013 Measure of concentration of a chemicalPages displaying short descriptions of redirect targets\nPercentage solution (disambiguation)\nSolubility equilibrium \u2013 Thermodynamic equilibrium between a solid and a solution of the same compound\nTotal dissolved solids \u2013 Measurement in environmental chemistry is a common term in a range of disciplines, and can have different meanings depending on the analytical method used. In water quality, it refers to the amount of residue remaining after the evaporation of water from a sample.\nUpper critical solution temperature \u2013 Critical temperature of miscibility in a mixture\nLower critical solution temperature \u2013 Critical temperature below which components of a mixture are miscible for all compositions\nCoil\u2013globule transition \u2013 Collapse of a macromolecule from an expanded coil state to a collapsed globule state\n\n\n== References ==\n\nIUPAC, Compendium of Chemical Terminology, 2nd ed. (the \"Gold Book\") (1997). Online corrected version: (2006\u2013) \"solution\". doi:10.1351/goldbook.S05746\n\n\n== External links ==\n Media related to Solutions at Wikimedia Commons"}, {"id": 44, "title": "Quantum key distribution", "content": "Quantum key distribution (QKD) is a secure communication method that implements a cryptographic protocol involving components of quantum mechanics. It enables two parties to produce a shared random secret key known only to them, which then can be used to encrypt and decrypt messages. The process of quantum key distribution is not to be confused with quantum cryptography, as it is the best-known example of a quantum-cryptographic task.\nAn important and unique property of quantum key distribution is the ability of the two communicating users to detect the presence of any third party trying to gain knowledge of the key. This results from a fundamental aspect of quantum mechanics: the process of measuring a quantum system in general disturbs the system. A third party trying to eavesdrop on the key must in some way measure it, thus introducing detectable anomalies. By using quantum superpositions or quantum entanglement and transmitting information in quantum states, a communication system can be implemented that detects eavesdropping. If the level of eavesdropping is below a certain threshold, a key can be produced that is guaranteed to be secure (i.e., the eavesdropper has no information about it). Otherwise no secure key is possible, and communication is aborted.\nThe security of encryption that uses quantum key distribution relies on the foundations of quantum mechanics, in contrast to traditional public key cryptography, which relies on the computational difficulty of certain mathematical functions, and cannot provide any mathematical proof as to the actual complexity of reversing the one-way functions used. QKD has provable security based on information theory, and forward secrecy.\nThe main drawback of quantum-key distribution is that it usually relies on having an authenticated classical channel of communication. In modern cryptography, having an authenticated classical channel means that one already has exchanged either a symmetric key of sufficient length or public keys of sufficient security level. With such information already available, in practice one can achieve authenticated and sufficiently secure communication without using QKD, such as by using the Galois/Counter Mode of the Advanced Encryption Standard. Thus QKD does the work of a stream cipher at many times the cost.\nQuantum key distribution is used to produce and distribute only a key, not to transmit any message data. This key can then be used with any chosen encryption algorithm to encrypt (and decrypt) a message, which can then be transmitted over a standard communication channel. The algorithm most commonly associated with QKD is the one-time pad, as it is provably secure when used with a secret, random key. In real-world situations, it is often also used with encryption using symmetric key algorithms like the Advanced Encryption Standard algorithm.\n\n\n== Quantum key exchange ==\nQuantum communication involves encoding information in quantum states, or qubits, as opposed to classical communication's use of bits. Usually, photons are used for these quantum states. Quantum key distribution exploits certain properties of these quantum states to ensure its security. There are several different approaches to quantum key distribution, but they can be divided into two main categories depending on which property they exploit.\n\nPrepare and measure protocols\nIn contrast to classical physics, the act of measurement is an integral part of quantum mechanics. In general, measuring an unknown quantum state changes that state in some way. This is a consequence of quantum indeterminacy and can be exploited in order to detect any eavesdropping on communication (which necessarily involves measurement) and, more importantly, to calculate the amount of information that has been intercepted.Entanglement based protocols\nThe quantum states of two (or more) separate objects can become linked together in such a way that they must be described by a combined quantum state, not as individual objects. This is known as entanglement and means that, for example, performing a measurement on one object affects the other. If an entangled pair of objects is shared between two parties, anyone intercepting either object alters the overall system, revealing the presence of the third party (and the amount of information they have gained).These two approaches can each be further divided into three families of protocols: discrete variable, continuous variable and distributed phase reference coding. Discrete variable protocols were the first to be invented, and they remain the most widely implemented. The other two families are mainly concerned with overcoming practical limitations of experiments. The two protocols described below both use discrete variable coding.\n\n\n=== BB84 protocol: Charles H. Bennett and Gilles Brassard (1984) ===\n\nThis protocol, known as BB84 after its inventors and year of publication, was originally described using photon polarization states to transmit the information. However, any two pairs of conjugate states can be used for the protocol, and many optical-fibre-based implementations described as BB84 use phase encoded states. The sender (traditionally referred to as Alice) and the receiver (Bob) are connected by a quantum communication channel which allows quantum states to be transmitted. In the case of photons this channel is generally either an optical fibre or simply free space. In addition they communicate via a public classical channel, for example using broadcast radio or the internet. The protocol is designed with the assumption that an eavesdropper (referred to as Eve) can interfere in any way with the quantum channel, while the classical channel needs to be authenticated.The security of the protocol comes from encoding the information in non-orthogonal states. Quantum indeterminacy means that these states cannot in general be measured without disturbing the original state (see No-cloning theorem). BB84 uses two pairs of states, with each pair conjugate to the other pair, and the two states within a pair orthogonal to each other. Pairs of orthogonal states are referred to as a basis. The usual polarization state pairs used are either the rectilinear basis of vertical (0\u00b0) and horizontal (90\u00b0), the diagonal basis of 45\u00b0 and 135\u00b0 or the circular basis of left- and right-handedness. Any two of these bases are conjugate to each other, and so any two can be used in the protocol. Below the rectilinear and diagonal bases are used.\n\nThe first step in BB84 is quantum transmission. Alice creates a random bit (0 or 1) and then randomly selects one of her two bases (rectilinear or diagonal in this case) to transmit it in. She then prepares a photon polarization state depending both on the bit value and basis, as shown in the adjacent table. So for example a 0 is encoded in the rectilinear basis (+) as a vertical polarization state, and a 1 is encoded in the diagonal basis (x) as a 135\u00b0 state. Alice then transmits a single photon in the state specified to Bob, using the quantum channel. This process is then repeated from the random bit stage, with Alice recording the state, basis and time of each photon sent.\nAccording to quantum mechanics (particularly quantum indeterminacy), no possible measurement distinguishes between the 4 different polarization states, as they are not all orthogonal. The only possible measurement is between any two orthogonal states (an orthonormal basis). So, for example, measuring in the rectilinear basis gives a result of horizontal or vertical. If the photon was created as horizontal or vertical (as a rectilinear eigenstate) then this measures the correct state, but if it was created as 45\u00b0 or 135\u00b0 (diagonal eigenstates) then the rectilinear measurement instead returns either horizontal or vertical at random. Furthermore, after this measurement the photon is polarized in the state it was measured in (horizontal or vertical), with all information about its initial polarization lost.\nAs Bob does not know the basis the photons were encoded in, all he can do is to select a basis at random to measure in, either rectilinear or diagonal. He does this for each photon he receives, recording the time, measurement basis used and measurement result. After Bob has measured all the photons, he communicates with Alice over the public classical channel. Alice broadcasts the basis each photon was sent in, and Bob the basis each was measured in. They both discard photon measurements (bits) where Bob used a different basis, which is half on average, leaving half the bits as a shared key.\n\nTo check for the presence of an eavesdropper, Alice and Bob now compare a predetermined subset of their remaining bit strings. If a third party (usually referred to as Eve, for \"eavesdropper\") has gained any information about the photons' polarization, this introduces errors in Bob's measurements. Other environmental conditions can cause errors in a similar fashion. If more than \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   bits differ they abort the key and try again, possibly with a different quantum channel, as the security of the key cannot be guaranteed. \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is chosen so that if the number of bits known to Eve is less than this, privacy amplification can be used to reduce Eve's knowledge of the key to an arbitrarily small amount at the cost of reducing the length of the key.\n\n\n=== E91 protocol: Artur Ekert (1991) ===\nArtur Ekert's scheme uses entangled pairs of photons. These can be created by Alice, by Bob, or by some source separate from both of them, including eavesdropper Eve. The photons are distributed so that Alice and Bob each end up with one photon from each pair.\nThe scheme relies on two properties of entanglement. First, the entangled states are perfectly correlated in the sense that if Alice and Bob both measure whether their particles have vertical or horizontal polarizations, they always get the same answer with 100% probability. The same is true if they both measure any other pair of complementary (orthogonal) polarizations. This necessitates that the two distant parties have exact directionality synchronization. However, the particular results are completely random; it is impossible for Alice to predict if she (and thus Bob) will get vertical polarization or horizontal polarization. Second, any attempt at eavesdropping by Eve destroys these correlations in a way that Alice and Bob can detect.\nSimilarly to BB84, the protocol involves a private measurement protocol before detecting the presence of Eve. The measurement stage involves Alice measuring each photon she receives using some basis from the set \n  \n    \n      \n        \n          Z\n          \n            0\n          \n        \n        ,\n        \n          Z\n          \n            \n              \u03c0\n              8\n            \n          \n        \n        ,\n        \n          Z\n          \n            \n              \u03c0\n              4\n            \n          \n        \n      \n    \n    {\\displaystyle Z_{0},Z_{\\frac {\\pi }{8}},Z_{\\frac {\\pi }{4}}}\n   while Bob chooses from  \n  \n    \n      \n        \n          Z\n          \n            0\n          \n        \n        ,\n        \n          Z\n          \n            \n              \u03c0\n              8\n            \n          \n        \n        ,\n        \n          Z\n          \n            \u2212\n            \n              \n                \u03c0\n                8\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Z_{0},Z_{\\frac {\\pi }{8}},Z_{-{\\frac {\\pi }{8}}}}\n   where \n  \n    \n      \n        \n          Z\n          \n            \u03b8\n          \n        \n      \n    \n    {\\displaystyle Z_{\\theta }}\n   is the \n  \n    \n      \n        {\n        \n          |\n        \n        \n          \u2191\n        \n        \u27e9\n        ,\n        \n        \n          |\n        \n        \n          \u2192\n        \n        \u27e9\n        }\n      \n    \n    {\\displaystyle \\{|{\\uparrow }\\rangle ,\\;|{\\rightarrow }\\rangle \\}}\n   basis rotated by \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  . They keep their series of basis choices private until measurements are completed. Two groups of photons are made: the first consists of photons measured using the same basis by Alice and Bob while the second contains all other photons. To detect eavesdropping, they can compute the test statistic \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n   using the correlation coefficients between Alice's bases and Bob's similar to that shown in the Bell test experiments. Maximally entangled photons would result in \n  \n    \n      \n        \n          |\n        \n        S\n        \n          |\n        \n        =\n        2\n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |S|=2{\\sqrt {2}}}\n  . If this were not the case, then Alice and Bob can conclude Eve has introduced local realism to the system, violating Bell's Theorem. If the protocol is successful, the first group can be used to generate keys since those photons are completely anti-aligned between Alice and Bob.\n\n\n=== Device Independent Quantum Key Distribution ===\nIn traditional QKD, the quantum devices used must be perfectly calibrated, trustworthy, and working exactly as they are expected to. Deviations from expected measurements can be extremely hard to detect, which leaves the entire system vulnerable. A new protocol called Device Independent QKD (DIQKD) or Measurement Device Independent QKD (MDIQKD) allows for the use of uncharacterized or untrusted devices, and for deviations from expected measurements to be included in the overall system. These deviations will cause the protocol to abort when detected, rather than resulting in incorrect data.DIQKD was first proposed by Mayers and Yao, building off of the BB84 protocol. They presented that in DIQKD, the quantum device, which they refer to as the photon source, be manufactured to come with tests that can be run by Alice and Bob to \u201cself-check\u201d if their device is working properly. Such a test would only need to consider the classical inputs and outputs in order to determine how much information is at risk of being intercepted by Eve. A self checking, or \u201cideal\u201d source would not have to be characterized, and would therefore not be susceptible to implementation flaws.Recent research has proposed using a Bell test to check that a device is working properly. Bell\u2019s theorem ensures that a device can create two outcomes that are exclusively correlated, meaning that Eve could not intercept the results, without making any assumptions about said device. This requires highly entangled states, and a low quantum bit error rate. DIQKD presents difficulties in creating qubits that are in such high quality entangled states, which makes it a challenge to realize experimentally.\n\n\n=== Twin Fields Quantum Key Distribution ===\nTwin Fields Quantum Key Distribution (TFQKD) was introduced in 2018, and is a version of DIQKD designed to overcome the fundamental rate-distance limit of traditional quantum key distribution. The rate-distance limit, also known as the rate-loss trade off, describes how as distance increases between Alice and Bob, the rate of key generation decreases exponentially. In traditional QKD protocols, this decay has been eliminated via the addition of physically secured relay nodes, which can be placed along the quantum link with the intention of dividing it up into several low-loss sections. Researchers have also recommended the use of quantum repeaters, which when added to the relay nodes make it so that they no longer need to be physically secured. Quantum repeaters, however, are difficult to create and have yet to be implemented on a useful scale. TFQKD aims to bypass the rate-distance limit without the use of quantum repeaters or relay nodes, creating manageable levels of noise and a process that can be repeated much more easily with today's existing technology.The original protocol for TFQKD is as follows: Alice and Bob each have a light source and one arm on an interferometer in their laboratories. The light sources create two dim optical pulses with a randomly phase pa or pb in the interval [0, 2\u03c0) and an encoding phase \u03b3a or \u03b3b. The pulses are sent along a quantum to Charlie, a third party who can be malicious or not. Charlie uses a beam splitter to overlap the two pulses and perform a measurement. He has two detectors in his own lab, one of which will light up if the bits are equal (00) or (11), and the other when they are different (10, 01). Charlie will announce to Alice and Bob which of the detectors lit up, at which point they publicly reveal the phases p and \u03b3. This is different from traditional QKD, in which the phases used are never revealed.\n\n\n== Information reconciliation and privacy amplification ==\nThe quantum key distribution protocols described above provide Alice and Bob with nearly identical shared keys, and also with an estimate of the discrepancy between the keys. These differences can be caused by eavesdropping, but also by imperfections in the transmission line and detectors. As it is impossible to distinguish between these two types of errors, guaranteed security requires the assumption that all errors are due to eavesdropping. Provided the error rate between the keys is lower than a certain threshold (27.6% as of 2002), two steps can be performed to first remove the erroneous bits and then reduce Eve's knowledge of the key to an arbitrary small value. These two steps are known as information reconciliation and privacy amplification respectively, and were first described in 1992.Information reconciliation is a form of error correction carried out between Alice and Bob's keys, in order to ensure both keys are identical. It is conducted over the public channel and as such it is vital to minimise the information sent about each key, as this can be read by Eve. A common protocol used for information reconciliation is the cascade protocol, proposed in 1994. This operates in several rounds, with both keys divided into blocks in each round and the parity of those blocks compared. If a difference in parity is found then a binary search is performed to find and correct the error. If an error is found in a block from a previous round that had correct parity then another error must be contained in that block; this error is found and corrected as before. This process is repeated recursively, which is the source of the cascade name. After all blocks have been compared, Alice and Bob both reorder their keys in the same random way, and a new round begins. At the end of multiple rounds Alice and Bob have identical keys with high probability; however, Eve has additional information about the key from the parity information exchanged. However, from a coding theory point of view information reconciliation is essentially source coding with side information. In consequence any coding scheme that works for this problem can be used for information reconciliation. Lately turbocodes, LDPC codes and polar codes have been used for this purpose improving the efficiency of the cascade protocol.\nPrivacy amplification is a method for reducing (and effectively eliminating) Eve's partial information about Alice and Bob's key. This partial information could have been gained both by eavesdropping on the quantum channel during key transmission (thus introducing detectable errors), and on the public channel during information reconciliation (where it is assumed Eve gains all possible parity information). Privacy amplification uses Alice and Bob's key to produce a new, shorter key, in such a way that Eve has only negligible information about the new key. This can be done using a universal hash function, chosen at random from a publicly known set of such functions, which takes as its input a binary string of length equal to the key and outputs a binary string of a chosen shorter length. The amount by which this new key is shortened is calculated, based on how much information Eve could have gained about the old key (which is known due to the errors this would introduce), in order to reduce the probability of Eve having any knowledge of the new key to a very low value.\n\n\n== Implementations ==\n\n\n=== Experimental ===\nIn 1991, John Rarity, Paul Tapster and Artur Ekert, researchers from the UK Defence Research Agency in Malvern and Oxford University, demonstrated quantum key distribution protected by the violation of the Bell inequalities. \nIn 2008, exchange of secure keys at 1 Mbit/s (over 20 km of optical fibre) and 10 kbit/s (over 100 km of fibre), was achieved by a collaboration between the University of Cambridge and Toshiba using the BB84 protocol with decoy state pulses.In 2007, Los Alamos National Laboratory/NIST achieved quantum key distribution over a 148.7 km of optic fibre using the BB84 protocol. Significantly, this distance is long enough for almost all the spans found in today's fibre networks. A European collaboration achieved free space QKD over 144 km between two of the Canary Islands using entangled photons (the Ekert scheme) in 2006, and using BB84 enhanced with decoy states in 2007.As of August 2015 the longest distance for optical fiber (307 km) was achieved by University of Geneva and Corning Inc. In the same experiment, a secret key rate of 12.7 kbit/s was generated, making it the highest bit rate system over distances of 100 km.  In 2016 a team from Corning and various institutions in China achieved a distance of 404 km, but at a bit rate too slow to be practical.In June 2017, physicists led by Thomas Jennewein at the Institute for Quantum Computing and the University of Waterloo in Waterloo, Canada achieved the first demonstration of quantum key distribution from a ground transmitter to a moving aircraft. They reported optical links with distances between 3\u201310 km and generated secure keys up to 868 kilobytes in length.Also in June 2017, as part of the Quantum Experiments at Space Scale project, Chinese physicists led by Pan Jianwei at the University of Science and Technology of China measured entangled photons over a distance of 1203 km between two ground stations, laying the groundwork for future intercontinental quantum key distribution experiments. Photons were sent from one ground station to the satellite they had named Micius and back down to another ground station, where they \"observed a survival of two-photon entanglement and a violation of Bell inequality by 2.37 \u00b1 0.09 under strict Einstein locality conditions\" along a \"summed length varying from 1600 to 2400 kilometers.\" Later that year BB84 was successfully implemented over satellite links from Micius to ground stations in China and Austria.  The keys were combined and the result was used to transmit images and video between Beijing, China, and Vienna, Austria.In August 2017, a group at Shanghai Jiaotong University experimentally demonstrate that polarization quantum states including general qubits of single photon and entangled states can survive well after travelling through seawater, representing the first step towards underwater quantum communication.\nIn May 2019 a group lead by Hong Guo at Peking University and Beijing University of Posts and Telecommunications reported field tests of a continuous-variable QKD system through commercial fiber networks in Xi'an and Guangzhou over distances of 30.02 km (12.48 dB) and 49.85 km (11.62 dB) respectively.In December 2020, Indian Defence Research and Development Organisation tested a QKD between two of its laboratories in Hyderabad facility. The setup also demonstrated the validation of detection of a third party trying to gain knowledge of the communication. Quantum based security against eavesdropping was validated for the deployed system at over 12 km (7.5 mi) range and 10 dB attenuation over fibre optic channel. A continuous wave laser source was used to generate photons without depolarization effect and timing accuracy employed in the setup was of the order of picoseconds. The Single photon avalanche detector (SPAD) recorded arrival of photons and key rate was achieved in the range of kbps with low Quantum bit error rate.In March 2021, Indian Space Research Organisation also demonstrated a free-space Quantum Communication over a distance of 300 meters. A free-space QKD was demonstrated at Space Applications Centre (SAC), Ahmedabad, between two line-of-sight buildings within the campus for video conferencing by quantum-key encrypted signals. The experiment utilised a NAVIC receiver for time synchronization between the transmitter and receiver modules. Later in January 2022, Indian scientists were able to successfully create an atmospheric channel for exchange of crypted messages and images. After demonstrating quantum communication between two ground stations, India has plans to develop Satellite Based Quantum Communication (SBQC).In July of  2022, researchers published their work experimentally implementing a device-independent quantum key distribution (DIQKD) protocol that uses quantum entanglement (as suggested by Ekert) to insure resistance to quantum hacking attacks. They were able to create two ions, about two meters apart that were in a high quality entangled state using the following process: Alice and Bob each have ion trap nodes with an 88Sr+ qubit inside. Initially, they excite the ions to an electronic state, which creates an entangled state. This process also creates two photons, which are then captured and transported using an optical fiber, at which point a Bell-basis measurement is performed and the ions are projected to a highly entangled state. Finally the qubits are returned to new locations in the ion traps disconnected from the optical link so that no information can be leaked. This is repeated many times before the key distribution proceeds.A separate experiment published in July 2022 demonstrated implementation of DIQKD that also uses a Bell inequality test to ensure that the quantum device is functioning, this time at a much larger distance of about 400m, using an optical fiber 700m long.  The set up for the experiment was similar to the one in the paragraph above, with some key differences. Entanglement was generated in a Quantum Network Link (QNL) between two 87Rb atoms in separate laboratories located 400m apart, connected by the 700m channel.The atoms are entangled by electronic excitation, at which point two photons are generated and collected, to be sent to the bell state measurement (BSM) setup. The photons are projected onto a |\u03c8+ state, indicating maximum entanglement. The rest of the key exchange protocol used is similar to the original QKD protocol, with the only difference being that keys are generated with two measurement settings instead of one.Since the proposal of Twin Field Quantum Key Distribution in 2018, a myriad of experiments have been performed with the goal of increasing the distance in a QKD system. The most successful of which was able to distribute key information across a distance of 833.8 km.In 2023, Scientists at Indian Institute of Technology (IIT) Delhi have achieved a trusted-node-free quantum key distribution (QKD) up to 380 km in standard telecom fiber with a very low quantum bit error rate (QBER).\n\n\n=== Commercial ===\nMany companies around the world offer commercial quantum key distribution, for example: ID Quantique (Geneva), MagiQ Technologies, Inc. (New York), QNu Labs (Bengaluru, India), QuintessenceLabs (Australia), QRate (Russia), SeQureNet (Paris), Quantum Optics Jena (Germany) and KEEQuant (Germany). Several other companies also have active research programs, including KETS Quantum Security (UK), Toshiba, HP, IBM, Mitsubishi, NEC and NTT (See External links for direct research links).\nIn 2004, the world's first bank transfer using quantum key distribution was carried out in Vienna, Austria. Quantum encryption technology provided by the Swiss company Id Quantique was used in the Swiss canton (state) of Geneva to transmit ballot results to the capital in the national election occurring on 21 October 2007. In 2013, Battelle Memorial Institute installed a QKD system built by ID Quantique between their main campus in Columbus, Ohio and their manufacturing facility in nearby Dublin. Field tests of Tokyo QKD network have been underway for some time.\n\n\n=== Quantum key distribution networks ===\n\n\n==== DARPA ====\nThe DARPA Quantum Network, was a 10-node quantum key distribution network, which ran continuously for four years, 24 hours a day, from 2004 to 2007 in Massachusetts in the United States. It was developed by BBN Technologies, Harvard University, Boston University, with collaboration from IBM Research, the National Institute of Standards and Technology, and QinetiQ. It supported a standards-based Internet computer network protected by quantum key distribution.\n\n\n==== SECOQC ====\n\nThe world's first computer network protected by quantum key distribution was implemented in October 2008, at a scientific conference in Vienna. The name of this network is SECOQC (Secure Communication Based on Quantum Cryptography) and the EU funded this project. The network used 200 km of standard fibre-optic cable to interconnect six locations across Vienna and the town of St Poelten located 69 km to the west.\n\n\n==== SwissQuantum ====\nId Quantique has successfully completed the longest running project for testing Quantum Key Distribution (QKD) in a field environment. The main goal of the SwissQuantum network project installed in the Geneva metropolitan area in March 2009, was to validate the reliability and robustness of QKD in continuous operation over a long time period in a field environment. The quantum layer operated for nearly 2 years until the project was shut down in January 2011 shortly after the initially planned duration of the test.\n\n\n==== Chinese networks ====\nIn May 2009, a hierarchical quantum network was demonstrated in Wuhu, China. The hierarchical network consisted of a backbone network of four nodes connecting a number of subnets. The backbone nodes were connected through an optical switching quantum router. Nodes within each subnet were also connected through an optical switch, which were connected to the backbone network through a trusted relay.Launched in August 2016, the QUESS space mission created an international QKD channel between China and the Institute for Quantum Optics and Quantum Information in Vienna, Austria \u2212 a ground distance of 7,500 km (4,700 mi), enabling the first intercontinental secure quantum video call. By October 2017, a 2,000-km fiber line was operational between Beijing, Jinan, Hefei and Shanghai. Together they constitute the world's first space-ground quantum network. Up to 10 Micius/QUESS satellites are expected, allowing a European\u2013Asian quantum-encrypted network by 2020, and a global network by 2030.\n\n\n==== Tokyo QKD Network ====\nThe Tokyo QKD Network was inaugurated on the first day of the UQCC2010 conference. The network involves an international collaboration between 7 partners; NEC, Mitsubishi Electric, NTT and NICT from Japan, and participation from Europe by Toshiba Research Europe Ltd. (UK), Id Quantique (Switzerland) and All Vienna (Austria). \"All Vienna\" is represented by researchers from the Austrian Institute of Technology (AIT), the Institute for Quantum Optics and Quantum Information (IQOQI) and the University of Vienna.\n\n\n==== Los Alamos National Laboratory ====\nA hub-and-spoke network has been operated by Los Alamos National Laboratory since 2011. All messages are routed via the hub. The system equips each node in the network with quantum transmitters\u2014i.e., lasers\u2014but not with expensive and bulky photon detectors. Only the hub receives quantum messages. To communicate, each node sends a one-time pad to the hub, which it then uses to communicate securely over a classical link. The hub can route this message to another node using another one time pad from the second node. The entire network is secure only if the central hub is secure. Individual nodes require little more than a laser: Prototype nodes are around the size of a box of matches.\n\n\n==== Eagle-1 ====\nIn 2024, the ESA plans to launch the satellite Eagle-1, an experimental space-based quantum key distribution system.\n\n\n== Attacks and security proofs ==\n\n\n=== Intercept and resend ===\nThe simplest type of possible attack is the intercept-resend attack, where Eve measures the quantum states (photons) sent by Alice and then sends replacement states to Bob, prepared in the state she measures. In the BB84 protocol, this produces errors in the key Alice and Bob share. As Eve has no knowledge of the basis a state sent by Alice is encoded in, she can only guess which basis to measure in, in the same way as Bob. If she chooses correctly, she measures the correct photon polarization state as sent by Alice, and resends the correct state to Bob. However, if she chooses incorrectly, the state she measures is random, and the state sent to Bob cannot be the same as the state sent by Alice. If Bob then measures this state in the same basis Alice sent, he too gets a random result\u2014as Eve has sent him a state in the opposite basis\u2014with a 50% chance of an erroneous result (instead of the correct result he would get without the presence of Eve). The table below shows an example of this type of attack.\n\nThe probability Eve chooses the incorrect basis is 50% (assuming Alice chooses randomly), and if Bob measures this intercepted photon in the basis Alice sent he gets a random result, i.e., an incorrect result with probability of 50%. The probability an intercepted photon generates an error in the key string is then 50% \u00d7 50% = 25%. If Alice and Bob publicly compare \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   of their key bits (thus discarding them as key bits, as they are no longer secret) the probability they find disagreement and identify the presence of Eve is\n\nSo to detect an eavesdropper with probability \n  \n    \n      \n        \n          P\n          \n            d\n          \n        \n        =\n        0.999999999\n      \n    \n    {\\displaystyle P_{d}=0.999999999}\n   Alice and Bob need to compare \n  \n    \n      \n        n\n        =\n        72\n      \n    \n    {\\displaystyle n=72}\n   key bits.\n\n\n=== Man-in-the-middle attack ===\nQuantum key distribution is vulnerable to a man-in-the-middle attack when used without authentication to the same extent as any classical protocol, since no known principle of quantum mechanics can distinguish friend from foe. As in the classical case, Alice and Bob cannot authenticate each other and establish a secure connection without some means of verifying each other's identities (such as an initial shared secret). If Alice and Bob have an initial shared secret then they can use an unconditionally secure authentication scheme (such as Carter-Wegman,) along with quantum key distribution to exponentially expand this key, using a small amount of the new key to authenticate the next session. Several methods to create this initial shared secret have been proposed, for example using a 3rd party or chaos theory. Nevertheless, only \"almost strongly universal\" family of hash functions can be used for unconditionally secure authentication.\n\n\n=== Photon number splitting attack ===\nIn the BB84 protocol Alice sends quantum states to Bob using single photons. In practice many implementations use laser pulses attenuated to a very low level to send the quantum states. These laser pulses contain a very small number of photons, for example 0.2 photons per pulse, which are distributed according to a Poisson distribution. This means most pulses actually contain no photons (no pulse is sent), some pulses contain 1 photon (which is desired) and a few pulses contain 2 or more photons. If the pulse contains more than one photon, then Eve can split off the extra photons and transmit the remaining single photon to Bob. This is the basis of the photon number splitting attack, where Eve stores these extra photons in a quantum memory until Bob detects the remaining single photon and Alice reveals the encoding basis. Eve can then measure her photons in the correct basis and obtain information on the key without introducing detectable errors.\nEven with the possibility of a PNS attack a secure key can still be generated, as shown in the GLLP security proof; however, a much higher amount of privacy amplification is needed reducing the secure key rate significantly (with PNS the rate scales as \n  \n    \n      \n        \n          t\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle t^{2}}\n   as compared to \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   for a single photon sources, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is the transmittance of the quantum channel).\nThere are several solutions to this problem. The most obvious is to use a true single photon \nsource instead of an attenuated laser. While such sources are still at a developmental stage QKD has been carried out successfully with them. However, as current sources operate at a low efficiency and frequency key rates and transmission distances are limited. Another solution is to modify the BB84 protocol, as is done for example in the SARG04 protocol, in which the secure key rate scales as \n  \n    \n      \n        \n          t\n          \n            3\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle t^{3/2}}\n  . The most promising solution is the decoy states in which Alice randomly sends some of her laser pulses with a lower average photon number. These decoy states can be used to detect a PNS attack, as Eve has no way to tell which pulses are signal and which decoy. Using this idea the secure key rate scales as \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  , the same as for a single photon source. This idea has been implemented successfully first at the University of Toronto, and in several follow-up QKD experiments, allowing for high key rates secure against all known attacks.\n\n\n=== Denial of service ===\nBecause currently a dedicated fibre optic line (or line of sight in free space) is required between the two points linked by quantum key distribution, a denial of service attack can be mounted by simply cutting or blocking the line. This is one of the motivations for the development of quantum key distribution networks, which would route communication via alternate links in case of disruption.\n\n\n=== Trojan-horse attacks ===\nA quantum key distribution system may be probed by Eve by sending bright light into the quantum channel and analyzing the back-reflections in a Trojan-horse attack. In a recent research study it has been shown that Eve discerns Bob's secret basis choice with higher than 90% probability, breaching the security of the system.\n\n\n=== Security proofs ===\nIf Eve is assumed to have unlimited resources, for example both classical and quantum computing power, there are many more attacks possible. BB84 has been proven secure against any attacks allowed by quantum mechanics, both for sending information using an ideal photon source which only ever emits a single photon at a time, and also using practical photon sources which sometimes emit multiphoton pulses. These proofs are unconditionally secure in the sense that no conditions are imposed on the resources available to the eavesdropper; however, there are other conditions required:\n\nEve cannot physically access Alice and Bob's encoding and decoding devices.\nThe random number generators used by Alice and Bob must be trusted and truly random (for example a Quantum random number generator).\nThe classical communication channel must be authenticated using an unconditionally secure authentication scheme.\nThe message must be encrypted using one-time pad like scheme\n\n\n== Quantum hacking ==\nHacking attacks target vulnerabilities in the operation of a QKD protocol or deficiencies in the components of the physical devices used in construction of the QKD system. If the equipment used in quantum key distribution can be tampered with, it could be made to generate keys that were not secure using a random number generator attack. Another common class of attacks is the Trojan horse attack which does not require physical access to the endpoints: rather than attempt to read Alice and Bob's single photons, Eve sends a large pulse of light back to Alice in between transmitted photons. Alice's equipment reflects some of Eve's light, revealing the state of Alice's basis (e.g., a polarizer). This attack can be detected, e.g. by using a classical detector to check the non-legitimate signals (i.e. light from Eve) entering Alice's system. It is also conjectured that most hacking attacks can similarly be defeated by modifying the implementation, though there is no formal proof.\nSeveral other attacks including faked-state attacks, phase remapping attacks, and time-shift attacks are now known. The time-shift attack has even been demonstrated on a commercial quantum cryptosystem. This is the first demonstration of quantum hacking against a non-homemade quantum key distribution system. Later on, the phase-remapping attack was also demonstrated on a specially configured, research oriented open QKD system (made and provided by the Swiss company Id Quantique under their Quantum Hacking program). It is one of the first 'intercept-and-resend' attacks on top of a widely used QKD implementation in commercial QKD systems. This work has been widely reported in media.The first attack that claimed to be able to eavesdrop the whole key without leaving any trace was demonstrated in 2010. It was experimentally shown that the single-photon detectors in two commercial devices could be fully remote-controlled using specially tailored bright illumination. In a spree of publications thereafter, the collaboration between the Norwegian University of Science and Technology in Norway and Max Planck Institute for the Science of Light in Germany, has now demonstrated several methods to successfully eavesdrop on commercial QKD systems based on weaknesses of avalanche photodiodes (APDs) operating in gated mode. This has sparked research on new approaches to securing communications networks.\n\n\n== Counterfactual quantum key distribution ==\n\nThe task of distributing a secret key could be achieved even when the particle (on which the secret information, e.g. polarization, has been encoded) does not traverse through the quantum channel using a protocol developed by Tae-Gon Noh. Here Alice generates a photon which, by not taking a measurement until later, exists in a superposition of being in paths (a) and (b) simultaneously. Path (a) stays inside Alice's secure device and path (b) goes to Bob. By rejecting the photons that Bob receives and only accepting the ones he doesn't receive, Bob & Alice can set up a secure channel, i.e. Eve's attempts to read the counterfactual photons would still be detected. This protocol uses the quantum phenomenon whereby the possibility that a photon can be sent has an effect even when it isn't sent. So-called interaction-free measurement also uses this quantum effect, as for example in the bomb testing problem, whereby an experimenter can conceptually determine which bombs are not duds without setting them off, except in a counterfactual sense.\n\n\n== History ==\nQuantum cryptography was proposed first by Stephen Wiesner, then at Columbia University in New York, who, in the early 1970s, introduced the concept of quantum conjugate coding. His seminal paper titled \"Conjugate Coding\" was rejected by IEEE Information Theory but was eventually published in 1983 in SIGACT News (15:1 pp. 78\u201388, 1983). In this paper he showed how to store or transmit two messages by encoding them in two \"conjugate observables\", such as linear and circular polarization of light, so that either, but not both, of which may be received and decoded. He illustrated his idea with a design of unforgeable bank notes. A decade later, building upon this work, Charles H. Bennett, of the IBM Thomas J. Watson Research Center, and Gilles Brassard, of the University of Montreal, proposed a method for secure communication based on Wiesner's \"conjugate observables\". In 1990, Artur Ekert, then a PhD student at Wolfson College, University of Oxford, developed a different approach to quantum key distribution based on quantum entanglement.\n\n\n== Future ==\nThe current commercial systems are aimed mainly at governments and corporations with high security requirements. Key distribution by courier is typically used in such cases, where traditional key distribution schemes are not believed to offer enough guarantee. This has the advantage of not being intrinsically distance limited, and despite long travel times the transfer rate can be high due to the availability of large capacity portable storage devices. The major difference of quantum key distribution is the ability to detect any interception of the key, whereas with courier the key security cannot be proven or tested. QKD (Quantum Key Distribution) systems also have the advantage of being automatic, with greater reliability and lower operating costs than a secure human courier network.\nKak's three-stage protocol has been proposed as a method for secure communication that is entirely quantum unlike quantum key distribution in which the cryptographic transformation uses classical algorithms.Factors preventing wide adoption of quantum key distribution outside high security areas include the cost of equipment, and the lack of a demonstrated threat to existing key exchange protocols. However, with optic fibre networks already present in many countries the infrastructure is in place for a more widespread use.\nAn Industry Specification Group (ISG) of the European Telecommunications Standards Institute (ETSI) has been set up to address standardisation issues in quantum cryptography.European Metrology Institutes, in the context of dedicated projects, are developing measurements required to characterise components of QKD systems.\nToshiba Europe has been awarded a prestigious Institute of Physics Award for Business Innovation. This recognises Toshiba\u2019s pioneering QKD technology developed over two decades of research, protecting communication infrastructure from present and future cyber-threats, and commercialising UK-manufactured products which pave the road to the quantum internet.\nToshiba also took the Semi Grand Prix award in the Solutions Category for the QKD has won the Minister of Economy, Trade and Industry Award in CEATEC AWARD 2021, the prestigious awards presented at CEATEC, Japan\u2019s premier electronics industry trade show.\n\n\n== Deprecation of quantum key distributions from governmental institutions ==\nSome organizations have recommended using \"Post-Quantum Cryptography (or quantum-resistant cryptography)\" as an alternative because of the problems it raises in practical use. For example, National Security Agency of USA, European Union Agency for Cybersecurity of EU (ENISA), National Cyber Security Centre (United Kingdom), and French Secretariat for Defense and Security (ANSSI) recommend it. (read through the bibliography for details).For example, the U.S. National Security Agency addresses five issues:\nQuantum key distribution is only a partial solution. QKD generates keying material for an encryption algorithm that provides confidentiality. Such keying material could also be used in symmetric key cryptographic algorithms to provide integrity and authentication if one has the cryptographic assurance that the original QKD transmission comes from the desired entity (i.e. entity source authentication). QKD does not provide a means to authenticate the QKD transmission source. Therefore, source authentication requires the use of asymmetric cryptography or preplaced keys to provide that authentication. Moreover, the confidentiality services QKD offers can be provided by quantum-resistant cryptography, which is typically less expensive with a better understood risk profile.\nQuantum key distribution requires special purpose equipment. QKD is based on physical properties, and its security derives from unique physical layer communications. This requires users to lease dedicated fiber connections or physically manage free-space transmitters. It cannot be implemented in software or as a service on a network, and cannot be easily integrated into existing network equipment. Since QKD is hardware-based it also lacks flexibility for upgrades or security patches.\nQuantum key distribution increases infrastructure costs and insider threat risks. QKD networks frequently necessitate the use of trusted relays, entailing additional cost for secure facilities and additional security risk from insider threats. This eliminates many use cases from consideration.\nSecuring and validating quantum key distribution is a significant challenge. The actual security provided by a QKD system is not the theoretical unconditional security from the laws of physics (as modeled and often suggested), but rather the more limited security that can be achieved by hardware and engineering designs. The tolerance for error in cryptographic security, however, is many orders of magnitude smaller than in most physical engineering scenarios making it very difficult to validate. The specific hardware used to perform QKD can introduce vulnerabilities, resulting in several well-publicized attacks on commercial QKD systems.\nQuantum key distribution increases the risk of denial of service. The sensitivity to an eavesdropper as the theoretical basis for QKD security claims also shows that denial of service is a significant risk for QKD.In response to problem 1 above, attempts to deliver authentication keys using post-quantum cryptography (or quantum-resistant cryptography) have been proposed worldwide. On the other hand, quantum-resistant cryptography is cryptography belonging to the class of computational security. In 2015, a research result was already published that \"sufficient care must be taken in implementation to achieve information-theoretic security for the system as a whole when authentication keys that are not information-theoretic secure are used\" (when the authentication key is not information-theoretic secure (If the authentication key is not information-theoretically secure, an attacker can break it to bring all classical and quantum communications under control and relay them to launch a man-in-the-middle attack).\nEricsson, a private company, also cites and points out the above problems and then presents a report that it may not be able to support the Zero trust security model, which is a recent trend in network security technology.\n\n\n== See also ==\nList of quantum key distribution protocols\nQuantum computing\nQuantum cryptography\nQuantum information science\nQuantum network\n\n\n== References ==\n\n\n== External links ==\nGeneral and reviewQuantum Computing 101\nScientific American Magazine (January 2005 Issue) Best-Kept Secrets Non-technical article on quantum cryptography\nPhysics World Magazine (March 2007 Issue) Non-technical article on current state and future of quantum communication\nScarani, Valerio; Bechmann-Pasquinucci, Helle; Cerf, Nicolas J.; Du\u0161ek, Miloslav; L\u00fctkenhaus, Norbert; Peev, Momtchil (2009). \"The Security of Practical Quantum Key Distribution\". Rev. Mod. Phys. 81 (3): 1301\u20131350. arXiv:0802.4155. Bibcode:2009RvMP...81.1301S. doi:10.1103/RevModPhys.81.1301. S2CID 15873250.\nNguyen, Kim-Chi; Gilles Van Assche; Cerf, Nicolas J. (2007). \"Quantum Cryptography: From Theory to Practice\". arXiv:quant-ph/0702202.\nSECOQC White Paper on Quantum Key Distribution and Cryptography European project to create a large scale quantum cryptography network, includes discussion of current QKD approaches and comparison with classical cryptography\nThe future of cryptography  May 2003 Tomasz Grabowski\nARDA Quantum Cryptography Roadmap\nLectures at the Institut Henri Poincar\u00e9 (slides and videos)\nInteractive quantum cryptography demonstration experiment with single photons for educationMore specific informationEkert, Artur (30 April 2005). \"Cracking codes, part II | plus.maths.org\". Pass.maths.org.uk. Retrieved 28 December 2013. Description of entanglement based quantum cryptography from Artur Ekert.\nXu, Qing (2009). Optical Homodyne Detections and Applications in Quantum Cryptography (PDF) (Thesis). Paris: T\u00e9l\u00e9com ParisTech. Retrieved 14 February 2017.\n\"Quantum Cryptography and Privacy Amplification\". Ai.sri.com. Retrieved 28 December 2013.  Description of BB84 protocol and privacy amplification by Sharon Goldwater.\nBennett, Charles H.; Brassard, Gilles (2014). \"Quantum cryptography: Public key distribution and coin tossing\". Theoretical Computer Science. 560: 7\u201311. doi:10.1016/j.tcs.2014.05.025.\nPublic debate on the Security of Quantum Key Distribution at the conference Hot Topics in Physical Informatics, 11 November 2013 Archived 4 March 2016 at the Wayback MachineFurther informationQuantiki.org - Quantum Information portal and wiki\nInteractive BB84 simulationQuantum key distribution simulationOnline Simulation and Analysis Toolkit for Quantum Key Distribution Archived 25 October 2016 at the Wayback MachineQuantum cryptography research groupsExperimental Quantum Cryptography with Entangled Photons\nNIST Quantum Information Networks\nFree Space Quantum Cryptography\nExperimental Continuous Variable QKD, MPL Erlangen\nExperimental Quantum Hacking, MPL Erlangen\nQuantum cryptography lab. Pljonkin A.P.Companies selling quantum devices for cryptographyAUREA Technology sells the optical building blocks for Quantum cryptography\nid Quantique sells Quantum Key Distribution products\nMagiQ Technologies sells quantum devices for cryptography\nQuintessenceLabs Solutions based on continuous wave lasers\nSeQureNet sells Quantum Key Distribution products using continuous-variablesCompanies with quantum cryptography research programmesToshiba\nHewlett Packard\nIBM\nMitsubishi\nNEC\nNTT"}, {"id": 45, "title": "Cars 2", "content": "Cars 2 is a 2011 American animated spy comedy film produced by Pixar Animation Studios for Walt Disney Pictures. It is the sequel to Cars (2006), the second film in the Cars franchise, and the 12th animated film from the studio. The film was directed by John Lasseter (in his final outing as director of a Pixar film to date), co-directed by Brad Lewis, produced by Denise Ream, and written by Ben Queen, Lasseter, Lewis, and Dan Fogelman. In the film's ensemble voice cast, Owen Wilson, Larry the Cable Guy, Tony Shalhoub, Guido Quaroni, Bonnie Hunt, and John Ratzenberger reprise their roles from the first film. George Carlin, who previously voiced Fillmore, died in 2008, and his role was passed to Lloyd Sherr. They are joined by newcomers Michael Caine, Emily Mortimer, John Turturro, Eddie Izzard, and Thomas Kretschmann. In the film, Lightning McQueen and Mater head overseas to compete in the first ever World Grand Prix promoting a new alternative fuel called Allinol, but Mater accidentally becomes involved in international espionage that could determine both his and Lightning's fate.\nCars 2 was first announced in April 2008 with a tentative 2012 release date, making Cars the second Pixar film to spawn a sequel after Toy Story (1995), as well as becoming a franchise. Lasseter was confirmed to be returning as director, while Lewis was designated as co-director in June 2010. The film's story was conceived by Lasseter while he was traveling around the world promoting the first film. Michael Giacchino composed the film's score, with artists such as Weezer, Robbie Williams, Brad Paisley and B\u00e9nabar contributing tracks for the film. This was the final Pixar film animated with their old software system, Marionette, before being officially replaced with Presto in 2012. With an estimated budget of $200 million, Cars 2 is one of the most expensive films ever made.\nCars 2 premiered at the El Capitan Theatre in Los Angeles on June 18, 2011, and was released in the United States on June 24, in Disney Digital 3D and IMAX 3D as well as traditional two-dimensional and IMAX formats. Despite receiving mixed reviews from critics, Cars 2 continued Pixar's streak of box office success, grossing over $559 million worldwide, becoming the tenth-highest-grossing film of 2011 and the highest-grossing film of the Cars trilogy. The film was nominated for Best Animated Feature Film at the 69th Golden Globe Awards, but lost to The Adventures of Tintin. A sequel, Cars 3, was released on June 16, 2017.\n\n\n== Plot ==\nBritish spy Finn McMissile infiltrates an oil rig owned by criminal lemon cars to rescue fellow spy Leland Turbo. He witnesses the lemons, seemingly led by weapons designer Professor Z\u00fcndapp, loading an electromagnetic pulse generator, disguised as a TV camera, onto a shipping crate. After discovering Turbo's death, Finn's presence gets exposed to the lemons, and he escapes by faking his death.\nAfter winning his fourth Piston Cup, Lightning McQueen returns to Radiator Springs to spend his off-season with his friends. Italian formula race car Francesco Bernoulli challenges Lightning to participate in the World Grand Prix, an international three-race event created by former oil tycoon turned electric car Sir Miles Axlerod, who intends to promote his new environmentally friendly fuel, Allinol. Lightning and his best friend Mater \u2014 along with Luigi, Guido, Fillmore, and Sarge \u2014 depart for Tokyo, where the first race takes place.\nAt a World Grand Prix promotional event, Mater makes a scene after eating wasabi and seemingly leaking on stage, embarrassing Lightning. While cleaning up, Mater interrupts a fight between American spy Rod \"Torque\" Redline and lemons Grem and Acer. Redline plants his tracking device on Mater, causing the spy Finn McMissile and his associate Holley Shiftwell to mistake Mater for the spy. Meanwhile, Redline is captured and killed by Z\u00fcndapp, who reveals that Allinol ignites when hit with an EMP. He informs his superior, an unknown mastermind, that Redline passed on his information. Holley finds and recruits Mater to stop Z\u00fcndapp's plot.\nDuring the race, three racers are ignited by the camera. Lightning places second in the race after Bernoulli, due to miscommunication with Mater, who was evading Z\u00fcndapp's henchmen. Mater is abducted by Finn, and they escape from the lemons in his jet, Siddeley. After traveling to Paris to gather intel from Finn's old friend Tomber, they head to Porto Corsa, Italy, where the second race takes place. During the race, Mater infiltrates the lemons' meeting, just as the camera ignites some more racers, causing a multi-car pile-up, while Lightning wins. Due to controversy over Allinol's safety, Axlerod lifts its requirement for use in the final race in London. When Lightning decides to continue using it, the lemons plan to kill him in the race. This spooks Mater, and accidentally blows his cover, causing him, Finn, and Holley to be captured and tied up inside Big Bentley, where he admits to them that he is not the spy they think he is.\nWhen the race starts, Lightning takes the lead before passing Big Bentley, but the camera is inexplicably defective on him. The lemons tell Mater that they planted a time bomb in Lightning's pits as a backup plan, spurring him to escape. Finn and Holley escape but realize that the bomb was fitted on Mater's air filter instead. As Mater flees down the track, Lightning pursues him to apologize for his outburst, while Finn apprehends Z\u00fcndapp. The other lemons arrive and outnumber Finn, Holley, Mater, and Lightning, but they are rescued by the other Radiator Springs residents and Sarge's colleagues in the British Army. Mater and Lightning go to Buckingham Palace, where Mater exposes Axlerod as the mastermind, proven when he is forced to disable the bomb. It is then revealed that the World Grand Prix was his cover-up to turn the world against alternative fuels. Mater also reveals that the supposedly electric Axlerod was instead running on an old gasoline engine, and that he leaked oil in Tokyo and blamed it on Mater. After Axlerod and the lemons are arrested by the London police, Mater receives a knighthood from the Queen, and he and Lightning reconcile.\nBack in Radiator Springs, as Mater tells everyone about his experience, Fillmore reveals that Sarge has swapped Allinol with his organic fuel, explaining the camera's ineffectiveness on Lightning. A \"Radiator Springs Grand Prix\" is held, featuring the World Grand Prix contenders. Finn and Holley invite Mater to go on another mission, but he declines. While his weapons get confiscated, he keeps the rockets and speeds off with Lightning, just as Siddeley speeds into the distance.\n\n\n== Voice cast ==\n\nMuch of the cast from the original Cars returned for the sequel, but three voice actors of the original film have died since its release. Joe Ranft (who voiced Red) died in an automobile accident on August 16, 2005, ten months before Cars (which was dedicated in memorial to him) was released, and therefore Red played no vocal role in the film. George Carlin (who voiced Fillmore) died of heart failure on June 22, 2008, so Fillmore was voiced by Lloyd Sherr (who also voices Tony Trihull). Paul Newman (who voiced Doc Hudson) died of cancer on September 26, 2008. After Newman's death, Lasseter said they would \"see how the story goes with Doc Hudson.\" Doc was eventually dropped, and implied to have died a few years before the events of Cars 2.\n\nIn international versions of the film, the character Jeff Gorvette is replaced with race car drivers better known in the specific countries in his dialogue scenes (however, he still appears as a competitor).\nMark Winterbottom as Frosty (Australian release)\nFernando Alonso as Fernando Alonso (Spanish release)\nVitaly Petrov as Vitaly Petrov (Russian release)\nJan Nilsson as Flash (Swedish release)\nMemo Rojas (Latin American release)\nSebastian Vettel as Sebastian Schnell (German release)In Brazil, Gorvette is replaced by Carla Veloso in his dialogue scenes (Carla appears in all other versions of the film, but with no lines); Carla is voiced by Brazilian singer Claudia Leitte. Sportspeople still appear, with Lewis Hamilton becoming Formula One champion Emerson Fittipaldi, while Brent Mustangburger and David Hobbscap were done by sports announcers Jos\u00e9 Trajano and Luciano do Valle.\n\n\n== Production ==\n\n\n=== Development ===\nCars is the second Pixar film, after Toy Story, to have a sequel as well as becoming a franchise. John Lasseter, the director of the film, stated that he conceived the sequel's story while traveling around the world promoting the first film. He said:\n\nI kept looking out thinking, 'What would Mater do in this situation, you know?' I could imagine him driving around on the wrong side of the road in the UK, going around in big, giant traveling circles in Paris, on the autobahn in Germany, dealing with the motor scooters in Italy, trying to figure out road signs in Japan.\nIn April 2008, Pixar unveiled its latest animation slate, with Cars 2 scheduled for a summer 2012 release. Brad Lewis, who had served as producer on Ratatouille, was announced as the film's director. In June 2010, it was announced that Lasseter had been designated as co-director.In 2009, Disney registered several domain names, hinting to audiences that the title and theme of the film would be in relation to a \"World Grand Prix\".In November 2010, the film's synopsis was announced, revealing the espionage racing storyline, along with a first look image and official poster.In March 2011, Jake Mandeville-Anthony, a U.K. screenwriter, sued Disney and Pixar alleging copyright infringement and breach of implied contract. In his complaint he alleged that Cars and Cars 2 are based in part on work that he had submitted in the early 1990s and he sought an injunction to stop the release of Cars 2 and requested actual or statutory damages. On May 13, 2011, Disney responded to the lawsuit, denying \"each and every one of Plaintiff's legal claims concerning the purported copyright infringement and substantial similarity of the parties' respective works.\" On July 27, 2011, the lawsuit was dismissed by a district court judge who, in her ruling, wrote that the \"Defendants have sufficiently shown that the Parties' respective works are not substantially similar in their protectable elements as a matter of law\".\n\n\n=== Casting ===\nIn November 2010, Owen Wilson, Larry the Cable Guy, Michael Caine, Emily Mortimer, Jason Isaacs, Joe Mantegna, Peter Jacobson, Bonnie Hunt, Tony Shalhoub, Cheech Marin, and Thomas Kretschmann were confirmed as the voice talent featured in the film. From November 2010 until May 2011, Disney released information about the other voice talent, including Jenifer Lewis, Katherine Helmond, Michael Wallis, Darrell Waltrip, Franco Nero, Vanessa Redgrave, Bruce Campbell, Sig Hansen, Michel Michelis, Jeff Gordon, Lewis Hamilton, Brent Musburger, David Hobbs, John Turturro, and Eddie Izzard.\n\n\n== Soundtrack ==\n\nThe soundtrack for the film was released on both CD and digital download on June 14, 2011. Cars 2 is the fourth Pixar film to be scored by Michael Giacchino, after The Incredibles, Ratatouille and Up. It was also the first and only Pixar film directed by John Lasseter not to be scored by Randy Newman, who scored the first and third films of the Cars franchise.\n\n\n== Release ==\nDuring the summer of 2008, John Lasseter announced that Cars 2 would be pushed forward and released in the summer of 2011, one year earlier than its original 2012 release date. The US release date was later confirmed to be June 24, 2011, with a UK release date set for July 22, 2011. The world premiere of the film took place at the El Capitan Theatre in Hollywood on June 18, 2011. Cars 2 was released in 4,115 theaters in the USA and Canada, setting a record-high for a G-rated film and for Pixar. The latter was surpassed by Brave (4,164 theaters). The film was presented in Disney Digital 3D and IMAX 3D, as well as traditional two-dimensional and IMAX formats.\n\n\n=== Short film ===\n\nThe film was preceded by a short film titled Hawaiian Vacation, directed by Gary Rydstrom and starring the characters of the Toy Story franchise.\n\n\n=== Home media ===\nThe film was released by Walt Disney Studios Home Entertainment on DVD, Blu-ray, Blu-ray 3D, and digital download on November 1, 2011. This release was produced in four different physical packages: a 1-disc DVD, a 2-disc combo pack (DVD and Blu-ray), a 5-disc combo pack (DVD, Blu-ray, Blu-ray 3D, and Digital Copy), and an 11-disc three movie collector's set, which features Cars, Cars 2, and Cars Toons: Mater's Tall Tales. The film was also released as a Movie Download edition in both standard and high definition.The Movie Download release includes four bonus features: Cars Toons \"Air Mater\", the Toy Story Toon \"Hawaiian Vacation\", \"World Tour Interactive Feature\", and \"Bringing Cars 2 to the World\". The 1-disc DVD and 2-disc Blu-ray/DVD combo pack releases include the shorts \"Air Mater\" and \"Hawaiian Vacation\", plus the Director John Lasseter Commentary. The 5-disc combo pack includes all of the same bonus features as the 1-disc DVD and 2-disc Blu-ray/DVD combo pack versions, in addition to \"World Tour Interactive Feature\" and \"Sneak Peek: The Nuts and Bolts of Cars Land.\" The 11-disc three movie collection comes packaged with Cars (DVD, Blu-ray, and Digital Copy), Cars 2 (DVD, Blu-ray, Blu-ray 3D, and Digital Copy), and Mater's Tall Tales (DVD, Blu-ray, and Digital Copy).Cars 2 sold a total of 1,983,374 DVD units during its opening week, generating $31.24 million and claiming first place. It also finished on the top spot on the Blu-ray chart during its first week, selling 1.76 million units and generating $44.57 million. Its Blu-ray share of home media was 47 percent, indicating an unexpectedly major shift of sales from DVD to Blu-ray. Blu-ray 3D contributed to this, accounting for 17% of total disc sales. On September 10, 2019, Cars 2 was released on 4K Ultra HD Blu-ray.\n\n\n== Reception ==\n\n\n=== Box office ===\nCars 2 grossed $191.5 million in the United States and Canada, and $370.7 million in other countries for a worldwide total of $562.1 million. Worldwide on its opening weekend it grossed $109 million, marking the largest opening weekend for a 2011 animated title. Overall, Cars 2 became the seventh-biggest Pixar film in worldwide box office among the fourteen released, and was the tenth-highest-grossing film of 2011.Cars 2 made $25.7 million on its debut Friday (June 24, 2011), marking the second-largest opening day for a Pixar film, at the time, after Toy Story 3's $41.1 million. During this time, though, it was the third least-attended opening day for a Pixar film, only ahead of Up and Ratatouille. It also scored the sixth largest opening day for an animated feature. On its opening weekend as a whole, Cars 2 debuted at No.1 ahead of Green Lantern and Bad Teacher with $66.1 million, marking the largest opening weekend for a 2011 animated feature, the seventh largest opening for Pixar, the eighth largest among films released in June, and the fourth largest for a G-rated film. In its second weekend, however, the film was overtaken by Transformers: Dark of the Moon, dropping 60.3% and grossing $26.2 million.Outside North America, it grossed $42.9 million during its first weekend from 3,129 theaters in 18 countries, topping the box office. It performed especially well in Russia where it grossed $9.42 million, marking the best opening weekend for a Disney or Pixar animated feature and surpassing the entire runs of Cars and Toy Story 3. In Mexico, it made $8.24 million during its first weekend, while in Brazil, it topped the box office with $5.19 million ($7.08 million with previews). It also premiered at No.1 with $5.16 million in Australia, where it debuted simultaneously with Kung Fu Panda 2 and out-grossed it. It is the highest-grossing film of 2011 in Lithuania ($477,117), Argentina ($12 million). It is the highest-grossing animated film of 2011 in Estonia ($442,707), Finland ($3.2 million), Norway ($5.8 million).\n\n\n=== Critical response ===\nOn the review aggregator website Rotten Tomatoes, 40% of 220 critics' reviews are positive, with an average rating of 5.50/10. The website's consensus reads: \"Cars 2 is as visually appealing as any other Pixar production, but all that dazzle can't disguise the rusty storytelling under the hood.\" It is the lowest-rated Pixar film on the site to date and the only one to earn a \"rotten\" certification. Another review aggregator, Metacritic, which assigns a weighted average score to reviews from mainstream critics, gave the film an average score of 57 out of 100, based on 38 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A\u2212\" on an A+ to F scale.\"The original Cars was not greeted with exceptional warmth,\" said The New York Times, \"but the sequel generated Pixar's first truly negative response.\" \nCritics generally criticized the focus on Mater and felt the film lacked warmth and charm, while also feeling the film was made as an exercise in target marketing and was too violent to be given a G rating. Reviewing the film for The Wall Street Journal, Joe Morgenstern wrote, \"This frenzied sequel seldom gets beyond mediocrity.\" Entertainment Weekly critic Owen Gleiberman said, \"Cars 2 is a movie so stuffed with \"fun\" that it went right off the rails. What on earth was the gifted director-mogul John Lasseter thinking \u2013 that he wanted kids to come out of this movie was [sic] more ADD?\" Although Leonard Maltin on IndieWire claimed that he had \"such high regard for Pixar and its creative team led by John Lasseter\" he said he found the plot \"confusing\" and felt that Mater's voice was annoying, saying that he'd \"rather listen to chalk on a blackboard than spend nearly two hours with Tow Mater.\" Considering the low reviews given to the Pixar production, critic Kyle Smith of the New York Post said, \"They said it couldn't be done. But Pixar proved the yaysayers wrong when it made its first bad movie, Cars. Now it has worsted itself with the even more awful Cars 2.\"Conversely, Peter Travers of Rolling Stone gave the film 3\u00bd stars out of four, and said that \"the sequel is a tire-burning burst of action and fun with a beating heart under its hood.\" He also praised its \"fluid script\" and called it a \"winner\". Roger Ebert was the most effusive of the more positive reviews, praising Lasseter's channeling of childhood playtime for the film's spirit and writing, \"At a time when some 'grown-up' action films are relentlessly shallow and stupid, here is a movie with such complexity that even the cars sometimes have to pause and explain it to themselves.\" Justin Chang of Variety commented, \"The rare sequel that not only improves on but retroactively justifies its predecessor.\" Ticket buyers also gave the film an A\u2212 in exit polls, on par with other Pixar titles.A central vein of many negative reviews was the theory that the Walt Disney Company forced Cars 2 into production at Pixar out of greed in order to drive merchandising sales. Lasseter vehemently denied these claims, which he attributed to \"people who don't know the facts, rushing to judge.\" Some theorized that the vitriol was less about the film but more about Pixar's broadened focus to sequels. The New York Times reported that although one negatively reviewed film would not be enough to scratch the studio, \"the commentary did dent morale at the studio, which until then had enjoyed an unbroken and perhaps unprecedented run of critical acclaim.\"\n\n\n=== Accolades ===\nCars 2 marks the first Pixar film not to be nominated for an Oscar. It is also the first Pixar film not nominated for Best Animated Feature since its introduction in 2001.\n\n\n== Video games ==\n\nA video game based on the film was developed by Avalanche Software and published by Disney Interactive Studios for the PlayStation 3, Xbox 360, Wii, PC and Nintendo DS on June 21, 2011. The PlayStation 3 version of the game was reported to be compatible with stereoscopic 3D gameplay. A Nintendo 3DS version was released on November 1, 2011, and a PSP version was released on November 8, 2011.An app based on the film was released on iTunes for a dollar on June 23, 2011. The Lite version was released for free that same day. The object of the game was to complete each race, unlock new levels, and get a high score. As of June 28, 2011, the app had hit No. 1 on the App Store. The game was retired on August 29, 2014.\nA V.Smile version was also released.\n\n\n== Future ==\n\n\n=== Sequel ===\nA sequel, titled Cars 3, was released on June 16, 2017. Directed by Brian Fee, the film focuses on Lightning McQueen, now a veteran racer, who after being overshadowed by a new wave of rookies, gets help from a younger car, Cruz Ramirez, to instruct him for the increasingly high-tech world and defeat new rival Jackson Storm.\n\n\n=== Spin-offs ===\nAn animated feature film spin-off called Planes, produced by DisneyToon Studios, was released on August 9, 2013. A sequel to Planes, titled Planes: Fire & Rescue, was later released the following year on July 18, 2014.\n\n\n== References ==\n\n\n== Notes ==\n\n\n== External links ==\n\nOfficial website\nCars 2 at IMDb \nCars 2 at AllMovie \nCars 2 at the TCM Movie Database \nCars 2 at the Internet Movie Cars Database"}, {"id": 46, "title": "Grid energy storage", "content": "Grid energy storage (also called large-scale energy storage) is a collection of methods used for energy storage on a large scale within an electrical power grid. Electrical energy is stored during times when electricity is plentiful and inexpensive (especially from intermittent power sources such as renewable electricity from wind power, tidal power and solar power) or when demand is low, and later returned to the grid when demand is high, and electricity prices tend to be higher.\nAs of 2020, the largest form of grid energy storage is dammed hydroelectricity, with both conventional hydroelectric generation as well as pumped-storage hydroelectricity.Developments in battery storage have enabled commercially viable projects to store energy during peak production and release during peak demand, and for use when production unexpectedly falls giving time for slower responding resources to be brought online. Green hydrogen, which is generated from electrolysis of water via electricity generated by renewables or relatively lower carbon emission sources, is a more economical means of long-term renewable energy storage in terms of capital expenditures than pumped-storage hydroelectricity or batteries.Two alternatives to grid storage are the use of peaking power plants to fill in supply gaps and demand response to shift load to other times.\n\n\n== Benefits ==\nAny electrical power grid must match electricity production to consumption, both of which vary drastically over time.  Any combination of energy storage and demand response has these advantages:\n\nfuel-based power plants (i.e. coal, oil, gas, nuclear) can be more efficiently and easily operated at constant production levels\nelectricity generated by intermittent sources can be stored and used later, whereas it would otherwise have to be transmitted for sale elsewhere, or shut down\npeak generating or transmission capacity can be reduced by the total potential of all storage plus deferrable loads (see demand side management), saving the expense of this capacity\nmore stable pricing \u2013 the cost of the storage or demand management is included in pricing so there is less variation in power rates charged to customers, or alternatively (if rates are kept stable by law) less loss to the utility from expensive on-peak wholesale power rates when peak demand must be met by imported wholesale power\nemergency preparedness \u2013 vital needs can be met reliably even with no transmission or generation going on while non-essential needs are deferredEnergy derived from solar, tidal and wind sources inherently varies on time scales ranging from minutes to weeks or longer \u2013 the amount of electricity produced varies with time of day, moon phase, season, and random factors such as the weather.  Thus, renewables in the absence of storage present special challenges to electric utilities.  While hooking up many separate wind sources can reduce the overall variability, solar is reliably not available at night, and tidal power shifts with the moon, so slack tides occur four times a day.\nHow much this affects any given utility varies significantly. In a summer peak utility, more solar can generally be absorbed and matched to demand.  In winter peak utilities, to a lesser degree, wind correlates to heating demand and can be used to meet that demand.  Depending on these factors, beyond about 20\u201340% of total generation, grid-connected intermittent sources such as solar power and wind power tend to require investment in grid interconnections, grid energy storage or demand-side management.\nIn an electrical grid without energy storage, generation that relies on energy stored within fuels (coal, biomass, natural gas, nuclear) must be scaled up and down to match the rise and fall of electrical production from intermittent sources (see load following power plant).  While hydroelectric and natural gas plants can be quickly scaled up or down to follow the wind, coal and nuclear plants take considerable time to respond to load.  Utilities with less natural gas or hydroelectric generation are thus more reliant on demand management, grid interconnections or costly pumped storage.\nThe French consulting firm Yole D\u00e9veloppement estimates the \"stationary storage\" market could be a $13.5 billion opportunity by 2023, compared with less than $1 billion in 2015.\n\n\n=== Demand side management and grid storage ===\nThe demand side can also store electricity from the grid, for example charging a battery electric vehicle stores energy for a vehicle and storage heaters, district heating storage or ice storage provide thermal storage for buildings. At present this storage serves only to shift consumption to the off-peak time of day, no electricity is returned to the grid.\nThe need for grid storage to provide peak power is reduced by demand side time of use pricing, one of the benefits of smart meters. At the household level, consumers may choose less expensive off-peak times to wash and dry clothes, use dishwashers, take showers and cook. As well, commercial and industrial users will take advantage of cost savings by deferring some processes to off-peak times.\nRegional impacts from the unpredictable operation of wind power has created a new need for interactive demand response, where the utility communicates with the demand. Historically this was only done in cooperation with large industrial consumers, but now may be expanded to entire grids. For instance, a few large-scale projects in Europe link variations in wind power to change industrial food freezer loads, causing small variations in temperature. If communicated on a grid-wide scale, small changes to heating/cooling temperatures would instantly change consumption across the grid.\nA report released in December 2013 by the United States Department of Energy further describes the potential benefits of energy storage and demand side technologies to the electric grid: \"Modernizing the electric system will help the nation meet the challenge of handling projected energy needs\u2014including addressing climate change by integrating more energy from renewable sources and enhancing efficiency from non-renewable energy processes. Advances to the electric grid must maintain a robust and resilient electricity delivery system, and energy storage can play a significant role in meeting these challenges by improving the operating capabilities of the grid, lowering cost and ensuring high reliability, as well as deferring and reducing infrastructure investments. Finally, energy storage can be instrumental for emergency preparedness because of its ability to provide backup power as well as grid stabilization services\". The report was written by a core group of developers representing Office of Electricity Delivery and Energy Reliability, ARPA-E, Office of Science, Office of Energy Efficiency and Renewable Energy, Sandia National Laboratories, and Pacific Northwest National Laboratory; all of whom are engaged in the development of grid energy storage.\n\n\n=== Energy storage for grid applications ===\nEnergy storage assets are a valuable asset for the electrical grid. They can provide benefits and services such as load management, power quality and uninterruptable power supply to increase the efficiency and supply security. This becomes more and more important in regard to the energy transition and the need for a more efficient and sustainable energy system.\nNumerous energy storage technologies (pumped-storage hydroelectricity, electric battery, flow battery, flywheel energy storage, supercapacitor etc.) are suitable for grid-scale applications, however their characteristics differ. For example, a pumped-hydro station is well suited for bulk load management applications due to their large capacities and power capabilities. However, suitable locations are limited and their usefulness fades when dealing with localized power quality issues. On the other hand, flywheels and capacitors are most effective in maintaining power quality but lack storage capacities to be used in larger applications. These constraints are a natural limitation to the storage's applicability.\nSeveral studies have developed interest and investigated the suitability or selection of the optimal energy storage for certain applications. Literature surveys comprise the available information of the state-of-the-art and compare the storage's uses based on current existing projects. Other studies take a step further in evaluating energy storage with each other and rank their fitness based on multiple-criteria decision analysis. Another paper proposed an evaluation scheme through the investigation and modelling of storage as equivalent circuits. An indexing approach has also been suggested in a few studies, but is still in the novel stages. In order to gain increased economic potential of grid connected energy storage systems, it is of interest to consider a portfolio with several services for one or more applications for an energy storage system. By doing so, several revenue streams can be achieved by a single storage and thereby also increasing the degree of utilization. To mention two examples, a combination of frequency response and reserve services is examined in, meanwhile load peak shaving together with power smoothing is considered in.\n\n\n== Forms ==\n\n\n=== Air ===\n\n\n==== Compressed air ====\n\nOne grid energy storage method is to use off-peak or renewably generated electricity to compress air, which is usually stored in an old mine or some other kind of geological feature. When electricity demand is high, the compressed air is heated with a small amount of natural gas and then goes through turboexpanders to generate electricity.Compressed air storage is typically around 60\u201390% efficient.\n\n\n==== Liquid air ====\n\nAnother electricity storage method is to compress and cool air, turning it into liquid air, which can be stored, and expanded when needed, turning a turbine, generating electricity, with a storage efficiency of up to 70%.A commercial liquid-air energy storage plant is under construction in the North of England,\n\nwith commercial operation planned for 2022.\nThe energy storage capacity of 250MWh of the plant will be nearly twice the capacity of the world's largest existing lithium-ion battery, the Hornsdale Power Reserve in South Australia.\n\n\n==== Compressed carbon dioxide ====\n\nGaseous carbon dioxide can be compressed to store energy at grid scale. The gas is well suited to this role because, unlike air, it liquifies at ambient temperatures. Liquid CO2 can be stored indefinitely in high-pressure cylinders, for use when needed. \nThe main proponent of the technology is start-up company Energy Dome, which in 2022 built a 2.5MW/4MWh demonstrator plant in Sardinia. The company claim a round trip efficiency of 75% and a projected cost of EUR220/kWh of storage capacity, which is half that of Li-ion batteries.\n\n\n=== Batteries ===\n\nBattery storage was used in the early days of direct current electric power. Where AC grid power was not readily available, isolated lighting plants run by wind turbines or internal combustion engines provided lighting and power to small motors. The battery system could be used to run the load without starting the engine or when the wind was calm. A bank of lead\u2013acid batteries in glass jars both supplied power to illuminate lamps, as well as to start an engine to recharge the batteries. Battery storage technology is typically around 80% to more than 90% efficient for newer lithium-ion devices.Battery systems connected to large solid-state converters have been used to stabilize power distribution networks. Some grid batteries are co-located with renewable energy plants, either to smooth the power supplied by the intermittent wind or solar output, or to shift the power output into other hours of the day when the renewable plant cannot produce power directly (see Installation examples). These hybrid systems (generation and storage) can either alleviate the pressure on the grid when connecting renewable sources or be used to reach self-sufficiency and work \"off-the-grid\" (see Stand-alone power system).\nContrary to electric vehicle applications, batteries for stationary storage do not suffer from mass or volume constraints. However, due to the large amounts of energy and power implied, the cost per power or energy unit is crucial. The relevant metrics to assess the interest of a technology for grid-scale storage is the $/Wh (or $/W) rather than the Wh/kg (or W/kg). The electrochemical grid storage was made possible thanks to the development of the electric vehicle, that induced a fast decrease in the production costs of batteries below $300/kWh. By optimizing the production chain, major industrials aimed to reach $150/kWh by the end of 2020, but actually achieved $140/kWh. The rate of decline in battery prices has consistently outpaced most estimates, reaching $132/kWh in 2021. These batteries rely on a lithium-ion technology, which is suited for mobile applications (high cost, high density). Technologies optimized for the grid should focus on low cost per kWh.   Lithium iron phosphate batteries are increasingly being used in both vehicles and grid storage because of their low cost, scale and acceptable energy density for many applications.\n\n\n==== Grid-oriented battery technologies ====\nSodium-ion batteries are a cheap and sustainable alternative to lithium-ion, because sodium is far more abundant and cheaper than lithium, but it has a lower power density. However, they are still on the early stages of their development.\nAutomotive-oriented technologies rely on solid electrodes, which feature a high energy density but require an expensive manufacturing process. Liquid electrodes represent a cheaper and less dense alternative as they do not need any processing.\n\n\n===== Molten-salt/liquid-metal batteries =====\nThese batteries are composed of two molten metal alloys separated by an electrolyte. They are simple to manufacture but require a temperature of several hundred degree Celsius to keep the alloys in a liquid state. This technology includes ZEBRA, sodium-sulfur batteries and liquid metal. Sodium sulphur batteries are being used for grid storage in Japan and in the United States. The electrolyte is composed of solid beta alumina. The liquid metal battery, developed by the group of Pr. Donald Sadoway, uses molten alloys of magnesium and antimony separated by an electrically insulating molten salt. It is being brought to market by MIT spinoff company Ambri, which is currently contracted to install a first 250MWh system for TerraScale data centre company near Reno, Nevada.\n\n\n===== Flow batteries =====\nIn rechargeable flow batteries, that store energy in liquids, such solutions of transition metal ions in water at room temperature. Flow batteries have the advantages of low capital cost for charge-discharge duration over 2-4 h, and of long durability (many years). Flow batteries are inferior to lithium-ion batteries in terms of energy efficiency. Flow batteries are currently deployed for storing energy from intermittent renewable sources, such as wind and solar.Vanadium redox batteries is most technologically and commercially advanced type of flow battery. Currently there are dozens of Vanadium Redox Flow batteries installed at different sites including; Huxley Hill wind farm (Australia), Tomari Wind Hills at Hokkaid\u014d (Japan), as well as in non-wind farm applications. A 12 MW\u00b7h flow battery was to be installed at the Sorne Hill wind farm (Ireland). These storage systems are designed to smooth out transient wind fluctuations.\n\n\n==== Examples ====\nIn Puerto Rico a system with a capacity of 20 megawatts for 15 minutes (5 megawatt hour) stabilizes the frequency of electric power produced on the island. A 27 megawatt 15-minute (6.75 megawatt hour) nickel-cadmium battery bank was installed at Fairbanks Alaska in 2003 to stabilize voltage at the end of a long transmission line.In 2014, the Tehachapi Energy Storage Project was commissioned by Southern California Edison.In 2016, a zinc-ion battery was proposed for use in grid storage applications.In 2017, the California Public Utilities Commission installed 396 refrigerator-sized stacks of Tesla batteries at the Mira Loma substation in Ontario, California. The stacks are deployed in two modules of 10 MW each (20 MW in total), each capable of running for 4 hours, thus adding up to 80 MWh of storage. The array is capable of powering 15,000 homes for over four hours.BYD proposes to use conventional consumer battery technologies such as lithium iron phosphate (LiFePO4) battery, connecting many batteries in parallel.\nThe largest grid storage batteries in the United States include the 31.5 MW battery at Grand Ridge Power plant in Illinois and the 31.5 MW battery at Beech Ridge, West Virginia. Two batteries under construction in 2015 include the 400 MWh (100 MW for 4 hours) Southern California Edison project and the 52 MWh project on Kauai, Hawaii to entirely time shift a 13MW solar farm's output to the evening. Two batteries are in Fairbanks, Alaska (40 MW for 7 minutes using Ni-Cd cells), and in Notrees, Texas (36 MW for 40 minutes using lead\u2013acid batteries). A 13 MWh battery made of used batteries from Daimler's Smart electric drive cars is being constructed in L\u00fcnen, Germany, with an expected second life of 10 years.In 2015, a 221 MW battery storage was installed in the US, with total capacity expected to reach 1.7 GW in 2020.The UK had a 50 MW lithium-ion grid-battery installed in Hertfordshire in 2018. In February 2021, construction began on a 50 MW battery storage development in Burwell, Cambridgeshire and a 40 MW site in Barnsley, South Yorkshire.In November 2017 Tesla installed a 100 MW, 129 MWh battery system in South Australia. The Australian Energy Market Operator stated that this \"is both rapid and precise, compared to the service typically provided by a conventional synchronous generation unit\".\n\n\n=== Electric vehicles ===\n\nCompanies are researching the possible use of electric vehicles to meet peak demand. A parked and plugged-in electric vehicle could sell the electricity from the battery during peak loads and charge either during night (at home) or during off-peak.Plug-in hybrid or electric cars could be used for their energy storage capabilities.  Vehicle-to-grid technology can be employed, turning each vehicle with its 20 to 50 kWh battery pack into a distributed load-balancing device or emergency power source. This represents two to five days per vehicle of average household requirements of 10 kWh per day, assuming annual consumption of 3,650 kWh. This quantity of energy is equivalent to between 60 and 480 kilometres (40 and 300 mi) of range in such vehicles consuming 0.1 to 0.3 kilowatt-hours per kilometre (0.16 to 0.5 kWh/mi). These figures can be achieved even in home-made electric vehicle conversions. Some electric utilities plan to use old plug-in vehicle batteries (sometimes resulting in a giant battery) to store electricity However, a large disadvantage of using vehicle to grid energy storage would be if each storage cycle stressed the battery with one complete charge-discharge cycle. However, one major study showed that used intelligently, vehicle-to-grid storage actually improved the batteries longevity. Conventional (cobalt-based) lithium-ion batteries break down with the number of cycles \u2013 newer li-ion batteries do not break down significantly with each cycle, and so have much longer lives. One approach is to reuse unreliable vehicle batteries in dedicated grid storage as they are expected to be good in this role for ten years. If such storage is done on a large scale it becomes much easier to guarantee replacement of a vehicle battery degraded in mobile use, as the old battery has value and immediate use.\n\n\n=== Flywheel ===\n\nMechanical inertia is the basis of this storage method. When the electric power flows into the device, an electric motor accelerates a heavy rotating disc. The motor acts as a generator when the flow of power is reversed, slowing down the disc and producing electricity. Electricity is stored as the kinetic energy of the disc. Friction must be kept to a minimum to prolong the storage time. This is often achieved by placing the flywheel in a vacuum and using magnetic bearings, tending to make the method expensive. Greater flywheel speeds allow greater storage capacity but require strong materials such as steel or composite materials to resist the centrifugal forces. The ranges of power and energy storage technology that make this method economic, however, tends to make flywheels unsuitable for general power system application; they are probably best suited to load-leveling applications on railway power systems and for improving power quality in renewable energy systems such as the 20MW system in Ireland.Applications that use flywheel storage are those that require very high bursts of power for very short durations such as tokamak and laser experiments where a motor generator is spun up to operating speed and is partially slowed down during discharge.\nFlywheel storage is also currently used in the form of the Diesel rotary uninterruptible power supply to provide uninterruptible power supply systems (such as those in large datacenters) for ride-through power necessary during transfer \u2013 that is, the relatively brief amount of time between a loss of power to the mains and the warm-up of an alternate source, such as a diesel generator.\nThis potential solution has been implemented by EDA in the Azores on the islands of Graciosa and Flores. This system uses an 18 megawatt-second flywheel to improve power quality and thus allow increased renewable energy usage. As the description suggests, these systems are again designed to smooth out transient fluctuations in supply, and could never be used to cope with an outage exceeding a couple of days.\nPowercorp in Australia have been developing applications using wind turbines, flywheels and low load diesel (LLD) technology to maximize the wind input to small grids. A system installed in Coral Bay, Western Australia, uses wind turbines coupled with a flywheel based control system and LLDs. The flywheel technology enables the wind turbines to supply up to 95 percent of Coral Bay's energy supply at times, with a total annual wind penetration of 45 percent.\n\n\n=== Hydrogen ===\n\nHydrogen is being developed as an electrical energy storage medium. Hydrogen is produced, then compressed or liquefied, cryogenically stored at \u2212252.882 \u00b0C, and then converted back to electrical energy or heat. Hydrogen can be used as a fuel for portable (vehicles) or stationary energy generation. Compared to pumped water storage and batteries, hydrogen has the advantage that it is a high energy density fuel. Green hydrogen, from electrolysis of water, is a more economical means of long-term renewable energy storage in terms of capital expenditures than pumped-storage hydroelectricity or batteries.Hydrogen can be produced either by reforming natural gas with steam or by the electrolysis of water into hydrogen and oxygen (see hydrogen production). Reforming natural gas produces carbon dioxide as a by-product. High temperature electrolysis and high pressure electrolysis are two techniques by which the efficiency of hydrogen production may be able to be increased. Hydrogen is then converted back to electricity in an internal combustion engine, or a fuel cell.\nThe AC-to-AC efficiency of hydrogen storage has been shown to be on the order of 20 to 45%, which imposes economic constraints. The price ratio between purchase and sale of electricity must be at least proportional to the efficiency in order for the system to be economic. Hydrogen fuel cells can respond quickly enough to correct rapid fluctuations in electricity demand or supply and regulate frequency. Whether hydrogen can use natural gas infrastructure depends on the network construction materials, standards in joints, and storage pressure.The equipment necessary for hydrogen energy storage includes an electrolysis plant, hydrogen compressors or liquifiers, and storage tanks.\nBiohydrogen is a process being investigated for producing hydrogen using biomass.\nMicro combined heat and power (microCHP) can use hydrogen as a fuel.\nSome nuclear power plants may be able to benefit from a symbiosis with hydrogen production. High temperature (950 to 1,000 \u00b0C) gas cooled nuclear generation IV reactors have the potential to electrolyze hydrogen from water by thermochemical means using nuclear heat as in the sulfur-iodine cycle. The first commercial reactors are expected in 2030.\nA community based pilot program using wind turbines and hydrogen generators was started in 2007 in the remote community of Ramea, Newfoundland and Labrador. A similar project has been going on since 2004 in Utsira, a small Norwegian island municipality.\n\n\n==== Underground hydrogen storage ====\nUnderground hydrogen storage is the practice of hydrogen storage in caverns, salt domes and depleted oil and gas fields. Large quantities of gaseous hydrogen have been stored in caverns by Imperial Chemical Industries (ICI) for many years without any difficulties. The European project Hyunder indicated in 2013 that for the storage of wind and solar energy an additional 85 caverns are required as it cannot be covered by PHES and CAES systems.\n\n\n==== Power to gas ====\nPower to gas is a technology which converts electrical power to a gas fuel. There are 2 methods, the first is to use the electricity for water splitting and inject the resulting hydrogen into the natural gas grid. The second less efficient method is used to convert carbon dioxide and water to methane, (see natural gas) using electrolysis and the Sabatier reaction. The excess power or off peak power generated by wind generators or solar arrays is then used for load balancing in the energy grid. Using the existing natural gas system for hydrogen, fuel cell maker Hydrogenics and natural gas distributor Enbridge have teamed up to develop such a power to gas system in Canada.Pipeline storage of hydrogen where a natural gas network is used for the storage of hydrogen. Before switching to natural gas, the German gas networks were operated using towngas, which for the most part consisted of hydrogen. The storage capacity of the German natural gas network is more than 200,000 GW\u00b7h which is enough for several months of energy requirement. By comparison, the capacity of all German pumped-storage power plants amounts to only about 40 GW\u00b7h. The transport of energy through a gas network is done with much less loss (<0.1%) than in a power network (8%). The use of the existing natural gas pipelines for hydrogen was studied by NaturalHy\n\n\n==== The power-to-ammonia concept ====\nThe power-to-ammonia concept offers a carbon-free energy storage route with a diversified application palette. At times when there is surplus low-carbon power, it can be used to create ammonia fuel. Ammonia may be produced by splitting water into hydrogen and oxygen with electricity, then high temperature and pressure are used to combine nitrogen from the air with the hydrogen, creating ammonia. As a liquid it is similar to propane, unlike hydrogen alone, which is difficult to store as a gas under pressure or to cryogenically liquefy and store at \u2212253 \u00b0C.\nJust like natural gas, the stored ammonia can be used as a thermal fuel for transportation and electricity generation or used in a fuel cell. A standard 60,000 m\u00b3 tank of liquid ammonia contains about 211 GWh of energy, equivalent to the annual production of roughly 30 wind turbines. Ammonia can be burned cleanly: water and nitrogen are released, but no CO2 and little or no nitrogen oxides. Ammonia has multiple uses besides being an energy carrier, it is the basis for the production of many chemicals, the most common use is for fertilizer. Given this flexibility of usage, and given that the infrastructure for the safe transport, distribution and usage of ammonia is already in place, it makes ammonia a good candidate to be a large-scale, non-carbon, energy carrier of the future.\n\n\n=== Hydroelectricity ===\n\n\n==== Pumped water ====\n\nIn 2008, world pumped-storage generating capacity was 104 GW, while other sources claim 127 GW, which comprises the vast majority of all types of grid electric storage \u2013 all other types combined are some hundreds of MW.In many places, pumped-storage hydroelectricity is used to even out the daily generating load, by pumping water to a high storage reservoir during off-peak hours and weekends, using the excess base-load capacity from coal or nuclear sources.  During peak hours, this water can be used for hydroelectric generation, often as a high value rapid-response reserve to cover transient peaks in demand. Pumped storage recovers about 70% to 85% of the energy consumed, and is currently the most cost effective form of mass power storage. The chief problem with pumped storage is that it usually requires two nearby reservoirs at considerably different heights, and often requires considerable capital expenditure.Pumped water systems have high dispatchability, meaning they can come on-line very quickly, typically within 15 seconds, which makes these systems very efficient at soaking up variability in electrical demand from consumers. There is over 90 GW of pumped storage in operation around the world, which is about 3% of instantaneous global generation capacity. Pumped water storage systems, such as the Dinorwig storage system in Britain, hold five or six hours of generating capacity, and are used to smooth out demand variations.\nAnother example is the 1836 MW Tianhuangping Pumped-Storage Hydro Plant in China, which has a reservoir capacity of eight million cubic meters (2.1 billion U.S. gallons or the volume of water over Niagara Falls in 25 minutes) with a vertical distance of 600 m (1970 feet). The reservoir can provide about 13 GW\u00b7h of stored gravitational potential energy (convertible to electricity at about 80% efficiency), or about 2% of China's daily electricity consumption.A new concept in pumped-storage is utilizing wind energy or solar power to pump water. Wind turbines or solar cells that direct drive water pumps for an energy storing wind or solar dam can make this a more efficient process but are limited. Such systems can only increase kinetic water volume during windy and daylight periods. A study published in 2013 showed rooftop solar, coupled to existing pumped-storage, could replace the reactors lost at Fukushima with an equivalent capacity factor.\n\n\n==== Hydroelectric dams ====\nHydroelectric dams with large reservoirs can also be operated to provide peak generation at times of peak demand. Water is stored in the reservoir during periods of low demand and released through the plant when demand is higher.  The net effect is the same as pumped storage, but without the pumping loss. Depending on the reservoir capacity the plant can provide daily, weekly, or seasonal load following.\nMany existing hydroelectric dams are fairly old (for example, the Hoover Dam was built in the 1930s), and their original design predated the newer intermittent power sources such as wind and solar by decades. A hydroelectric dam originally built to provide baseload power will have its generators sized according to the average flow of water into the reservoir. Uprating such a dam with additional generators increases its peak power output capacity, thereby increasing its capacity to operate as a virtual grid energy storage unit. The United States Bureau of Reclamation reports an investment cost of $69 per kilowatt capacity to uprate an existing dam, compared to more than $400 per kilowatt for oil-fired peaking generators. While an uprated hydroelectric dam does not directly store excess energy from other generating units, it behaves equivalently by accumulating its own fuel \u2013 incoming river water \u2013 during periods of high output from other generating units. Functioning as a virtual grid storage unit in this way, the uprated dam is one of the most efficient forms of energy storage, because it has no pumping losses to fill its reservoir, only increased losses to evaporation and leakage.\nA dam which impounds a large reservoir can store and release a correspondingly large amount of energy, by controlling river outflow and raising or lowering its reservoir level a few meters. Limitations do apply to dam operation, their releases are commonly subject to government regulated water rights to limit downstream effect on rivers. For example, there are grid situations where baseload thermal plants, nuclear or wind turbines are already producing excess power at night, dams are still required to release enough water to maintain adequate river levels, whether electricity is generated or not. Conversely there's a limit to peak capacity, which if excessive could cause a river to flood for a few hours each day.\n\n\n=== Superconducting magnetic energy ===\n\nSuperconducting magnetic energy storage (SMES) systems store energy in the magnetic field created by the flow of direct current in a superconducting coil which has been cryogenically cooled to a temperature below its superconducting critical temperature. A typical SMES system includes three parts: superconducting coil, power conditioning system and cryogenically cooled refrigerator. Once the superconducting coil is charged, the current will not decay and the magnetic energy can be stored indefinitely. The stored energy can be released back to the network by discharging the coil. The power conditioning system uses an inverter/rectifier to transform alternating current (AC) power to direct current or convert DC back to AC power. The inverter/rectifier accounts for about 2\u20133% energy loss in each direction. SMES loses the least amount of electricity in the energy storage process compared to other methods of storing energy. SMES systems are highly efficient; the round-trip efficiency is greater than 95%. The high cost of superconductors is the primary limitation for commercial use of this energy storage method.\nDue to the energy requirements of refrigeration, and the limits in the total energy able to be stored, SMES is currently used for short duration energy storage. Therefore, SMES is most commonly devoted to improving power quality.  If SMES were to be used for utilities it would be a diurnal storage device, charged from base load power at night and meeting peak loads during the day.\nThere are significant technical challenges yet to be solved for superconducting magnetic energy storage to become practical.\n\n\n=== Thermal ===\n\nIn Denmark the direct storage of electricity is perceived as too expensive for very large scale usage, albeit significant usage is made of existing Norwegian Hydro.  Instead, the use of existing hot water storage tanks connected to district heating schemes, heated by either electrode boilers or heat pumps, is seen as a preferable approach.  The stored heat is then transmitted to dwellings using district heating pipes.\nMolten salt is used to store heat collected by a solar power tower so that it can be used to generate electricity in bad weather or at night.Building heating and cooling systems can be controlled to store thermal energy in either the building's mass or dedicated thermal storage tanks. This thermal storage can provide load-shifting or even more complex ancillary services by increasing power consumption (charging the storage) during off-peak times and lowering power consumption (discharging the storage) during higher-priced peak times. For example, off-peak electricity can be used to make ice from water, and the ice can be stored.  The stored ice can be used to cool the air in a large building which would have normally used electric AC, thereby shifting the electric load to off-peak hours. On other systems stored ice is used to cool the intake air of a gas turbine generator, thus increasing the on-peak generation capacity and the on-peak efficiency.\nA pumped-heat electricity storage system uses a highly reversible heat engine/heat pump to pump heat between two storage vessels, heating one and cooling the other. The UK-based engineering company Isentropic that is developing the system claims a potential electricity-in to electricity-out round-trip efficiency of 72\u201380%.A Carnot battery is a type of energy storage systems that stores electricity in heat storage and converts the stored heat back to electricity via thermodynamics cycles. This concept has been investigated and developed by many research projects recently. One of the advantage of this type of system is that the cost at large-scale and long-duration of thermal storage could be much lower than other storage technologies.\n\n\n=== Physical battery; Gravitational potential energy storage with solid masses ===\n\nAlternatives include storing energy by moving large solid masses upward against gravity. This can be achieved inside old mine shafts or in specially constructed towers where heavy weights are winched up to store energy and allowed a controlled descent to release it. In rail energy storage, rail cars carrying large weights are moved up or down a section of inclined rail track, storing or releasing energy as a result;\nIn disused oil-well potential energy storage, weights are raised or lowered in a deep, decommissioned oil well.\n\n\n== Economics ==\nThe levelized cost of storing electricity depends highly on storage type and purpose; as subsecond-scale frequency regulation, minute/hour-scale peaker plants, or day/week-scale season storage.Using battery storage is said to have a levelized cost of $120 to $170 per MWh. This compares with open cycle gas turbines which, as of 2020, have a cost of around $151\u2013198 per MWh.Generally speaking, energy storage is economical when the marginal cost of electricity varies more than the costs of storing and retrieving the energy plus the price of energy lost in the process. For instance, assume a pumped-storage reservoir can pump to its upper reservoir a volume of water capable of producing 1,200 MW\u00b7h after all losses are factored in (evaporation and seeping in the reservoir, efficiency losses, etc.). If the marginal cost of electricity during off-peak times is $15 per MW\u00b7h, and the reservoir operates at 75% efficiency (i.e., 1,500 MW\u00b7h are consumed and 1,200 MW\u00b7h of energy are retrieved), then the total cost of filling the reservoir is $22,500. If all of the stored energy is sold the following day during peak hours for an average $40 per MW\u00b7h, then the reservoir will see revenues of $48,000 for the day, for a gross profit of $25,500.\nHowever, the marginal cost of electricity varies because of the varying operational and fuel costs of different classes of generators. At one extreme, base load power plants such as coal-fired power plants and nuclear power plants are low marginal cost generators, as they have high capital and maintenance costs but low fuel costs. At the other extreme, peaking power plants such as gas turbine natural gas plants burn expensive fuel but are cheaper to build, operate and maintain. To minimize the total operational cost of generating power, base load generators are dispatched most of the time, while peak power generators are dispatched only when necessary, generally when energy demand peaks.  This is called \"economic dispatch\".\nDemand for electricity from the world's various grids varies over the course of the day and from season to season. For the most part, variation in electric demand is met by varying the amount of electrical energy supplied from primary sources. Increasingly, however, operators are storing lower-cost energy produced at night, then releasing it to the grid during the peak periods of the day when it is more valuable. In areas where hydroelectric dams exist, release can be delayed until demand is greater; this form of storage is common and can make use of existing reservoirs. This is not storing \"surplus\" energy produced elsewhere, but the net effect is the same \u2013 although without the efficiency losses.  Renewable supplies with variable production, like wind and solar power, tend to increase the net variation in electric load, increasing the opportunity for grid energy storage.\nIt may be more economical to find an alternative market for unused electricity, rather than try and store it. High Voltage Direct Current allows for transmission of electricity, losing only 3% per 1000 km.\nThe United States Department of Energy's International Energy Storage Database provides a free list of grid energy storage projects, many of which show funding sources and amounts.\n\n\n=== Load leveling ===\nThe demand for electricity from consumers and industry is constantly changing, broadly within the following categories:\n\nSeasonal (during dark winters more electric lighting and heating is required, while in other climates hot weather boosts the requirement for air conditioning)\nWeekly (most industry closes at the weekend, lowering demand)\nDaily (such as the morning peak as offices open and air conditioners get switched on)\nHourly (one method for estimating television viewing figures in the United Kingdom is to measure the power spikes during advertisement breaks or after programmes when viewers go to switch a kettle on)\nTransient (fluctuations due to individual's actions, differences in power transmission efficiency and other small factors that need to be accounted for)There are currently three main methods for dealing with changing demand:\n\nElectrical devices generally having a working voltage range that they require, commonly 110\u2013120 V or 220\u2013240 V.  Minor variations in load are automatically smoothed by slight variations in the voltage available across the system.\nPower plants can be run below their normal output, with the facility to increase the amount they generate almost instantaneously.  This is termed 'spinning reserve'.\nAdditional generation can be brought online.  Typically, these would be hydroelectric or gas turbines, which can be started in a matter of minutes.The problem with standby gas turbines is higher costs; expensive generating equipment is unused much of the time. Spinning reserve also comes at a cost; plants running below maximum output are usually less efficient. Grid energy storage is used to shift generation from times of peak load to off-peak hours. Power plants are able to run at their peak efficiency during nights and weekends.\nSupply-demand leveling strategies may be intended to reduce the cost of supplying peak power or to compensate for the intermittent generation of wind and solar power.\n\n\n=== Portability ===\nThis is the area of greatest success for current energy storage technologies.  Single-use and rechargeable batteries are ubiquitous, and provide power for devices with demands as varied as digital watches and cars.  Advances in battery technology have generally been slow, however, with much of the advance in battery life that consumers see being attributable to efficient power management rather than increased storage capacity. Portable consumer electronics have benefited greatly from size and power reductions associated with Moore's law. Unfortunately, Moore's law does not apply to hauling people and freight; the underlying energy requirements for transportation remain much higher than for information and entertainment applications. Battery capacity has become an issue as pressure grows for alternatives to internal combustion engines in cars, trucks, buses, trains, ships, and aeroplanes.  These uses require far more energy density (the amount of energy stored in a given volume or weight) than current battery technology can deliver. Liquid hydrocarbon fuel (such as gasoline/petrol and diesel), as well as alcohols (methanol, ethanol, and butanol) and lipids (straight vegetable oil, biodiesel) have much higher energy densities.\nThere are synthetic pathways for using electricity to reduce carbon dioxide and water to liquid hydrocarbon or alcohol fuels. These pathways begin with electrolysis of water to generate hydrogen, and then reducing carbon dioxide with excess hydrogen in variations of the reverse water gas shift reaction. Non-fossil sources of carbon dioxide include fermentation plants and sewage treatment plants. Converting electrical energy to carbon-based liquid fuel has potential to provide portable energy storage usable by the large existing stock of motor vehicles and other engine-driven equipment, without the difficulties of dealing with hydrogen or another exotic energy carrier. These synthetic pathways may attract attention in connection with attempts to improve energy security in nations that rely on imported petroleum, but have or can develop large sources of renewable or nuclear electricity, as well as to deal with possible future declines in the amount of petroleum available to import.\nBecause the transport sector uses the energy from petroleum very inefficiently, replacing petroleum with electricity for mobile energy will not require very large investments over many years.\n\n\n=== Reliability ===\nVirtually all devices that operate on electricity are adversely affected by the sudden removal of their power supply.  Solutions such as UPS (uninterruptible power supplies) or backup generators are available, but these are expensive.  Efficient methods of power storage would allow for devices to have a built-in backup for power cuts, and also reduce the impact of a failure in a generating station.  Examples of this are currently available using fuel cells and flywheels.\n\n\n== See also ==\n\n\n== References ==\n\nSaving For a Windless day by Sean Davies in The E&T Magazine Vol 5 Issue 9 from the www.IET.org\n\n\n== Further reading ==\nBaxter, Richard (2006). Energy Storage: A Nontechnical Guide. PennWell Books. ISBN 978-1-59370-027-0.\n\n\n== External links ==\nUK Government report on the Benefits of long-duration electricity storage (Aug 2022)\nA large grid-connected nickel-cadmium battery\nStationary Energy Storage\u2026Key to the Renewable Grid\nElectricity Storage FactBook"}, {"id": 47, "title": "Forms of Cricket", "content": "Cricket is a multi-faceted sport with different formats, depending on the standard of play, the desired level of formality, and the time available. One of the main differences is between matches limited by time in which the teams have two innings apiece, and those limited by number of overs in which they have a single innings each. The former, known as first-class cricket if played at the senior level, has a scheduled duration of three to five days (there have been examples of \"timeless\" matches too); the latter, known as limited overs cricket because each team bowls a limit of typically 50 overs, has a planned duration of one day only. A separate form of limited overs is Twenty20, originally designed so that the whole game could be played in a single evening (3 hours), in which each team has an innings limited to twenty overs.\nDouble innings matches usually have at least six hours of playing time each day, with formal intervals on each day for lunch and tea, and additional brief informal breaks for drinks. There is also a short interval between innings. Limited overs matches often last at least six hours, with similar intervals and breaks, whilst the more streamlined Twenty20 matches are generally completed in under four hours. T10 cricket is a newer version of the game, based on the principles of other limited overs formats, but with only 10 overs per innings, and the total playing time limited to 90 minutes.\nLocal club cricket teams, which consist of amateur players, rarely play matches that last longer than a single day; these may loosely be divided into \n\ndeclaration matches, in which a specified maximum time or number of overs is assigned to the game in total and the teams swap roles only when the batting team is either completely dismissed or declares\nlimited overs matches, in which a specified maximum number of overs is assigned for each team's innings individually. These will vary in length between 30 and 60 overs per side at the weekend and the 20-over format in the evenings.Indoor cricket is a variant of the sport played in sports halls during the winter months.\nAt still lower levels, the rules are often changed simply to make the game playable with limited resources, or to render it more convenient and enjoyable for the participants. Informal variants of the sport can be played almost anywhere, if there is enough space.\n\n\n== Professional cricket ==\nFour forms of cricket have been played at what may be termed the highest international or domestic level of the game. Three are contested currently and one is historic. There is no official term for this level of cricket collectively, although the individual forms do have official designations and are defined by the International Cricket Council (ICC). In the past, before any official definition was agreed upon, highest standard matches were routinely described as \"great\" or \"important\" or \"top-class\"; or even \"first-class\" before this became the official term for one type of cricket (see below). Note that \"minor cricket\" is a term used officially in England and Wales at least.\nMatches played at the highest international and domestic levels are those in which players and/or teams of a recognized high standard are taking part. In modern domestic cricket, it includes first-class cricket, List A cricket and top-class Twenty20 competitions for both men and women. Test cricket, One Day Internationals (ODIs) and Twenty20 Internationals (T20Is) are variations of those forms within the international sphere. Historically (see History of cricket), top-class matches were those held by substantial sources to have historical significance including single wicket and those double innings matches without statistical significance: i.e., lacking scorecards and other statistical data.\nThe oldest known English county teams are Kent, Surrey and Sussex, all of which have histories commencing in the early 18th century. These counties had achieved a high standard long before their modern county clubs were founded (from 1839 to 1845), and so they have always had first-class status. Following a meeting in May 1894 of Marylebone Cricket Club (MCC) and the County Championship clubs, the concept of \"first-class cricket\" was officially defined. By 1895, several other counties had also been recognized as having first-class status, as had MCC itself from its foundation in 1787. Top-class limited overs cricket began in 1963 when the County Championship clubs took part in the first seasonal knockout tournament, which was won by Sussex. Hence, like all the other first-class counties, Sussex for example is classified as a List A team from 1963; and as a top-class Twenty20 team since 2003.\n\n\n=== First-class matches ===\n\nFirst-class cricket is a form of the game in which teams of a recognized high standard compete. Test cricket is first-class at international level; the term \"first-class\" is habitually applied to domestic matches only, although a player's Test statistics are included in their overall first-class statistics. A first-class match must have eleven players per side, two innings apiece and a scheduled duration of at least three days. Historically, however, there have been instances of first-class matches being arranged for less than three days, and there have been others with twelve or thirteen players per side; these are exceptional cases and form a tiny percentage of the whole. If the game is not completed within the allotted time then it is drawn, regardless of who has scored the most runs when time expires. Limited overs matches in which the teams have only one innings each are not first-class (see List A and Twenty20 sections below) and these cannot result in a draw (they can, however, result in a tie or be declared a \"no result\").\nTest matches, other games between two Test nations, games between two domestic teams deemed first-class in countries holding full membership of the ICC, and games between a Test nation's national side (or a team drawn from a national touring squad) and a first-class domestic team from a Test nation, are deemed to be first-class. A match between a leading ICC associate member and another team adjudged first-class would be granted first-class status, but domestic matches in the associate member country are minor.\nThe origin of the term \"first-class cricket\" is unknown but, along with other terms, it was used loosely for top-class eleven-a-side matches before it acquired its official status in 1894 (see above). Subsequently, at a meeting of the Imperial Cricket Conference (ICC) in May 1947, it was formally defined on a global basis. A key omission of both the MCC and ICC rulings was any attempt to define first-class cricket retrospectively and it was stipulated in the ICC ruling that the definition \"will not have retrospective effect\". Many historians and statisticians have subjectively classified chosen pre-1895 matches as first-class but these are unofficial ratings and differences of opinion among the experts has led to variations in published cricket statistics. The main problem with \"first-class cricket\" is that it can be a misleading concept as it is essentially statistical and may typically ignore the historical aspect of a match if statistical information is missing, as is invariably the case with matches played up to 1825. Nevertheless, the recognition of any match as first-class by a substantial source qualifies it as such and it follows that the teams, venues and players involved in such matches before 1895 are the equivalent of first-class teams, venues and players since 1895. Substantial sources interested in 18th and 19th century cricket include Arthur Haygarth, F. S. Ashley-Cooper, H. T. Waghorn, G. B. Buckley, H. S. Altham, Roy Webber, John Arlott, Bill Frindall, the ACS and various internet sites (see Historical sources). Writing in 1951, Roy Webber drew a line between what is important historically and what should form part of the statistical record when he argued that the majority of matches prior to 1864 (i.e., the year in which overarm bowling was legalized) \"cannot be regarded as (statistically) first-class\" and their records are used \"for their historical associations\".\n\n\n=== Limited overs cricket ===\n\nLimited overs cricket played with 40 to 60 overs per team, known statistically as List A cricket, is the second form of cricket which differs from first-class as the teams play one inning each and are allowed a maximum number of overs per innings. Matches are scheduled for completion in a single day's play, though they can in theory continue into a second day if impacted by bad weather. Most cricketing nations have some form of domestic List A competition. The over limits range from forty to sixty. The categorization of \"List A\" was only endorsed by the ICC in 2006; the Association of Cricket Statisticians and Historians created it for the purpose of providing a parallel to first-class cricket in their record books.\n\n\n==== 100-ball cricket ====\n100-ball cricket is a form of cricket in which each team has an innings of at most 100 legal balls. Ties are, in some cases, broken by having each team play a \"Super Five\", which is a 5-ball innings for each team. Subsequent Super Fives may be played if the first Super Five is tied. This format is played professionally in The Hundred competition, which started in 2021 in England and Wales.\n\n\n==== Double wicket ====\nDouble-wicket or \"pairs\" cricket is a form of cricket with two teams of two players each which are pitched against each other for a limited number of overs. A player getting out in this form of cricket does not retire but continues to bat but is penalized a stipulated number of runs for each time he gets out. There have been a number of international double wicket cricket tournaments, between 1978 and 2001.\n\n\n==== One-vs-One Cricket ====\nA very similar format was used in the Ultimate Kricket Challenge, held from 24 December 2020 to 1 January 2021 in Dubai. It was a one-on-one format, with players taking turns bowling 15 ball innings to each other. It was played indoors, and the bowling player was assisted by a wicketkeeper and one fielder, as well as being allowed a substitute bowler for up to 7 balls per innings.\n\n\n==== T10 cricket ====\nT10 format is a limited-overs evolution of cricket, following the success of the T20 game, with play limited to just 10 overs per team. It was first played from 14 to 17 December 2017 at the Sharjah Cricket Stadium, approved by the Emirates Cricket Board in a professional cricket league owned and launched by T10 Sports Management. Each team has one inning of 10 overs, also time-limited to 90 minutes. The league is played in a round-robin format that is followed by the semifinals and the final. If there is a tie, the result is decided by means of a Super Over. In August 2018, the International Cricket Council (ICC) officially sanctioned the second season of T10 to be held in Sharjah starting on 23 November that year, with six teams competing.\n\n\n==== Twenty20 cricket ====\n\nTwenty20 is a separate form of limited-overs cricket and is not part of List A. It is the third form of cricket originally devised in England in 2003. The teams have one inning each in which the maximum number of overs is twenty. Twenty20 competitions are held internationally and there are domestic championships, sometimes called franchise cricket in several cricketing nations.\n\n\n=== Single wicket ===\n\nA match in which, as the name implies, there is a single batsman at any time. It is probably the oldest form of cricket as, at its most basic level, it involves one player against another. Historically, its matches were top-class and it has known periods of huge popularity, especially in the mid-18th century when it was the most popular form of cricket thanks to its gambling associations, and in the first half of the 19th century. Matches can involve teams with a single player only but the lucrative 18th century games were mostly between teams of three to five players known as \"threes\", \"fours\" or \"fives\". Only those players designated as team members can bat or bowl but it is normal to have the full quota of fielders including a wicket-keeper.\n\n\n=== Three team cricket ===\nThree team cricket, branded as 3TeamCricket (3TC), is an experimental format that was devised by Paul Harris, former CEO of FirstRand Bank. A 3TC match is contested between 3 teams of 8 players each. Teams bat for one innings of 12 overs, split between two 6-over periods, facing one opponent in the first half and the other opponent in the second half.\nOn 18 July 2020, the 3TC Solidarity Cup became the first 3TeamCricket match to be played. It was held in South Africa as a charity exhibition match.\n\n\n== Amateur cricket ==\n\n\n=== Club cricket ===\n\nClub cricket, by far and away the widest form of cricket played worldwide, is largely amateur, but still formal, cricket, with the teams organised into leagues. The games are sometimes limited-overs, with each innings usually lasting between twenty and fifty overs. Other matches are played to time restrictions. Restrictions in overs or time may be placed on each side individually, or they may stipulate the total length of the match. The latter more traditional case is often known as declaration cricket.\nClub cricket is played extensively in cricketing nations, and also by immigrants from cricketing nations. Club cricket most often takes place on a natural grass wicket, often maintained by the players themselves, although at a lower level it may take place on an artificial turf pitch, though the rest of actual field will be natural grass.\nThere are numerous forms of cricket which, although they are not played professionally or at a recognized high standard, are still popular as common formats of amateur cricket. The double innings, limited overs, Twenty20 and single wicket forms are played by amateur teams: for example, Grade cricket in Australia and the Minor Counties Cricket Championship in England and Wales play the double innings form.\n\n\n=== Declaration cricket ===\nThis is the most traditional version of cricket, with rules most closely replicating the original rules of cricket from the 16th and 17th century. It is a single innings game with a set time limit for the entire game to be completed in. To win the game, a side must both score the highest aggregate number of runs and take all ten of the opposition wickets. It is up to the side batting first to declare when they feel they have enough runs to be able to win the match. In this format of cricket, if the side batting second do not lose all ten of their wickets, the match is said to have ended in a draw.\nDeclaration cricket is generally played over a single day, although two day games lasting an entire weekend are also common. This format is often seen as \"old-fashioned\" and is typically used for friendly matches rather than in organised league play.\n\n\n=== Short format cricket ===\n\nCricket is also played in several different shortened forms, designed to pack as much action as possible into an hour or two, enabling them to be played as a single contest in an evening, or as a series of multiple contests between different teams that cover the entire day. Such forms have evolved since the 1980s, and take cricket an additional step beyond one-day cricket. Most forms will resemble twenty-twenty cricket in nature, although shorter formats with reduced numbers of players, typically 6-aside or 8-aside, are also common for tournament play.\n\n\n==== Rules ====\nDifferent forms of short format cricket have different rules for certain situations:\n\nWhen all but one of a team's batsmen are out:\nIn Last man stands cricket, the last batsman who is not out bats alone, can only run even numbers of runs, and can only make their ground at the striker's end.\nIn six-a-side cricket, the last batsman to be out acts as the nonstriker, while only the not-out batsman can take strike.\n\n\n=== Blind cricket ===\n\nBlind cricket is a variant for blind and partially sighted players. The most obvious difference is that the ball is contains ball bearings to that it can be heard, and that it is rolled along the ground. Blind cricket was invented in 1922, and has been governed by the World Blind Cricket Council since 1996.\n\n\n=== Indoor cricket ===\n\nIndoor cricket is a format of the game designed to be played in an indoor sports hall at times of the year when outdoor play is not possible. There are two recognized forms of indoor cricket. The traditional version played with a hard ball is popular in the UK. This format is played with six players per side and features modified rules designed specifically for indoor play. A soft ball version is played by junior cricketers in the UK and is also popular among adults in the Southern Hemisphere.\n\n\n=== Kwik cricket ===\n\nIt is a simplified, high-speed version of the game played on a small pitch with plastic equipment, aimed mainly at encouraging youngsters to take part.\n\n\n=== Table cricket ===\n\nTable Cricket is an indoor version of the game designed primarily for physically challenged cricketers.\n\n\n== Informal forms of cricket ==\n\n\n=== Backyard cricket ===\nBackyard cricket, Beach cricket, Street cricket and Garden cricket are all different names used to describe a wide range of related informal games. The rules are often ad hoc, and the laws of cricket, such as those involving leg before wicket, penalty runs, and others, are ignored or modified to suit both the setting and participants' preferences. In India and Pakistan, there is Gali cricket ('gali' in Hindi means 'street'. It is pronounced as 'gully' but should not be confused with the fielding position). Often, there are no teams, and each player plays for himself, and fields when he is not batting. Often, there is one wicket, and one bowling position, and no overs. If the batsman runs a single run, he is allowed to walk back to the wicket before the next ball is bowled.Informal cricket in the UK is often known as garden cricket and is played in gardens and recreation grounds around the country. Because of limited space in gardens and the potential damage to property, one particular version of garden cricket is unique in that there are no concept of runs as aerial attacking shots are expressly forbidden, and instead, the winning batsman is the one who can survive the longest number of deliveries. Typically this will be played with a tennis ball or other soft bouncy ball, and modified rules, such as one hand one bounce are often employed. The length of the wicket will typically be roughly 15 meters, and the non-bowling fielders will be encircled close round the bat looking for a catching chance. There are quite often other rules such as not out the first ball and not out leg before wicket\n\n\n=== French cricket ===\n\nIt is a game in which the ball is bowled at the legs of the batsman, with the batsman's legs forming the wicket. It is often played by children. A tennis ball is often used rather than the harder cricket ball. Much like beach cricket, the rules may vary wildly.\n\n\n=== Non-stop (continuous) cricket ===\nContinuous cricket is a game involving one batsman, who upon hitting the ball, must run to a marker, which is square of the wicket. The bowler may bowl as soon as the ball is returned, regardless of whether or not the batsman is still running. The game can be played in teams, or as a group, where players rotate between fielding positions, batting and bowling.\n\n\n==== Bete-ombro ====\n\nA similar version is played on the streets of Brazil and is known as bete-ombro, bats or taco ('taco' being Portuguese for 'bat').\n\n\n==== Placa or plaquita ====\n\n\"La plaquita\" ('The little plate') or \"la placa\" ('The plate') is an obscure variation, played in the streets of Caribbean countries such as the Dominican Republic between two couples, usually making use of broomsticks as bats, rubber or tennis balls, and old licence plates as wickets (with their ends twisted to make them stand up). The game is divided in alternate 3-out innings as in baseball. The first team to reach 100 or 200 runs wins.\n\n\n==== South American variants ====\nPlaquita and Bete-ombro are two South American versions of street cricket that are very similar.\n\n\n=== Tape ball cricket ===\n\nTape ball cricket was invented in Karachi, Pakistan as an attempt to replicate the feeling of a standard game. It does so by covering a tennis ball with electrical tape to make it heavier and give it a smoother surface, similar to that of a hard cricket ball, which also creates extra swing in the air. This concept has the added advantage of not requiring any protective gear, which has seen it spread a sometimes exclusive sport to people from all walks of life. Since its inception in the 1960s-70s, it has been enjoyed in several countries and Pakistanis who have settled abroad have introduced the idea to others by founding tape ball leagues in the UK, USA, and Canada. It remains the most popular pastime for many in Pakistan and was named as one of cricket's ten greatest inventions by Wisden in 2020.\n\n\n=== Tennis ball cricket ===\n\nThis type of cricket is popular in the South Asian sub-continent, USA and Canada. In this game a harder version of tennis ball is used. The number of overs in the game varies from 6 to 25 overs. Considering that the ball is not as hard as the professional cricket ball, the use of protective gear like gloves, pads and helmets is optional. As tennis ball cricket games are shorter when compared to the conventional version, it suits the US and Canadian lifestyle where one would see a large number of people participating. Where cricket pitches are not available, part of a baseball diamond is used as a pitch in most parts of USA and Canada.\n\n\n== Unorthodox forms of cricket ==\n\n\n=== Kilikiti ===\n\nAlso known as Kirikiti, or Samoan Cricket, it is the national game of Samoa and is especially popular in New Zealand. The game is descended from the cricket brought to Samoa by British missionaries; teams of unlimited size follow rules opaque to outside observers in a game/dance/feast event that can last several days.\n\n\n=== Leg cricket ===\n\nSimilar to kickball, it is a form of cricket which involves kicking the ball instead of hitting with a bat.\n\n\n=== Trobriand cricket ===\n\nIt is a peculiar form of cricket played in the Trobriand Islands, in Papua New Guinea. Although cricket was introduced by the British as part of colonial agenda, it was adopted into local Trobriand culture and many modifications and cultural adaptations were made over the years. Some of these include: under-arm bowling; outs are celebrated with dances; the \"home\" team (the tribal community which organised a match) always wins; any number of players can take part in a match; players dress in traditional war costumes.\n\n\n=== Vigoro ===\n\nIt is a form of cricket that also resembles baseball, mainly played by women.\n\n\n== Cricket simulations without a ball or pitch ==\n\n\n=== Book cricket ===\nBook cricket is played by school children in India, Pakistan, Bangladesh and Sri Lanka. It has several variants and is usually played by 2 teams consisting of 3-4 players each. If there are an odd number of players then the person who is left at the end of distribution of teams can play for both teams and is often called a common player. The runs are scored by flipping a book open at random and counting as the number of runs scored the last digit of the page-number of the verso (the left-side or even-numbered page). 0 and sometimes 8 are assigned special rules: typically a wicket is lost when a person scores 0, and a No-ball run and an additional chance are assigned when a player scores 8. To give an example, if the batting side opened the book at page 26, then 6 runs would be scored. For the toss, both players open a page and the one who scores the higher number of runs wins.\nAnother version of cricket appeared during the 1950s in the UK in the Eagle comic. A page was chosen and each letter or symbol was counted according to a formula. This produced a  scorecard with the majority of innings around 150 to 300 scored at about 4 runs per over.\n\n\n=== Calculator cricket ===\nThis form is played by school children who use scientific calculators for maths and science.A player starts by clearing the memory on their calculator. The player will then use the random number generator on their calculator to bring up a number between 0 and 1. The number of runs scored is the first digit after the decimal point (for example, if the random number generator provides 0.521, 5 runs are scored). Scoring is kept by using the memory addition function on the calculator, or by pen and paper. Scoring a 0 is considered out. The player who has the highest score wins.\n\n\n=== Hand cricket ===\nHand cricket is played through gestures (called 'throws') similar to rock paper scissors. The total number of fingers extended equates to the equivalent number, with a thumb counting as 6. In some variants, this is the maximum number of runs possible in a throw, but in others, it extends to 10 runs, with 7 indicated by the thumb and index finger, 8 by the thumb, index and middle finger, 9 by the thumb, index, middle, and ring fingers, and 10 by a scrunched up (as opposed to an open) palm. Throws are made simultaneously by both players, one designated as the batter and the other as the bowler. Runs scored according to the batter's throws until the bowler throws the same, in which case the batter is \"out\". It is played by school children in India, Sri Lanka and Pakistan.\n\n\n=== Pencil cricket ===\n\nA one-person game played with pencils marked by hand to function as 'long dice'. A Japanese variant of these for use in other games are called 'battle pencils'. It may also simply be played with conventional dice. The aim is to generate scores and attribute them to imaginary players and teams by compiling a scorecard. The game has been marketed commercially featuring plastic or metal long dice (rollers) and playing rules.The board game Test Match operates on a similar principle.\n\n\n=== Pub cricket ===\n\nAlso called car cricket. A travel game based on the names of public houses passed on the route. Runs are scored according to the number of legs, arms or other items featured in the pub name. The exact rules vary according to the participants.\n\n\n== Bibliography ==\nACS (1981). A Guide to Important Cricket Matches Played in the British Isles 1709 \u2013 1863. Nottingham: ACS.\nACS (1982). A Guide to First-Class Cricket Matches Played in the British Isles. Nottingham: ACS.\nBirley, Derek (1999). A Social History of English Cricket. Aurum.\nWebber, Roy (1951). The Playfair Book of Cricket Records. Playfair Books.\nWisden Cricketers' Almanack, 32nd edition, editor Sydney Pardon, John Wisden & Co., 1895\nWisden Cricketers' Almanack, 85th edition, editor Hubert Preston, Sporting Handbooks Ltd, 1948\n\n\n== See also ==\nWicket (sport) - a North American historical version of cricket\nBat-and-ball sports\n\n\n== References =="}, {"id": 48, "title": "Cryptography", "content": "Cryptography, or cryptology (from Ancient Greek: \u03ba\u03c1\u03c5\u03c0\u03c4\u03cc\u03c2, romanized: krypt\u00f3s \"hidden, secret\"; and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd graphein, \"to write\", or -\u03bb\u03bf\u03b3\u03af\u03b1 -logia, \"study\", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, information security, electrical engineering, digital signal processing, physics, and others. Core concepts related to  information security (data confidentiality, data integrity, authentication, and non-repudiation) are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.\nCryptography prior to the modern age was effectively synonymous with encryption, converting readable information (plaintext) to unintelligible nonsense text (ciphertext), which can only be read by reversing the process (decryption). The sender of an encrypted (coded) message shares the decryption (decoding) technique only with the intended recipients to preclude access from adversaries. The cryptography literature often uses the names \"Alice\" (or \"A\") for the sender, \"Bob\" (or \"B\") for the intended recipient, and \"Eve\" (or \"E\") for the eavesdropping adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and their applications more varied.\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed \"computationally secure\". Theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated and, if necessary, adapted. Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable but computationally secure schemes.\nThe growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes with regard to digital media.\n\n\n== Terminology ==\nThe first use of the term \"cryptograph\" (as opposed to \"cryptogram\") dates back to the 19th century\u2014originating from \"The Gold-Bug,\" a story by Edgar Allan Poe.Until modern times, cryptography referred almost exclusively to \"encryption\", which is the process of converting ordinary information (called plaintext) into an unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a \"key\". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext.  In formal mathematical terms, a \"cryptosystem\" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms that correspond to each key.  Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks.\nThere are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message. Data manipulation in symmetric systems is significantly faster than in asymmetric systems. Asymmetric systems use a \"public key\" to encrypt a message and a related \"private key\" to decrypt it. The advantage of asymmetric systems is that the public key can be freely published, allowing parties to establish secure communication without having a shared secret key.  In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie\u2013Hellman key exchange, RSA (Rivest\u2013Shamir\u2013Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).  Insecure symmetric algorithms include children's language tangling schemes such as Pig Latin or other cant, and all historical cryptographic schemes, however seriously intended, prior to the invention of the one-time pad early in the 20th century.\nIn colloquial use, the term \"code\" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, \"wallaby\" replaces \"attack at dawn\").  A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, a syllable, or a pair of letters, etc.) in order to produce a cyphertext.\nCryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to \"crack\" encryption algorithms or their implementations.\nSome use the terms \"cryptography\" and \"cryptology\" interchangeably in English, while others (including US military practice generally) use \"cryptography\" to refer specifically to the use and practice of cryptographic techniques and \"cryptology\" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which \"cryptology\" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics. Cryptolingusitics is especially used in military intelligence applications for deciphering foreign communications.\n\n\n== History ==\n\nBefore the modern era, cryptography focused on message confidentiality (i.e., encryption)\u2014conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.\n\n\n=== Classic cryptography ===\nThe main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter some fixed number of positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (c.\u20091900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.\nThe Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military). Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair. More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.\nIn India, the 2000-year-old Kamasutra of V\u0101tsy\u0101yana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.In Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the \u0161\u0101h-dab\u012br\u012bya (literally \"King's script\") which was used for official correspondence, and the r\u0101z-sahar\u012bya which was used to communicate secret messages with other countries.David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\nCiphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as Alkindus) in the 9th century, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.\nLanguage letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.\nEssentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel that implemented a partial realization of his invention. In the Vigen\u00e8re cipher, a polyalphabetic cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigen\u00e8re cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim\u2014'the enemy knows the system'.\nDifferent physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher. In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines\u2014famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.\n\n\n=== Early computer-era cryptography ===\nCryptanalysis of the new mechanical ciphering devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitive tasks, such as military code breaking (decryption). This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.\nExtensive open academic research into cryptography is relatively recent, beginning in the mid-1970s. In the early 1970s IBM personnel designed the Data Encryption Standard (DES) algorithm that became the first federal government cryptography standard in the United States. In 1976 Whitfield Diffie and Martin Hellman published the Diffie\u2013Hellman key exchange algorithm. In 1977 the RSA algorithm was published in Martin Gardner's Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally.\nSome modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so, proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers.  There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring  n = pq is impossible;  it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem.As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing. The potential impact of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography. The announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative.\n\n\n=== Modern cryptography ===\nPrior to the early 20th century, cryptography was mainly concerned with linguistic and lexicographic patterns. Since then cryptography has broadened in scope, and now makes extensive use of mathematical subdisciplines, including information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics.\nJust as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.\n\n\n== Modern cryptography ==\n\n\n=== Symmetric-key cryptography ===\n\nSymmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.\nSymmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.\nThe Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted). Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption to e-mail privacy and secure remote access. Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL.Stream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher. Block ciphers can be used as stream ciphers by generating blocks of a keystream (in place of a Pseudorandom number generator) and applying an XOR operation to each bit of the plaintext with each bit of the keystream.Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort. Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.\n\n\n=== Public-key cryptography ===\n\nSymmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret.\n\nIn a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used\u2014a public key and a private key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as \"the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance\".In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. In a public-key encryption system, the public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie\u2013Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.\nThe X.509 standard defines the most commonly used format for public key certificates.Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.The Diffie\u2013Hellman and RSA algorithms, in addition to being the first publicly known examples of high-quality public-key algorithms, have been among the most widely used. Other asymmetric-key algorithms include the Cramer\u2013Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques.A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that was very similar in design rationale to RSA. In 1974, Malcolm J. Williamson is claimed to have developed the Diffie\u2013Hellman key exchange.\nPublic-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).Public-key algorithms are most often based on the computational complexity of \"hard\" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie\u2013Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.\n\n\n=== Cryptographic hash functions ===\nCryptographic hash functions are functions that take a variable-length input and return a fixed-length output, which can be used in, for example, a digital signature. For a hash function to be secure, it must be difficult to compute two inputs that hash to the same value (collision resistance) and to compute an input that hashes to a given output (preimage resistance). MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.\n\n\n=== Cryptanalysis ===\n\nThe goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.\nIt is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible.\nThere are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved).\n\nCryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations. This is a considerable improvement over brute force attacks.\nPublic-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time (P) using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, to achieve an equivalent strength of encryption, techniques that depend upon the difficulty of factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.\nWhile pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, they may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, rubber-hose cryptanalysis or torture) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin.\n\n\n=== Cryptographic primitives ===\nMuch of the theoretical work in cryptography concerns cryptographic primitives\u2014algorithms with basic cryptographic properties\u2014and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.\n\n\n=== Cryptosystems ===\n\nOne or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols.\nSome widely known cryptosystems include RSA, Schnorr signature, ElGamal encryption, and Pretty Good Privacy (PGP). More complex cryptosystems include electronic cash systems, signcryption systems, etc. Some more 'theoretical' cryptosystems include interactive proof systems, (like zero-knowledge proofs), systems for secret sharing, etc.\n\n\n=== Lightweight cryptography ===\nLightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. The growth of Internet of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security. Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed to achieve the standard set by the National Institute of Standards and Technology.\n\n\n== Applications ==\n\nCryptography is widely used on the internet to help protect user-data and prevent eavesdropping. To ensure secrecy during transmission, many systems use private key cryptography to protect transmitted information. With public-key systems, one can maintain secrecy without a master key or a large number of keys. But, some algorithms like Bitlocker and Veracrypt are generally not private-public key cryptography. For example, Veracrypt uses a password hash to generate the single private key. However, it can be configured to run in public-private key systems. The C++ opensource encryption library OpenSSL provides free and opensource encryption software and tools. The most commonly used encryption cipher suit is AES, as it has hardware acceleration for all x86 based processors that has AES-NI. A close contender is ChaCha20-Poly1305, which is a stream cipher, however it is commonly used for mobile devices as they are ARM based which does not feature AES-NI instruction set extension.\n\n\n=== Cybersecurity ===\nCryptography can be used to secure communications by encrypting them. Websites use encryption via HTTPS. \"End-to-end\" encryption, where only sender and receiver can read messages, is implemented for email in Pretty Good Privacy and for secure messaging in general in WhatsApp, Signal and Telegram.Operating systems use encryption to keep passwords secret, conceal parts of the system, and ensure that software updates are truly from the system maker. Instead of storing plaintext passwords, computer systems store hashes thereof; then, when a user logs in, the system passes the given password through a cryptographic hash function and compares it to the hashed value on file. In this manner, neither the system nor an attacker has at any point access to the password in plaintext.Encryption is sometimes used to encrypt one's entire drive. For example, University College London has implemented BitLocker (a program by Microsoft) to render drive data opaque without users logging in.\n\n\n=== Cryptocurrencies and cryptoeconomics ===\nCryptographic techniques enable cryptocurrency technologies, such as distributed ledger technologies (e.g., blockchains), which finance cryptoeconomics applications such as decentralized finance (DeFi). Key cryptographic techniques that enable cryptocurrencies and cryptoeconomics include, but are not limited to: cryptographic keys, cryptographic hash function, asymmetric (public key) encryption, Multi-Factor Authentication (MFA), End-to-End Encryption (E2EE), and Zero Knowledge Proofs (ZKP).\n\n\n== Legal issues ==\n\n\n=== Prohibitions ===\nCryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible.\nIn some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.\n\n\n=== Export controls ===\n\nIn the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and \"dual-use\" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users do not realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally do not find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.\n\n\n=== NSA involvement ===\n\nAnother contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.\nAnother instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).\n\n\n=== Digital rights management ===\n\nCryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.\nThe United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech.\n\n\n=== Forced disclosure of encryption keys ===\n\nIn the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.\nIn the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.In many jurisdictions, the legal status of forced disclosure remains unclear.\nThe 2016 FBI\u2013Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected.\nAs a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).\n\n\n== See also ==\nCollision attack\nComparison of cryptography libraries\nCrypto Wars \u2013 Attempts to limit access to strong cryptography\nEncyclopedia of Cryptography and Security \u2013 Book by Technische Universiteit Eindhoven\nGlobal surveillance \u2013 Mass surveillance across national borders\nIndistinguishability obfuscation \u2013 Type of cryptographic software obfuscation\nInformation theory \u2013 Scientific study of digital information\nOutline of cryptography \u2013 Overview of and topical guide to cryptography\nList of cryptographers\nList of important publications in cryptography\nList of multiple discoveries\nList of unsolved problems in computer science \u2013 List of unsolved computational problems\nSecure cryptoprocessor\nStrong cryptography \u2013 Term applied to cryptographic systems that are highly resistant to cryptanalysis\nSyllabical and Steganographical Table \u2013 Eighteenth-century work believed to be the first cryptography chart \u2013 first cryptography chart\nWorld Wide Web Consortium's Web Cryptography API \u2013 World Wide Web Consortium cryptography standard\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\n The dictionary definition of cryptography at Wiktionary\n Media related to Cryptography at Wikimedia Commons\nCryptography on In Our Time at the BBC\nCrypto Glossary and Dictionary of Technical Cryptography\nA Course in Cryptography by Raphael Pass & Abhi Shelat \u2013 offered at Cornell in the form of lecture notes.\nFor more on the use of cryptographic elements in fiction, see: Dooley, John F., William and Marilyn Ingersoll Professor of Computer Science, Knox College (23 August 2012). \"Cryptology in Fiction\". Archived from the original on 29 July 2020. Retrieved 20 February 2015.{{cite web}}:  CS1 maint: multiple names: authors list (link)\nThe George Fabyan Collection at the Library of Congress has early editions of works of seventeenth-century English literature, publications relating to cryptography."}, {"id": 49, "title": "FIFA World Cup", "content": "The FIFA World Cup, often simply called the World Cup, is an international association football competition between the senior men's national teams of the members of the F\u00e9d\u00e9ration Internationale de Football Association (FIFA), the sport's global governing body. The tournament has been held every four years since the inaugural tournament in 1930, with the exception of 1942 and 1946 due to the Second World War. The reigning champions are Argentina, who won their third title at the 2022 tournament.\nThe contest starts with the qualification phase, which takes place over the preceding three years to determine which teams qualify for the tournament phase. In the tournament phase, 32 teams compete for the title at venues within the host nation(s) over the course of about a month. The host nation(s) automatically qualify for the group stage of the tournament. The next FIFA World Cup is scheduled to expand to 48 teams for the 2026 tournament.\nAs of the 2022 FIFA World Cup, 22 final tournaments have been held since the event's inception in 1930, and a total of 80 national teams have competed. The trophy has been won by eight national teams. Brazil, with five wins, are the only team to have played in every tournament. The other World Cup winners are Germany and Italy, with four titles each; Argentina, with three titles; France and inaugural winner Uruguay, each with two titles; and England and Spain, with one title each.\nThe World Cup is the most prestigious association football tournament in the world, as well as the most widely viewed and followed single sporting event in the world. The viewership of the 2018 World Cup was estimated to be 3.57 billion, close to half of the global population, while the engagement with the 2022 World Cup was estimated to be 5 billion, with about 1.5 billion people watching the final match.Seventeen countries have hosted the World Cup, most recently Qatar, who hosted the 2022 event. The 2026 tournament will be jointly hosted by Canada, the United States and Mexico, which will give Mexico the distinction of being the first country to host games in three World Cups.\n\n\n== History ==\n\n\n=== Previous international competitions ===\nThe world's first international football match was a challenge match played in Glasgow in 1872 between Scotland and England. The first international tournament for nations, the inaugural British Home Championship, took place in 1884 and included games between England, Scotland, Wales, and Ireland. As football grew in popularity in other parts of the world at the start of the 20th century, it was held as a demonstration sport with no medals awarded at the 1900 and 1904 Summer Olympics; however, the International Olympic Committee has retroactively upgraded their status to official events, as well as the 1906 Intercalated Games.After FIFA was founded in 1904, it tried to arrange an international football tournament between nations outside the Olympic framework in Switzerland in 1906. These were very early days for international football, and the official history of FIFA describes the competition as having been unsuccessful.At the 1908 Summer Olympics in London, football became an official Olympic sport. Planned by The Football Association (FA), England's football governing body, the event was for amateur players only and was regarded suspiciously as a show rather than a competition. Great Britain (represented by the England national amateur football team) won the gold medals. They repeated the feat at the 1912 Summer Olympics in Stockholm.With the Olympic event continuing to be a contest between amateur teams only, Sir Thomas Lipton organised the Sir Thomas Lipton Trophy tournament in Turin in 1909. The Lipton tournament was a championship between individual clubs (not national teams) from different nations, each of which represented an entire nation. The competition is sometimes described as The First World Cup, and featured the most prestigious professional club sides from Italy, Germany and Switzerland, but the FA of England refused to be associated with the competition and declined the offer to send a professional team. Lipton invited West Auckland, an amateur side from County Durham, to represent England instead. West Auckland won the tournament and returned in 1911 to successfully defend their title. Prior to the Lipton competition, from 1876 to 1904, games that were considered to be the \"football world championship\" were meetings between leading English and Scottish clubs, such as the 1895 game between Sunderland A.F.C. and the Heart of Midlothian F.C., which Sunderland won.In 1914, FIFA agreed to recognise the Olympic tournament as a \"world football championship for amateurs\", and took responsibility for managing the event. This paved the way for the world's first intercontinental football competition for nations, at the 1920 Summer Olympics, contested by Egypt and 13 European teams, and won by Belgium. Uruguay won the next two Olympic football tournaments in 1924 and 1928. Those were also the first two open world championships, as 1924 was the start of FIFA's professional era, and is the reason why Uruguay is allowed to wear 4 stars.\n\n\n=== World Cups before World War II ===\nDue to the success of the Olympic football tournaments, FIFA, with President Jules Rimet as the driving force, again started looking at staging its own international tournament outside of the Olympics. On 28 May 1928, the FIFA Congress in Amsterdam decided to stage a world championship. With Uruguay now two-time official football world champions and to celebrate their centenary of independence in 1930, FIFA named Uruguay as the host country of the inaugural World Cup tournament.The national associations of selected nations were invited to send a team, but the choice of Uruguay as a venue for the competition meant a long and costly trip across the Atlantic Ocean for European sides, especially in the midst of the Great Depression. As such, no European country pledged to send a team until two months before the start of the competition. Rimet eventually persuaded teams from Belgium, France, Romania, and Yugoslavia to make the trip. In total, 13 nations took part: seven from South America, four from Europe, and two from North America.\nThe first two World Cup matches took place simultaneously on 13 July 1930, and were won by France and the United States, who defeated Mexico 4\u20131 and Belgium 3\u20130 respectively. The first goal in World Cup history was scored by Lucien Laurent of France. In the final, Uruguay defeated Argentina 4\u20132 in front of 93,000 spectators in Montevideo, and became the first nation to win the World Cup. After the creation of the World Cup, FIFA and the IOC disagreed over the status of amateur players; football was dropped from the 1932 Summer Olympics. After the IOC and FIFA worked out their differences, Olympic football returned at the 1936 Summer Olympics, but was now overshadowed by the more prestigious World Cup.The issues facing the early World Cup tournaments were the difficulties of intercontinental travel, and war. Few South American teams were willing to travel to Europe for the 1934 World Cup and all North and South American nations except Brazil and Cuba boycotted the 1938 tournament. Brazil was the only South American team to compete in both. The 1942 and 1946 competitions, which Germany and Brazil sought to host, were cancelled due to World War II.\n\n\n=== World Cups after World War II ===\nThe 1950 World Cup, held in Brazil, was the first to include British football associations. Scotland, England, Wales, and Northern Ireland had withdrawn from FIFA in 1920, partly out of unwillingness to play against the countries they had been at war with, and partly as a protest against foreign influence on football. The teams rejoined in 1946 following FIFA's invitation. The tournament also saw the return of 1930 champions Uruguay, who had boycotted the previous two World Cups. Uruguay won the tournament again after defeating the host nation Brazil, in the match called \"Maracanazo\" (Portuguese: Maracana\u00e7o).In the tournaments between 1934 and 1978, 16 teams competed in each tournament, except in 1938, when Austria was absorbed into Germany after qualifying, leaving the tournament with 15 teams, and in 1950, when India, Scotland, and Turkey withdrew, leaving the tournament with 13 teams. Most of the participating nations were from Europe and South America, with a small minority from North America, Africa, Asia, and Oceania. These teams were usually defeated easily by the European and South American teams. Until 1982, the only teams from outside Europe and South America to advance out of the first round were: United States, semi-finalists in 1930; Cuba, quarter-finalists in 1938; North Korea, quarter-finalists in 1966; and Mexico, quarter-finalists in 1970.\n\n\n=== Expansion to 24 and 32 teams ===\nThe tournament was expanded to 24 teams in 1982, and then to 32 in 1998, allowing more teams from Africa, Asia and North America to take part. Since then, teams from these regions have enjoyed more success, with several having reached the quarter-finals: Mexico, quarter-finalists in 1986; Cameroon, quarter-finalists in 1990; South Korea, finishing in fourth place in 2002; Senegal, along with USA, both quarter-finalists in 2002; Ghana, quarter-finalists in 2010; Costa Rica, quarter-finalists in 2014; and Morocco, finishing in fourth place in 2022. European and South American teams continue to dominate, e.g., the quarter-finalists in 1994, 1998, 2006 and 2018 were all from Europe or South America and so were the finalists of all tournaments so far.\nTwo hundred teams entered the 2002 FIFA World Cup qualification rounds. 198 nations attempted to qualify for the 2006 FIFA World Cup. A record 204 countries entered qualification for the 2010 FIFA World Cup.\n\n\n=== Expansion to 48 teams ===\nIn October 2013, Sepp Blatter spoke of guaranteeing the Caribbean Football Union's region a position in the World Cup. In the edition of 25 October 2013 of the FIFA Weekly Blatter wrote that: \"From a purely sporting perspective, I would like to see globalisation finally taken seriously, and the African and Asian national associations accorded the status they deserve at the FIFA World Cup. It cannot be that the European and South American confederations lay claim to the majority of the berths at the World Cup.\" Those two remarks suggested to commentators that Blatter could be putting himself forward for re-election to the FIFA Presidency.Following the magazine's publication, Blatter's would-be opponent for the FIFA Presidency, UEFA President Michel Platini, responded that he intended to extend the World Cup to 40 national associations, increasing the number of participants by eight. Platini said that he would allocate an additional berth to UEFA, two each to the Asian Football Confederation and the Confederation of African Football, two shared between CONCACAF and CONMEBOL, and a guaranteed place for the Oceania Football Confederation. Platini was clear about why he wanted to expand the World Cup. He said: \"[The World Cup is] not based on the quality of the teams because you don't have the best 32 at the World Cup ... but it's a good compromise. ... It's a political matter so why not have more Africans? The competition is to bring all the people of all the world. If you don't give the possibility to participate, they don't improve.\"In October 2016, FIFA president Gianni Infantino stated his support for a 48-team World Cup in 2026. On 10 January 2017, FIFA confirmed the 2026 World Cup will have 48 finalist teams.\n\n\n=== 2015 FIFA corruption case ===\n\nBy May 2015, the games were under a particularly dark cloud because of the 2015 FIFA corruption case, allegations and criminal charges of bribery, fraud and money laundering to corrupt the issuing of media and marketing rights (rigged bids) for FIFA games, with FIFA officials accused of taking bribes totaling more than $150 million over 24 years. In late May, the U.S. Department of Justice announced a 47-count indictment with charges of racketeering, wire fraud and money laundering conspiracy against 14 people. Arrests of over a dozen FIFA officials were made since that time, particularly on 29 May and 3 December. By the end of May 2015, a total of nine FIFA officials and five executives of sports and broadcasting markets had already been charged on corruption. At the time, FIFA president Sepp Blatter announced he would relinquish his position in February 2016.On 4 June 2015, Chuck Blazer while co-operating with the FBI and the Swiss authorities admitted that he and the other members of FIFA's then-executive committee were bribed in order to promote the 1998 and 2010 World Cups. On 10 June 2015, Swiss authorities seized computer data from the offices of Sepp Blatter. The same day, FIFA postponed the bidding process for the 2026 FIFA World Cup in light of the allegations surrounding bribery in the awarding of the 2018 and 2022 tournaments. Then-secretary general J\u00e9r\u00f4me Valcke stated, \"Due to the situation, I think it's nonsense to start any bidding process for the time being.\" On 28 October 2015, Blatter and FIFA VP Michel Platini, a potential candidate for presidency, were suspended for 90 days; both maintained their innocence in statements made to the news media.On 3 December 2015 two FIFA vice-presidents were arrested on suspicion of bribery in the same Zurich hotel where seven FIFA officials had been arrested in May. An additional 16 indictments by the US Department of Justice were announced on the same day.\n\n\n=== Biennial World Cup proposition ===\nA biennial World Cup plan was first proposed by the Saudi Arabian Football Federation at the 71st FIFA Congress on 21 May 2021 and prominently backed by former Arsenal manager Ars\u00e8ne Wenger and national federations in Africa and Asia.\nContinental confederations such as UEFA and CONMEBOL are not on board with the plan but, in total, the idea is supported by 166 of the 210 member associations of FIFA.\n\n\n=== Other FIFA tournaments ===\nAn equivalent tournament for women's football, the FIFA Women's World Cup, was first held in 1991 in China. The women's tournament is smaller in scale and profile than the men's, but is growing; the number of entrants for the 2007 tournament was 120, more than double that of 1991.Men's football has been included in every Summer Olympic Games except 1896 and 1932. Unlike many other sports, the men's football tournament at the Olympics is not a top-level tournament, and since 1992, an under-23 tournament with each team allowed three over-age players. Women's football made its Olympic debut in 1996.\nThe FIFA Confederations Cup was a tournament held one year before the World Cup at the World Cup host nation(s) as a dress rehearsal for the upcoming World Cup. It is contested by the winners of each of the six FIFA confederation championships, along with the FIFA World Cup champion and the host country. The first edition took place in 1992 and the last edition was played in 2017. In March 2019, FIFA confirmed that the tournament would no longer be active owing to an expansion of the FIFA Club World Cup in 2021.FIFA also organises international tournaments for youth football (FIFA U-20 World Cup, FIFA U-17 World Cup, FIFA U-20 Women's World Cup, FIFA U-17 Women's World Cup), club football (FIFA Club World Cup), and football variants such as futsal (FIFA Futsal World Cup) and beach soccer (FIFA Beach Soccer World Cup). The latter three do not have a women's version, although a FIFA Women's Club World Cup has been proposed.The FIFA U-20 Women's World Cup is held biannually, including the year before each Women's World Cup. Both tournaments were awarded in a single bidding process on three occasions, with the U-20 tournament serving as a dress rehearsal for the larger competition each time (2010, 2014 and 2018).\n\n\n== Trophy ==\n\nFrom 1930 to 1970, the Jules Rimet Trophy was awarded to the World Cup winning team. It was originally simply known as the World Cup or Coupe du Monde, but in 1946 it was renamed after the FIFA president Jules Rimet who set up the first tournament. In 1970, Brazil's third victory in the tournament entitled them to keep the trophy permanently. However, the trophy was stolen in 1983 and has never been recovered, apparently melted down by the thieves.\nAfter 1970, a new trophy, known as the FIFA World Cup Trophy, was designed. The experts of FIFA, coming from seven countries, evaluated the 53 presented models, finally opting for the work of the Italian designer Silvio Gazzaniga. The new trophy is 36 cm (14.2 in) high, made of solid 18 carat (75%) gold and weighs 6.175 kg (13.6 lb).The base contains two layers of semi-precious malachite while the bottom side of the trophy bears the engraved year and name of each FIFA World Cup winner since 1974. The description of the trophy by Gazzaniga was: \"The lines spring out from the base, rising in spirals, stretching out to receive the world. From the remarkable dynamic tensions of the compact body of the sculpture rise the figures of two athletes at the stirring moment of victory.\"This new trophy is not awarded to the winning nation permanently. World Cup winners retain the trophy only until the post-match celebration is finished. They are awarded a gold-plated replica rather than the solid gold original immediately afterwards.All members (players, coaches, and managers) of the top three teams receive medals with an insignia of the World Cup Trophy; winners' (gold), runners-up' (silver), and third-place (bronze). In the 2002 edition, fourth-place medals were awarded to hosts South Korea. Before the 1978 tournament, medals were only awarded to the eleven players on the pitch at the end of the final and the third-place match. In November 2007, FIFA announced that all members of World Cup-winning squads between 1930 and 1974 were to be retroactively awarded winners' medals.Since 2006, winners of the competition are also awarded the right to wear the FIFA Champions Badge, up until the time at which the winner of the next competition is decided.\n\n\n== Format ==\n\n\n=== Qualification ===\n\nSince the second World Cup in 1934, qualifying tournaments have been held to thin the field for the final tournament. They are held within the six FIFA continental zones (Africa, Asia, North and Central America and Caribbean, South America, Oceania, and Europe), overseen by their respective confederations. For each tournament, FIFA decides the number of places awarded to each of the continental zones beforehand, generally based on the relative strength of the confederations' teams.\nThe qualification process can start as early as almost three years before the final tournament and last over a two-year period. The formats of the qualification tournaments differ between confederations. Usually, one or two places are awarded to winners of intercontinental play-offs. For example, the winner of the Oceanian zone and the fifth-placed team from the Asian zone entered a play-off for a spot in the 2010 World Cup. From the 1938 World Cup onwards, host nations receive automatic qualification to the final tournament. This right was also granted to the defending champions between 1938 and 2002, but was withdrawn from the 2006 FIFA World Cup onward, requiring the champions to qualify. Brazil, winners in 2002, were the first defending champions to play qualifying matches.\n\n\n=== Final tournament ===\n\nThe final tournament format since 1998 has had 32 national teams competing over the course of a month in the host nations. There are two stages: the group stage, followed by the knockout stage.In the group stage, teams compete within eight groups of four teams each. Eight teams are seeded, including the hosts, with the other seeded teams selected using a formula based on the FIFA World Rankings or performances in recent World Cups, and drawn to separate groups. The other teams are assigned to different \"pots\", usually based on geographical criteria, and teams in each pot are drawn at random to the eight groups. Since 1998, constraints have been applied to the draw to ensure that no group contains more than two European teams or more than one team from any other confederation.Each group plays a round-robin tournament in which each team is scheduled for three matches against other teams in the same group. This means that a total of six matches are played within a group. The last round of matches of each group is scheduled at the same time to preserve fairness among all four teams. The top two teams from each group advance to the knockout stage. Points are used to rank the teams within a group. Since 1994, three points have been awarded for a win, one for a draw and none for a loss (before, winners received two points).\nConsidering all possible outcomes (win, draw, loss) for all six matches in a group, there are 729 (= 36) combinations possible. However, 207 of these combinations lead to ties between the second and third places. In such case, the ranking among these teams is determined by:\nGreatest combined goal difference in all group matches\nGreatest combined number of goals scored in all group matches\nIf more than one team remain level after applying the above criteria, their ranking will be determined as follows:\nGreatest number of points in head-to-head matches among those teams\nGreatest goal difference in head-to-head matches among those teams\nGreatest number of goals scored in head-to-head matches among those teams\nFair play points, defined by the number of yellow and red cards received in the group stage:\nYellow card: minus 1 point\nIndirect red card (as a result of a second yellow card): minus 3 points\nDirect red card: minus 4 points\nYellow card and direct red card: minus 5 points\nIf any of the teams above remain level after applying the above criteria, their ranking will be determined by the drawing of lotsThe knockout stage is a single-elimination tournament in which teams play each other in one-off matches, with extra time and penalty shootouts used to decide the winner if necessary. It begins with the round of 16 (or the second round) in which the winner of each group plays against the runner-up of another group. This is followed by the quarter-finals, the semi-finals, the third-place match (contested by the losing semi-finalists), and the final.On 10 January 2017, FIFA approved a new format, the 48-team World Cup (to accommodate more teams), which was to consist of 16 groups of three teams each, with two teams qualifying from each group, to form a round of 32 knockout stage, to be implemented by 2026. On 14 March 2023, FIFA approved a revised format of the 2026 tournament, which features 12 groups of four teams each, with the top 8 third-placed teams joining the group winners and runners-up in a new round of 32.\n\n\n== Hosts ==\n\n\n=== Selection process ===\nEarly World Cups were given to countries at meetings of FIFA's congress. The locations were controversial because South America and Europe were by far the two centres of strength in football and travel between them required three weeks by boat. The decision to hold the first World Cup in Uruguay, for example, led to only four European nations competing. The next two World Cups were both held in Europe. The decision to hold the second of these in France was disputed, as the South American countries understood that the location would alternate between the two continents. Both Argentina and Uruguay thus boycotted the 1938 FIFA World Cup.Since the 1958 FIFA World Cup, to avoid future boycotts or controversy, FIFA began a pattern of alternating the hosts between the Americas and Europe, which continued until the 1998 FIFA World Cup. The 2002 FIFA World Cup, hosted jointly by South Korea and Japan, was the first one held in Asia, and the first tournament with multiple hosts. South Africa became the first African nation to host the World Cup in 2010. The 2014 FIFA World Cup was hosted by Brazil, the first held in South America since Argentina 1978, and was the first occasion where consecutive World Cups were held outside Europe.\nThe host country is now chosen in a vote by FIFA's Council. This is done under an exhaustive ballot system. The national football association of a country desiring to host the event receives a \"Hosting Agreement\" from FIFA, which explains the steps and requirements that are expected from a strong bid. The bidding association also receives a form, the submission of which represents the official confirmation of the candidacy. After this, a FIFA designated group of inspectors visit the country to identify that the country meets the requirements needed to host the event and a report on the country is produced. The decision on who will host the World Cup is usually made six or seven years in advance of the tournament. There have been occasions where the hosts of multiple future tournaments were announced at the same time, as was the case for the 2018 and 2022 World Cups, which were awarded to Russia and Qatar, with Qatar becoming the first Middle Eastern country to host the tournament.For the 2010 and 2014 World Cups, the final tournament was rotated between confederations, allowing only countries from the chosen confederation (Africa in 2010, South America in 2014) to bid to host the tournament. The rotation policy was introduced after the controversy surrounding Germany's victory over South Africa in the vote to host the 2006 tournament. However, the policy of continental rotation did not continue beyond 2014, so any country, except those belonging to confederations that hosted the two preceding tournaments, can apply as hosts for World Cups starting from 2018. This is partly to avoid a similar scenario to the bidding process for the 2014 tournament, where Brazil was the only official bidder.The 2026 FIFA World Cup was chosen to be held in the United States, Canada and Mexico, marking the first time a World Cup has been shared by three host nations. The 2026 tournament will be the biggest World Cup ever held, with 48 teams playing 104 matches. Sixty matches will take place in the US, including all matches from the quarter-finals onward, while Canada and Mexico will host 10 games each.\n\n\n=== Selection results ===\n* West Germany was the host of the 1974 Cup, and (reunited) Germany host to the one in 2006\n\n\n=== Performances ===\n\nSix of the eight champions have won one of their titles while playing in their own homeland, the exceptions being Brazil, who finished as runners-up after losing the deciding match on home soil in 1950 and lost their semi-final against Germany in 2014, and Spain, which reached the second round on home soil in 1982. England (1966) won its only title while playing as a host nation. Uruguay (1930), Italy (1934), Argentina (1978), and France (1998) won their first titles as host nations but have gone on to win again, while Germany (1974) won their second title on home soil.Other nations have also been successful when hosting the tournament. Switzerland (quarter-finals 1954), Sweden (runners-up in 1958), Chile (third place in 1962), South Korea (fourth place in 2002), Russia (quarter-finals 2018), and Mexico (quarter-finals in 1970 and 1986) all have their best results when serving as hosts. So far, South Africa (2010) and Qatar (2022) failed to advance beyond the first round.\n\n\n== Attendance ==\n\n\u2020 Source: FIFA\u2021 The best-attended single match has been the final in 11 of the 21 World Cups as of 2018. Another match or matches drew more attendance than the final in 1930, 1938, 1958, 1962, 1970\u20131982, 1990, and 2006.\n\n\n== Broadcasting and promotion ==\nThe World Cup was first televised in 1954 and as of 2006 is the most widely viewed and followed sporting event in the world. The cumulative viewership of all matches of the 2006 World Cup was estimated to be 26.29 billion. 715.1 million individuals watched the final match of the tournament, almost a ninth of the entire population of the planet. The 2006 World Cup draw, which decided the distribution of teams into groups, was watched by 300 million viewers. The World Cup attracts major sponsors such as Coca-Cola, McDonald's and Adidas. For these companies and many more, being a sponsor strongly impacts their global brands. Host countries typically experience a multimillion-dollar revenue increase from the month-long event.\nThe governing body of the sport, FIFA, generated $4.8 billion in revenue from the 2014 tournament, and $6.1 billion from the 2018 tournament.\nEach FIFA World Cup since 1966 has its own mascot or logo. World Cup Willie, the mascot for the 1966 competition, was the first World Cup mascot. World Cups feature official match balls specially designed for each tournament. After Slazenger produced the ball for the 1966 World Cup Adidas became the official supplier to FIFA. Each World Cup also has an official song, which have been performed by artists ranging from Shakira to Will Smith. Other songs, such as \u201cNessun dorma\u201d, performed by The Three Tenors at four World Cup concerts, have also become identified with the tournament.\nForming a partnership with FIFA in 1970, Panini published its first sticker album for the 1970 World Cup. Since then, collecting and trading stickers and cards has become part of the World Cup experience, especially for the younger generation. FIFA has licensed World Cup video games since 1986, sponsored by Electronic Arts.\n\n\n== Results ==\n\nNotes\nIn all, 80 nations have played in at least one World Cup. Of these, eight national teams have won the World Cup, and they have added stars to their badges, with each star representing a World Cup victory. (Uruguay, however, choose to display four stars on their badge, representing their two gold medals at the 1924 and 1928 Summer Olympics, which are recognised by FIFA as World Championships, and their two World Cup titles in 1930 and 1950).\nWith five titles, Brazil are the most successful World Cup team and also the only nation to have played in every World Cup (22) to date. Brazil were also the first team to win the World Cup for the third (1970), fourth (1994) and fifth (2002) time. Italy (1934 and 1938) and Brazil (1958 and 1962) are the only nations to have won consecutive titles. West Germany (1982\u20131990) and Brazil (1994\u20132002) are the only nations to appear in three consecutive World Cup finals. Germany has made the most top-four finishes (13), medals (12), as well as the most finals (8).\n\n\n=== Teams reaching the top four ===\n\n\n=== Best performances by confederations ===\n\nTo date, the final of the World Cup has only been contested by teams from the UEFA (Europe) and CONMEBOL (South America) confederations. European nations have won twelve titles, while South American nations have won ten. Only three teams from outside these two continents have ever reached the semi-finals of the competition: United States (North, Central America and Caribbean) in 1930; South Korea (Asia) in 2002; and Morocco (Africa) in 2022. Only one Oceanian qualifier, Australia in 2006, has advanced to the second round, a feat they later reaccomplished in 2022.Brazil, Argentina, Spain and Germany are the only teams to win a World Cup hosted outside their continental confederation; Brazil came out victorious in Europe (1958), North America (1970 and 1994) and Asia (2002). Argentina won a World Cup in North America in 1986 and in Asia in 2022. Spain won in Africa in 2010. In 2014, Germany became the first European team to win in the Americas. Only on five occasions have consecutive World Cups been won by teams from the same continent; the longest streak of tournaments won by a single confederation is four, with the 2006, 2010, 2014, and 2018 tournaments all won by UEFA teams (Italy, Spain, Germany, and France, respectively).\n\n\n== Records and statistics ==\n\nFive players share the record for playing in the most World Cups; Mexico's Antonio Carbajal (1950\u20131966) and Rafael M\u00e1rquez (2002\u20132018); Germany's Lothar Matth\u00e4us (1982\u20131998); Argentina's Lionel Messi (2006\u20132022); and Portugal's Cristiano Ronaldo (2006\u20132022) all played in five tournaments with Ronaldo also being the first and only player to score in five tournaments. Messi has played the most World Cup matches overall, with 26 appearances. Brazil's Djalma Santos (1954\u20131962), West Germany's Franz Beckenbauer (1966\u20131974), and Germany's Philipp Lahm (2006\u20132014) are the only players to be named to three World Cup All-Star Teams.Miroslav Klose of Germany (2002\u20132014) is the all-time top scorer at the World Cup with 16 goals. He broke Ronaldo of Brazil's record of 15 goals (1998\u20132006) during the 2014 semi-final match against Brazil. West Germany's Gerd M\u00fcller (1970\u20131974) is third, with 14 goals. The fourth-placed goalscorer, France's Just Fontaine, holds the record for the most goals scored in a single World Cup; all his 13 goals were scored in the 1958 tournament.\nIn November 2007, FIFA announced that all members of World Cup-winning squads between 1930 and 1974 were to be retroactively awarded winners' medals. This made Brazil's Pel\u00e9 the only player to have won three World Cup winners' medals (1958, 1962, and 1970, although he did not play in the 1962 final due to injury), with 20 other players who have won two winners' medals. Seven players have collected all three types of World Cup medals (winners', runner- ups', and third-place); five players were from West Germany's squad of 1966\u20131974: Franz Beckenbauer, J\u00fcrgen Grabowski, Horst-Dieter H\u00f6ttges, Sepp Maier, and Wolfgang Overath (1966\u20131974), Italy's Franco Baresi (1982, 1990, 1994) and the most recent has been Miroslav Klose of Germany (2002\u20132014) with four consecutive medals.Brazil's M\u00e1rio Zagallo, West Germany's Franz Beckenbauer and France's Didier Deschamps are the only people to date to win the World Cup as both player and head coach. Zagallo won in 1958 and 1962 as a player and in 1970 as head coach. Beckenbauer won in 1974 as captain and in 1990 as head coach, and Deschamps repeated the feat in 2018, after having won in 1998 as captain. Italy's Vittorio Pozzo is the only head coach to ever win two World Cups (1934 and 1938). All World Cup-winning head coaches were natives of the country they coached to victory.Among the national teams, Brazil has played the most World Cup matches (114), Germany appeared in the most finals (8), semi-finals (13), and quarter-finals (16), while Brazil has appeared in the most World Cups (22), has the most wins (76) and has scored the most goals (237). The two teams have played each other twice in the World Cup, in the 2002 final and in the 2014 semi-final.\n\n\n=== Top goalscorers ===\n\nIndividualPlayers in bold are still active.\n\nCountry\n\n\n== Awards ==\n\nAt the end of each World Cup, awards are presented to the players and teams for accomplishments other than their final team positions in the tournament. \n\nThere are five post-tournament awards from the FIFA Technical Study Group:the Golden Ball (named for its sponsor \"Adidas Golden Ball\") for best player, first awarded in 1982;\nthe Golden Boot (named for its sponsor \"Adidas Golden Boot\", formerly known as the \"adidas Golden Shoe\" from 1982 to 2006) for top goalscorer, first awarded in 1982;\nthe Golden Glove (named for its sponsor \"Adidas Golden Glove\", formerly known as the \"Lev Yashin Award\" from 1994 to 2006) for best goalkeeper, first awarded in 1994;\nthe FIFA Young Player Award (formerly known as the \"Best Young Player Award\" from 2006 to 2010) for best player under 21 years of age at the start of the calendar year, first awarded in 2006;\nthe FIFA Fair Play Trophy for the team that advanced to the second round with the best record of fair play, first awarded in 1970.\nThere is currently one award voted on by fans during the tournament.:\nthe Player of the Match (currently commercially termed \"Budweiser Player of the Match\", formerly known as the \"Man of the Match\" from 2002 to 2018) for outstanding performance during each match of the tournament, first awarded in 2002.\nThere are two awards voted on by fans after the conclusion of the tournament:\nthe Goal of the Tournament, (currently commercially termed \"Hyundai Goal of the Tournament\") for the fans' best goal scored during the tournament, first awarded in 2006;\nthe Most Entertaining Team during the World Cup final tournament, as determined by a poll of the general public.\nOne other award was given between 1994 and 2006:an All-Star Team comprising the best players of the tournament chosen by the FIFA Technical Study Group. From 2010 onwards, all Dream Teams or Statistical Teams are unofficial, as reported by FIFA itself.\n\n\n== See also ==\nList of FIFA World Cup finals\nFIFA World Cup records and statistics\nFIFA World Cup awards\nFIFA U-20 World Cup\nFIFA U-17 World Cup\nFIFA Club World Cup\nFIFA Beach Soccer World Cup\nFIFA Futsal World Cup\nFIFA Confederations Cup\nList of association football competitions\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== Cited works ==\n\n\n== External links ==\n\nOfficial website \nWorld Cup overview at the RSSSF"}, {"id": 50, "title": "Solar eclipse", "content": "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby obscuring the view of the Sun from a small part of the Earth, totally or partially. Such an alignment occurs approximately every six months, during the eclipse season in its new moon phase, when the Moon's orbital plane is closest to the plane of the Earth's orbit. In a total eclipse, the disk of the Sun is fully obscured by the Moon. In partial and annular eclipses, only part of the Sun is obscured. Unlike a lunar eclipse, which may be viewed from anywhere on the night side of Earth, a solar eclipse can only be viewed from a relatively small area of the world. As such, although total solar eclipses occur somewhere on Earth every 18 months on average, they recur at any given place only once every 360 to 410 years.\nIf the Moon were in a perfectly circular orbit and in the same orbital plane as Earth, there would be total solar eclipses once a month, at every new moon. Instead, because the Moon's orbit is tilted at about 5 degrees to Earth's orbit, its shadow usually misses Earth. Solar (and lunar) eclipses therefore happen only during eclipse seasons, resulting in at least two, and up to five, solar eclipses each year, no more than two of which can be total. Total eclipses are more rare because they require a more precise alignment between the centers of the Sun and Moon, and because the Moon's apparent size in the sky is sometimes too small to fully cover the Sun.\nAn eclipse is a natural phenomenon. In some ancient and modern cultures, solar eclipses were attributed to supernatural causes or regarded as bad omens. Astronomers' predictions of eclipses began in China as early as the 4th century BC; eclipses hundreds of years into the future may now be predicted with high accuracy. \nLooking directly at the Sun can lead to permanent eye damage, so special eye protection or indirect viewing techniques are used when viewing a solar eclipse. Only the total phase of a total solar eclipse is safe to view without protection. Enthusiasts known as eclipse chasers or umbraphiles travel to remote locations to see solar eclipses.\n\n\n== Types ==\nThere are four types of solar eclipses:\n\nA total eclipse occurs in average every 18 months when the dark silhouette of the Moon completely obscures the intensely bright light of the Sun, allowing the much fainter solar corona to be visible. During any one eclipse, totality occurs at best only in a narrow track on the surface of Earth. This narrow track is called the path of totality.\nAn annular eclipse occurs once every one or two years when the Sun and Moon are exactly in line with the Earth, but the apparent size of the Moon is smaller than that of the Sun. Hence the Sun appears as a very bright ring, or annulus, surrounding the dark disk of the Moon.\nA hybrid eclipse (also called annular/total eclipse) shifts between a total and annular eclipse. At certain points on the surface of Earth, it appears as a total eclipse, whereas at other points it appears as annular. Hybrid eclipses are comparatively rare.\nA partial eclipse occurs about twice a year, when the Sun and Moon are not exactly in line with the Earth and the Moon only partially obscures the Sun. This phenomenon can usually be seen from a large part of the Earth outside of the track of an annular or total eclipse. However, some eclipses can be seen only as a partial eclipse, because the umbra passes above the Earth's polar regions and never intersects the Earth's surface. Partial eclipses are virtually unnoticeable in terms of the Sun's brightness, as it takes well over 90% coverage to notice any darkening at all. Even at 99%, it would be no darker than civil twilight.The Sun's distance from Earth is about 400 times the Moon's distance, and the Sun's diameter is about 400 times the Moon's diameter. Because these ratios are approximately the same, the Sun and the Moon as seen from Earth appear to be approximately the same size: about 0.5 degree of arc in angular measure.The Moon's orbit around the Earth is slightly elliptical, as is the Earth's orbit around the Sun. The apparent sizes of the Sun and Moon therefore vary. The magnitude of an eclipse is the ratio of the apparent size of the Moon to the apparent size of the Sun during an eclipse. An eclipse that occurs when the Moon is near its closest distance to Earth (i.e., near its perigee) can be a total eclipse because the Moon will appear to be large enough to completely cover the Sun's bright disk or photosphere; a total eclipse has a magnitude greater than or equal to 1.000. Conversely, an eclipse that occurs when the Moon is near its farthest distance from Earth (i.e., near its apogee) can be only an annular eclipse because the Moon will appear to be slightly smaller than the Sun; the magnitude of an annular eclipse is less than 1.A hybrid eclipse occurs when the magnitude of an eclipse changes during the event from less to greater than one, so the eclipse appears to be total at locations nearer the midpoint, and annular at other locations nearer the beginning and end, since the sides of the Earth are slightly further away from the Moon.  These eclipses are extremely narrow in their path width and relatively short in their duration at any point compared with fully total eclipses; the 2023 April 20 hybrid eclipse's totality is over a minute in duration at various points along the path of totality.  Like a focal point, the width and duration of totality and annularity are near zero at the points where the changes between the two occur.Because the Earth's orbit around the Sun is also elliptical, the Earth's distance from the Sun similarly varies throughout the year. This affects the apparent size of the Sun in the same way, but not as much as does the Moon's varying distance from Earth. When Earth approaches its farthest distance from the Sun in early July, a total eclipse is somewhat more likely, whereas conditions favour an annular eclipse when Earth approaches its closest distance to the Sun in early January.\n\n\n=== Terminology for central eclipse ===\n\nCentral eclipse is often used as a generic term for a total, annular, or hybrid eclipse. This is, however, not completely correct: the definition of a central eclipse is an eclipse during which the central line of the umbra touches the Earth's surface. It is possible, though extremely rare, that part of the umbra intersects with the Earth (thus creating an annular or total eclipse), but not its central line. This is then called a non-central total or annular eclipse. Gamma is a measure of how centrally the shadow strikes. The last (umbral yet) non-central solar eclipse was on April 29, 2014. This was an annular eclipse. The next non-central total solar eclipse will be on April 9, 2043.The visual phases observed during a total eclipse are called:\nFirst contact\u2014when the Moon's limb (edge) is exactly tangential to the Sun's limb.\nSecond contact\u2014starting with Baily's Beads (caused by light shining through valleys on the Moon's surface) and the diamond ring effect. Almost the entire disk is covered.\nTotality\u2014the Moon obscures the entire disk of the Sun and only the solar corona is visible.\nThird contact\u2014when the first bright light becomes visible and the Moon's shadow is moving away from the observer. Again a diamond ring may be observed.\nFourth contact\u2014when the trailing edge of the Moon ceases to overlap with the solar disk and the eclipse ends.\n\n\n== Predictions ==\n\n\n=== Geometry ===\nThe diagrams to the right show the alignment of the Sun, Moon, and Earth during a solar eclipse. The dark gray region between the Moon and Earth is the umbra, where the Sun is completely obscured by the Moon. The small area where the umbra touches Earth's surface is where a total eclipse can be seen. The larger light gray area is the penumbra, in which a partial eclipse can be seen. An observer in the antumbra, the area of shadow beyond the umbra, will see an annular eclipse.The Moon's orbit around the Earth is inclined at an angle of just over 5 degrees to the plane of the Earth's orbit around the Sun (the ecliptic). Because of this, at the time of a new moon, the Moon will usually pass to the north or south of the Sun. A solar eclipse can occur only when a new moon occurs close to one of the points (known as nodes) where the Moon's orbit crosses the ecliptic.As noted above, the Moon's orbit is also elliptical. The Moon's distance from the Earth can vary by about 6% from its average value. Therefore, the Moon's apparent size varies with its distance from the Earth, and it is this effect that leads to the difference between total and annular eclipses. The distance of the Earth from the Sun also varies during the year, but this is a smaller effect. On average, the Moon appears to be slightly smaller than the Sun as seen from the Earth, so the majority (about 60%) of central eclipses are annular. It is only when the Moon is closer to the Earth than average (near its perigee) that a total eclipse occurs.\nThe Moon orbits the Earth in approximately 27.3 days, relative to a fixed frame of reference. This is known as the sidereal month. However, during one sidereal month, Earth has revolved part way around the Sun, making the average time between one new moon and the next longer than the sidereal month: it is approximately 29.5 days. This is known as the synodic month and corresponds to what is commonly called the lunar month.The Moon crosses from south to north of the ecliptic at its ascending node, and vice versa at its descending node. However, the nodes of the Moon's orbit are gradually moving in a retrograde motion, due to the action of the Sun's gravity on the Moon's motion, and they make a complete circuit every 18.6 years. This regression means that the time between each passage of the Moon through the ascending node is slightly shorter than the sidereal month. This period is called the nodical or draconic month.Finally, the Moon's perigee is moving forwards or precessing in its orbit and makes a complete circuit in 8.85 years. The time between one perigee and the next is slightly longer than the sidereal month and known as the anomalistic month.The Moon's orbit intersects with the ecliptic at the two nodes that are 180 degrees apart. Therefore, the new moon occurs close to the nodes at two periods of the year approximately six months (173.3 days) apart, known as eclipse seasons, and there will always be at least one solar eclipse during these periods. Sometimes the new moon occurs close enough to a node during two consecutive months to eclipse the Sun on both occasions in two partial eclipses. This means that, in any given year, there will always be at least two solar eclipses, and there can be as many as five.Eclipses can occur only when the Sun is within about 15 to 18 degrees of a node, (10 to 12 degrees for central eclipses). This is referred to as an eclipse limit, and is given in ranges because the apparent sizes and speeds of the Sun and Moon vary throughout the year. In the time it takes for the Moon to return to a node (draconic month), the apparent position of the Sun has moved about 29 degrees, relative to the nodes. Since the eclipse limit creates a window of opportunity of up to 36 degrees (24 degrees for central eclipses), it is possible for partial eclipses (or rarely a partial and a central eclipse) to occur in consecutive months.\n\n\n=== Path ===\nDuring a central eclipse, the Moon's umbra (or antumbra, in the case of an annular eclipse) moves rapidly from west to east across the Earth. The Earth is also rotating from west to east, at about 28 km/min at the Equator, but as the Moon is moving in the same direction as the Earth's rotation at about 61 km/min, the umbra almost always appears to move in a roughly west\u2013east direction across a map of the Earth at the speed of the Moon's orbital velocity minus the Earth's rotational velocity.The width of the track of a central eclipse varies according to the relative apparent diameters of the Sun and Moon. In the most favourable circumstances, when a total eclipse occurs very close to perigee, the track can be up to 267 km (166 mi) wide and the duration of totality may be over 7 minutes. Outside of the central track, a partial eclipse is seen over a much larger area of the Earth. Typically, the umbra is 100\u2013160 km wide, while the penumbral diameter is in excess of 6400 km.Besselian elements are used to predict whether an eclipse will be partial, annular, or total (or annular/total), and what the eclipse circumstances will be at any given location.:\u200aChapter 11\u200aCalculations with Besselian elements can determine the exact shape of the umbra's shadow on the Earth's surface. But at what longitudes on the Earth's surface the shadow will fall, is a function of the Earth's rotation, and on how much that rotation has slowed down over time. A number called \u0394T is used in eclipse prediction to take this slowing into account. As the Earth slows, \u0394T increases. \u0394T for dates in the future can only be roughly estimated because the Earth's rotation is slowing irregularly. This means that, although it is possible to predict that there will be a total eclipse on a certain date in the far future, it is not possible to predict in the far future exactly at what longitudes that eclipse will be total. Historical records of eclipses allow estimates of past values of \u0394T and so of the Earth's rotation.\n:\u200aEquation 11.132\u200a\n\n\n=== Duration ===\nThe following factors determine the duration of a total solar eclipse (in order of decreasing importance):\nThe Moon being almost exactly at perigee (making its angular diameter as large as possible).\nThe Earth being very near aphelion (furthest away from the Sun in its elliptical orbit, making its angular diameter nearly as small as possible).\nThe midpoint of the eclipse being very close to the Earth's equator, where the rotational velocity is greatest and is closest to the speed of the lunar shadow moving over Earth's surface.\nThe vector of the eclipse path at the midpoint of the eclipse aligning with the vector of the Earth's rotation (i.e. not diagonal but due east).\nThe midpoint of the eclipse being near the subsolar point (the part of the Earth closest to the Sun).The longest eclipse that has been calculated thus far is the eclipse of July 16, 2186 (with a maximum duration of 7 minutes 29 seconds over northern Guyana).\n\n\n== Occurrence and cycles ==\n\nTotal solar eclipses are rare events. Although they occur somewhere on Earth every 18 months on average, it is estimated that they recur at any given place only once every 360 to 410 years, on average. The total eclipse lasts for only a maximum of a few minutes at any location, because the Moon's umbra moves eastward at over 1700 km/h. Totality currently can never last more than 7 min 32 s. This value changes over the millennia and is currently decreasing. By the 8th millennium, the longest theoretically possible total eclipse will be less than 7 min 2 s. The last time an eclipse longer than 7 minutes occurred was June 30, 1973 (7 min 3 sec). Observers aboard a Concorde supersonic aircraft were able to stretch totality for this eclipse to about 74 minutes by flying along the path of the Moon's umbra. The next total eclipse exceeding seven minutes in duration will not occur until June 25, 2150. The longest total solar eclipse during the 11,000 year period from 3000 BC to at least 8000 AD will occur on July 16, 2186, when totality will last 7 min 29 s. For comparison, the longest total eclipse of the 20th century at 7 min 8 s occurred on June 20, 1955, and there will be no total solar eclipses over 7 min in duration in the 21st century.It is possible to predict other eclipses using eclipse cycles. The saros is probably the best known and one of the most accurate. A saros lasts 6,585.3 days (a little over 18 years), which means that, after this period, a practically identical eclipse will occur. The most notable difference will be a westward shift of about 120\u00b0 in longitude (due to the 0.3 days) and a little in latitude (north-south for odd-numbered cycles, the reverse for even-numbered ones). A saros series always starts with a partial eclipse near one of Earth's polar regions, then shifts over the globe through a series of annular or total eclipses, and ends with a partial eclipse at the opposite polar region. A saros series lasts 1226 to 1550 years and 69 to 87 eclipses, with about 40 to 60 of them being central.\n\n\n=== Frequency per year ===\nBetween two and five solar eclipses occur every year, with at least one per eclipse season. Since the Gregorian calendar was instituted in 1582, years that have had five solar eclipses were 1693, 1758, 1805, 1823, 1870, and 1935. The next occurrence will be 2206. On average, there are about 240 solar eclipses each century.\n\n\n=== Final totality ===\nTotal solar eclipses are seen on Earth because of a fortuitous combination of circumstances. Even on Earth, the diversity of eclipses familiar to people today is a temporary (on a geological time scale) phenomenon. Hundreds of millions of years in the past, the Moon was closer to the Earth and therefore apparently larger, so every solar eclipse was total or partial, and there were no annular eclipses. Due to tidal acceleration, the orbit of the Moon around the Earth becomes approximately 3.8 cm more distant each year. Millions of years in the future, the Moon will be too far away to fully occlude the Sun, and no total eclipses will occur. In the same timeframe, the Sun may become brighter, making it appear larger in size. Estimates of the time when the Moon will be unable to occlude the entire Sun when viewed from the Earth range between 650 million and 1.4 billion years in the future.\n\n\n== Historical eclipses ==\nHistorical eclipses are a very valuable resource for historians, in that they allow a few historical events to be dated precisely, from which other dates and ancient calendars may be deduced. A solar eclipse of June 15, 763 BC mentioned in an Assyrian text is important for the chronology of the ancient Near East. There have been other claims to date earlier eclipses. The legendary Chinese king Zhong Kang supposedly beheaded two astronomers, Hsi and Ho, who failed to predict an eclipse 4,000 years ago. Perhaps the earliest still-unproven claim is that of archaeologist Bruce Masse, who putatively links an eclipse that occurred on May 10, 2807, BC with a possible meteor impact in the Indian Ocean on the basis of several ancient flood myths that mention a total solar eclipse. The earliest preserved depiction of a partial solar eclipse from 1143 BCE might be the one in tomb KV9 of Ramses V and Ramses VI.\nEclipses have been interpreted as omens, or portents. The ancient Greek historian Herodotus wrote that Thales of Miletus predicted an eclipse that occurred during a battle between the Medes and the Lydians. Both sides put down their weapons and declared peace as a result of the eclipse. The exact eclipse involved remains uncertain, although the issue has been studied by hundreds of ancient and modern authorities. One likely candidate took place on May 28, 585 BC, probably near the Halys river in Asia Minor. An eclipse recorded by Herodotus before Xerxes departed for his expedition against Greece, which is traditionally dated to 480 BC, was matched by John Russell Hind to an annular eclipse of the Sun at Sardis on February 17, 478 BC. Alternatively, a partial eclipse was visible from Persia on October 2, 480 BC. Herodotus also reports a solar eclipse at Sparta during the Second Persian invasion of Greece. The date of the eclipse (August 1, 477 BC) does not match exactly the conventional dates for the invasion accepted by historians.Chinese records of eclipses begin at around 720 BC. The 4th century BC astronomer Shi Shen described the prediction of eclipses by using the relative positions of the Moon and Sun.\nAttempts have been made to establish the exact date of Good Friday by assuming that the darkness described at Jesus's crucifixion was a solar eclipse. This research has not yielded conclusive results, and Good Friday is recorded as being at Passover, which is held at the time of a full moon. Further, the darkness lasted from the sixth hour to the ninth, or three hours, which is much, much longer than the eight-minute upper limit for any solar eclipse's totality. Contemporary chronicles wrote about an eclipse at the beginning of May 664 that coincided with the beginning of the plague of 664 in the British isles. In the Western hemisphere, there are few reliable records of eclipses before AD 800, until the advent of Arab and monastic observations in the early medieval period. The Cairo astronomer Ibn Yunus wrote that the calculation of eclipses was one of the many things that connect astronomy with the Islamic law, because it allowed knowing when a special prayer can be made. The first recorded observation of the corona was made in Constantinople in AD 968.The first known telescopic observation of a total solar eclipse was made in France in 1706. Nine years later, English astronomer Edmund Halley accurately predicted and observed the solar eclipse of May 3, 1715. By the mid-19th century, scientific understanding of the Sun was improving through observations of the Sun's corona during solar eclipses. The corona was identified as part of the Sun's atmosphere in 1842, and the first photograph (or daguerreotype) of a total eclipse was taken of the solar eclipse of July 28, 1851. Spectroscope observations were made of the solar eclipse of August 18, 1868, which helped to determine the chemical composition of the Sun.John Fiske summed up myths about the solar eclipse like this in his 1872 book Myth and Myth-Makers,  the myth of Hercules and Cacus, the fundamental idea is the victory of the solar god over the robber who steals the light. Now whether the robber carries off the light in the evening when Indra has gone to sleep, or boldly rears his black form against the sky during the daytime, causing darkness to spread over the earth, would make little difference to the framers of the myth. To a chicken a solar eclipse is the same thing as nightfall, and he goes to roost accordingly. Why, then, should the primitive thinker have made a distinction between the darkening of the sky caused by black clouds and that caused by the rotation of the earth? He had no more conception of the scientific explanation of these phenomena than the chicken has of the scientific explanation of an eclipse. For him it was enough to know that the solar radiance was stolen, in the one case as in the other, and to suspect that the same demon was to blame for both robberies.\n\n\n== Viewing ==\nLooking directly at the photosphere of the Sun (the bright disk of the Sun itself), even for just a few seconds, can cause permanent damage to the retina of the eye, because of the intense visible and invisible radiation that the photosphere emits. This damage can result in impairment of vision, up to and including blindness. The retina has no sensitivity to pain, and the effects of retinal damage may not appear for hours, so there is no warning that injury is occurring.Under normal conditions, the Sun is so bright that it is difficult to stare at it directly. However, during an eclipse, with so much of the Sun covered, it is easier and more tempting to stare at it. Looking at the Sun during an eclipse is as dangerous as looking at it outside an eclipse, except during the brief period of totality, when the Sun's disk is completely covered (totality occurs only during a total eclipse and only very briefly; it does not occur during a partial or annular eclipse). Viewing the Sun's disk through any kind of optical aid (binoculars, a telescope, or even an optical camera viewfinder) is extremely hazardous and can cause irreversible eye damage within a fraction of a second.\n\n\n=== Partial and annular eclipses ===\n\nViewing the Sun during partial and annular eclipses (and during total eclipses outside the brief period of totality) requires special eye protection, or indirect viewing methods if eye damage is to be avoided. The Sun's disk can be viewed using appropriate filtration to block the harmful part of the Sun's radiation. Sunglasses do not make viewing the Sun safe. Only properly designed and certified solar filters should be used for direct viewing of the Sun's disk. Especially, self-made filters using common objects such as a floppy disk removed from its case, a Compact Disc, a black colour slide film, smoked glass, etc. must be avoided.The safest way to view the Sun's disk is by indirect projection. This can be done by projecting an image of the disk onto a white piece of paper or card using a pair of binoculars (with one of the lenses covered), a telescope, or another piece of cardboard with a small hole in it (about 1 mm diameter), often called a pinhole camera. The projected image of the Sun can then be safely viewed; this technique can be used to observe sunspots, as well as eclipses. Care must be taken, however, to ensure that no one looks through the projector (telescope, pinhole, etc.) directly. A kitchen colander with small holes can also be used to project multiple images of the partially eclipsed Sun onto the ground or a viewing screen. Viewing the Sun's disk on a video display screen (provided by a video camera or digital camera) is safe, although the camera itself may be damaged by direct exposure to the Sun. The optical viewfinders provided with some video and digital cameras are not safe. Securely mounting #14 welder's glass in front of the lens and viewfinder protects the equipment and makes viewing possible. Professional workmanship is essential because of the dire consequences any gaps or detaching mountings will have. In the partial eclipse path, one will not be able to see the corona or nearly complete darkening of the sky. However, depending on how much of the Sun's disk is obscured, some darkening may be noticeable. If three-quarters or more of the Sun is obscured, then an effect can be observed by which the daylight appears to be dim, as if the sky were overcast, yet objects still cast sharp shadows.\n\n\n=== Totality ===\n\nWhen the shrinking visible part of the photosphere becomes very small, Baily's beads will occur. These are caused by the sunlight still being able to reach the Earth through lunar valleys. Totality then begins with the diamond ring effect, the last bright flash of sunlight.It is safe to observe the total phase of a solar eclipse directly only when the Sun's photosphere is completely covered by the Moon, and not before or after totality. During this period, the Sun is too dim to be seen through filters. The Sun's faint corona will be visible, and the chromosphere, solar prominences, and possibly even a solar flare may be seen. At the end of totality, the same effects will occur in reverse order, and on the opposite side of the Moon.\n\n\n=== Eclipse chasing ===\n\nA dedicated group of eclipse chasers have pursued the observation of solar eclipses when they occur around the Earth. A person who chases eclipses is known as an umbraphile, meaning shadow lover. Umbraphiles travel for eclipses and use various tools to help view the sun including solar viewing glasses, also known as eclipse glasses, as well as telescopes.\n\n\n=== Photography ===\nPhotographing an eclipse is possible with fairly common camera equipment. In order for the disk of the Sun/Moon to be easily visible, a fairly high magnification long focus lens is needed (at least 200 mm for a 35 mm camera), and for the disk to fill most of the frame, a longer lens is needed (over 500 mm). As with viewing the Sun directly, looking at it through the optical viewfinder of a camera can produce damage to the retina, so care is recommended. Solar filters are required for digital photography even if an optical viewfinder is not used. Using a camera's live view feature or an electronic viewfinder is safe for the human eye, but the Sun's rays could potentially irreparably damage digital image sensors unless the lens is covered by a properly designed solar filter.\n\n\n== Other observations ==\nA total solar eclipse provides a rare opportunity to observe the corona (the outer layer of the Sun's atmosphere). Normally this is not visible because the photosphere is much brighter than the corona. According to the point reached in the solar cycle, the corona may appear small and symmetric, or large and fuzzy. It is very hard to predict this in advance.\nAs the light filters through leaves of trees during a partial eclipse, the overlapping leaves create natural pinholes, displaying mini eclipses on the ground.Phenomena associated with eclipses include shadow bands (also known as flying shadows), which are similar to shadows on the bottom of a swimming pool. They occur only just prior to and after totality, when a narrow solar crescent acts as an anisotropic light source.\n\n\n=== 1919 observations ===\n\nThe observation of a total solar eclipse of May 29, 1919, helped to confirm Einstein's theory of general relativity. By comparing the apparent distance between stars in the constellation Taurus, with and without the Sun between them, Arthur Eddington stated that the theoretical predictions about gravitational lenses were confirmed. The observation with the Sun between the stars was possible only during totality since the stars are then visible. Though Eddington's observations were near the experimental limits of accuracy at the time, work in the later half of the 20th century confirmed his results.\n\n\n=== Gravity anomalies ===\nThere is a long history of observations of gravity-related phenomena during solar eclipses, especially during the period of totality. In 1954, and again in 1959, Maurice Allais reported observations of strange and unexplained movement during solar eclipses. The reality of this phenomenon, named the Allais effect, has remained controversial. Similarly, in 1970, Saxl and Allen observed the sudden change in motion of a torsion pendulum; this phenomenon is called the Saxl effect.Observation during the 1997 solar eclipse by Wang et al. suggested a possible gravitational shielding effect, which generated debate. In 2002, Wang and a collaborator published detailed data analysis, which suggested that the phenomenon still remains unexplained.\n\n\n=== Eclipses and transits ===\nIn principle, the simultaneous occurrence of a solar eclipse and a transit of a planet is possible. But these events are extremely rare because of their short durations. The next anticipated simultaneous occurrence of a solar eclipse and a transit of Mercury will be on July 5, 6757, and a solar eclipse and a transit of Venus is expected on April 5, 15232.More common, but still infrequent, is a conjunction of a planet (especially, but not only, Mercury or Venus) at the time of a total solar eclipse, in which event the planet will be visible very near the eclipsed Sun, when without the eclipse it would have been lost in the Sun's glare. At one time, some scientists hypothesized that there may be a planet (often given the name Vulcan) even closer to the Sun than Mercury; the only way to confirm its existence would have been to observe it in transit or during a total solar eclipse. No such planet was ever found, and general relativity has since explained the observations that led astronomers to suggest that Vulcan might exist.\n\n\n=== Artificial satellites ===\nArtificial satellites can also pass in front of the Sun as seen from the Earth, but none is large enough to cause an eclipse. At the altitude of the International Space Station, for example, an object would need to be about 3.35 km (2.08 mi) across to blot the Sun out entirely. These transits are difficult to watch because the zone of visibility is very small. The satellite passes over the face of the Sun in about a second, typically. As with a transit of a planet, it will not get dark.Observations of eclipses from spacecraft or artificial satellites orbiting above the Earth's atmosphere are not subject to weather conditions. The crew of Gemini 12 observed a total solar eclipse from space in 1966. The partial phase of the 1999 total eclipse was visible from Mir.\n\n\n=== Impact ===\nThe solar eclipse of March 20, 2015, was the first occurrence of an eclipse estimated to potentially have a significant impact on the power system, with the electricity sector taking measures to mitigate any impact. The continental Europe and Great Britain synchronous areas were estimated to have about 90 gigawatts of solar power and it was estimated that production would temporarily decrease by up to 34 GW compared to a clear sky day.Eclipses may cause the temperature to decrease by 3 \u00b0C, with wind power potentially decreasing as winds are reduced by 0.7 m/s.In addition to the drop in light level and air temperature, animals change their behavior during totality. For example, birds and squirrels return to their nests and crickets chirp.\n\n\n== Recent and forthcoming solar eclipses ==\n\nEclipses occur only in the eclipse season, when the Sun is close to either the ascending or descending node of the Moon. Each eclipse is separated by one, five or six lunations (synodic months), and the midpoint of each season is separated by 173.3 days, which is the mean time for the Sun to travel from one node to the next. The period is a little less than half a calendar year because the lunar nodes slowly regress. Because 223 synodic months is roughly equal to 239 anomalistic months and 242 draconic months, eclipses with similar geometry recur 223 synodic months (about 6,585.3 days) apart. This period (18 years 11.3 days) is a saros. Because 223 synodic months is not identical to 239 anomalistic months or 242 draconic months, saros cycles do not endlessly repeat. Each cycle begins with the Moon's shadow crossing the Earth near the north or south pole, and subsequent events progress toward the other pole until the Moon's shadow misses the Earth and the series ends. Saros cycles are numbered; currently, cycles 117 to 156 are active.\n\n\n=== 1997\u20132000 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\nPartial solar eclipses on July 1, 2000 and December 25, 2000 occur in the next lunar year eclipse set.\n\n\n=== 2000\u20132003 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.Partial solar eclipses on February 5, 2000 and July 31, 2000 occur in the previous lunar year set.\n\n\n=== 2004\u20132007 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\n\n\n=== 2008\u20132011 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\nPartial solar eclipses on June 1, 2011, and November 25, 2011, occur on the next lunar year eclipse set.\n\n\n=== 2011\u20132014 ===\nThis eclipse is a member of the 2011\u20132014 solar eclipse semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\n\n\n=== 2015\u20132018 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\nPartial solar eclipses on July 13, 2018, and January 6, 2019, occur during the next semester series.\n\n\n=== 2018\u20132021 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.Note: Partial solar eclipses on February 15, 2018, and August 11, 2018, occurred during the previous semester series.\n\n\n=== 2022\u20132025 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\n\n\n=== 2026\u20132029 ===\nThis eclipse is a member of a semester series. An eclipse in a semester series of solar eclipses repeats approximately every 177 days and 4 hours (a semester) at alternating nodes of the Moon's orbit.\nPartial solar eclipses on June 12, 2029, and December 5, 2029, occur in the next lunar year eclipse set.\n\n\n== See also ==\nLists of solar eclipses\nList of films featuring eclipses\nApollo\u2013Soyuz: First joint U.S.\u2013Soviet space flight. Mission included an arranged eclipse of the Sun by the Apollo module to allow instruments on the Soyuz to take photographs of the solar corona.\nEclipse chasing: Travel to eclipse locations for study and enjoyment\nOccultation: Generic term for occlusion of an object by another object that passes between it and the observer, thus revealing (for example) the presence of an exoplanet orbiting a distant star by eclipsing it as seen from Earth\nSolar eclipses in fiction\nSolar eclipses on the Moon: Eclipse of the Sun by planet Earth, as seen from the Moon\nLunar eclipse: Solar eclipse of the Moon, as seen from Earth; the shadow cast on the Moon by that eclipse\nTransit of Venus: Passage of the planet Venus between the Sun and the Earth, as seen from Earth. Technically a partial eclipse.\nTransit of Deimos from Mars: Passage of the Martian moon Deimos between the Sun and Mars, as seen from Mars\nTransit of Phobos from Mars: Passage of the Martian moon Phobos between the Sun and Mars, as seen from Mars\n\n\n== Footnotes ==\n\n\n== Notes ==\n\n\n== References ==\nMucke, Hermann; Meeus, Jean (1992). Canon of Solar Eclipses \u22122003 to +2526 (2 ed.). Vienna: Astronomisches B\u00fcro.\nHarrington, Philip S. (1997). Eclipse! The What, Where, When, Why and How Guide to Watching Solar and Lunar Eclipses. New York: John Wiley and Sons. ISBN 0-471-12795-7.\nSteel, Duncan (1999). Eclipse: The celestial phenomenon which has changed the course of history. London: Headline. ISBN 0-7472-7385-5.\nMobberley, Martin (2007). Total Solar Eclipses and How to Observe Them. Astronomers' Observing Guides. New York: Springer. ISBN 978-0-387-69827-4.\nEspenak, Fred (2015). Thousand Year Canon of Solar Eclipses 1501 to 2500. Portal AZ: Astropixels Publishing. ISBN 978-1-941983-02-7.\nEspenak, Fred (2016). 21st Century Canon of Solar Eclipses. Portal AZ: Astropixels Publishing. ISBN 978-1-941983-12-6.\nFotheringham, John Knight (1921). Historical eclipses: being the Halley lecture delivered 17 May 1921. Oxford: Clarendon Press.\n\n\n== External links ==\n\nNASA Eclipse Web Site, with information on future eclipses and eye safety information\nNASA Eclipse Web Site (older version)\nEclipsewise, Fred Espenak's new eclipse site\nAndrew Lowe's Eclipse Page, with maps and circumstances for 5000 years of solar eclipses\nA Guide to Eclipse Activities for Educators, Explaining eclipses in educational settings\nDetailed eclipse explanations and predictions, Hermit Eclipse\nEclipse Photography, Prof. Miroslav Druckm\u00fcller\nAnimated maps of August 21, 2017 solar eclipses, Larry Koehn\nFive Millennium (\u22121999 to +3000) Canon of Solar Eclipses Database, Xavier M. Jubier\nAnimated explanation of the mechanics of a solar eclipse Archived 2013-05-25 at the Wayback Machine, University of South Wales\nEclipse Image Gallery Archived 2016-10-15 at the Wayback Machine, The World at Night\nRing of Fire Eclipse: 2012, Photos\n\"Sun, Eclipses of the\" . Collier's New Encyclopedia. 1921.\nCentered and aligned video recording of Total Solar Eclipse 20th March 2015 on YouTube\nSolar eclipse photographs taken from the Lick Observatory from the Lick Observatory Records Digital Archive, UC Santa Cruz Library\u2019s Digital Collections Archived 2020-06-05 at the Wayback Machine\nVideo with Total Solar Eclipse March 09 2016 (from the beginning to the total phase) on YouTube\nTotal Solar Eclipse Shadow on Earth March 09 2016 CIMSSSatelite\nList of all solar eclipses\nNational Geographic Solar Eclipse 101 video Archived 2018-08-04 at the Wayback Machine\n Wikiversity has a solar eclipse lab that students can do on any sunny day."}, {"id": 51, "title": "Social media", "content": "Social media are interactive technologies that facilitate the creation and sharing of content, ideas, interests, and other forms of expression through virtual communities and networks. Social media refers to new forms of media that involve interactive participation. While challenges to the definition of social media arise due to the variety of stand-alone and built-in social media services currently available, there are some common features:\nSocial media are interactive Web 2.0 Internet-based applications.\nUser-generated content\u2014such as text posts or comments, digital photos or videos, and data generated through all online interactions\u2014is the lifeblood of social media.\nUsers create service-specific profiles for the website or app that are designed and maintained by the social media organization.\nSocial media helps the development of online social networks by connecting a user's profile with those of other individuals or groups.The term social in regard to media suggests that platforms are user-centric and enable communal activity. As such, social media can be viewed as online facilitators or enhancers of human networks\u2014webs of individuals who enhance social connectivity.Users usually access social media services through web-based apps on desktops or download services that offer social media functionality to their mobile devices (e.g. smartphones and tablets). As users engage with these electronic services, they create highly interactive platforms in which individuals, communities, and organizations can share, co-create, discuss, participate, and modify user-generated or self-curated content posted online. Additionally, social media are used to document memories, learn about and explore things, advertise oneself, and form friendships along with the growth of ideas from the creation of blogs, podcasts, videos, and gaming sites. This changing relationship between humans and technology is the focus of the emerging field of technological self-studies. Some of the most popular social media websites, with more than 100 million registered users, include Twitter, Facebook (and its associated Messenger), WeChat, ShareChat, Instagram, QZone, Weibo, VK, Tumblr, Baidu Tieba, and LinkedIn. Depending on interpretation, other popular platforms that are sometimes referred to as social media services include YouTube, QQ, Quora, Telegram, WhatsApp, Signal, LINE, Snapchat, Pinterest, Viber, Reddit, Discord, TikTok, Microsoft Teams, and more. Wikis are examples of collaborative content creation.\nSocial media outlets differ from traditional media (e.g. print magazines and newspapers, TV, and radio broadcasting) in many ways, including quality, reach, frequency, usability, relevancy, and permanence. Additionally, social media outlets operate in a dialogic transmission system (i.e., many sources to many receivers) while traditional media outlets operate under a monologic transmission model (i.e., one source to many receivers). For instance, a newspaper is delivered to many subscribers, and a radio station broadcasts the same programs to an entire city.Since the dramatic expansion of the Internet, digital media or digital rhetoric can be used to represent or identify a culture. Studying the rhetoric that exists in the digital environment has become a crucial new process for many scholars.\nObservers have noted a wide range of positive and negative impacts when it comes to the use of social media. Social media can help to improve an individual's sense of connectedness with real or online communities and can be an effective communication (or marketing) tool for corporations, entrepreneurs, non-profit organizations, advocacy groups, political parties, and governments. Observers have also seen that there has been a rise in social movements using social media as a tool for communicating and organizing in times of political unrest.\nSocial media can also be used to read or share news, whether it is true or false.\n\n\n== History ==\n\n\n=== Early computing ===\n\nThe PLATO system was launched in 1960 after being developed at the University of Illinois and subsequently commercially marketed by Control Data Corporation. It offered early forms of social media features with 1973-era innovations such as Notes, PLATO's message-forum application; TERM-talk, its instant-messaging feature; Talkomatic, perhaps the first online chat room; News Report, a crowdsourced online newspaper, and blog and Access Lists, enabling the owner of a note file or other application to limit access to a certain set of users, for example, only friends, classmates, or co-workers.\nARPANET, which first came online in 1967, had by the late 1970s developed a rich cultural exchange of non-government/business ideas and communication, as evidenced by the network etiquette (or \"netiquette\") described in a 1982 handbook on computing at MIT's Artificial Intelligence Laboratory. ARPANET evolved into the Internet following the publication of the first Transmission Control Protocol (TCP) specification, RFC 675 (Specification of Internet Transmission Control Program), written by Vint Cerf, Yogen Dalal, and Carl Sunshine in 1974. This became the foundation of Usenet, conceived by Tom Truscott and Jim Ellis in 1979 at the University of North Carolina at Chapel Hill and Duke University, and established in 1980.\nA precursor of the electronic bulletin board system (BBS), known as Community Memory, appeared by 1973. True electronic BBSs arrived with the Computer Bulletin Board System in Chicago, which first came online on February 16, 1978. Before long, most major cities had more than one BBS running on TRS-80, Apple II, Atari, IBM PC, Commodore 64, Sinclair, and similar personal computers. The IBM PC was introduced in 1981, and subsequent models of both Mac computers and PCs were used throughout the 1980s. Multiple modems, followed by specialized telecommunication hardware, allowed many users to be online simultaneously. CompuServe, Prodigy, and AOL were three of the largest BBS companies and were the first to migrate to the Internet in the 1990s. Between the mid-1980s and the mid-1990s, BBSes numbered in the tens of thousands in North America alone. Message forums (a specific structure of social media) arose with the BBS phenomenon throughout the 1980s and early 1990s. When the World Wide Web (WWW, or \"the web\") was added to the Internet in the mid-1990s, message forums migrated to the web, becoming Internet forums, primarily due to cheaper per-person access as well as the ability to handle far more people simultaneously than telco modem banks.\nDigital imaging and semiconductor image sensor technology facilitated the development and rise of social media. Advances in metal\u2013oxide\u2013semiconductor (MOS) semiconductor device fabrication, reaching smaller micron and then sub-micron levels during the 1980s\u20131990s, led to the development of the NMOS (n-type MOS) active-pixel sensor (APS) at Olympus in 1985, and then the complementary MOS (CMOS) active-pixel sensor (CMOS sensor) at NASA's Jet Propulsion Laboratory (JPL) in 1993. CMOS sensors enabled the mass proliferation of digital cameras and camera phones, which bolstered the rise of social media.\n\n\n=== Development of social-media platforms ===\nIn 1991, when Tim Berners-Lee integrated hypertext software with the Internet, he created the World Wide Web, marking the beginning of the modern era of networked communication. This breakthrough facilitated the formation of online communities and enabled support for offline groups through the use of weblogs, list servers, and email services. The evolution of online services progressed from serving as channels for networked communication to becoming interactive platforms for networked social interaction with the advent of Web 2.0.Social media started in the mid-1990s with the advent of platforms like GeoCities, Classmates.com, and SixDegrees.com. While instant messaging and chat clients existed at the time, SixDegrees was unique as it was the first online service designed for real people to connect using their actual names. It boasted features like profiles, friends lists, and school affiliations, making it \"the very first social networking site\" according to CBS News. The platform's name was inspired by the \"six degrees of separation\" concept, which suggests that every person on the planet is just six connections away from everyone else.In the early 2000s, social media platforms gained widespread popularity with the likes of Friendster and Myspace, followed by Facebook, YouTube, and Twitter, among others.Research from 2015 shows that the world spent 22% of their online time on social networks, thus suggesting the popularity of social media platforms, likely fueled by the widespread adoption of smartphones. There are as many as 4.76 billion social media users in the world which, as of January 2023, equates to 59.4% of the total global population.\n\n\n== Definition and features ==\nThe idea that social media are defined simply by their ability to bring people together has been seen as too broad, as this would suggest that fundamentally different technologies like the telegraph and telephone are also social media. The terminology is unclear, with some early researchers referring to social media as social networks or social networking services in the mid-2000s. A more recent paper from 2015 reviewed the prominent literature in the area and identified four common features unique to then-current social media services:\nWeb 2.0 Internet-based applications.User-generated content\nUser-created self profiles\nSocial network formed by connections between profiles, such as followers or groupsIn 2019, Merriam-Webster defined social media as \"forms of electronic communication (such as websites for social networking and microblogging) through which users create online communities to share information, ideas, personal messages, and other content (such as videos).\"While the variety of evolving stand-alone and built-in social media services makes it challenging to define them, marketing and social media experts broadly agree that social media includes the following 13 types:\nBlogs (ex. HuffPost, Boing Boing)\nBusiness networks (ex. LinkedIn, XING)\nCollaborative projects (ex. Mozilla)\nEnterprise social networks (ex. Yammer, Socialcast)\nForums (ex. Gaia Online, IGN)\nMicroblogs (ex. Twitter, Tumblr)\nPhoto sharing (ex. Flickr, Photobucket)\nProducts/services review (ex. Amazon, Upwork)\nSocial bookmarking (ex. Delicious, Pinterest)\nSocial gaming (ex. Mafia Wars, World of Warcraft)\nSocial network sites (ex. Facebook, Google+)\nVideo sharing (ex. YouTube, Vimeo)\nVirtual worlds (ex. Second Life, Twinity)Some services of other social media subtypes (such as Twitter and YouTube) also allow users to create a social network, and so are sometimes also included in the social network subtype.\n\n\n=== Mobile social media ===\nMobile social media refers to the use of social media on mobile devices such as smartphones and tablet computers. Mobile social media are useful applications of mobile marketing because the creation, exchange, and circulation of user-generated content can assist companies with marketing research, communication, and relationship development. Mobile social media differ from others because they incorporate the current location of the user (location-sensitivity) or the time delay between sending and receiving messages.\nSocial media promotes users to share content with others and display content in order to enhance a particular brand or product. Social media allows people to be creative and share interesting ideas with their followers or fans. Certain social media applications such as Twitter, Facebook, and Instagram are places where users share specific political or sports content. Many reporters and journalists produce updates and information on sports and political news. It can truly give users pertinent and necessary information to stay up to date on relevant news stories and topics. However, there is a downside to it. Users are advised to exercise due diligence when they are using social media platforms.\nAccording to Andreas Kaplan, mobile social media applications can be differentiated among four types:\nSpace-timers (location and time-sensitive): Exchange of messages with relevance mostly for one specific location at one specific point in time (e.g. Facebook Places, WhatsApp, Telegram, Foursquare)\nSpace-locators (only location sensitive): Exchange of messages with relevance for one specific location, which is tagged to a certain place and read later by others (e.g. Yelp, Qype, Tumblr, Fishbrain)\nQuick-timers (only time sensitive): Transfer of traditional social media mobile apps to increase immediacy (e.g. posting on Twitter or status updates on Facebook)\nSlow-timers (neither location nor time sensitive): Transfer of traditional social media applications to mobile devices (e.g. watching a YouTube video)\n\n\n=== Elements and function ===\n\n\n==== Viral content ====\n\nSocial media sites are powerful tools for sharing content across networks. Certain content has the potential to spread virally, an analogy for the way viral infections spread from individual to individual. When content or websites go viral, users are more likely to share them with their social network, which leads to even more sharing.\nViral marketing campaigns are particularly attractive to businesses because they can achieve widespread advertising coverage at a fraction of the cost of traditional marketing campaigns. Nonprofit organizations and activists may also use social media to post content with the aim of it going viral.\nMany social media sites provide specific functionality to help users re-share content, such as Twitter's \"retweet\" button or Facebook's \"share\" option. This feature is especially popular on Twitter, allowing users to keep up with important events and stay connected with their peers. When certain posts become popular, they start to get retweeted over and over again, becoming viral. Hashtags can also be used in tweets to take count of how many people have used that hashtag.\nHowever, not all content has the potential to go viral, and it's difficult to predict what content will take off. Despite this, viral marketing campaigns can still be a cost-effective and powerful tool for promoting a message or product.\n\n\n==== Bots ====\n\nBots are automated programs that operate on the internet, which have become increasingly popular due to their ability to automate many communication tasks. This has led to the creation of a new industry of bot providers.Chatbots and social bots are programmed to mimic natural human interactions such as liking, commenting, following, and unfollowing on social media platforms. As companies aim for greater market shares and increased audiences, internet bots have also been developed to facilitate social media marketing. With the existence of social bots and chatbots, however, the marketing industry has also met an analytical crisis, as these bots make it difficult to differentiate between human interactions and automated bot interactions. For instance, marketing data has been negatively affected by some bots, causing \"digital cannibalism\" in social media marketing. Additionally, some bots violate the terms of use on many social media platforms such as Instagram, which can result in profiles being taken down and banned.'Cyborgs'\u2014either bot-assisted humans or human-assisted bots\u2014are used for a number of different purposes both legitimate and illegitimate, from spreading fake news to creating marketing buzz. A common legitimate use includes using automated programs to post on social media at a specific time. In these cases, often, the human writes the post content and the bot schedules the time of posting. In other cases, the cyborgs are more nefarious, e.g., contributing to the spread of fake news and misinformation. Often these accounts blend human and bot activity in a strategic way, so that when an automated account is publicly identified, the human half of the cyborg is able to take over and could protest that the account has been used manually all along. In many cases, these accounts that are being used in a more illegitimate fashion try to pose as real people; in particular, the number of their friends or followers resemble that of a real person. Cyborgs are also related to sock puppet accounts, where one human pretends to be someone else, but can also include one human operating multiple cyborg accounts.\n\n\n==== New social media technology ====\n\nThere has been rapid growth in the number of United States patent applications that cover new technologies that are related to social media, and the number of them that are published has been growing rapidly over the past five years. As of 2020, there are over 5000 published patent applications in the United States. As many as 7000 applications may be currently on file including those that have not been published yet; however, only slightly over 100 of these applications have issued as patents, largely due to the multi-year backlog in examination of business method patents, i.e., patents that outline and claim new methods of doing business.\n\n\n=== Platform convergence ===\n\nAs an instance of technological convergence, various social media platforms of different kinds adapted functionality beyond their original scope, increasingly overlapping with each other over time, albeit usually not implemented as completely as on dedicated platforms.\nExamples are the social hub site Facebook launching an integrated video platform in May 2007, and Instagram, whose original scope was low-resolution photo sharing, introducing the ability to share quarter-minute 640\u00d7640 pixel videos in 2013 (later extended to a minute with increased resolution), acting like a minimal video platform without video seek bar. Instagram later implemented stories (short videos self-destructing after 24 hours), a concept popularized by Snapchat, as well as IGTV, for seekable videos of up to ten minutes or one hour depending on account status. Stories have been later adapted by the dedicated video platform YouTube in 2018, although access is restricted to the mobile apps, excluding mobile and desktop websites.Twitter, whose original scope was text-based microblogging, later adapted photo sharing functionality (deprecating third-party services such as TwitPic), later video sharing with 140-second time limit and view counter but no manual quality selection or subtitles like on dedicated video platforms, and originally only available to mobile app users but later implemented in their website front ends. Then a media studio feature for business users, which resembles YouTube's Creator Studio.The discussion platform Reddit added an integrated image hoster in June 2016 after Reddit users commonly relied on the external standalone image sharing platform Imgur, and an internal video hosting service around a year later. In July 2020, the ability to share multiple images in a single post (image galleries), a feature known from Imgur, was implemented. Imgur itself implemented sharing videos of up to 30 seconds in May 2018, later extended to one minute.Starting in 2018, the dedicated video platform YouTube rolled out a Community feature accessible through a channel tab (which usurps the previous Discussion channel tab), where text-only posts, as well as polls can be shared. To be enabled, channels have to pass a subscriber count threshold which has been lowered over time.\n\n\n== Statistics on usage and membership ==\n\nAccording to Statista, it is estimated that, in 2022, there are around 3.96 billion people who are using social media around the globe. This number is up from 3.6 billion in 2020 and is expected to increase to 4.41 billion in 2025.\n\n\n=== Most popular social networking services ===\nThe following is a list of the most popular social networking services based on the number of active users as of January 2022 per Statista.\n\n\n=== Usage: Before the COVID-19 pandemic ===\nA study from 2009 suggests that there may be individual differences that help explain who uses social media and who does not: extraversion and openness have a positive relationship with social media, while emotional stability has a negative sloping relationship with social media. A separate study from 2015 found that people with a higher social comparison orientation appear to use social media more heavily than people with low social comparison orientation.Data from Common Sense Media has suggested that children under the age of 13 in the United States use social networking services despite the fact that many social media sites have policies that state one must be at least 13 years old or older to join. In 2017, Common Sense Media conducted a nationally representative survey of parents of children from birth to age 8 and found that 4% of children at this age used social media sites such as Instagram, Snapchat, or (now-defunct) Musical.ly \"often\" or \"sometimes\". A different nationally representative survey by Common Sense in 2019 surveyed young Americans ages 8\u201316 and found that about 31% of children ages 8\u201312 ever use social media such as Snapchat, Instagram, or Facebook. In that same survey, when American teens ages 16\u201318 were asked when they started using social media, 28% said they started to use it before they were 13 years old. However, the median age of starting to use social media was 14 years old.\n\n\n=== Usage: During the COVID-19 pandemic ===\n\n\n==== Amount of usage by minors ====\nSocial media plays a role in communication during COVID-19 pandemic. In June 2020, during the COVID-19 pandemic, a nationally representative survey by Cartoon Network and the Cyberbullying Research Center surveyed Americans tweens (ages 9\u201312) found that the most popular overall application in the past year was YouTube (67%). (In general, as age increased, the tweens were more likely to have used major social media apps and games.) Similarly, a nationally representative survey by Common Sense Media conducted in 2020 of Americans ages 13\u201318 found that YouTube was also the most popular social media service (used by 86% of 13- to 18-year-old Americans in the past year). As children grow older, they utilize certain social media services on a frequent basis and often use the application YouTube to consume content. The use of social media certainly increases as people grow older and it has become a customary thing to have an Instagram and Twitter account.\n\n\n==== Reasons for use by adults ====\nWhile adults were already using social media before the COVID-19 pandemic, more started using it to stay socially connected and to get updates on the pandemic. \"Social media have become popularly use to seek for medical information and have fascinated the general public to collect information regarding corona virus pandemics in various perspectives. During these days, people are forced to stay at home and the social media have connected and supported awareness and pandemic updates.\"This also made healthcare workers and systems more aware of social media as a place people were getting health information about the pandemic:\"During the COVID-19 pandemic, social media use has accelerated to the point of becoming a ubiquitous part of modern healthcare systems.\"Though this also led to the spread of disinformation, indeed, on December 11, 2020, the CDC put out a \"Call to Action: Managing the Infodemic\".\nSome healthcare organizations even used hashtags as interventions and published articles on their Twitter data: \"Promotion of the joint usage of #PedsICU and #COVID19 throughout the international pediatric critical care community in tweets relevant to the coronavirus disease 2019 pandemic and pediatric critical care.\" However others in the medical community were concerned about social media addiction, due to it as an increasingly important context and therefore \"source of social validation and reinforcement\" and are unsure if increased social media use is a coping mechanism or harmful.\n\n\n== Timeline of social media (1973\u20132023) ==\n\n\n== Use by organizations ==\n\n\n=== Governments ===\nGovernments may use social media to (for example):\ninform their opinions to public\ninteract with citizens\nfoster citizen participation\nfurther open government\nanalyze/monitor public opinion and activities\neducate the public about risks and public health.\n\n\n==== Law enforcement and investigations ====\nSocial media has been used extensively in civil and criminal investigations. It has also been used to assist in searches for missing persons. Police departments often make use of official social media accounts to engage with the public, publicize police activity, and burnish law enforcement's image; conversely, video footage of citizen-documented police brutality and other misconduct has sometimes been posted to social media.In the United States, U.S. Immigration and Customs Enforcement identifies and track individuals via social media, and also has apprehended some people via social media based sting operations. U.S. Customs and Border Protection (also known as CPB) and the United States Department of Homeland Security use social media data as influencing factors during the visa process, and continue to monitor individuals after they have entered the country. CPB officers have also been documented performing searches of electronics and social media behavior at the border, searching both citizens and non-citizens without first obtaining a warrant.\n\n\n==== Government reputation management ====\nAs social media gained momentum among the younger generations, governments began using it to improve their image, especially among the youth. In January 2021, Egyptian authorities were found to be using Instagram influencers as part of its media ambassadors program. The program was designed to revamp Egypt's image and to counter the bad press Egypt had received because of the country's human rights record. Saudi Arabia and the United Arab Emirates participated in similar programs. Similarly, Dubai has also extensively relied on social media and influencers to promote tourism. However, the restrictive laws of Dubai have always kept these influencers within the limits to not offend the authorities, or to criticize the city, politics or religion. The content of these foreign influencers is controlled to make sure that nothing portrays Dubai in a negative light.\n\n\n=== Businesses ===\n\nBusinesses can use social media tools for marketing research, communication, sales promotions/discounts, informal employee-learning/organizational development, relationship development/loyalty programs, and e-Commerce. Companies are increasingly using social-media monitoring tools to monitor, track, and analyze online conversations on the Web about their brand or products or about related topics of interest. This can prove useful in public relations management and advertising-campaign tracking, allowing analysts to measure return on investment for their social media ad spending, competitor-auditing, and for public engagement. Tools range from free, basic applications to subscription-based, more in-depth tools. Often social media can become a good source of information and explanation of industry trends for a business to embrace change. Within the financial industry, companies can utilize the power of social media as a tool for analyzing the sentiment of financial markets. These range from the marketing of financial products, gaining insights into market sentiment, future market predictions, and as a tool to identify insider trading.To properly take advantage of these benefits, businesses need to have a set of guidelines that they can use on different social media platforms. Social media can enhance a brand through a process called \"building social authority\". However, this process can be difficult, because one of the foundational concepts in social media is that one cannot completely control one's message through social media but rather one can simply begin to participate in the \"conversation\" expecting that one can achieve a significant influence in that conversation. Because of the wide use of social media by consumers and their own employees, companies use social media on a customer-organizational level; and an intra-organizational level. Social media, by connecting individuals to new ties via the social network can increase entrepreneurship and innovation, especially for those individuals who lack conventional information channels due to their lower socioeconomic background.\n\n\n==== Social media marketing ====\n\nSocial media marketing is the use of social media platforms and websites to promote a product or service and also to establish a connection with its customers. Social media marketing has increased due to the growing active user rates on social media sites. Though these numbers are not exponential. For example, as of 2018 Facebook had 2.2 billion users, Twitter had 330 million active users and Instagram had 800 million users. Then in 2021 Facebook had 2.89 billion users and Twitter had 206 million users. Similar to traditional advertising, all of social media marketing can be divided into three types: (1) paid media, (2) earned media, and (3) owned media. Paid social media is when a firm directly buys advertising on a social media platform. Earned social media is when the firms does something that impresses its consumers or other stakeholders and they spontaneously post their own content about it on social media. Owned social media is when the firm itself owns the social media channel and creates content for its followers.One of the main uses of social media marketing is to create brand awareness of a company or organization, creating a customer engagement by directly interacting with customers (e.g., customers can provide feedback on the firm's products) and providing support for customer service. However, since social media allows consumers to spread opinions and share experiences in a peer-to-peer fashion, this has shifted some of the power from the organization to consumers, since these messages can be transparent and honest and the company can not control the content of the messages posted by consumers.Social media personalities, often referred to as \"influencers\", are internet celebrities who have been employed or sponsored by marketers to promote products online. Research shows that digital endorsements seem to be successfully attracting social media users, especially younger consumers who have grown up in the digital age. In 2013, the United Kingdom Advertising Standards Authority (ASA) began to advise celebrities and sports stars to make it clear if they had been paid to tweet about a product or service by using the hashtag #spon or #ad in tweets containing endorsements, and the US Federal Trade Commission has issued similar guidelines. The practice of harnessing social media personalities to market or promote a product or service to their following is commonly referred to as influencer marketing.\nSocial media can also be used to directly advertise. Placing an advertisement on Facebook's Newsfeed, for example, can provide exposure of the brand to a large number of people. Social media platforms also enable targeting specific audiences with advertising. Users of social media are then able to like, share, and comment on the advertisement; this turns the passive advertising consumers into active advertising producers since they can pass the advertisement's message on to their friends. Companies using social media marketing have to keep up with the different social media platforms and stay on top of ongoing trends. Since the different platforms and trends attract different audiences, firms must be strategic about their use of social media to attract the right audience. Moreover, the tone of the content can affect the efficacy of social media marketing. Companies such as fast food franchise Wendy's have used humor (such as shitposting) to advertise their products by poking fun at competitors such as McDonald's and Burger King. This particular example spawned a lot of fanart of the Wendy's mascot which circulated widely online, (particularly on sites like DeviantArt) increasing the effect of the marketing campaign. Other companies such as Juul have used hashtags (such as #ejuice and #eliquid) to promote themselves and their products.Marketing efforts can also take advantage of the peer effects in social media. Consumers tend to treat content on social media differently from traditional advertising (such as print ads), but these messages may be part of an interactive marketing strategy involving modeling, reinforcement, and social interaction mechanisms. A 2012 study focused on this communication described how communication between peers through social media can affect purchase intentions: a direct impact through conformity, and an indirect impact by stressing product engagement. This study indicated that social media communication between peers about a product had a positive relationship with product engagement.\n\n\n=== Politics ===\n\nSocial media have a range of uses in political processes and activities. Social media have been championed as allowing anyone with access to an Internet connection to become a content creator and as empowering users. The role of social media in democratizing media participation, which proponents herald as ushering in a new era of participatory democracy, with all users able to contribute news and comments, may fall short of the ideals, given that many often follow like-minded individuals, as noted by Philip Pond and Jeff Lewis. Online-media audience-members are largely passive consumers, while content creation is dominated by a small number of users who post comments and write new content.:\u200a78\u200a Online engagement does not always translate into real-world action, and Howard, Busch and Sheets have argued that there is a digital divide in North America because of the continent's history, culture, and geography.Younger generations are becoming more involved in politics due to the increase of political news posted on social media. Political campaigns are targeting millennials online via social-media posts in hope that they will increase their political engagement. Social media was influential in the widespread attention given to the revolutionary outbreaks in the Middle East and North Africa during 2011. During the Tunisian revolution in 2011, people used Facebook to organize meetings and protests.\nHowever, debate persists about the extent to which social media facilitated this kind of political change.Social-media footprints of candidates for political office have grown during the last decade\u2014the 2016 United States presidential election provided good examples. Dounoucos et al. noted that Twitter use by candidates was unprecedented during that election cycle. Most candidates in the United States have a Twitter account. The public has also increased their reliance on social-media sites for political information. In the European Union, social media have amplified political messages.Militant groups have begun to see social media as a major organizing and recruiting tool. The Islamic State of Iraq and the Levant (also known as ISIL, ISIS, and Daesh) has used social media to promote its cause. In 2014, #AllEyesonISIS went viral on Arabic Twitter. ISIS produces an online magazine named the Islamic State Report to recruit more fighters. State-sponsored cyber-groups have weaponized social-media platforms to attack governments in the United States, the European Union, and the Middle East. Although phishing attacks via email are the most commonly used tactic to breach government networks, phishing attacks on social media rose 500% in 2016.Increasing political influence on social media saw several campaigns running from one political side against another. Often, foreign-originated social-media campaigns have sought to influence political opinion in another country. For example, in October 2020, a Twitter campaign in Saudi Arabia caused #HillaryEmails to trend by supporters of Crown Prince Mohammed bin Salman. It also involved Riyadh's social-marketing firm, SMAAT, which had a history of running such campaigns on Twitter.\nPoliticians themselves use social media to their advantage\u2014and to spread their campaign messages and to influence voters.\nDue to the growing abuse of human rights in Bahrain, activists have used social media to report acts of violence and injustice. They publicized the brutality of government authorities and police, who were detaining, torturing and threatening many individuals. On the other hand, Bahrain's government was using social media to track and target rights activists and individuals who were critical of the authorities; the government has stripped citizenship from over 1,000 activists as punishment.\n\n\n=== Hiring ===\n\nSome employers examine job applicants' social media profiles as part of the hiring assessment. This issue raises many ethical questions that some consider an employer's right and others consider discrimination. Many Western-European countries have already implemented laws that restrict the regulation of social media in the workplace. States including Arkansas, California, Colorado, Illinois, Maryland, Michigan, Nevada, New Jersey, New Mexico, Utah, Washington, and Wisconsin have passed legislation that protects potential employees and current employees from employers that demand that they provide their usernames and passwords for any social media accounts. Use of social media by young people has caused significant problems for some applicants who are active on social media when they try to enter the job market. A survey of 17,000 young people in six countries in 2013 found that one in ten people aged 16 to 34 have been rejected for a job because of online comments they made on social media websites.For potential employees, Social media services such as LinkedIn have shown to affect deception in resumes. While these services do not affect how often deception happens, they affect the types of deception that occur. LinkedIn resumes are less deceptive about prior work experience but more deceptive about interests and hobbies.\n\n\n=== Science ===\nThe use of social media in science communications offers extensive opportunities for exchanging scientific information, ideas, opinions and publications. Scientists use social media to share their scientific knowledge and new findings on platforms such as ResearchGate, LinkedIn, Facebook, Twitter and Academia.edu. Among these the most common type of social media that scientists use is Twitter and blogs. It has been found that Twitter increased the scientific impact in the community. The use of social media has improved and elevated the interaction between scientists, reporters, and the general public. Over 495,000 opinions were shared on Twitter related to science in one year (between September 1, 2010, and August 31, 2011), which was an increase compared with past years. Science related blogs motivate public interest in learning, following, and discussing science. Blogs use textual depth and graphical videos that provide the reader with a dynamic way to interact with scientific information. Both Twitter and blogs can be written quickly and allow the reader to interact in real time with the authors. However, the popularity of social media platforms changes quickly and scientists need to keep pace with changes in social media. In terms of organized uses of scientific social media, one study in the context of climate change has shown that climate scientist and scientific institutions played a minimal role in online debate, while nongovernmental organizations played a larger role.\n\n\n=== Academia ===\nSignals from social media are used to assess academic publications, as well as for different scientific approaches, such as gaining better understanding of the public sentiment concerning relevant topics, identifying influencer accounts shaping the public opinion in specific domains, or crowdsourcing for new ideas or solutions. Another study found that most of the health science students acquiring academic materials from others through social media.\n\n\n=== School admissions ===\nIt is not only an issue in the workplace but an issue in post-secondary school admissions as well. There have been situations where students have been forced to give up their social media passwords to school administrators. There are inadequate laws to protect a student's social media privacy, and organizations such as the ACLU are pushing for more privacy protection, as it is an invasion. They urge students who are pressured to give up their account information to tell the administrators to contact a parent or lawyer before they take the matter any further. Although they are students, they still have the right to keep their password-protected information private.\nAccording to a 2007 journal, before social media admissions officials in the United States used SAT and other standardized test scores, extra-curricular activities, letters of recommendation, and high school report cards to determine whether to accept or deny an applicant. In the 2010s, while colleges and universities still used these traditional methods to evaluate applicants, these institutions were increasingly accessing applicants' social media profiles to learn about their character and activities. According to Kaplan, Inc, a corporation that provides higher education preparation, in 2012 27% of admissions officers used Google to learn more about an applicant, with 26% checking Facebook. Students whose social media pages include offensive jokes or photos, racist or homophobic comments, photos depicting the applicant engaging in illegal drug use or drunkenness, and so on, may be screened out from admission processes.\"One survey in July 2017, by the American Association of College Registrars and Admissions Officers, found that 11 percent of respondents said they had refused to admit an applicant based on social media content. This includes 8 percent of public institutions, where the First Amendment applies. The survey found that 30 percent of institutions acknowledged reviewing the personal social media accounts of applicants at least some of the time.\"\n\n\n=== Court cases ===\nSocial media comments and images are being used in a range of court cases including employment law, child custody/child support and insurance disability claims. After an Apple employee criticized his employer on Facebook, he was fired. When the former employee sued Apple for unfair dismissal, the court, after seeing the man's Facebook posts, found in favor of Apple, as the man's social media comments breached Apple's policies. After a heterosexual couple broke up, the man posted \"violent rap lyrics from a song that talked about fantasies of killing the rapper's ex-wife\" and made threats against him. The court found him guilty and he was sentenced to jail. In a disability claims case, a woman who fell at work claimed that she was permanently injured; the employer used the social media posts of her travels and activities to counter her claims.Courts do not always admit social media evidence, in part, because screenshots can be faked or tampered with. Judges are taking emojis into account to assess statements made on social media; in one Michigan case where a person alleged that another person had defamed them in an online comment, the judge disagreed, noting that there was an emoji after the comment which indicated that it was a joke. In a 2014 case in Ontario against a police officer regarding alleged assault of a protester during the G20 summit, the court rejected the Crown's application to use a digital photo of the protest that was anonymously posted online, because there was no metadata proving when the photo was taken and it could have been digitally altered.\n\n\n== Use by individuals ==\n\n\n=== As a news source ===\n\nAs of March 2010, in the United States, 81% of users look online for news of the weather, first and foremost, with the percentage seeking national news at 73%, 52% for sports news, and 41% for entertainment or celebrity news. According to CNN, in 2010 75% of people got their news forwarded through e-mail or social media posts, whereas 37% of people shared a news item via Facebook or Twitter. Facebook and Twitter make news a more participatory experience than before as people share news articles and comment on other people's posts. Rainie and Wellman (2012) have argued that media making now has become a participation work, which changes communication systems. However, 27% of respondents worry about the accuracy of a story on a blog. From a 2019 poll, Pew Research Center found that Americans are wary about the ways that social media sites share news and certain content. This wariness of accuracy is on the rise as social media sites are increasingly exploited by aggregated new sources which stitch together multiple feeds to develop plausible correlations. Hemsley and colleagues (2018) refer to this phenomenon as \"pseudo-knowledge\" which develop false narratives and fake news that are supported through general analysis and ideology rather than facts. Social media as a news source was further questioned as spikes in evidence surround major news events such as was captured in the United States 2016 presidential election and again during the COVID-19 Pandemic.\n\n\n=== As a social tool ===\nSocial media are used to fulfill perceived social needs such as socializing with friends and family as well as romance and flirting, but not all needs can be fulfilled by social media. For example, a 2003 article found that lonely individuals are more likely to use the Internet for emotional support than those who are not lonely. A nationally representative survey from Common Sense Media in 2018 found that 40% of American teens ages 13\u201317 thought that social media was \"extremely\" or \"very\" important for them to keep up with their friends on a day-to-basis. The same survey found that 33% of teens said social media was extremely or very important to have meaningful conversations with close friends, and 23% of teens said social media was extremely or very important to document and share highlights from their lives. Recently, a Gallup poll from May 2020 showed that 53% of adult social media users in the United States thought that social media was a very or moderately important way to keep in touch with those they cannot otherwise see in-person due to social distancing measures related to the COVID-19 pandemic.Sherry Turkle explores this topic in her book Alone Together as she discusses how people confuse social media usage with authentic communication. She posits that people tend to act differently online and are less afraid to hurt each other's feelings. Additionally, some online behaviors can cause stress and anxiety, due to the permanence of online posts, the fear of being hacked, or of universities and employers exploring social media pages. Turkle also speculates that people are beginning to prefer texting to face-to-face communication, which can contribute to feelings of loneliness. Nationally representative surveys from 2019 have found this to be the case with teens in the United States and Mexico. Some researchers have also found that exchanges that involved direct communication and reciprocation of messages correlated with fewer feelings of loneliness. However, that same study showed that passively using social media without sending or receiving messages does not make people feel less lonely unless they were lonely to begin with.\nThe term social media \"stalking\" or \"creeping\" have been popularized over the years, and this refers to looking at the person's \"timeline, status updates, tweets, and online bios\" to find information about them and their activities. While social media creeping is common, it is considered to be poor form to admit to a new acquaintance or new date that you have looked through his or her social media posts, particularly older posts, as this will indicate that you were going through their old history. A sub-category of creeping is creeping ex-partners' social media posts after a breakup to investigate if there is a new partner or new dating; this can lead to preoccupation with the ex, rumination, and negative feelings, all of which postpone recovery and increase feelings of loss.Catfishing has become more prevalent since the advent of social media. Relationships formed with catfish can lead to actions such as supporting them with money and catfish will typically make excuses as to why they cannot meet up or be viewed on camera.\n\n\n=== As a self-presentational tool ===\nThe more time people spend on Facebook, the less satisfied they feel about their life. Self-presentation theory explains that people will consciously manage their self-image or identity related information in social contexts. In fact, a critical aspect of social networking sites is the time invested in customizing a personal profile, and encourage a sort of social currency based on likes, followers, and comments. Users also tend to segment their audiences based on the image they want to present, pseudonymity and use of multiple accounts across the same platform remain popular ways to negotiate platform expectations and segment audiences.However, users may feel pressure to gain their peers' acceptance of their self-presentation. For example, in a 2016 peer-reviewed article by Trudy Hui Hui Chua and Leanne Chang, the authors found that teenage girls manipulate their self-presentation on social media to achieve a sense of beauty that is projected by their peers. These authors also discovered that teenage girls compare themselves to their peers on social media and present themselves in certain ways in an effort to earn regard and acceptance. However, when users do not feel like they reached this regard and acceptance, this can actually lead to problems with self-confidence and self-satisfaction. A nationally representative survey of American teens ages 13\u201317 by Common Sense Media found that 45% said getting \"likes\" on posts is at least somewhat important, and 26% at least somewhat agreed that they feel bad about themselves if nobody comments on or \"likes\" their photos. Some evidence suggests that perceived rejection may lead to feeling emotional pain, and some may partake in online retaliation such as online bullying. Conversely, according to research from UCLA, users' reward circuits in their brains are more active when their own photos are liked by more peers.Literature suggests that social media can breed a negative feedback loop of viewing and uploading photos, self-comparison, feelings of disappointment when perceived social success is not achieved, and disordered body perception. In fact, one study shows that the microblogging platform, Pinterest is directly associated with disordered dieting behavior, indicating that for those who frequently look at exercise or dieting \"pins\" there is a greater chance that they will engage in extreme weight-loss and dieting behavior.\n\n\n=== As a health behavior change and reinforcement tool ===\n\nSocial media can also function as a supportive system for adolescents' health, because by using social media, adolescents are able to mobilize around health issues that they themselves deem relevant. For example, in a clinical study among adolescent patients undergoing treatment for obesity, the participants' expressed that through social media, they could find personalized weight-loss content as well as social support among other adolescents with obesity. Whilst, social media can provide such information there are a considerable amount of uninformed and incorrect sources which promote unhealthy and dangerous methods of weight loss. As stated by the national eating disorder association there is a high correlation between weight loss content and disorderly eating among women who have been influenced by this negative content. Therefore, there is a need for people to evaluate and identify reliable health information, competencies commonly known as health literacy. This has led to efforts by governments and public health organizations to use social media to interact with users, to limited success.Other social media, such as pro-anorexia sites, have been found in studies to cause significant risk of harm by reinforcing negative health-related behaviors through social networking, especially in adolescents. Social media affects the way a person views themself. The constant comparison to edited photos, of other individual's and their living situations, can cause many negative emotions. This can lead to not eating, and isolation. As more and more people continue to use social media for the wrong reasons, it increases the feeling of loneliness in adults.During the coronavirus pandemic, the spread of information throughout social media regarding treatments against the virus has also influenced different health behaviors. For example, People who use more social media and belief more in conspiracy theory in social media during the COVID-19 pandemic had worse mental health and is predictive of their compliance to health behaviors such as hand-washing during the pandemic.Social media platforms can serve as a breeding ground for addiction-related behaviors, with studies showing that excessive use can lead to the development of addiction-like symptoms. These symptoms include compulsive checking, mood modification, and withdrawal when not using social media, which can result in decreased face-to-face social interactions and contribute to the deterioration of interpersonal relationships and a sense of loneliness.For example, adolescents who rely heavily on social media for health information and support may be more prone to these addiction-like behaviors. In a clinical study among adolescent patients undergoing treatment for obesity, participants expressed that they could find personalized weight-loss content and social support among other adolescents with obesity through social media. However, social media also hosts a considerable amount of uninformed and incorrect sources promoting unhealthy and dangerous methods of weight loss. The National Eating Disorder Association states that there is a high correlation between weight loss content on social media and disordered eating among women influenced by this negative content.\n\n\n=== Effects on individual and collective memory ===\nNews media and television journalism have been a key feature in the shaping of American collective memory for much of the 20th century. Indeed, since the colonial era of the United States, news media has influenced collective memory and discourse about national development and trauma. In many ways, mainstream journalists have maintained an authoritative voice as the storytellers of the American past. Their documentary-style narratives, detailed expos\u00e9s, and their positions in the present make them prime sources for public memory. Specifically, news media journalists have shaped collective memory on nearly every major national event\u2014from the deaths of social and political figures to the progression of political hopefuls. Journalists provide elaborate descriptions of commemorative events in U.S. history and contemporary popular cultural sensations. Many Americans learn the significance of historical events and political issues through news media, as they are presented on popular news stations. However, journalistic influence has grown less important, whereas social networking sites such as Facebook, YouTube and Twitter, provide a constant supply of alternative news sources for users.\nAs social networking becomes more popular among older and younger generations, sites such as Facebook and YouTube gradually undermine the traditionally authoritative voices of news media. For example, American citizens contest media coverage of various social and political events as they see fit, inserting their voices into the narratives about America's past and present and shaping their own collective memories. An example of this is the public explosion of the Trayvon Martin shooting in Sanford, Florida. News media coverage of the incident was minimal until social media users made the story recognizable through their constant discussion of the case. Approximately one month after Martin's death, its online coverage by everyday Americans garnered national attention from mainstream media journalists, in turn exemplifying media activism.\n\n\n=== Negative interpersonal interactions ===\n\nSocial media use sometimes involves negative interactions between users. Angry or emotional conversations can lead to real-world interactions, which can get users into dangerous situations. Some users have experienced threats of violence online and have feared these threats manifesting themselves offline. Related issues include cyberbullying, online harassment, and 'trolling'. According to cyberbullying statistics from the i-Safe Foundation, over half of adolescents and teens have been bullied online, and about the same number have engaged in cyberbullying. Both the bully and the victim are negatively affected, and the intensity, duration, and frequency of bullying are the three aspects that increase the negative effects on both of them.\n\n\n=== Social comparison ===\nOne phenomenon that is commonly studied with social media is the issue of social comparison. People compare their own lives to the lives of their friends through their friends' posts. Because people are motivated to portray themselves in a way that is appropriate to the situation and serves their best interests, often the things posted online are the positive aspects of people's lives, making other people question why their own lives are not as exciting or fulfilling. One study in 2017 found that problematic social media use (i.e., feeling addicted to social media) was related to lower life satisfaction and self-esteem scores; the authors speculate that users may feel if their life is not exciting enough to put online it is not as good as their friends or family.Studies have shown that self-comparison on social media can have dire effects on physical and mental health because they give us the ability to seek approval and compare ourselves. In one study, women reported that social media are the most influential sources of their body image satisfaction; while men reported them as the second most impacting factor.Social media has allowed for people to be constantly surrounded and aware of celebrity images and influencers who hold strong online presence with the number of followers they have. This constant online presence has meant that people are far more aware of what others look like and as such body comparisons have become an issue, as people are far more aware of what the desired body type is. A study produced by King university showed that 87% of women and 65% of men compared themselves to images found on social media.There are efforts to combat these negative effects, such as the use of the tag #instagramversusreality and #instagramversusreallife, that have been used to promote body positivity. In a related study, women aged 18\u201330 were shown posts using this hashtag that contained side-by-side images of women in the same clothes and setting, but one image was enhanced for Instagram, while the other was an unedited, \"realistic\" version. Women who participated in this experiment noted a decrease in body dissatisfaction.\n\n\n=== Sleep disturbance ===\nAccording to a study released in 2017 by researchers from the University of Pittsburgh, the link between sleep disturbance and the use of social media was clear. It concluded that blue light had a part to play\u2014and how often they logged on, rather than time spent on social media sites, was a higher predictor of disturbed sleep, suggesting \"an obsessive 'checking'\". The strong relationship of social media use and sleep disturbance has significant clinical ramifications for young adults health and well-being. In a recent study, we have learned that people in the highest quartile for social media use per week report the most sleep disturbance. The median number of minutes of social media use per day is 61 minutes. Lastly, we have learned that females are more inclined to experience high levels of sleep disturbance than males. Many teenagers suffer from sleep deprivation as they spend long hours at night on their phones, and this, in turn, could affect grades as they will be tired and unfocused in school. In a study from 2011, it was found that time spent on Facebook has a strong negative relationship with overall GPA, but it was unclear if this was related to sleep disturbances.\n\n\n=== Emotional effects ===\n\nOne studied emotional effect of social media is 'Facebook depression', which is a type of depression that affects adolescents who spend too much of their free time engaging with social media sites. This may lead to problems such as reclusiveness which can negatively damage one's health by creating feelings of loneliness and low self-esteem among young people. Using a phone to look at social media before bed has become a popular trend among teenagers and this has led to a lack of sleep and inability to stay awake during school. Social media applications curate content that encourages users to keep scrolling to the point where they lose track of time. There are studies that show children's self-esteem is positively affected by positive comments on social media and negatively affected self-esteem by negative comments. This affects the way that people look at themselves on a \"worthiness\" scale. A 2017 study of almost 6,000 adolescent students showed that those who self-reported addiction-like symptoms of social media use were more likely to report low self-esteem and high levels of depressive symptoms. From the findings on a population-based study, there is about 37% increase in the likelihood of major depression among adolescents. In a different study conducted in 2007, those who used the most multiple social media platforms (7 to 11) had more than three times the risk of depression and anxiety than people who used the fewest (0 to 2).A second emotional effect is social media burnout, which is defined by Bo Han as ambivalence, emotional exhaustion, and depersonalization. Ambivalence refers to a user's confusion about the benefits she can get from using a social media site. Emotional exhaustion refers to the stress a user has when using a social media site. Depersonalization refers to the emotional detachment from a social media site a user experiences. The three burnout factors can all negatively influence the user's social media continuance. This study provides an instrument to measure the burnout a user can experience when his or her social media \"friends\" are generating an overwhelming amount of useless information (e.g., \"what I had for dinner\", \"where I am now\").\nA third emotional effect is the \"fear of missing out\" (FOMO), which is defined as the \"pervasive apprehension that others might be having rewarding experiences from which one is absent.\" FOMO has been classified by some as a form of social anxiety. It is associated with checking updates on friends' activities on social media. Some speculate that checking updates on friends' activities on social media may be associated with negative influences on people's psychological health and well-being because it could contribute to negative mood and depressed feelings. Looking at friends' stories or posts of them attending parties, music festivals, vacations and other events on various social media applications can lead users to feel left out and upset because they are not having as much fun as others. This is a very common issue between young people using certain apps and it continues to affect their personal well-being.On the other hand, social media can sometimes have a supportive effect on individuals who use it. Twitter has been used more by the medical community. While Twitter can facilitate academic discussion among health professionals and students, it can also provide a supportive community for these individuals by fostering a sense of community and allowing individuals to support each other through tweets, likes, and comments. Access to social media has also been seen a way to keep older adults connected, after the deaths of partners and the increased geographical distance between friends and loved ones.\n\n\n== Social impacts and regulation ==\n\n\n=== Disparity ===\n\nThe digital divide is a measure of disparity in the level of access to technology between households, socioeconomic levels or other demographic categories. People who are homeless, living in poverty, elderly people and those living in rural or remote communities may have little or no access to computers and the Internet; in contrast, middle class and upper-class people in urban areas have very high rates of computer and Internet access. Other models argue that within a modern information society, some individuals produce Internet content while others only consume it, which could be a result of disparities in the education system where only some teachers integrate technology into the classroom and teach critical thinking. While social media has differences among age groups, a 2010 study in the United States found no racial divide. Some zero-rating programs offer subsidized data access to certain websites on low-cost plans. Critics say that this is an anti-competitive program that undermines net neutrality and creates a \"walled garden\" for platforms like Facebook Zero. A 2015 study found that 65% of Nigerians, 61% of Indonesians, and 58% of Indians agree with the statement that \"Facebook is the Internet\" compared with only 5% in the US.Eric Ehrmann contends that social media in the form of public diplomacy create a patina of inclusiveness that covers traditional economic interests that are structured to ensure that wealth is pumped up to the top of the economic pyramid, perpetuating the digital divide and post-Marxian class conflict. He also voices concern over the trend that finds social utilities operating in a quasi-libertarian global environment of oligopoly that requires users in economically challenged nations to spend high percentages of annual income to pay for devices and services to participate in the social media lifestyle. Neil Postman also contends that social media will increase an information disparity between \"winners\" who are able to use the social media actively and \"losers\" who are not familiar with modern technologies or who do not have access to them. People with high social media skills may have better access to information about job opportunities, potential new friends, and social activities in their area, which may enable them to improve their standard of living and their quality of life.\n\n\n=== Political polarization ===\n\nAccording to the Pew Research Center and other research works, a majority of Americans at least occasionally receive news from social media. Because of recommendation algorithms on social media which filter and display news content which are likely to match their users' political preferences (known as a filter bubble), a potential impact of receiving news from social media includes an increase in political polarization due to selective exposure (see also: algorithmic radicalization). Political polarization refers to when an individual's stance on a topic is more likely to be strictly defined by their identification with a specific political party or ideology than on other factors. Selective exposure occurs when an individual favors information that supports their beliefs and avoids information that conflicts with their beliefs. A 2016 study using U.S. elections, conducted by Evans and Clark, revealed gender differences in the political use of Twitter between candidates. Whilst politics is a male dominated arena, on social media the situation appears to be the opposite, with women discussing policy issues at a higher rate than their male counterparts. The study concluded that an increase in female candidates directly correlates to an increase in the amount of attention paid to policy issues, potentially heightening political polarization.Efforts to combat selective exposure in social media may also cause an increase in political polarization. A study examining Twitter activity conducted by Bail et al. paid Democrat and Republican participants to follow Twitter handles whose content was different from their political beliefs (Republicans received liberal content and Democrats received conservative content) over a six-week period. At the end of the study, both Democrat and Republican participants were found to have increased political polarization in favor of their own parties, though only Republican participants had an increase that was statistically significant.Though research has shown evidence that social media plays a role in increasing political polarization, it has also shown evidence that social media use leads to a persuasion of political beliefs. An online survey consisting of 1,024 U.S. participants was conducted by Diehl, Weeks, and Gil de Zu\u00f1iga, which found that individuals who use social media were more likely to have their political beliefs persuaded than those who did not. In particular, those using social media as a means to receive their news were the most likely to have their political beliefs changed. Diehl et al. found that the persuasion reported by participants was influenced by the exposure to diverse viewpoints they experienced, both in the content they saw as well as the political discussions they participated in. Similarly, a study by Hardy and colleagues conducted with 189 students from a Midwestern state university examined the persuasive effect of watching a political comedy video on Facebook. Hardy et al. found that after watching a Facebook video of the comedian/political commentator John Oliver performing a segment on his show, participants were likely to be persuaded to change their viewpoint on the topic they watched (either payday lending or the Ferguson protests) to one that was closer to the opinion expressed by Oliver. Furthermore, the persuasion experienced by the participants was found to be reduced if they viewed comments by Facebook users which contradicted the arguments made by Oliver.Research has also shown that social media use may not have an effect on polarization at all. A U.S. national survey of 1,032 participants conducted by Lee et al. found that participants who used social media were more likely to be exposed to a diverse number of people and amount of opinion than those who did not, although using social media was not correlated with a change in political polarization for these participants.In a study examining the potential polarizing effects of social media on the political views of its users, Mihailidis and Viotty suggest that a new way of engaging with social media must occur to avoid polarization. The authors note that media literacies (described as methods which give people skills to critique and create media) are important to using social media in a responsible and productive way, and state that these literacies must be changed further in order to have the most effectiveness. In order to decrease polarization and encourage cooperation among social media users, Mihailidis and Viotty suggest that media literacies must focus on teaching individuals how to connect with other people in a caring way, embrace differences, and understand the ways in which social media has a real impact on the political, social, and cultural issues of the society they are a part of.\n\n\n=== Stereotyping ===\nRecent research has demonstrated that social media, and media in general, have the power to increase the scope of stereotypes not only in children but people of all ages. Both cases of stereotyping of the youth and the elderly are prime examples of ageism. The presumed characteristics of the individual being stereotyped can have both negative and positive connotations but frequently carry an opposing viewpoint. For example, the youth on social media platforms are often depicted as lazy, immature individuals who oftentimes have no drive or passion for other activities. For example, during the COVID-19 pandemic, much of the youth were accused for the spread of the disease and were blamed for the continuous lockdowns across the world. These misrepresentations make it difficult for the youth to find new efforts and prove others wrong, especially when a large group of individuals believe that the stereotypes are highly accurate. Considering the youthful groups that are present on social media are frequently in a new stage of their lives and preparing to make life-changing decisions, it is essential that the stereotypes are diminished so that they do not feel invalidated. Further, stereotyping often occurs for the elderly as they are presumed to be a group of individuals who are unaware of the proper functions and slang usage on social media. These stereotypes often seek to exclude older generations from participating in trends or engaging them in other activities on digital platforms.\n\n\n=== Effects on youth communication ===\nSocial media has allowed for mass cultural exchange and intercultural communication. As different cultures have different value systems, cultural themes, grammar, and world views, they also communicate differently. The emergence of social media platforms fused together different cultures and their communication methods, blending together various cultural thinking patterns and expression styles.Social media has affected the way youth communicate, by introducing new forms of language. Abbreviations have been introduced to cut down on the time it takes to respond online. The commonly known \"LOL\" has become globally recognized as the abbreviation for \"laugh out loud\" thanks to social media and use by people of all ages particularly as people grow up.\nAnother trend that influences the way youth communicates is the use of hashtags. With the introduction of social media platforms such as Twitter, Facebook, and Instagram the hashtag was created to easily organize and search for information. Hashtags can be used when people want to advocate for a movement, store content or tweets from a movement for future use, and allow other social media users to contribute to a discussion about a certain movement by using existing hashtags. Using hashtags as a way to advocate for something online makes it easier and more accessible for more people to acknowledge it around the world. As hashtags such as #tbt (\"throwback Thursday\") become a part of online communication, it influenced the way in which youth share and communicate in their daily lives. Because of these changes in linguistics and communication etiquette, researchers of media semiotics have found that this has altered youth's communications habits and more.Social media is a great way to learn about your community and the world around you, but as social media progressed younger audiences have lowered their ability to effectively communicate. Because of the digital nature, teens have stopped worrying about the consequences that social media has. They often do not think about what they are sending and take longer to figure out what to say. In return, during real-life settings, it's harder for them to carry on conversations. Social media also creates a toxic environment where people cyberbully each other, so in person they act the same way and do not worry about the consequences. This can not only affect themselves but people around them.Social media has offered a new platform for peer pressure with both positive and negative communication. From Facebook comments to likes on Instagram, how the youth communicate, and what is socially acceptable is now heavily based on social media. Social media does make kids and young adults more susceptible to peer pressure. The American Academy of Pediatrics has also shown that bullying, the making of non-inclusive friend groups, and sexual experimentation have increased situations related to cyberbullying, issues with privacy, and the act of sending sexual images or messages to someone's mobile device. This includes issues of sexting and revenge porn among minors, and the resulting legal implications and issues, and resulting risk of trauma. On the other hand, social media also benefits the youth and how they communicate. Adolescents can learn basic social and technical skills that are essential in society. Through the use of social media, kids and young adults are able to strengthen relationships by keeping in touch with friends and family, making more friends, and participating in community engagement activities and services.\n\n\n=== Regulation of negative social externalities ===\nDue to the business model of social media platforms - which is based on selling slots of highly personalized ads to advertising partners by collecting large amounts of user data - these platforms incentivize the distribution of content that keeps users on the platform for as long as possible. Socio-psychological research has already shown that populist, often extreme content in particular encourages users to stay on these platforms for longer, which in turn leads to such content being prioritized by the platforms' algorithms purely for economic reasons. Various whistleblowers have already highlighted this problem with the platforms on several occasions. It is therefore a prime example of negative social externalities that are actually unplanned with the spread of technology, but are incentivized due to the business model and the way the technology works.\n\n\n==== Status quo: Europe ====\nUntil recently, the narrative of self-regulation by platform providers, who determined the rules and processes of content moderation and the design of the algorithms used, largely prevailed in the European Union. At the end of 2020, the European Commission presented two legislative proposals: The Digital Services Act (DSA) and the Digital Markets Act (DMA). Both proposals were adopted by the European Parliament in July 2022. The DSA will enter into force on 17 February 2024, the DMA in March 2024. This legislation can basically be summarized in the following four objectives, articulated by MEPs: \"What is illegal offline must also be illegal online\".\"Very large online platforms\" must therefore, among other things (a) delete illegal content (Russian propaganda, election interference, hate crimes and online harms such as harassment and child abuse) and better protect fundamental rights, (b) redesign their systems to ensure a \"high level of privacy, security and protection of minors\", by prohibiting advertising based on personal data, redesigning recommender systems to minimize risks for children and demonstrating this to the European Commission in a risk assessment, and (c) not using sensitive personal data such as race, gender and religion to target users with advertising.  The legislative package therefore requires extensive content moderation and adaptation of the respective algorithm.\nAccording to the directive, a company that does not comply with the law could face a complete ban in Europe or fines of up to 6% of its global turnover. It remains to be seen whether this will actually have a deterrent effect on large platform providers such as Meta. Iverna McGowan, the director of the Centre for Democracy & Technology's Europe office, said that civil society in particular has a role to play in overseeing platforms, but also that national authorities lack adequate resources to enforce the law.\n\n\n==== Regulatory shortcomings ====\nThe regulatory problem is that both the prescribed rules for content moderation and the corresponding adaptation of the algorithms require extensive intervention by the platforms. However, it is not in their financial interest to identify and delete polarizing content, as they are incentivized to disseminate divisive content due to their advertising-based business model.It is therefore unlikely that digital platforms such as Meta will make sufficient adjustments - also because the effectiveness of monitoring mechanisms to ensure that companies comply with the new European regulations will be low due to resource issues.Another problem is that, according to current regulations under the DSA, \"a country can have information deleted that is only illegal there but is not a problem at all elsewhere\", says Patrick Breyer (MEP). If, for example, Hungary deletes a video that is critical of Viktor Orban's government from the internet throughout the EU, this creates a problem. A different policy approach is therefore needed to solve the problem.\n\n\n==== Other regulatory solutions ====\nRepresentatives of Ashoka's Tech & Humanity initiative, \"a global network of leading social entrepreneurs committed to ensuring tech works for the good of people and planet\" and 2018 Nobel Laureate Paul Romer are in favor of taxing negative externalities of social media platforms due to the resource problem. Similar to a CO2 tax - the resulting negative social effects should be measured and compensated for by a financial levy on the part of the companies. The capital raised could then be used for awareness campaigns or education to offset negative effects such as political polarization, social division or increased suicide rates among minors. However, no consensus has yet emerged in the scientific community on how to measure the corresponding damage and convert it into a tax.\nAnother proposal that is and has been the subject of lively academic debate and complements the proposals implemented by the EU Commission on data privacy, consumer protection and the fundamental intermediary liability of social media platforms is competition law. The basic idea is to prevent the emergence of overly strong platforms or to restrict the market power of existing platforms by controlling mergers ex ante and tightening the relevant competition law. This is to be achieved through a supranational enforcement mechanism and the deterrent effect of high fines.\n\n\n== Criticism, debate and controversy ==\nCriticisms of social media range from criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented, the impact of social media use on an individual's concentration, ownership of media content, and the meaning of interactions created by social media. Although some social media platforms, such as servers in the decentralized Fediverse, offer users the opportunity to cross-post between independently run servers using a standard protocol such as ActivityPub, the dominant social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos, viz. isolated pockets of data contained in one social media platform. However, it is also argued that social media has positive effects, such as allowing the democratization of the Internet while also allowing individuals to advertise themselves and form friendships. have noted that the term \"social\" cannot account for technological features of a platform alone, hence the level of sociability should be determined by the actual performances of its users. There has been a dramatic decrease in face-to-face interactions as more and more social media platforms have been introduced with the threat of cyber-bullying and online sexual predators including groomers being more prevalent. Social media may expose children to images of alcohol, tobacco, and sexual behaviors. In regards to cyber-bullying, it has been proven that individuals who have no experience with cyber-bullying often have a better well-being than individuals who have been bullied online.Twitter is increasingly a target of heavy activity of marketers. Their actions focused on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques that distort the prime idea of social media by abusing human trustfulness. British-American entrepreneur and author Andrew Keen criticized social media in his 2007 book The Cult of the Amateur, writing, \"Out of this anarchy, it suddenly became clear that what was governing the infinite monkeys now inputting away on the Internet was the law of digital Darwinism, the survival of the loudest and most opinionated. Under these rules, the only way to intellectually prevail is by infinite filibustering.\"\nThis is also relative to the issue \"justice\" in the social network. For example, the phenomenon \"Human flesh search engine\" in Asia raised the discussion of \"private-law\" brought by social network platform. Comparative media professor Jos\u00e9 van Dijck contends in her book The Culture of Connectivity (2013) that to understand the full weight of social media, their technological dimensions should be connected to the social and the cultural. She critically describes six social media platforms. One of her findings is the way Facebook had been successful in framing the term 'sharing' in such a way that third party use of user data is neglected in favor of intra-user connectedness. The fragmentation of modern society, in part due to social media, has been likened to a modern Tower of Babel.Essena O'Neill attracted international coverage when she explicitly left social media.\n\n\n=== Trustworthiness and reliability ===\nSocial media has increasingly become a regular source of news and information for their users. Users have become so reliant on social media for news that a 2021 poll by the Pew Research Center found that roughly 70% of users regularly get their news from the site. While social media has become an increasingly used source for news, these platform's reliability and trustworthiness are questionable as a result of the amount of fake news and misinformation present on these sites. This is due to the lack of verification and regulation on information posted on social media platforms, as users can anonymously create posts containing misinformation and pass it off as truthful news. While some social media platforms have started to employ fact-checking to combat this, a majority of social media sites either lack said functionality or simply don't employ it enough to fully combat the issue.\nFurthermore, social media platforms have been found to magnify the spread of misinformation. In 2018, researchers found that fake news spreads almost 70% faster than truthful news on Twitter. Social media accelerates the spread of misinformation as a result of two main reasons.\nThe first reason is due to the heavy prevalence of bots on social media. Bad actors can use bots to mass post misinformation. This is because bots can generate and publish posts significantly faster than human users, thus creating a platform where fake news posts greatly outnumber truthful reports. Most platforms attempt to combat botting by implementing CAPTCHAs and other forms of human verification, yet even with these barriers, heavy botting is still a frequent issue on most sites.The second reason is that fake news tends to receive more user engagement. This is because fake news posts contain \"novel\" or more \"new\" information, meaning that users out of curiosity are more likely to engage with the post. As a result, because these posts receive more engagement they are more likely to be recommended to other users, thus mass spreading the misinformation within them.This issue becomes particularly bad in the immediate aftermath of a crisis event before much information is known about said event. These \"information holes\" in the wake of a major disaster become filled by speculation and false information on social media platforms, which are then shared by other users and sometimes by news organizations, amplifying the spread of said misinformation in a positive feedback loop.A fairly recent example of social media being used to spread fake news came in the wake of the BLM protests in 2020. On social media, users were exposed to fake news surrounding the protests resulting in political divisions and increased racial tension. As a result, a 2022 study found that users exposed to fake news during that time were more likely to be against the protests than users who interacted with mostly accurate news.Evgeny Morozov, a 2009\u20132010 Yahoo fellow at Georgetown University, contended that information uploaded to Twitter may have little relevance to the masses of people who do not use Twitter. In an article for the magazine Dissent titled \"Iran: Downside to the 'Twitter Revolution'\", Morozov wrote:\n\n[B]y its very design Twitter only adds to the noise: it's simply impossible to pack much context into its 140 characters. All other biases are present as well: in a country like Iran it's mostly pro-Western, technology-friendly and iPod-carrying young people who are the natural and most frequent users of Twitter. They are a tiny and, most important, extremely untypical segment of the Iranian population (the number of Twitter users in Iran \u2014 a country of more than seventy million people \u2014 was estimated at less than twenty thousand before the protests).\nProfessor Matthew Auer of Bates College casts doubt on the conventional wisdom that social media are open and participatory. He also speculates on the emergence of \"anti-social media\" used as \"instruments of pure control\".\n\n\n==== Data harvesting and data mining ====\n\nSocial media 'mining' is a type of data mining, a technique of analyzing data to detect patterns. Social media mining is a process of representing, analyzing, and extracting actionable patterns from data collected from people's activities on social media. Google mines data in many ways including using an algorithm in Gmail to analyze information in emails. This use of the information will then affect the type of advertisements shown to the user when they use Gmail. Facebook has partnered with many data mining companies such as Datalogix and BlueKai to use customer information for targeted advertising. Massive amounts of data from social platforms allows scientists and machine learning researchers to extract insights and build product features.Ethical questions of the extent to which a company should be able to utilize a user's information have been called \"big data\". Users tend to click through Terms of Use agreements when signing up on social media platforms, and they do not know how their information will be used by companies. This leads to questions of privacy and surveillance when user data is recorded. Some social media outlets have added capture time and geotagging that helps provide information about the context of the data as well as making their data more accurate.\nOn April 10, 2018, in a US Senate hearing held in response to revelations of data harvesting by Cambridge Analytica, Facebook chief executive Mark Zuckerberg faced questions from senators on a variety of issues, from privacy to the company's business model and the company's mishandling of data. This was Mr. Zuckerberg's first appearance before Congress, prompted by the revelation that Cambridge Analytica, a political consulting firm linked to the Trump campaign, harvested the data of an estimated 87 million Facebook users to psychologically profile voters during the 2016 election. Zuckerberg was pressed to account for how third-party partners could take data without users' knowledge. Lawmakers questioned him on the proliferation of so-called fake news on Facebook, Russian interference during the 2016 presidential election and censorship of conservative media.\n\n\n=== Critique of activism ===\n\nFor The New Yorker writer Malcolm Gladwell, the role of social media, such as Twitter and Facebook, in revolutions and protests is overstated. On one hand, social media makes it easier for individuals, and in this case activists, to express themselves. On the other hand, it is harder for that expression to have an impact. Gladwell distinguishes between social media activism and high-risk activism, which brings real changes. Activism and especially high-risk activism involves strong-tie relationships, hierarchies, coordination, motivation, exposing oneself to high risks, making sacrifices. Gladwell discusses that social media are built around weak ties and he argues that \"social networks are effective at increasing participation\u2014by lessening the level of motivation that participation requires.\" According to him, \"Facebook activism succeeds not by motivating people to make a real sacrifice, but by motivating them to do the things that people do when they are not motivated enough to make a real sacrifice.\"Disputing Gladwell's theory, in the study \"Perceptions of Social Media for Politics: Testing the Slacktivism Hypothesis\" (2018), Nojin Kwak and colleagues conducted a survey which found that people who are politically expressive on social media are also more likely to participate in offline political activity.\n\n\n=== Ownership of content ===\nSocial media content is generated through social media interactions done by users through the site. There has always been a huge debate on the ownership of the content on social media platforms because it is generated by the users and hosted by the company. Added to this is the danger to the security of information, which can be leaked to third parties with economic interests in the platform, or parasites who comb the data for their own databases.\nIn order for social media platforms such as Facebook, Instagram, Twitter, and YouTube to publish user content online, they must be issued a license from the copyright owners. A license is a legitimate right that allows them to carry out a specific task. Users grant a platform permission to use their content in accordance with its terms and conditions, even if users control the content. Although each platform's terms are different, generally they all give social media sites permission to utilize users' copyrighted works however they see fit. Theoretically, platforms could make commercial use of and even sell or sublicense their license, and because each license specifically states that it is \"royalty free\", users wouldn't be entitled to a share of the revenue.\nAfter being acquired by Facebook (now Meta) in 2012, Instagram made headlines when it revealed it intended to use user posts in advertisements without seeking permission from or paying its users. The next day, it backed down from these changes, with then-CEO Kevin Systrom writing in a blog post that \"it's not our intention to sell your photos\" and promising to update the terms of service to clarify this point.\n\n\n=== Privacy ===\n\nPrivacy rights advocates warn users on social media about the collection of their personal data. Some information is captured without the user's knowledge or consent through electronic tracking and third-party applications. Data may also be collected for law enforcement and governmental purposes, by social media intelligence using data mining techniques. Data and information may also be collected for third party use. When information is shared on social media, that information is no longer private. There have been many cases in which young persons especially, share personal information, which can attract predators. It is very important to monitor what you share and to be aware of who you could potentially be sharing that information with. Teens especially share significantly more information on the internet now than they have in the past. Teens are much more likely to share their personal information, such as email address, phone number, and school names. Studies suggest that teens are not aware of what they are posting and how much of that information can be accessed by third parties.\nThere are arguments that \"privacy is dead\" and that with social media growing more and more, some heavy social media users appear to have become quite unconcerned with privacy. Others argue, however, that people are still very concerned about their privacy, but are being ignored by the companies running these social networks, who can sometimes make a profit off of sharing someone's personal information. There is also a disconnect between social media user's words and their actions. Studies suggest that surveys show that people want to keep their lives private, but their actions on social media suggest otherwise. Everyone leaves a trail when they use social media. Every time someone creates a new social media account, they provide personal information that can include their name, birthdate, geographic location, and personal interests. In addition, companies collect data on user behaviors. All of this data is stored and leveraged by companies to better target advertising to their users.Another factor is ignorance of how accessible social media posts are. Some social media users who have been criticized for inappropriate comments stated that they did not realize that anyone outside their circle of friends would read their posts; in fact, on some social media sites, unless a user selects higher privacy settings, their content is shared with a wide audience.\nAccording to a 2016 article diving into the topic of sharing privately and the effect social media has on expectations of privacy, \"1.18 billion people will log into their Facebook accounts, 500 million tweets will be sent, and there will be 95 million photos and videos posted on Instagram\" in a day. Much of the privacy concerns individuals face stem from their own posts on a form of a social network. Users have the choice to share voluntarily and have been ingrained into society as routine and normative. Social media are a snapshot of our lives; a community we have created on the behaviors of sharing, posting, liking, and communicating. Sharing has become a phenomenon which social media and networks have uprooted and introduced to the world. The idea of privacy is redundant; once something is posted, its accessibility remains constant even if we select who is potentially able to view it. People desire privacy in some shape or form, yet also contribute to social media, which makes it difficult to maintain privacy. Mills offers options for reform which include copyright and the application of the law of confidence; more radically, a change to the concept of privacy itself.\nA 2014 Pew Research Center survey found that 91% of Americans \"agree\" or \"strongly agree\" that people have lost control over how personal information is collected and used by all kinds of entities. Some 80% of social media users said they were concerned about advertisers and businesses accessing the data they share on social media platforms, and 64% said the government should do more to regulate advertisers.According to The Wall Street Journal published on February 17, 2019, According to UK law, Facebook did not protect certain aspects of the user data.The US government announced banning TikTok and WeChat from the States over national security concerns. The shutdown was announced for September 20, 2020. Access to TikTok was extended until 12 November 2020, and a federal court ruling on October 30, 2020, has blocked further implementation of restrictions that would lead to TikTok's shutdown.Additionally, in 2019 the Pentagon issued guidance to the US Army, Navy, Air Force, Marine Corps, Coast Guard and other government agencies that identified \"the potential risk associated with using the TikTok app and directs appropriate action for employees to take in order to safeguard their personal information.\" As a result, the Army, Navy, Air Force, Marine Corps, Coast Guard, Transportation Security Administration, and Department of Homeland Security banned the installation and use of TikTok on government devices, including blacklisting it on intranet services.\n\n\n=== Criticism of commercialization ===\nThe commercial development of social media has been criticized as the actions of consumers in these settings have become increasingly value-creating, for example when consumers contribute to the marketing and branding of specific products by posting positive reviews. As such, value-creating activities also increase the value of a specific product, which could, according to marketing professors Bernad Cova and Daniele Dalli (2009), lead to what they refer to as \"double exploitation\".As social media usage has become increasingly widespread, social media has to a large extent come to be subjected to commercialization by marketing companies and advertising agencies. In 2014 Christofer Laurell, a digital marketing researcher, suggested that the social media landscape currently consists of three types of places because of this development: consumer-dominated places, professionally dominated places and places undergoing commercialization. As social media becomes commercialized, this process has been shown to create novel forms of value networks stretching between consumer and producer in which a combination of personal, private and commercial contents are created.\n\n\n=== Debate over addiction ===\n\nSocial media addiction has various social effects.\nAs one of the biggest preoccupations among adolescents is social media usage, in 2011 researchers began using the term \"Facebook addiction disorder\" (F.A.D.), a form of internet addiction disorder. FAD is characterized by compulsive use of the social networking site Facebook, which generally results in physical or psychological complications. The disorder, although not classified in the latest Diagnostic and Statistical Manual of Mental Disorders (DSM-5) or by the World Health Organization, has been the subject of several studies focusing on the negative effects of social media use on the psyche. One German study published in 2017 investigated a correlation between excessive use of the social networking site and narcissism; the results were published in the journal PLoS One. According to the findings: \"FAD was significantly positively related to the personality trait narcissism and to negative mental health variables (depression, anxiety, and stress symptoms)\".In 2020, Netflix released The Social Dilemma, which raises concerns about the problematic effects of social media. In the documentary, mental health experts and former employees of social media companies explain how social media is designed to be addictive. One example that's shown is when an AI detects that someone has not visited Facebook for some time, it may choose different notifications that it predicts are most likely to cause them to re-visit the platform. This AI takes into account everything that each person has done on that platform.\nThe documentary also raises concerns about the correlation between child and teen suicides and suicide attempts and increasing social media usage in the United States, particularly usage on mobile.Turning off social media notifications temporarily or long-term may help reduce problematic social media use. In certain cases and for some users, changes in their web browsing environments can be helpful in compensating for self-regulatory problems. For instance, a study involving 157 online learners on massive open online courses examined the impact of self-regulatory intervention on learners\u2019 web browsing behavior. The results showed that, on average, learners spend half of their time online on YouTube and social media, and Less than 2% of visited websites account for nearly 80% of their time spent online. Further, the study found that modifying the learners' web environment, specifically by providing support in self-regulation, was associated with changes in behavior, including a reduction in time spent online, particularly on websites related to entertainment. This suggests there is a potential for interventions to improve self-regulatory skills, which may effectively help learners reduce excessive social media usage and manage their signs of social media misuse more effectively.\n\n\n=== Debate over use in academic settings ===\n\nHaving social media in the classroom was a controversial topic in the 2010s. Many parents and educators have been fearful of the repercussions of having social media in the classroom. There are concerns that social media tools can be misused for cyberbullying or sharing inappropriate content. As result, cell phones have been banned from some classrooms, and some schools have blocked many popular social media websites. Many schools have realized that they need to loosen restrictions, teach digital citizenship skills, and even incorporate these tools into classrooms. Some schools permit students to use smartphones or tablet computers in class, as long as the students are using these devices for academic purposes, such as doing research. Using Facebook in class allows for the integration of multimodal content such as student-created photographs and video and URLs to other texts, in a platform that many students are already familiar with. Twitter can be used to enhance communication building and critical thinking and it provides students with an informal \"back channel\", and extend discussion outside of class time.\n\n\n=== Censorship by governments ===\n\nSocial media often features in political struggles to control public perception and online activity. In some countries, Internet police or secret police monitor or control citizens' use of social media. For example, in 2013 some social media was banned in Turkey after the Taksim Gezi Park protests. Both Twitter and YouTube were temporarily suspended in the country by a court's decision. A new law, passed by Turkish Parliament, has granted immunity to Telecommunications Directorate (T\u0130B) personnel. The T\u0130B was also given the authority to block access to specific websites without the need for a court order. Yet T\u0130B's 2014 blocking of Twitter was ruled by the constitutional court to violate free speech. More recently, in the 2014 Thai coup d'\u00e9tat, the public was explicitly instructed not to 'share' or 'like' dissenting views on social media or face prison. In July of that same year, in response to WikiLeaks' release of a secret suppression order made by the Victorian Supreme Court, media lawyers were quoted in the Australian media to the effect that \"anyone who tweets a link to the WikiLeaks report, posts it on Facebook, or shares it in any way online could also face charges\". On 27 July 2020, in Egypt, two women were sentenced to two years of imprisonment for posting TikTok videos, which the government claims are \"violating family values\".\n\n\n=== Decentralization and open standards ===\nMastodon, GNU social, Diaspora, Friendica and other compatible software packages operate as a loose federation of mostly volunteer-operated servers, called the Fediverse, which connect with each other through the open source protocol ActivityPub. In early 2019, Mastodon successfully blocked the spread of violent right-wing extremism when the Twitter alternative Gab tried to associate with Mastodon, and their independent servers quickly contained its dissemination.In December 2019, Twitter CEO Jack Dorsey made a similar suggestion, stating that efforts would be taken to achieve an \"open and decentralized standard for social media\". Rather than \"deplatforming\", such standards would allow a more scalable, and customizable approach to content moderation and censorship, and involve a number of companies, in the way that e-mail servers work.\n\n\n=== Deplatforming ===\n\nDeplatforming is a form of Internet censorship in which controversial speakers or speech are suspended, banned, or otherwise shut down by social media platforms and other service providers that normally provide a venue for free expression. These kinds of actions are similar to alternative dispute resolution.:\u200a4\u200a As early as 2015, platforms such as Reddit began to enforce selective bans based, for example, on terms of service that prohibit \"hate speech\". According to technology journalist Declan McCullagh, \"Silicon Valley's efforts to pull the plug on dissenting opinions\" have included, as of 2018, Twitter, Facebook, and YouTube \"devising excuses to suspend ideologically disfavored accounts\".Most people see social media platforms as censoring objectionable political views.\n\n\n=== Reproduction of class distinctions ===\nAccording to Danah Boyd (2011), the media plays a large role in shaping people's perceptions of specific social networking services. When looking at the site MySpace, after adults started to realize how popular the site was becoming with teens, news media became heavily concerned with teen participation and the potential dangers they faced using the site. As a result, teens avoided joining the site because of the associated risks (e.g. child predators and lack of control), and parents began to publicly denounce the site. Ultimately, the site was labeled as dangerous, and many were detracted from interacting with the site.As Boyd also describes, when Facebook initially launched in 2004, it solely targeted college students and access was intentionally limited. Facebook started as a Harvard-only social networking service before expanding to all other Ivy League schools. It then made its way to other top universities and ultimately to a wider range of schools. Because of its origins, some saw Facebook as an \"elite\" social networking service. While it was very open and accepting to some, it seemed to outlaw and shun most others who did not fit that \"elite\" categorization. These narratives propagated by the media influenced the large movement of teenage users from one social networking service to another.\n\n\n=== Use by extremist groups ===\n\nAccording to LikeWar: The Weaponization of Social Media (2018) by P.W. Singer and Emerson T. Brooking, the use of effective social media marketing techniques is not only limited to celebrities, corporations, and governments, but also extremist groups to carry out political objectives based on extremist ideologies. The use of social media by ISIS and Al-Qaeda has been used primarily to influence operations in areas of operation and gain the attention of sympathizers of extremist ideologies. Social media platforms like YouTube, Twitter, Facebook, and various encrypted-messaging applications have been used to increase the recruiting of members into these extremist groups, both locally and internationally. Larger platforms like YouTube, Twitter, and various others have received backlash for allowing this type of content on their platform (see Use of social media by the Islamic State of Iraq and the Levant). The use of social media to further extremist objectives is not only limited to Islamic terrorism, but also extreme nationalist groups across the world, and more prominently, right-wing extremist groups based out of the United States.\n\n\n==== 2021 United States Capitol attack ====\n\nAs many of the traditional social media platforms banned hate speech (see Online hate speech), several platforms have become popular among right-wing extremists to carry out planning and communication of thoughts and organized events; these application became known as \"Alt-tech\". Platforms such as Telegram, Parler, and Gab were used during the 2021 storming of the US Capitol in Washington, D.C. The use of this social media was used to coordinate attacks on the Capitol. Several members within these groups shared tips on how to avoid law enforcement and what their plans were with regards to carrying out their objectives; some users called for killings of law enforcement and politicians.\n\n\n== Deceased users ==\n\nSocial media content, like most content on the web, will continue to persist unless the user deletes it. This brings up the inevitable question of what to do once a social media user dies, and no longer has access to their content. As it is a topic that is often left undiscussed, it is important to note that each social media platform, e.g., Twitter, Facebook, Instagram, LinkedIn, and Pinterest, has created its own guidelines for users who have died. In most cases on social media, the platforms require a next-of-kin to prove that the user is deceased, and then give them the option of closing the account or maintaining it in a 'legacy' status. Ultimately, social media users should make decisions about what happens to their social media accounts before they pass, and make sure their instructions are passed on to their next-of-kin.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nAgozzino, Alisa (2012). \"Building A Personal Relationship Through Social Media: A Study Of Millenial Students' Brand Engagement\". Ohio Communication Journal. 50: 181\u2013204.\nAl-Rahmi, Waleed Mugahed; Othman, Mohd Shahizan (2013). \"The Impact of Social Media use on Academic Performance among university students: A Pilot Study\". Journal of Information Systems Research and Innovation: 1\u201310.\nAral, Sinan (2020). The Hype Machine: How Social Media Disrupts Our Elections, Our Economy, and Our Health\u2014and How We Must Adapt. Currency. ISBN 978-0-525-57451-4.\nBenkler, Yochai (2006). The Wealth of Networks. New Haven: Yale University Press. ISBN 978-0-300-11056-2. OCLC 61881089.\nBeshears, Michael L. (2016). \"Effectiveness of Police Social Media Use\". American Journal of Criminal Justice. 42 (3): 489\u2013501. doi:10.1007/s12103-016-9380-4. S2CID 151928750.\nBlankenship, M (2011). \"How social media can and should impact higher education\". The Education Digest. 76 (7): 39. ProQuest 848431918.\nFuchs, Christian (2014). Social Media: A Critical Introduction. London: Sage. ISBN 978-1-4462-5731-9.\nGentle, Anne (2012). Conversation and Community: The Social Web for Documentation (2nd ed.). Laguna Hills, CA: XML Press. ISBN 978-1-937434-10-6. OCLC 794490599.\nJohnson, Steven Berlin (2005). Everything Bad Is Good for You. New York: Riverhead Books. ISBN 978-1-57322-307-2. OCLC 57514882.\nJordan, Kasteler (2017). \"How to use SEO data in your social media strategy\".\nJue, Arthur L.; Alcalde Marr, Jackie; Kassotakis, Mary Ellen (2010). Social media at work : how networking tools propel organizational performance (1st ed.). San Francisco, CA: Jossey-Bass. ISBN 978-0-470-40543-7.\nLardi, Kamales; Fuchs, Rainer (2013). Social Media Strategy \u2013 A step-by-step guide to building your social business (1st ed.). Zurich: vdf. ISBN 978-3-7281-3557-5.\nLi, Charlene; Bernoff, Josh (2008). Groundswell: Winning in a World Transformed by Social Technologies. Boston: Harvard Business Press. ISBN 978-1-4221-2500-7. OCLC 423555651.\nMateus, Samuel (2012). \"Social Networks Scopophilic dimension \u2013 social belonging through spectatorship\". Observatorio (OBS*) Journal (Special Issue). doi:10.15847/obsOBS000605. S2CID 142933378.\nMcHale, Robert; Garulay, Eric (2012). Navigating Social Media Legal Risks: Safeguarding Your Business. Que. ISBN 978-0-7897-4953-6.\nPiskorski, Miko\u0142aj Jan (2014). A Social Strategy: How We Profit from Social Media. Princeton, NJ: Princeton University Press. ISBN 978-0-691-15339-1.\nPowell, Guy R.; Groves, Steven W.; Dimos, Jerry (2011). ROI of Social Media: How to improve the return on your social marketing investment. New York: John Wiley & Sons. ISBN 978-0-470-82741-3. OCLC 0470827416.\nRheingold, Howard (2002). Smart mobs: The next social revolution (1st printing ed.). Cambridge, MA: Perseus Pub. p. 288. ISBN 978-0-7382-0608-0.\nSchoen, Harald; Gayo-Avello, Daniel; Takis Metaxas, Panagiotis; Mustafaraj, Eni; Strohmaier, Markus; Gloor, Peter (2013). \"The power of prediction with social media\". Internet Research. 23 (5): 528\u2013543. CiteSeerX 10.1.1.460.3885. doi:10.1108/IntR-06-2013-0115.\nSchrape, JF (2017). \"Reciprocal irritations: Social media, mass media and the public sphere\". Society, Regulation and Governance. pp. 138\u2013150. doi:10.4337/9781786438386.00016. ISBN 978-1-78643-838-6. {{cite book}}: |journal= ignored (help)\nScoble, Robert; Israel, Shel (2006). Naked Conversations: How Blogs are Changing the Way Businesses Talk with Customers. Hoboken, NJ: John Wiley. ISBN 978-0-471-74719-2. OCLC 61757953.\nShirky, Clay (2008). Here Comes Everybody. New York: Penguin Press. ISBN 978-1-59420-153-0. OCLC 458788924.\nSiegel, Alyssa (September 7, 2015). \"How Social Media Affects Our Relationships\". Psychology Tomorrow.\nSurowiecki, James (2004). The Wisdom of Crowds. New York: Anchor Books. ISBN 978-0-385-72170-7. OCLC 156770258.\nTapscott, Don; Williams, Anthony D. (2006). Wikinomics. New York: Portfolio. ISBN 978-1-59184-138-8. OCLC 318389282.\nTedesco, Laura Anne (October 2000). \"Lascaux (ca. 15,000 B.C.)\". Heilbrunn Timeline of Art History. New York: The Metropolitan Museum of Art.\nWatts, Duncan J. (2003). Six degrees: The science of a connected age. London: Vintage. p. 368. ISBN 978-0-09-944496-1.\n\n\n== External links ==\n\n Media related to Social media at Wikimedia Commons"}, {"id": 52, "title": "Quantum mechanics", "content": "Quantum mechanics is a fundamental theory in physics that describes the behavior of nature at the scale of atoms and subatomic particles.:\u200a1.1\u200a It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.\nClassical physics, the collection of theories that existed before the advent of quantum mechanics, describes many aspects of nature at an ordinary (macroscopic) scale, but is not sufficient for describing them at small (atomic and subatomic) scales. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization); measurements of systems show characteristics of both particles and waves (wave\u2013particle duality); and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).\nQuantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the \"old quantum theory\", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schr\u00f6dinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.\n\n\n== Overview and fundamental concepts ==\nQuantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and sub-atomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been  shown to agree with experiment to within 1 part in 108 for some atomic properties.\nA fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schr\u00f6dinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.\nOne consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between different measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.\nAnother consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.:\u200a102\u2013111\u200a:\u200a1.1\u20131.8\u200a The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen \u2013 a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).:\u200a109\u200a However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave\u2013particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.Another non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential. In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy and the tunnel diode.When quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schr\u00f6dinger called entanglement \"...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought\". Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding. Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.Another possibility opened by entanglement is testing for \"hidden variables\", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory can provide. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed, using entangled particles, and they have shown results incompatible with the constraints imposed by local hidden variables.It is not possible to present these concepts in more than a superficial way without introducing the actual mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects. Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.\n\n\n== Mathematical formulation ==\n\nIn the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   belonging to a (separable) complex Hilbert space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  . This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys \n  \n    \n      \n        \u27e8\n        \u03c8\n        ,\n        \u03c8\n        \u27e9\n        =\n        1\n      \n    \n    {\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n  , and it is well-defined up to a complex number of modulus 1 (the global phase), that is, \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   and \n  \n    \n      \n        \n          e\n          \n            i\n            \u03b1\n          \n        \n        \u03c8\n      \n    \n    {\\displaystyle e^{i\\alpha }\\psi }\n   represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system \u2013 for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions \n  \n    \n      \n        \n          L\n          \n            2\n          \n        \n        (\n        \n          C\n        \n        )\n      \n    \n    {\\displaystyle L^{2}(\\mathbb {C} )}\n  , while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors \n  \n    \n      \n        \n          \n            C\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {C} ^{2}}\n   with the usual inner product.\nPhysical quantities of interest \u2013 position, momentum, energy, spin \u2013 are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   is non-degenerate and the probability is given by \n  \n    \n      \n        \n          |\n        \n        \u27e8\n        \n          \n            \n              \u03bb\n              \u2192\n            \n          \n        \n        ,\n        \u03c8\n        \u27e9\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle |\\langle {\\vec {\\lambda }},\\psi \\rangle |^{2}}\n  , where \n  \n    \n      \n        \n          \n            \n              \u03bb\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\lambda }}}\n   is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by \n  \n    \n      \n        \u27e8\n        \u03c8\n        ,\n        \n          P\n          \n            \u03bb\n          \n        \n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle \\langle \\psi ,P_{\\lambda }\\psi \\rangle }\n  , where \n  \n    \n      \n        \n          P\n          \n            \u03bb\n          \n        \n      \n    \n    {\\displaystyle P_{\\lambda }}\n   is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.\nAfter the measurement, if result \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n   was obtained, the quantum state is postulated to collapse to \n  \n    \n      \n        \n          \n            \n              \u03bb\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {\\lambda }}}\n  , in the non-degenerate case, or to \n  \n    \n      \n        \n          P\n          \n            \u03bb\n          \n        \n        \u03c8\n        \n          \n            /\n          \n        \n        \n        \n          \n            \u27e8\n            \u03c8\n            ,\n            \n              P\n              \n                \u03bb\n              \n            \n            \u03c8\n            \u27e9\n          \n        \n      \n    \n    {\\textstyle P_{\\lambda }\\psi {\\big /}\\!{\\sqrt {\\langle \\psi ,P_{\\lambda }\\psi \\rangle }}}\n  , in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr\u2013Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a \"measurement\" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of \"wave function collapse\" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.The time evolution of a quantum state is described by the Schr\u00f6dinger equation:\n\n  \n    \n      \n        i\n        \u210f\n        \n          \n            d\n            \n              d\n              t\n            \n          \n        \n        \u03c8\n        (\n        t\n        )\n        =\n        H\n        \u03c8\n        (\n        t\n        )\n        .\n      \n    \n    {\\displaystyle i\\hbar {\\frac {d}{dt}}\\psi (t)=H\\psi (t).}\n  Here \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   denotes the Hamiltonian, the observable corresponding to the total energy of the system, and \n  \n    \n      \n        \u210f\n      \n    \n    {\\displaystyle \\hbar }\n   is the reduced Planck constant. The constant \n  \n    \n      \n        i\n        \u210f\n      \n    \n    {\\displaystyle i\\hbar }\n   is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.\nThe solution of this differential equation is given by\n\n  \n    \n      \n        \u03c8\n        (\n        t\n        )\n        =\n        \n          e\n          \n            \u2212\n            i\n            H\n            t\n            \n              /\n            \n            \u210f\n          \n        \n        \u03c8\n        (\n        0\n        )\n        .\n      \n    \n    {\\displaystyle \\psi (t)=e^{-iHt/\\hbar }\\psi (0).}\n  The operator \n  \n    \n      \n        U\n        (\n        t\n        )\n        =\n        \n          e\n          \n            \u2212\n            i\n            H\n            t\n            \n              /\n            \n            \u210f\n          \n        \n      \n    \n    {\\displaystyle U(t)=e^{-iHt/\\hbar }}\n   is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that \u2013 given an initial quantum state \n  \n    \n      \n        \u03c8\n        (\n        0\n        )\n      \n    \n    {\\displaystyle \\psi (0)}\n    \u2013 it makes a definite prediction of what the quantum state \n  \n    \n      \n        \u03c8\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\psi (t)}\n   will be at any later time.\nSome wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian. Many systems that are treated dynamically in classical mechanics are described by such \"static\" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).\nAnalytic solutions of the Schr\u00f6dinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom \u2013 which contains just two electrons \u2013 has defied all attempts at a fully analytic treatment.\nHowever, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy. Another method is called \"semi-classical equation of motion\", which applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.\n\n\n=== Uncertainty principle ===\nOne consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum. Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator \n  \n    \n      \n        \n          \n            \n              X\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {X}}}\n   and momentum operator \n  \n    \n      \n        \n          \n            \n              P\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {P}}}\n   do not commute, but rather satisfy the canonical commutation relation:\n\n  \n    \n      \n        [\n        \n          \n            \n              X\n              ^\n            \n          \n        \n        ,\n        \n          \n            \n              P\n              ^\n            \n          \n        \n        ]\n        =\n        i\n        \u210f\n        .\n      \n    \n    {\\displaystyle [{\\hat {X}},{\\hat {P}}]=i\\hbar .}\n  Given a quantum state, the Born rule lets us compute expectation values for both \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  , and moreover for powers of them. Defining \nthe uncertainty for an observable by a standard deviation, we have\n\n  \n    \n      \n        \n          \u03c3\n          \n            X\n          \n        \n        =\n        \n          \n            \n              \n                \n                  \u27e8\n                  \n                    X\n                    \n                      2\n                    \n                  \n                  \u27e9\n                \n                \u2212\n                \n                  \n                    \u27e8\n                    X\n                    \u27e9\n                  \n                  \n                    2\n                  \n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma _{X}={\\textstyle {\\sqrt {\\left\\langle X^{2}\\right\\rangle -\\left\\langle X\\right\\rangle ^{2}}}},}\n  and likewise for the momentum:\n\n  \n    \n      \n        \n          \u03c3\n          \n            P\n          \n        \n        =\n        \n          \n            \n              \u27e8\n              \n                P\n                \n                  2\n                \n              \n              \u27e9\n            \n            \u2212\n            \n              \n                \u27e8\n                P\n                \u27e9\n              \n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{P}={\\sqrt {\\left\\langle P^{2}\\right\\rangle -\\left\\langle P\\right\\rangle ^{2}}}.}\n  The uncertainty principle states that\n\n  \n    \n      \n        \n          \u03c3\n          \n            X\n          \n        \n        \n          \u03c3\n          \n            P\n          \n        \n        \u2265\n        \n          \n            \u210f\n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{X}\\sigma _{P}\\geq {\\frac {\\hbar }{2}}.}\n  Either standard deviation can in principle be made arbitrarily small, but not both simultaneously. This inequality generalizes to arbitrary pairs of self-adjoint operators \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  . The commutator of these two operators is\n\n  \n    \n      \n        [\n        A\n        ,\n        B\n        ]\n        =\n        A\n        B\n        \u2212\n        B\n        A\n        ,\n      \n    \n    {\\displaystyle [A,B]=AB-BA,}\n  and this provides the lower bound on the product of standard deviations:\n\n  \n    \n      \n        \n          \u03c3\n          \n            A\n          \n        \n        \n          \u03c3\n          \n            B\n          \n        \n        \u2265\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          |\n          \n            \n              \n                \u27e8\n              \n            \n            [\n            A\n            ,\n            B\n            ]\n            \n              \n                \u27e9\n              \n            \n          \n          |\n        \n        .\n      \n    \n    {\\displaystyle \\sigma _{A}\\sigma _{B}\\geq {\\tfrac {1}{2}}\\left|{\\bigl \\langle }[A,B]{\\bigr \\rangle }\\right|.}\n  Another consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an \n  \n    \n      \n        i\n        \n          /\n        \n        \u210f\n      \n    \n    {\\displaystyle i/\\hbar }\n   factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum \n  \n    \n      \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle p_{i}}\n   is replaced by \n  \n    \n      \n        \u2212\n        i\n        \u210f\n        \n          \n            \u2202\n            \n              \u2202\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle -i\\hbar {\\frac {\\partial }{\\partial x}}}\n  , and in particular in the non-relativistic Schr\u00f6dinger equation in position space the momentum-squared term is replaced with a Laplacian times \n  \n    \n      \n        \u2212\n        \n          \u210f\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle -\\hbar ^{2}}\n  .\n\n\n=== Composite systems and entanglement ===\nWhen two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{A}}\n   and \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{B}}\n  , respectively. The Hilbert space of the composite system is then\n\n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n            B\n          \n        \n        =\n        \n          \n            \n              H\n            \n          \n          \n            A\n          \n        \n        \u2297\n        \n          \n            \n              H\n            \n          \n          \n            B\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\mathcal {H}}_{AB}={\\mathcal {H}}_{A}\\otimes {\\mathcal {H}}_{B}.}\n  If the state for the first system is the vector \n  \n    \n      \n        \n          \u03c8\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\psi _{A}}\n   and the state for the second system is \n  \n    \n      \n        \n          \u03c8\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\psi _{B}}\n  , then the state of the composite system is\n\n  \n    \n      \n        \n          \u03c8\n          \n            A\n          \n        \n        \u2297\n        \n          \u03c8\n          \n            B\n          \n        \n        .\n      \n    \n    {\\displaystyle \\psi _{A}\\otimes \\psi _{B}.}\n  Not all states in the joint Hilbert space \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            A\n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{AB}}\n   can be written in this form, however, because the superposition principle implies that linear combinations of these \"separable\" or \"product states\" are also valid. For example, if \n  \n    \n      \n        \n          \u03c8\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\psi _{A}}\n   and \n  \n    \n      \n        \n          \u03d5\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\phi _{A}}\n   are both possible states for system \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , and likewise \n  \n    \n      \n        \n          \u03c8\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\psi _{B}}\n   and \n  \n    \n      \n        \n          \u03d5\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\phi _{B}}\n   are both possible states for system \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , then\n\n  \n    \n      \n        \n          \n            \n              1\n              \n                2\n              \n            \n          \n        \n        \n          (\n          \n            \n              \u03c8\n              \n                A\n              \n            \n            \u2297\n            \n              \u03c8\n              \n                B\n              \n            \n            +\n            \n              \u03d5\n              \n                A\n              \n            \n            \u2297\n            \n              \u03d5\n              \n                B\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{\\sqrt {2}}}\\left(\\psi _{A}\\otimes \\psi _{B}+\\phi _{A}\\otimes \\phi _{B}\\right)}\n  is a valid joint state that is not separable. States that are not separable are called entangled.If the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system. Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.\n\n\n=== Equivalence between formulations ===\nThere are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the \"transformation theory\" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics \u2013 matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schr\u00f6dinger). An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.\n\n\n=== Symmetries and conservation laws ===\n\nThe Hamiltonian \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   is known as the generator of time evolution, since it defines a unitary time-evolution operator \n  \n    \n      \n        U\n        (\n        t\n        )\n        =\n        \n          e\n          \n            \u2212\n            i\n            H\n            t\n            \n              /\n            \n            \u210f\n          \n        \n      \n    \n    {\\displaystyle U(t)=e^{-iHt/\\hbar }}\n   for each value of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  . From this relation between \n  \n    \n      \n        U\n        (\n        t\n        )\n      \n    \n    {\\displaystyle U(t)}\n   and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  , it follows that any observable \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   that commutes with \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   will be conserved: its expectation value will not change over time. This statement generalizes, as mathematically, any Hermitian operator \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   can generate a family of unitary operators parameterized by a variable \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  . Under the evolution generated by \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , any observable \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   that commutes with \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   will be conserved. Moreover, if \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is conserved by evolution under \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , then \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is conserved under the evolution generated by \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  . This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law.\n\n\n== Examples ==\n\n\n=== Free particle ===\n\nThe simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:\n\n  \n    \n      \n        H\n        =\n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          P\n          \n            2\n          \n        \n        =\n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              d\n              \n                2\n              \n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle H={\\frac {1}{2m}}P^{2}=-{\\frac {\\hbar ^{2}}{2m}}{\\frac {d^{2}}{dx^{2}}}.}\n  The general solution of the Schr\u00f6dinger equation is given by\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        ,\n        t\n        )\n        =\n        \n          \n            1\n            \n              2\n              \u03c0\n            \n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \n            \n              \u03c8\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n        \n          e\n          \n            i\n            (\n            k\n            x\n            \u2212\n            \n              \n                \n                  \u210f\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  m\n                \n              \n            \n            t\n            )\n          \n        \n        \n          d\n        \n        k\n        ,\n      \n    \n    {\\displaystyle \\psi (x,t)={\\frac {1}{\\sqrt {2\\pi }}}\\int _{-\\infty }^{\\infty }{\\hat {\\psi }}(k,0)e^{i(kx-{\\frac {\\hbar k^{2}}{2m}}t)}\\mathrm {d} k,}\n  which is a superposition of all possible plane waves \n  \n    \n      \n        \n          e\n          \n            i\n            (\n            k\n            x\n            \u2212\n            \n              \n                \n                  \u210f\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  m\n                \n              \n            \n            t\n            )\n          \n        \n      \n    \n    {\\displaystyle e^{i(kx-{\\frac {\\hbar k^{2}}{2m}}t)}}\n  , which are eigenstates of the momentum operator with momentum \n  \n    \n      \n        p\n        =\n        \u210f\n        k\n      \n    \n    {\\displaystyle p=\\hbar k}\n  . The coefficients of the superposition are \n  \n    \n      \n        \n          \n            \n              \u03c8\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle {\\hat {\\psi }}(k,0)}\n  , which is the Fourier transform of the initial quantum state \n  \n    \n      \n        \u03c8\n        (\n        x\n        ,\n        0\n        )\n      \n    \n    {\\displaystyle \\psi (x,0)}\n  .\nIt is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states. Instead, we can consider a Gaussian wave packet:\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        ,\n        0\n        )\n        =\n        \n          \n            1\n            \n              \n                \u03c0\n                a\n              \n              \n                4\n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n                \n                  2\n                  a\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\psi (x,0)={\\frac {1}{\\sqrt[{4}]{\\pi a}}}e^{-{\\frac {x^{2}}{2a}}}}\n  which has Fourier transform, and therefore momentum distribution\n\n  \n    \n      \n        \n          \n            \n              \u03c8\n              ^\n            \n          \n        \n        (\n        k\n        ,\n        0\n        )\n        =\n        \n          \n            \n              a\n              \u03c0\n            \n            \n              4\n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                \n                  a\n                  \n                    k\n                    \n                      2\n                    \n                  \n                \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\hat {\\psi }}(k,0)={\\sqrt[{4}]{\\frac {a}{\\pi }}}e^{-{\\frac {ak^{2}}{2}}}.}\n  We see that as we make \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.\nAs we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.\n\n\n=== Particle in a box ===\n\nThe particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region.:\u200a77\u201378\u200a For the one-dimensional case in the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   direction, the time-independent Schr\u00f6dinger equation may be written\n\n  \n    \n      \n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03c8\n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        E\n        \u03c8\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\hbar ^{2}}{2m}}{\\frac {d^{2}\\psi }{dx^{2}}}=E\\psi .}\n  With the differential operator defined by\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n        \n        =\n        \u2212\n        i\n        \u210f\n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}_{x}=-i\\hbar {\\frac {d}{dx}}}\n  the previous equation is evocative of the classic kinetic energy analogue,\n\n  \n    \n      \n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n          \n            2\n          \n        \n        =\n        E\n        ,\n      \n    \n    {\\displaystyle {\\frac {1}{2m}}{\\hat {p}}_{x}^{2}=E,}\n  with state \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   in this case having energy \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   coincident with the kinetic energy of the particle.\nThe general solutions of the Schr\u00f6dinger equation for the particle in a box are\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        )\n        =\n        A\n        \n          e\n          \n            i\n            k\n            x\n          \n        \n        +\n        B\n        \n          e\n          \n            \u2212\n            i\n            k\n            x\n          \n        \n        \n        \n        E\n        =\n        \n          \n            \n              \n                \u210f\n                \n                  2\n                \n              \n              \n                k\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\psi (x)=Ae^{ikx}+Be^{-ikx}\\qquad \\qquad E={\\frac {\\hbar ^{2}k^{2}}{2m}}}\n  or, from Euler's formula,\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        )\n        =\n        C\n        sin\n        \u2061\n        (\n        k\n        x\n        )\n        +\n        D\n        cos\n        \u2061\n        (\n        k\n        x\n        )\n        .\n        \n      \n    \n    {\\displaystyle \\psi (x)=C\\sin(kx)+D\\cos(kx).\\!}\n  The infinite potential walls of the box determine the values of \n  \n    \n      \n        C\n        ,\n        D\n        ,\n      \n    \n    {\\displaystyle C,D,}\n   and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n   and \n  \n    \n      \n        x\n        =\n        L\n      \n    \n    {\\displaystyle x=L}\n   where \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   must be zero. Thus, at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  ,\n\n  \n    \n      \n        \u03c8\n        (\n        0\n        )\n        =\n        0\n        =\n        C\n        sin\n        \u2061\n        (\n        0\n        )\n        +\n        D\n        cos\n        \u2061\n        (\n        0\n        )\n        =\n        D\n      \n    \n    {\\displaystyle \\psi (0)=0=C\\sin(0)+D\\cos(0)=D}\n  and \n  \n    \n      \n        D\n        =\n        0\n      \n    \n    {\\displaystyle D=0}\n  . At \n  \n    \n      \n        x\n        =\n        L\n      \n    \n    {\\displaystyle x=L}\n  ,\n\n  \n    \n      \n        \u03c8\n        (\n        L\n        )\n        =\n        0\n        =\n        C\n        sin\n        \u2061\n        (\n        k\n        L\n        )\n        ,\n      \n    \n    {\\displaystyle \\psi (L)=0=C\\sin(kL),}\n  in which \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   cannot be zero as this would conflict with the postulate that \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   has norm 1. Therefore, since \n  \n    \n      \n        sin\n        \u2061\n        (\n        k\n        L\n        )\n        =\n        0\n      \n    \n    {\\displaystyle \\sin(kL)=0}\n  , \n  \n    \n      \n        k\n        L\n      \n    \n    {\\displaystyle kL}\n   must be an integer multiple of \n  \n    \n      \n        \u03c0\n      \n    \n    {\\displaystyle \\pi }\n  ,\n\n  \n    \n      \n        k\n        =\n        \n          \n            \n              n\n              \u03c0\n            \n            L\n          \n        \n        \n        \n        n\n        =\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        \u2026\n        .\n      \n    \n    {\\displaystyle k={\\frac {n\\pi }{L}}\\qquad \\qquad n=1,2,3,\\ldots .}\n  This constraint on \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   implies a constraint on the energy levels, yielding\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        \n          \n            \n              \n                \u210f\n                \n                  2\n                \n              \n              \n                \u03c0\n                \n                  2\n                \n              \n              \n                n\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                h\n                \n                  2\n                \n              \n            \n            \n              8\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E_{n}={\\frac {\\hbar ^{2}\\pi ^{2}n^{2}}{2mL^{2}}}={\\frac {n^{2}h^{2}}{8mL^{2}}}.}\n  \nA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy.\n\n\n=== Harmonic oscillator ===\n\nAs in the classical case, the potential for the quantum harmonic oscillator is given by\n\n  \n    \n      \n        V\n        (\n        x\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          \u03c9\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle V(x)={\\frac {1}{2}}m\\omega ^{2}x^{2}.}\n  This problem can either be treated by directly solving the Schr\u00f6dinger equation, which is not trivial, or by using the more elegant \"ladder method\" first proposed by Paul Dirac. The eigenstates are given by\n\n  \n    \n      \n        \n          \u03c8\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              1\n              \n                \n                  2\n                  \n                    n\n                  \n                \n                \n                n\n                !\n              \n            \n          \n        \n        \u22c5\n        \n          \n            (\n            \n              \n                \n                  m\n                  \u03c9\n                \n                \n                  \u03c0\n                  \u210f\n                \n              \n            \n            )\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n        \u22c5\n        \n          e\n          \n            \u2212\n            \n              \n                \n                  m\n                  \u03c9\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \u210f\n                \n              \n            \n          \n        \n        \u22c5\n        \n          H\n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    m\n                    \u03c9\n                  \n                  \u210f\n                \n              \n            \n            x\n          \n          )\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\psi _{n}(x)={\\sqrt {\\frac {1}{2^{n}\\,n!}}}\\cdot \\left({\\frac {m\\omega }{\\pi \\hbar }}\\right)^{1/4}\\cdot e^{-{\\frac {m\\omega x^{2}}{2\\hbar }}}\\cdot H_{n}\\left({\\sqrt {\\frac {m\\omega }{\\hbar }}}x\\right),\\qquad }\n  \n  \n    \n      \n        n\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        \u2026\n        .\n      \n    \n    {\\displaystyle n=0,1,2,\\ldots .}\n  where Hn are the Hermite polynomials\n\n  \n    \n      \n        \n          H\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        (\n        \u2212\n        1\n        \n          )\n          \n            n\n          \n        \n        \n          e\n          \n            \n              x\n              \n                2\n              \n            \n          \n        \n        \n          \n            \n              d\n              \n                n\n              \n            \n            \n              d\n              \n                x\n                \n                  n\n                \n              \n            \n          \n        \n        \n          (\n          \n            e\n            \n              \u2212\n              \n                x\n                \n                  2\n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\\frac {d^{n}}{dx^{n}}}\\left(e^{-x^{2}}\\right),}\n  and the corresponding energy levels are\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        \u210f\n        \u03c9\n        \n          (\n          \n            n\n            +\n            \n              \n                1\n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle E_{n}=\\hbar \\omega \\left(n+{1 \\over 2}\\right).}\n  This is another example illustrating the discretization of energy for bound states.\n\n\n=== Mach\u2013Zehnder interferometer ===\nThe Mach\u2013Zehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the Elitzur\u2013Vaidman bomb tester, and in studies of quantum entanglement.We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the \"lower\" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the \"upper\" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector \n  \n    \n      \n        \u03c8\n        \u2208\n        \n          \n            C\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\psi \\in \\mathbb {C} ^{2}}\n   that is a superposition of the \"lower\" path \n  \n    \n      \n        \n          \u03c8\n          \n            l\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\psi _{l}={\\begin{pmatrix}1\\\\0\\end{pmatrix}}}\n   and the \"upper\" path \n  \n    \n      \n        \n          \u03c8\n          \n            u\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\psi _{u}={\\begin{pmatrix}0\\\\1\\end{pmatrix}}}\n  , that is, \n  \n    \n      \n        \u03c8\n        =\n        \u03b1\n        \n          \u03c8\n          \n            l\n          \n        \n        +\n        \u03b2\n        \n          \u03c8\n          \n            u\n          \n        \n      \n    \n    {\\displaystyle \\psi =\\alpha \\psi _{l}+\\beta \\psi _{u}}\n   for complex \n  \n    \n      \n        \u03b1\n        ,\n        \u03b2\n      \n    \n    {\\displaystyle \\alpha ,\\beta }\n  . In order to respect the postulate that \n  \n    \n      \n        \u27e8\n        \u03c8\n        ,\n        \u03c8\n        \u27e9\n        =\n        1\n      \n    \n    {\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n   we require that \n  \n    \n      \n        \n          |\n        \n        \u03b1\n        \n          \n            |\n          \n          \n            2\n          \n        \n        +\n        \n          |\n        \n        \u03b2\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}\n  .\nBoth beam splitters are modelled as the unitary matrix \n  \n    \n      \n        B\n        =\n        \n          \n            1\n            \n              2\n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  i\n                \n              \n              \n                \n                  i\n                \n                \n                  1\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\frac {1}{\\sqrt {2}}}{\\begin{pmatrix}1&i\\\\i&1\\end{pmatrix}}}\n  , which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of \n  \n    \n      \n        1\n        \n          /\n        \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 1/{\\sqrt {2}}}\n  , or be reflected to the other path with a probability amplitude of \n  \n    \n      \n        i\n        \n          /\n        \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle i/{\\sqrt {2}}}\n  . The phase shifter on the upper arm is modelled as the unitary matrix \n  \n    \n      \n        P\n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      i\n                      \u0394\n                      \u03a6\n                    \n                  \n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle P={\\begin{pmatrix}1&0\\\\0&e^{i\\Delta \\Phi }\\end{pmatrix}}}\n  , which means that if the photon is on the \"upper\" path it will gain a relative phase of \n  \n    \n      \n        \u0394\n        \u03a6\n      \n    \n    {\\displaystyle \\Delta \\Phi }\n  , and it will stay unchanged if it is in the lower path.\nA photon that enters the interferometer from the left will then be acted upon with a beam splitter \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , a phase shifter \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  , and another beam splitter \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  , and so end up in the state \n\n  \n    \n      \n        B\n        P\n        B\n        \n          \u03c8\n          \n            l\n          \n        \n        =\n        i\n        \n          e\n          \n            i\n            \u0394\n            \u03a6\n            \n              /\n            \n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \u2212\n                  sin\n                  \u2061\n                  (\n                  \u0394\n                  \u03a6\n                  \n                    /\n                  \n                  2\n                  )\n                \n              \n              \n                \n                  cos\n                  \u2061\n                  (\n                  \u0394\n                  \u03a6\n                  \n                    /\n                  \n                  2\n                  )\n                \n              \n            \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle BPB\\psi _{l}=ie^{i\\Delta \\Phi /2}{\\begin{pmatrix}-\\sin(\\Delta \\Phi /2)\\\\\\cos(\\Delta \\Phi /2)\\end{pmatrix}},}\n  and the probabilities that it will be detected at the right or at the top are given respectively by\n\n  \n    \n      \n        p\n        (\n        u\n        )\n        =\n        \n          |\n        \n        \u27e8\n        \n          \u03c8\n          \n            u\n          \n        \n        ,\n        B\n        P\n        B\n        \n          \u03c8\n          \n            l\n          \n        \n        \u27e9\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          cos\n          \n            2\n          \n        \n        \u2061\n        \n          \n            \n              \u0394\n              \u03a6\n            \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle p(u)=|\\langle \\psi _{u},BPB\\psi _{l}\\rangle |^{2}=\\cos ^{2}{\\frac {\\Delta \\Phi }{2}},}\n  \n\n  \n    \n      \n        p\n        (\n        l\n        )\n        =\n        \n          |\n        \n        \u27e8\n        \n          \u03c8\n          \n            l\n          \n        \n        ,\n        B\n        P\n        B\n        \n          \u03c8\n          \n            l\n          \n        \n        \u27e9\n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        \n          \n            \n              \u0394\n              \u03a6\n            \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle p(l)=|\\langle \\psi _{l},BPB\\psi _{l}\\rangle |^{2}=\\sin ^{2}{\\frac {\\Delta \\Phi }{2}}.}\n  One can therefore use the Mach\u2013Zehnder interferometer to estimate the phase shift by estimating these probabilities.\nIt is interesting to consider what would happen if the photon were definitely in either the \"lower\" or \"upper\" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given by \n  \n    \n      \n        p\n        (\n        u\n        )\n        =\n        p\n        (\n        l\n        )\n        =\n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle p(u)=p(l)=1/2}\n  , independently of the phase \n  \n    \n      \n        \u0394\n        \u03a6\n      \n    \n    {\\displaystyle \\Delta \\Phi }\n  . From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.\n\n\n== Applications ==\n\nQuantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained by classical methods. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.In many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.\n\n\n== Relation to other scientific theories ==\n\n\n=== Classical mechanics ===\nThe rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space \u2013 although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers. One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.\nWhen quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.\nComplications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.\nQuantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations. Quantum coherence is not typically evident at macroscopic scales, except maybe at temperatures approaching absolute zero at which quantum behavior may manifest macroscopically.Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.\n\n\n=== Special relativity and electrodynamics ===\nEarly attempts to merge quantum mechanics with special relativity involved the replacement of the Schr\u00f6dinger equation with a covariant equation such as the Klein\u2013Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical \n  \n    \n      \n        \n          \u2212\n          \n            e\n            \n              2\n            \n          \n          \n            /\n          \n          (\n          4\n          \u03c0\n          \n            \u03f5\n            \n              \n                \n                \n                  0\n                \n              \n            \n          \n          r\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle -e^{2}/(4\\pi \\epsilon _{_{0}}r)}\n   Coulomb potential. This \"semi-classical\" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.\nQuantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg.\n\n\n=== Relation to general relativity ===\nEven though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant \"Theory of Everything\" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.One proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.Another popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric \"woven\" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616\u00d710\u221235 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.\n\n\n== Philosophical implications ==\n\nSince its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, \"I think I can safely say that nobody understands quantum mechanics.\" According to Steven Weinberg, \"There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.\"The views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the \"Copenhagen interpretation\". According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of \"causality\". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr, Heisenberg, Schr\u00f6dinger, Feynman, and Zeilinger as well as 21st century researchers in quantum foundations.Albert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the Bohr\u2013Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the Einstein\u2013Podolsky\u2013Rosen paradox. In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles. Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.Bohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schr\u00f6dinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.Everett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes. This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule, with no consensus on whether they have been successful.Relational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas, and QBism was developed some years later.\n\n\n== History ==\n\nQuantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations. In 1803 English polymath Thomas Young described the famous double-slit experiment. This experiment played a major role in the general acceptance of the wave theory of light.\nDuring the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics. While the early conception of atoms from Greek philosophy had been that they were indivisible units \u2013  the word \"atom\" deriving from the Greek for \"uncuttable\" \u2013  the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Pl\u00fccker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.The black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete \"quanta\" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation. The word quantum derives from the Latin, meaning \"how great\" or \"how much\". According to Planck, quantities of energy could be thought of as divided into \"elements\" whose size (E) would be proportional to their frequency (\u03bd):\n\n  \n    \n      \n        E\n        =\n        h\n        \u03bd\n         \n      \n    \n    {\\displaystyle E=h\\nu \\ }\n  ,where h is Planck's constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation. In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery. However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen. Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency. In his paper \"On the Quantum Theory of Radiation\", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation, which became the basis of the laser.\n\nThis phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics. The theory is now understood as a semi-classical approximation to modern quantum mechanics. Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.\nIn the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan developed matrix mechanics and the Austrian physicist Erwin Schr\u00f6dinger invented wave mechanics. Born introduced the probabilistic interpretation of Schr\u00f6dinger's wave function in July 1926. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.By 1930 quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors and superfluids.\n\n\n== See also ==\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nJ. O'Connor and E. F. Robertson: A history of quantum mechanics.\nIntroduction to Quantum Theory at Quantiki.\nQuantum Physics Made Relatively Simple: three video lectures by Hans BetheCourse materialQuantum Cook Book and PHYS 201: Fundamentals of Physics II by Ramamurti Shankar, Yale OpenCourseware\nModern Physics: With waves, thermodynamics, and optics \u2013 an online textbook.\nMIT OpenCourseWare: Chemistry and Physics. See 8.04, 8.05 and 8.06\n5\u00bd Examples in Quantum Mechanics\nImperial College Quantum Mechanics Course.PhilosophyIsmael, Jenann. \"Quantum Mechanics\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nKrips, Henry. \"Measurement in Quantum Theory\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy."}, {"id": 53, "title": "Stock market", "content": "A stock market, equity market, or share market is the aggregation of buyers and sellers of stocks (also called shares), which represent ownership claims on businesses; these may include securities listed on a public stock exchange, as well as stock that is only traded privately, such as shares of private companies which are sold to investors through equity crowdfunding platforms. Investment is usually made with an investment strategy in mind.\n\n\n== Size of the market ==\nThe total market capitalization of all publicly traded stocks worldwide rose from US$2.5 trillion in 1980 to US$93.7 trillion at the end of 2020.As of 2016, there are 60 stock exchanges in the world. Of these, there are 16 exchanges with a market capitalization of $1 trillion or more, and they account for 87% of global market capitalization. Apart from the Australian Securities Exchange, these 16 exchanges are all in North America, Europe, or Asia.By country, the largest stock markets as of January 2022 are in the United States of America (about 59.9%), followed by Japan (about 6.2%) and United Kingdom (about 3.9%).\n\n\n== Stock exchange ==\n\nA stock exchange is an exchange (or bourse) where stockbrokers and traders can buy and sell shares (equity stock), bonds, and other securities. Many large companies have their stocks listed on a stock exchange. This makes the stock more liquid and thus more attractive to many investors. The exchange may also act as a guarantor of settlement. These and other stocks may also be traded \"over the counter\" (OTC), that is, through a dealer. Some large companies will have their stock listed on more than one exchange in different countries, so as to attract international investors.Stock exchanges may also cover other types of securities, such as fixed-interest securities (bonds) or (less frequently) derivatives, which are more likely to be traded OTC.\nTrade in stock markets means the transfer (in exchange for money) of a stock or security from a seller to a buyer. This requires these two parties to agree on a price. Equities (stocks or shares) confer an ownership interest in a particular company.\nParticipants in the stock market range from small individual stock investors to larger investors, who can be based anywhere in the world, and may include banks, insurance companies, pension funds and hedge funds. Their buy or sell orders may be executed on their behalf by a stock exchange trader.\nSome exchanges are physical locations where transactions are carried out on a trading floor, by a method known as open outcry. This method is used in some stock exchanges and commodities exchanges, and involves traders shouting bid and offer prices. The other type of stock exchange has a network of computers where trades are made electronically. An example of such an exchange is the NASDAQ.\nA potential buyer bids a specific price for a stock, and a potential seller asks a specific price for the same stock. Buying or selling at the Market means you will accept any ask price or bid price for the stock. When the bid and ask prices match, a sale takes place, on a first-come, first-served basis if there are multiple bidders at a given price.\nThe purpose of a stock exchange is to facilitate the exchange of securities between buyers and sellers, thus providing a marketplace. The exchanges provide real-time trading information on the listed securities, facilitating price discovery.\nThe New York Stock Exchange (NYSE) is a physical exchange, with a hybrid market for placing orders electronically from any location as well as on the trading floor. Orders executed on the trading floor enter by way of exchange members and flow down to a floor broker, who submits the order electronically to the floor trading post for the Designated market maker (\"DMM\") for that stock to trade the order. The DMM's job is to maintain a two-sided market, making orders to buy and sell the security when there are no other buyers or sellers. If a bid\u2013ask spread exists, no trade immediately takes place \u2013 in this case, the DMM may use their own resources (money or stock) to close the difference. Once a trade has been made, the details are reported on the \"tape\" and sent back to the brokerage firm, which then notifies the investor who placed the order. Computers play an important role, especially for program trading.\nThe NASDAQ is an electronic exchange, where all of the trading is done over a computer network. The process is similar to the New York Stock Exchange. One or more NASDAQ market makers will always provide a bid and ask the price at which they will always purchase or sell 'their' stock.\nThe Paris Bourse, now part of Euronext, is an order-driven, electronic stock exchange. It was automated in the late 1980s. Prior to the 1980s, it consisted of an open outcry exchange. Stockbrokers met on the trading floor of the Palais Brongniart. In 1986, the CATS trading system was introduced, and the order matching system was fully automated.\nPeople trading stock will prefer to trade on the most popular exchange since this gives the largest number of potential counter parties (buyers for a seller, sellers for a buyer) and probably the best price. However, there have always been alternatives such as brokers trying to bring parties together to trade outside the exchange. Some third markets that were popular are Instinet, and later Island and Archipelago (the latter two have since been acquired by Nasdaq and NYSE, respectively). One advantage is that this avoids the commissions of the exchange. However, it also has problems such as adverse selection. Financial regulators have probed dark pools.\n\n\n== Market participant ==\nMarket participants include individual retail investors, institutional investors (e.g., pension funds, insurance companies, mutual funds, index funds, exchange-traded funds, hedge funds, investor groups, banks and various other financial institutions), and also publicly traded corporations trading in their own shares. Robo-advisors, which automate investment for individuals are also major participants.\n\n\n=== Demographics of market participation ===\n\n\n==== Indirect vs. Direct Investment ====\nIndirect investment involves owning shares indirectly, such as via a mutual fund or an exchange traded fund. Direct investment involves direct ownership of shares.Direct ownership of stock by individuals rose slightly from 17.8% in 1992 to 17.9% in 2007, with the median value of these holdings rising from $14,778 to $17,000. Indirect participation in the form of retirement accounts rose from 39.3% in 1992 to 52.6% in 2007, with the median value of these accounts more than doubling from $22,000 to $45,000 in that time. Rydqvist, Spizman, and Strebulaev attribute the differential growth in direct and indirect holdings to differences in the way each are taxed in the United States. Investments in pension funds and 401ks, the two most common vehicles of indirect participation, are taxed only when funds are withdrawn from the accounts. Conversely, the money used to directly purchase stock is subject to taxation as are any dividends or capital gains they generate for the holder. In this way, the current tax code incentivizes individuals to invest indirectly.\n\n\n==== Participation by income and wealth strata ====\nRates of participation and the value of holdings differ significantly across strata of income. In the bottom quintile of income, 5.5% of households directly own stock and 10.7% hold stocks indirectly in the form of retirement accounts. The top decile of income has a direct participation rate of 47.5% and an indirect participation rate in the form of retirement accounts of 89.6%. The median value of directly owned stock in the bottom quintile of income is $4,000 and is $78,600 in the top decile of income as of 2007. The median value of indirectly held stock in the form of retirement accounts for the same two groups in the same year is $6,300 and $214,800 respectively. Since the Great Recession of 2008 households in the bottom half of the income distribution have lessened their participation rate both directly and indirectly from 53.2% in 2007 to 48.8% in 2013, while over the same period households in the top decile of the income distribution slightly increased participation 91.7% to 92.1%. The mean value of direct and indirect holdings at the bottom half of the income distribution moved slightly downward from $53,800 in 2007 to $53,600 in 2013. In the top decile, mean value of all holdings fell from $982,000 to $969,300 in the same time. The mean value of all stock holdings across the entire income distribution is valued at $269,900 as of 2013.\n\n\n==== Participation by race and gender ====\nThe racial composition of stock market ownership shows households headed by whites are nearly four and six times as likely to directly own stocks than households headed by blacks and Hispanics respectively. As of 2011 the national rate of direct participation was 19.6%, for white households the participation rate was 24.5%, for black households it was 6.4% and for Hispanic households it was 4.3%. Indirect participation in the form of 401k ownership shows a similar pattern with a national participation rate of 42.1%, a rate of 46.4% for white households, 31.7% for black households, and 25.8% for Hispanic households. Households headed by married couples participated at rates above the national averages with 25.6% participating directly and 53.4% participating indirectly through a retirement account. 14.7% of households headed by men participated in the market directly and 33.4% owned stock through a retirement account. 12.6% of female-headed households directly owned stock and 28.7% owned stock indirectly.\n\n\n==== Determinants and possible explanations of stock market participation ====\nIn a 2003 paper by Vissing-J\u00f8rgensen attempts to explain disproportionate rates of participation along wealth and income groups as a function of fixed costs associated with investing. Her research concludes that a fixed cost of $200 per year is sufficient to explain why nearly half of all U.S. households do not participate in the market. Participation rates have been shown to strongly correlate with education levels, promoting the hypothesis that information and transaction costs of market participation are better absorbed by more educated households. Behavioral economists Harrison Hong, Jeffrey Kubik and Jeremy Stein suggest that sociability and participation rates of communities have a statistically significant impact on an individual's decision to participate in the market. Their research indicates that social individuals living in states with higher than average participation rates are 5% more likely to participate than individuals that do not share those characteristics. This phenomenon also explained in cost terms. Knowledge of market functioning diffuses through communities and consequently lowers transaction costs associated with investing.\n\n\n== History ==\nIn 12th-century France, the courtiers de change were concerned with managing and regulating the debts of agricultural communities on behalf of the banks. Because these men also traded with debts, they could be called the first brokers. The Italian historian Lodovico Guicciardini described how, in late 13th-century Bruges, commodity traders gathered outdoors at a market square containing an inn owned by a family called Van der Beurze, and in 1409 they became the \"Brugse Beurse\", institutionalizing what had been, until then, an informal meeting. The idea quickly spread around Flanders and neighboring countries and \"Beurzen\" soon opened in Ghent and Rotterdam. International traders, and specially the Italian bankers, present in Bruges since the early 13th-century, took back the word in their countries to define the place for stock market exchange: first the Italians (Borsa), but soon also the French (Bourse), the Germans (b\u00f6rse), Russians (bir\u017ea), Czechs (burza), Swedes (b\u00f6rs), Danes and Norwegians (b\u00f8rs). In most languages, the word coincides with that for money bag, dating back to the Latin bursa, from which obviously also derives the name of the Van der Beurse family.\nIn the middle of the 13th century, Venetian bankers began to trade in government securities. In 1351 the Venetian government outlawed spreading rumors intended to lower the price of government funds. Bankers in Pisa, Verona, Genoa and Florence also began trading in government securities during the 14th century. This was only possible because these were independent city-states not ruled by a duke but a council of influential citizens. Italian companies were also the first to issue shares. Companies in England and the Low Countries followed in the 16th century. Around this time, a joint stock company\u2014one whose stock is owned jointly by the shareholders\u2014emerged and became important for the colonization of what Europeans called the \"New World\".\nThere are now stock markets in virtually every developed and most developing economies, with the world's largest markets being in the United States, United Kingdom, Japan, India, China, Canada, Germany (Frankfurt Stock Exchange), France, South Korea and the Netherlands.\n\n\n== Importance ==\nEven in the days before perestroika, socialism was never a monolith. Within the Communist countries, the spectrum of socialism ranged from the quasi-market, quasi-syndicalist system of Yugoslavia to the centralized totalitarianism of neighboring Albania. One time I asked Professor von Mises, the great expert on the economics of socialism, at what point on this spectrum of statism would he designate a country as \"socialist\" or not. At that time, I wasn't sure that any definite criterion existed to make that sort of clear-cut judgment. And so I was pleasantly surprised at the clarity and decisiveness of Mises's answer. \"A stock market,\" he answered promptly. \"A stock market is crucial to the existence of capitalism and private property. For it means that there is a functioning market in the exchange of private titles to the means of production. There can be no genuine private ownership of capital without a stock market: there can be no true socialism if such a market is allowed to exist.\"\n\n\n=== Function and purpose ===\nThe stock market is one of the most important ways for companies to raise money, along with debt markets which are generally more imposing but do not trade publicly. This allows businesses to be publicly traded, and raise additional financial capital for expansion by selling shares of ownership of the company in a public market. The liquidity that an exchange affords the investors enables their holders to quickly and easily sell securities. This is an attractive feature of investing in stocks, compared to other less liquid investments such as property and other immoveable assets.\nHistory has shown that the price of stocks and other assets is an important part of the dynamics of economic activity, and can influence or be an indicator of social mood. An economy where the stock market is on the rise is considered to be an up-and-coming economy. The stock market is often considered the primary indicator of a country's economic strength and development.Rising share prices, for instance, tend to be associated with increased business investment and vice versa. Share prices also affect the wealth of households and their consumption. Therefore, central banks tend to keep an eye on the control and behavior of the stock market and, in general, on the smooth operation of financial system functions. Financial stability is the raison d'\u00eatre of central banks.Exchanges also act as the clearinghouse for each transaction, meaning that they collect and deliver the shares, and guarantee payment to the seller of a security. This eliminates the risk to an individual buyer or seller that the counterparty could default on the transaction.The smooth functioning of all these activities facilitates economic growth in that lower costs and enterprise risks promote the production of goods and services as well as possibly employment. In this way the financial system is assumed to contribute to increased prosperity, although some controversy exists as to whether the optimal financial system is bank-based or market-based.Recent events such as the Global Financial Crisis have prompted a heightened degree of scrutiny of the impact of the structure of stock markets (called market microstructure), in particular to the stability of the financial system and the transmission of systemic risk.\n\n\n=== Relation to the modern financial system ===\nA transformation is the move to electronic trading to replace human trading of listed securities.\n\n\n=== Behavior of stock prices ===\n\nChanges in stock prices are mostly caused by external factors such as socioeconomic conditions, inflation, exchange rates. Intellectual capital does not affect a company stock's current earnings. Intellectual capital contributes to a stock's return growth.The efficient-market hypothesis (EMH) is a hypothesis in financial economics that states that asset prices reflect all available information at the current time.\nThe 'hard' efficient-market hypothesis does not explain the cause of events such as the crash in 1987, when the Dow Jones Industrial Average plummeted 22.6 percent\u2014the largest-ever one-day fall in the United States.This event demonstrated that share prices can fall dramatically even though no generally agreed upon definite cause has been found: a thorough search failed to detect any 'reasonable' development that might have accounted for the crash. (Note that such events are predicted to occur strictly by randomness, although very rarely.) It seems also to be true more generally that many price movements (beyond those which are predicted to occur 'randomly') are not occasioned by new information; a study of the fifty largest one-day share price movements in the United States in the post-war period seems to confirm this.A 'soft' EMH has emerged which does not require that prices remain at or near equilibrium, but only that market participants cannot systematically profit from any momentary 'market anomaly'. Moreover, while EMH predicts that all price movement (in the absence of change in fundamental information) is random (i.e. non-trending), many studies have shown a marked tendency for the stock market to trend over time periods of weeks or longer. Various explanations for such large and apparently non-random price movements have been promulgated. For instance, some research has shown that changes in estimated risk, and the use of certain strategies, such as stop-loss limits and value at risk limits, theoretically could cause financial markets to overreact. But the best explanation seems to be that the distribution of stock market prices is non-Gaussian (in which case EMH, in any of its current forms, would not be strictly applicable).Other research has shown that psychological factors may result in exaggerated (statistically anomalous) stock price movements (contrary to EMH which assumes such behaviors 'cancel out'). Psychological research has demonstrated that people are predisposed to 'seeing' patterns, and often will perceive a pattern in what is, in fact, just noise, e.g. seeing familiar shapes in clouds or ink blots. In the present context, this means that a succession of good news items about a company may lead investors to overreact positively, driving the price up. A period of good returns also boosts the investors' self-confidence, reducing their (psychological) risk threshold.Another phenomenon\u2014also from psychology\u2014that works against an objective assessment is group thinking. As social animals, it is not easy to stick to an opinion that differs markedly from that of a majority of the group. An example with which one may be familiar is the reluctance to enter a restaurant that is empty; people generally prefer to have their opinion validated by those of others in the group.\nIn one paper the authors draw an analogy with gambling. In normal times the market behaves like a game of roulette; the probabilities are known and largely independent of the investment decisions of the different players. In times of market stress, however, the game becomes more like poker (herding behavior takes over). The players now must give heavy weight to the psychology of other investors and how they are likely to react psychologically.Stock markets play an essential role in growing industries that ultimately affect the economy through transferring available funds from units that have excess funds (savings) to those who are suffering from funds deficit (borrowings) (Padhi and Naik, 2012). In other words, capital markets facilitate funds movement between the above-mentioned units. This process leads to the enhancement of available financial resources which in turn affects the economic growth positively.\nEconomic and financial theories argue that stock prices are affected by macroeconomic trends. Macroeconomic trends include such as changes in GDP, unemployment rates, national income, price indices, output, consumption, unemployment, inflation, saving, investment, energy, international trade, immigration, productivity, aging populations, innovations, international finance. increasing corporate profit, increasing profit margins, higher concentration of business, lower company income, less vigorous activity, less progress, lower investment rates, lower productivity growth, less employee share of corporate revenues, decreasing Worker to Beneficiary ratio (year 1960 5:1, year 2009 3:1, year 2030 2.2:1), increasing female to male ratio college graduates.\n\n\n=== Irrational behavior ===\nSometimes, the market seems to react irrationally to economic or financial news, even if that news is likely to have no real effect on the fundamental value of securities itself. However, this market behaviour may be more apparent than real, since often such news was anticipated, and a counter reaction may occur if the news is better (or worse) than expected. Therefore, the stock market may be swayed in either direction by press releases, rumors, euphoria and mass panic.\nOver the short-term, stocks and other securities can be battered or bought by any number of fast market-changing events, making the stock market behavior difficult to predict. Emotions can drive prices up and down, people are generally not as rational as they think, and the reasons for buying and selling are generally accepted.\nBehaviorists argue that investors often behave irrationally when making investment decisions thereby incorrectly pricing securities, which causes market inefficiencies, which, in turn, are opportunities to make money. However, the whole notion of EMH is that these non-rational reactions to information cancel out, leaving the prices of stocks rationally determined.\n\n\n=== Crashes ===\n\nA stock market crash is often defined as a sharp dip in share prices of stocks listed on the stock exchanges. In parallel with various economic factors, a reason for stock market crashes is also due to panic and investing public's loss of confidence. Often, stock market crashes end speculative economic bubbles.\nThere have been famous stock market crashes that have ended in the loss of billions of dollars and wealth destruction on a massive scale. An increasing number of people are involved in the stock market, especially since the social security and retirement plans are being increasingly privatized and linked to stocks and bonds and other elements of the market. There have been a number of famous stock market crashes like the Wall Street Crash of 1929, the stock market crash of 1973\u20134, the Black Monday of 1987, the Dot-com bubble of 2000, and the Stock Market Crash of 2008.\n\n\n=== 1929 ===\nOne of the most famous stock market crashes started October 24, 1929, on Black Thursday. The Dow Jones Industrial Average lost 50% during this stock market crash. It was the beginning of the Great Depression.\n\n\n=== 1987 ===\nAnother famous crash took place on October 19, 1987 \u2013 Black Monday. The crash began in Hong Kong and quickly spread around the world.\nBy the end of October, stock markets in Hong Kong had fallen 45.5%, Australia 41.8%, Spain 31%, the United Kingdom 26.4%, the United States 22.68%, and Canada 22.5%. Black Monday itself was the largest one-day percentage decline in stock market history \u2013 the Dow Jones fell by 22.6% in a day. The names \"Black Monday\" and \"Black Tuesday\" are also used for October 28\u201329, 1929, which followed Terrible Thursday\u2014the starting day of the stock market crash in 1929.\nThe crash in 1987 raised some puzzles \u2013 main news and events did not predict the catastrophe and visible reasons for the collapse were not identified. This event raised questions about many important assumptions of modern economics, namely, the theory of rational human conduct, the theory of market equilibrium and the efficient-market hypothesis. For some time after the crash, trading in stock exchanges worldwide was halted, since the exchange computers did not perform well owing to enormous quantity of trades being received at one time. This halt in trading allowed the Federal Reserve System and central banks of other countries to take measures to control the spreading of worldwide financial crisis. In the United States the SEC introduced several new measures of control into the stock market in an attempt to prevent a re-occurrence of the events of Black Monday.\n\n\n=== 2007-2009 ===\nThis marked the beginning of the Great Recession. Starting in 2007 and lasting through 2009, financial markets experienced one of the sharpest declines in decades. It was more widespread than just the stock market as well. The housing market, lending market, and even global trade experienced unimaginable decline. Sub-prime lending led to the housing bubble bursting and was made famous by movies like The Big Short where those holding large mortgages were unwittingly falling prey to lenders. This saw banks and major financial institutions completely fail in many cases and took major government intervention to remedy during the period. From October 2007 to March 2009, the S&P 500 fell 57% and wouldn't recover to its 2007 levels until April 2013.\n\n\n=== 2020 ===\nThe 2020 stock market crash was a major and sudden global stock market crash that began on 20 February 2020 and ended on 7 April. This market crash was due to the sudden outbreak of the global pandemic, COVID-19. The crash ended with a new deal that had a positive impact on the market.\n\n\n=== Circuit breakers ===\nSince the early 1990s, many of the largest exchanges have adopted electronic 'matching engines' to bring together buyers and sellers, replacing the open outcry system. Electronic trading now accounts for the majority of trading in many developed countries. Computer systems were upgraded in the stock exchanges to handle larger trading volumes in a more accurate and controlled manner. The SEC modified the margin requirements in an attempt to lower the volatility of common stocks, stock options and the futures market. The New York Stock Exchange and the Chicago Mercantile Exchange introduced the concept of a circuit breaker. The circuit breaker halts trading if the Dow declines a prescribed number of points for a prescribed amount of time. In February 2012, the Investment Industry Regulatory Organization of Canada (IIROC) introduced single-stock circuit breakers.\nNew York Stock Exchange (NYSE) circuit breakers\n\n\n== Stock market index ==\n\nThe movements of the prices in global, regional or local markets are captured in price indices called stock market indices, of which there are many, e.g. the S&P, the FTSE, the Euronext indices and the NIFTY & SENSEX of India. Such indices are usually market capitalization weighted, with the weights reflecting the contribution of the stock to the index. The constituents of the index are reviewed frequently to include/exclude stocks in order to reflect the changing business environment.\n\n\n== Derivative instruments ==\n\nFinancial innovation has brought many new financial instruments whose pay-offs or values depend on the prices of stocks. Some examples are exchange-traded funds (ETFs), stock index and stock options, equity swaps, single-stock futures, and stock index futures. These last two may be traded on futures exchanges (which are distinct from stock exchanges\u2014their history traces back to commodity futures exchanges), or traded over-the-counter. As all of these products are only derived from stocks, they are sometimes considered to be traded in a (hypothetical) derivatives market, rather than the (hypothetical) stock market.\n\n\n== Leveraged strategies ==\nStock that a trader does not actually own may be traded using short selling; margin buying may be used to purchase stock with borrowed funds; or, derivatives may be used to control large blocks of stocks for a much smaller amount of money than would be required by outright purchase or sales.\n\n\n=== Short selling ===\n\nIn short selling, the trader borrows stock (usually from his brokerage which holds its clients shares or its own shares on account to lend to short sellers) then sells it on the market, betting that the price will fall. The trader eventually buys back the stock, making money if the price fell in the meantime and losing money if it rose. Exiting a short position by buying back the stock is called \"covering\". This strategy may also be used by unscrupulous traders in illiquid or thinly traded markets to artificially lower the price of a stock. Hence most markets either prevent short selling or place restrictions on when and how a short sale can occur. The practice of naked shorting is illegal in most (but not all) stock markets.\n\n\n=== Margin buying ===\n\nIn margin buying, the trader borrows money (at interest) to buy a stock and hopes for it to rise. Most industrialized countries have regulations that require that if the borrowing is based on collateral from other stocks the trader owns outright, it can be a maximum of a certain percentage of those other stocks' value. In the United States, the margin requirements have been 50% for many years (that is, if you want to make a $1000 investment, you need to put up $500, and there is often a maintenance margin below the $500).\nA margin call is made if the total value of the investor's account cannot support the loss of the trade. (Upon a decline in the value of the margined securities additional funds may be required to maintain the account's equity, and with or without notice the margined security or any others within the account may be sold by the brokerage to protect its loan position. The investor is responsible for any shortfall following such forced sales.)\nRegulation of margin requirements (by the Federal Reserve) was implemented after the Crash of 1929. Before that, speculators typically only needed to put up as little as 10 percent (or even less) of the total investment represented by the stocks purchased. Other rules may include the prohibition of free-riding: putting in an order to buy stocks without paying initially (there is normally a three-day grace period for delivery of the stock), but then selling them (before the three-days are up) and using part of the proceeds to make the original payment (assuming that the value of the stocks has not declined in the interim).\n\n\n== Types of financial markets ==\nFinancial markets can be divided into different subtypes:\n\n\n=== For the assets transferred ===\nMoney market : It is traded with money or financial assets with short-term maturity and high liquidity, generally assets with a term of less than one year.\nCapital market : Financial assets with medium and long-term maturity are traded, which are basic for carrying out certain investment processes.\n\n\n=== Depending on its structure ===\nOrganized market\nNon-organized markets denominated in English (\" Over The Counter \").\n\n\n=== According to the negotiation phase of financial assets ===\nPrimary market : Financial assets are created. In this market, assets are transmitted directly by their issuer.\nSecondary market : Only existing financial assets are exchanged, which were issued at a previous time. This market allows holders of financial assets to sell instruments that were already issued in the primary market (or that had already been transmitted in the secondary market) and that are in their possession, or to buy other financial assets.\n\n\n=== According to the geographical perspective ===\nNational markets. The currency in which the financial assets are denominated and the residence of those involved is national.\nInternational markets. The markets situated outside a country's geographical area.\n\n\n=== According to the type of asset traded ===\nTraditional market. In which financial assets such as demand deposits, stocks or bonds are traded.\nAlternative market. In which alternative financial assets are traded such as portfolio investments, promissory notes, factoring, real estate (e.g. through fiduciary rights), in private equity funds, venture capital funds, hedge funds, investment projects (e.g. infrastructure, cinema, etc.) among many others.\n\n\n=== Other markets ===\nCommodity markets, which allow the trading of commodities\nDerivatives markets, which provide instruments for managing financial risk\nForward markets, which provide standardized forward contracts to trade products at a future date\nInsurance markets, which allows the redistribution of varied risks\nForeign exchange market, which allows the exchange of foreign currencies\n\n\n== Investment strategies ==\n\nMany strategies can be classified as either fundamental analysis or technical analysis. Fundamental analysis refers to analyzing companies by their financial statements found in SEC filings, business trends, and general economic conditions. Technical analysis studies price actions in markets through the use of charts and quantitative techniques to attempt to forecast price trends based on historical performance, regardless of the company's financial prospects. One example of a technical strategy is the Trend following method, used by John W. Henry and Ed Seykota, which uses price patterns and is also rooted in risk management and diversification.\nAdditionally, many choose to invest via passive index funds. In this method, one holds a portfolio of the entire stock market or some segment of the stock market (such as the S&P 500 Index or Wilshire 5000). The principal aim of this strategy is to maximize diversification, minimize taxes from realizing gains, and ride the general trend of the stock market to rise.\nResponsible investment emphasizes and requires a long-term horizon on the basis of fundamental analysis only, avoiding hazards in the expected return of the investment. Socially responsible investing is another investment preference.\n\n\n== Taxation ==\nTaxation is a consideration of all investment strategies; profit from owning stocks, including dividends received, is subject to different tax rates depending on the type of security and the holding period. Most profit from stock investing is taxed via a capital gains tax. In many countries, the corporations pay taxes to the government and the shareholders once again pay taxes when they profit from owning the stock, known as \"double taxation\".\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading =="}, {"id": 54, "title": "Cars", "content": "A car, or an automobile, is a motor vehicle with wheels. Most definitions of cars state that they run primarily on roads, seat one to eight people, have four wheels, and mainly transport people, not cargo. French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while French-born Swiss inventor Fran\u00e7ois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.\nThe modern car\u2014a practical, marketable automobile for everyday use\u2014was invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen. Commercial cars became widely available during the 20th century. One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced horse-drawn carriages. In Europe and other parts of the world, demand for automobiles did not increase until after World War II. The car is considered an essential part of the developed economy.\nCars have controls for driving, parking, passenger comfort, and a variety of lamps. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex. These include rear-reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the early 2020s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than petrol-driven cars before 2025. The transition from fossil fuels to electric cars features prominently in most climate change mitigation scenarios, such as Project Drawdown's 100 actionable solutions for climate change.There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance. The costs to society include maintaining roads, land use, road congestion, air pollution, noise pollution, public health, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide. Personal benefits include on-demand transportation, mobility, independence, and convenience. Societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies. There are around one billion cars in use worldwide. Car usage is increasing rapidly, especially in China, India, and other newly industrialized countries.\n\n\n== Etymology ==\nThe English word car is believed to originate from Latin carrus/carrum \"wheeled vehicle\" or (via Old North French) Middle English carre \"two-wheeled cart\", both of which in turn derive from Gaulish karros \"chariot\". It originally referred to any wheeled horse-drawn vehicle, such as a cart, carriage, or wagon.\"Motor car\", attested from 1895, is the usual formal term in British English. \"Autocar\", a variant likewise attested from 1895 and literally meaning \"self-propelled car\", is now considered archaic. \"Horseless carriage\" is attested from 1895.\"Automobile\", a classical compound derived from Ancient Greek aut\u00f3s (\u03b1\u1f50\u03c4\u03cc\u03c2) \"self\" and Latin mobilis \"movable\", entered English from French and was first adopted by the Automobile Club of Great Britain in 1897. It fell out of favour in Britain and is now used chiefly in North America, where the abbreviated form \"auto\" commonly appears as an adjective in compound formations like \"auto industry\" and \"auto mechanic\".\n\n\n== History ==\nThe first steam-powered vehicle was designed by Ferdinand Verbiest, a Flemish member of a Jesuit mission in China around 1672. It was a 65-centimetre-long (26 in) scale-model toy for the Kangxi Emperor that was unable to carry a driver or a passenger. It is not known with certainty if Verbiest's model was successfully built or run.Nicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle in about 1769; he created a steam-powered tricycle. He also constructed two steam tractors for the French Army, one of which is preserved in the French National Conservatory of Arts and Crafts. His inventions were limited by problems with water supply and maintaining steam pressure. In 1801, Richard Trevithick built and demonstrated his Puffing Devil road locomotive, believed by many to be the first demonstration of a steam-powered road vehicle. It was unable to maintain sufficient steam pressure for long periods and was of little practical use.\nThe development of external combustion (steam) engines is detailed as part of the history of the car but often treated separately from the development of true cars. A variety of steam-powered road vehicles were used during the first part of the 19th century, including steam cars, steam buses, phaetons, and steam rollers. In the United Kingdom, sentiment against them led to the Locomotive Acts of 1865.\nIn 1807, Nic\u00e9phore Ni\u00e9pce and his brother Claude created what was probably the world's first internal combustion engine (which they called a Pyr\u00e9olophore), but installed it in a boat on the river Saone in France. Coincidentally, in 1807, the Swiss inventor Fran\u00e7ois Isaac de Rivaz designed his own \"de Rivaz internal combustion engine\", and used it to develop the world's first vehicle to be powered by such an engine. The Ni\u00e9pces' Pyr\u00e9olophore was fuelled by a mixture of Lycopodium powder (dried spores of the Lycopodium plant), finely crushed coal dust and resin that were mixed with oil, whereas de Rivaz used a mixture of hydrogen and oxygen. Neither design was successful, as was the case with others, such as Samuel Brown, Samuel Morey, and Etienne Lenoir, who each built vehicles (usually adapted carriages or carts) powered by internal combustion engines.In November 1881, French inventor Gustave Trouv\u00e9 demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity. Although several other German engineers (including Gottlieb Daimler, Wilhelm Maybach, and Siegfried Marcus) were working on cars at about the same time, the year 1886 is regarded as the birth year of the modern car\u2014a practical, marketable automobile for everyday use\u2014when the German Carl Benz patented his Benz Patent-Motorwagen; he is generally acknowledged as the inventor of the car.In 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a cheaper model. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz car to his line of products. Because France was more open to the early cars, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888, Bertha Benz, the wife of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\nIn 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor. During the last years of the 19th century, Benz was the largest car company in the world with 572 units produced in 1899 and, because of its size, Benz & Cie., became a joint-stock company. The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Pr\u00e4sident automobil.\nDaimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890, and sold their first car in 1892 under the brand name Daimler. It was a horse-drawn stagecoach built by another manufacturer, which they retrofitted with an engine of their design. By 1895, about 30 vehicles had been built by Daimler and Maybach, either at the Daimler works or in the Hotel Hermann, where they set up shop after disputes with their backers. Benz, Maybach, and the Daimler team seem to have been unaware of each other's early work. They never worked together; by the time of the merger of the two companies, Daimler and Maybach were no longer part of DMG. Daimler died in 1900 and later that year, Maybach designed an engine named Daimler-Mercedes that was placed in a specially ordered model built to specifications set by Emil Jellinek. This was a production of a small number of vehicles for Jellinek to race and market in his country. Two years later, in 1902, a new model DMG car was produced and the model was named Mercedes after the Maybach engine, which generated 35 hp. Maybach quit DMG shortly thereafter and opened a business of his own. Rights to the Daimler brand name were sold to other manufacturers.\nIn 1890, \u00c9mile Levassor and Armand Peugeot of France began producing vehicles with Daimler engines, and so laid the foundation of the automotive industry in France. In 1891, Auguste Doriot and his Peugeot colleague Louis Rigoulot completed the longest trip by a petrol-driven vehicle when their self-designed and built Daimler powered Peugeot Type 3 completed 2,100 kilometres (1,300 mi) from Valentigney to Paris and Brest and back again. They were attached to the first Paris\u2013Brest\u2013Paris bicycle race, but finished six days after the winning cyclist, Charles Terront.\nThe first design for an American car with a petrol internal combustion engine was made in 1877 by George Selden of Rochester, New York. Selden applied for a patent for a car in 1879, but the patent application expired because the vehicle was never built. After a delay of 16 years and a series of attachments to his application, on 5 November 1895, Selden was granted a US patent (U.S. Patent 549,160) for a two-stroke car engine, which hindered, more than encouraged, development of cars in the United States. His patent was challenged by Henry Ford and others, and overturned in 1911.\nIn 1893, the first running, petrol-driven American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts. The first public run of the Duryea Motor Wagon took place on 21 September 1893, on Taylor Street in Metro Center Springfield. Studebaker, subsidiary of a long-established wagon and coach manufacturer, started to build cars in 1897:\u200a66\u200a and commenced sales of electric vehicles in 1902 and petrol vehicles in 1904.In Britain, there had been several attempts to build steam cars with varying degrees of success, with Thomas Rickett even attempting a production run in 1860. Santler from Malvern is recognized by the Veteran Car Club of Great Britain as having made the first petrol-driven car in the country in 1894, followed by Frederick William Lanchester in 1895, but these were both one-offs. The first production vehicles in Great Britain came from the Daimler Company, a company founded by Harry J. Lawson in 1896, after purchasing the right to use the name of the engines. Lawson's company made its first car in 1897, and they bore the name Daimler.In 1892, German engineer Rudolf Diesel was granted a patent for a \"New Rational Combustion Engine\". In 1897, he built the first diesel engine. Steam-, electric-, and petrol-driven vehicles competed for a few decades, with petrol internal combustion engines achieving dominance in the 1910s. Although various pistonless rotary engine designs have attempted to compete with the conventional piston and crankshaft design, only Mazda's version of the Wankel engine has had more than very limited success.\nAll in all, it is estimated that over 100,000 patents created the modern automobile and motorcycle.\n\n\n== Mass production ==\n\nLarge-scale, production-line manufacturing of affordable cars was started by Ransom Olds in 1901 at his Oldsmobile factory in Lansing, Michigan, and based upon stationary assembly line techniques pioneered by Marc Isambard Brunel at the Portsmouth Block Mills, England, in 1802. The assembly line style of mass production and interchangeable parts had been pioneered in the US by Thomas Blanchard in 1821, at the Springfield Armory in Springfield, Massachusetts. This concept was greatly expanded by Henry Ford, beginning in 1913 with the world's first moving assembly line for cars at the Highland Park Ford Plant.\nAs a result, Ford's cars came off the line in 15-minute intervals, much faster than previous methods, increasing productivity eightfold, while using less manpower (from 12.5 manhours to 1 hour 33 minutes). It was so successful, paint became a bottleneck. Only Japan black would dry fast enough, forcing the company to drop the variety of colors available before 1913, until fast-drying Duco lacquer was developed in 1926. This is the source of Ford's apocryphal remark, \"any color as long as it's black\". In 1914, an assembly line worker could buy a Model T with four months' pay.Ford's complex safety procedures\u2014especially assigning each worker to a specific location instead of allowing them to roam about\u2014dramatically reduced the rate of injury. The combination of high wages and high efficiency is called \"Fordism\" and was copied by most major industries. The efficiency gains from the assembly line also coincided with the economic rise of the US. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.\nIn the automotive industry, its success was dominating, and quickly spread worldwide seeing the founding of Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany 1925; in 1921, Citro\u00ebn was the first native European manufacturer to adopt the production method. Soon, companies had to have assembly lines, or risk going broke; by 1930, 250 companies which did not, had disappeared.Development of automotive technology was rapid, due in part to the hundreds of small manufacturers competing to gain the world's attention. Key developments included electric ignition and the electric self-starter (both by Charles Kettering, for the Cadillac Motor Company in 1910\u20131911), independent suspension, and four-wheel brakes.\nSince the 1920s, nearly all cars have been mass-produced to meet market needs, so marketing plans often have heavily influenced car design. It was Alfred P. Sloan who established the idea of different makes of cars produced by one company, called the General Motors Companion Make Program, so that buyers could \"move up\" as their fortunes improved.\nReflecting the rapid pace of change, makes shared parts with one another so larger production volume resulted in lower costs for each price range. For example, in the 1930s, LaSalles, sold by Cadillac, used cheaper mechanical parts made by Oldsmobile; in the 1950s, Chevrolet shared bonnet, doors, roof, and windows with Pontiac; by the 1990s, corporate powertrains and shared platforms (with interchangeable brakes, suspension, and other parts) were common. Even so, only major makers could afford high costs, and even companies with decades of production, such as Apperson, Cole, Dorris, Haynes, or Premier, could not manage: of some two hundred American car makers in existence in 1920, only 43 survived in 1930, and with the Great Depression, by 1940, only 17 of those were left.In Europe, much the same would happen. Morris set up its production line at Cowley in 1924, and soon outsold Ford, while beginning in 1923 to follow Ford's practice of vertical integration, buying Hotchkiss (engines), Wrigley (gearboxes), and Osberton (radiators), for instance, as well as competitors, such as Wolseley: in 1925, Morris had 41 per cent of total British car production. Most British small-car assemblers, from Abbey to Xtra, had gone under. Citro\u00ebn did the same in France, coming to cars in 1919; between them and other cheap cars in reply such as Renault's 10CV and Peugeot's 5CV, they produced 550,000 cars in 1925, and Mors, Hurtu, and others could not compete. Germany's first mass-manufactured car, the Opel 4PS Laubfrosch (Tree Frog), came off the line at R\u00fcsselsheim in 1924, soon making Opel the top car builder in Germany, with 37.5 per cent of the market.In Japan, car production was very limited before World War II. Only a handful of companies were producing vehicles in limited numbers, and these were small, three-wheeled for commercial uses, like Daihatsu, or were the result of partnering with European companies, like Isuzu building the Wolseley A-9 in 1922. Mitsubishi was also partnered with Fiat and built the Mitsubishi Model A based on a Fiat vehicle. Toyota, Nissan, Suzuki, Mazda, and Honda began as companies producing non-automotive products before the war, switching to car production during the 1950s. Kiichiro Toyoda's decision to take Toyoda Loom Works into automobile manufacturing would create what would eventually become Toyota Motor Corporation, the largest automobile manufacturer in the world. Subaru, meanwhile, was formed from a conglomerate of six companies who banded together as Fuji Heavy Industries, as a result of having been broken up under keiretsu legislation.\n\n\n== Fuel and propulsion technologies ==\n\nThe transport sector is a major contributor to air pollution, noise pollution and climate change.Most cars in use in the early 2020s run on petrol burnt in an internal combustion engine (ICE). The International Organization of Motor Vehicle Manufacturers says that, in countries that mandate low sulphur motor spirit, petrol-fuelled cars built to late 2010s standards (such as Euro-6) emit very little local air pollution. Some cities ban older petrol-driven cars and some countries plan to ban sales in future. However, some environmental groups say this phase-out of fossil fuel vehicles must be brought forwards to limit climate change. Production of petrol-fuelled cars peaked in 2017.Other hydrocarbon fossil fuels also burnt by deflagration (rather than detonation) in ICE cars include diesel, autogas, and CNG. Removal of fossil fuel subsidies, concerns about oil dependence, tightening environmental laws and restrictions on greenhouse gas emissions are propelling work on alternative power systems for cars. This includes hybrid vehicles, plug-in electric vehicles and hydrogen vehicles. Out of all cars sold in 2021, nine per cent were electric, and by the end of that year there were more than 16 million electric cars on the world's roads. Despite rapid growth, less than two per cent of cars on the world's roads were fully electric and plug-in hybrid cars by the end of 2021. Cars for racing or speed records have sometimes employed jet or rocket engines, but these are impractical for common use.\nOil consumption has increased rapidly in the 20th and 21st centuries because there are more cars; the 1980s oil glut even fuelled the sales of low-economy vehicles in OECD countries. The BRIC countries are adding to this consumption.\nAs of 2023 few production cars use wheel hub motors.\n\n\n=== Batteries ===\n\nIn almost all hybrid (even mild hybrid) and pure electric cars regenerative braking recovers and returns to a battery some energy which would otherwise be wasted by friction brakes getting hot. Although all cars must have friction brakes (front disc brakes and either disc or drum rear brakes) for emergency stops, regenerative braking improves efficiency, particularly in city driving.\n\n\n== User interface ==\n\nCars are equipped with controls used for driving, passenger comfort, and safety, normally operated by a combination of the use of feet and hands, and occasionally by voice on 21st-century cars. These controls include a steering wheel, pedals for operating the brakes and controlling the car's speed (and, in a manual transmission car, a clutch pedal), a shift lever or stick for changing gears, and a number of buttons and dials for turning on lights, ventilation, and other functions. Modern cars' controls are now standardized, such as the location for the accelerator and brake, but this was not always the case. Controls are evolving in response to new technologies, for example, the electric car and the integration of mobile communications.\nSome of the original controls are no longer required. For example, all cars once had controls for the choke valve, clutch, ignition timing, and a crank instead of an electric starter. However, new controls have also been added to vehicles, making them more complex. These include air conditioning, navigation systems, and in-car entertainment. Another trend is the replacement of physical knobs and switches by secondary controls with touchscreen controls such as BMW's iDrive and Ford's MyFord Touch. Another change is that while early cars' pedals were physically linked to the brake mechanism and throttle, in the early 2020s, cars have increasingly replaced these physical linkages with electronic controls.\n\n\n== Electronics and interior ==\nCars are typically equipped with interior lighting which can be toggled manually or be set to light up automatically with doors open, an entertainment system which originated from car radios, sideways windows which can be lowered or raised electrically (manually on earlier cars), and one or multiple auxiliary power outlets for supplying portable appliances such as mobile phones, portable fridges, power inverters, and electrical air pumps from the on-board electrical system. More costly upper-class and luxury cars are equipped with features earlier such as massage seats and collision avoidance systems.Dedicated automotive fuses and circuit breakers prevent damage from electrical overload.\n\n\n== Lighting ==\n\nCars are typically fitted with multiple types of lights. These include headlights, which are used to illuminate the way ahead and make the car visible to other users, so that the vehicle can be used at night; in some jurisdictions, daytime running lights; red brake lights to indicate when the brakes are applied; amber turn signal lights to indicate the turn intentions of the driver; white-colored reverse lights to illuminate the area behind the car (and indicate that the driver will be or is reversing); and on some vehicles, additional lights (e.g., side marker lights) to increase the visibility of the car. Interior lights on the ceiling of the car are usually fitted for the driver and passengers. Some vehicles also have a boot light and, more rarely, an engine compartment light.\n\n\n== Weight ==\nDuring the late 20th and early 21st century, cars increased in weight due to batteries, modern steel safety cages, anti-lock brakes, airbags, and \"more-powerful\u2014if more efficient\u2014engines\" and, as of 2019, typically weigh between 1 and 3 tonnes (1.1 and 3.3 short tons; 0.98 and 2.95 long tons). Heavier cars are safer for the driver from a crash perspective, but more dangerous for other vehicles and road users. The weight of a car influences fuel consumption and performance, with more weight resulting in increased fuel consumption and decreased performance. The Wuling Hongguang Mini EV, a typical city car, weighs about 700 kilograms (1,500 lb). Heavier cars include SUVs and extended-length SUVs like the Suburban.\nSome places tax heavier cars more: as well as improving pedestrian safety this can encourage manufacturers to use materials such as recycled aluminium instead of steel. It has been suggested that one benefit of subsidizing charging infrastructure is that cars can use lighter batteries.\n\n\n== Seating and body style ==\n\nMost cars are designed to carry multiple occupants, often with four or five seats. Cars with five seats typically seat two passengers in the front and three in the rear. Full-size cars and large sport utility vehicles can often carry six, seven, or more occupants depending on the arrangement of the seats. On the other hand, sports cars are most often designed with only two seats. Utility vehicles like pickup trucks, combine seating with extra cargo or utility functionality. The differing needs for passenger capacity and their luggage or cargo space has resulted in the availability of a large variety of body styles to meet individual consumer requirements that include, among others, the sedan/saloon, hatchback, station wagon/estate, coupe, and minivan.\n\n\n== Safety ==\n\nTraffic collisions are the largest cause of injury-related deaths worldwide. Mary Ward became one of the first documented car fatalities in 1869 in Parsonstown, Ireland, and Henry Bliss one of the US's first pedestrian car casualties in 1899 in New York City. There are now standard tests for safety in new cars, such as the Euro and US NCAP tests, and insurance-industry-backed tests by the Insurance Institute for Highway Safety (IIHS).\n\n\n== Costs and benefits ==\n\nThe costs of car usage, which may include the cost of: acquiring the vehicle, repairs and auto maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance, are weighed against the cost of the alternatives, and the value of the benefits\u2014perceived and real\u2014of vehicle usage. The benefits may include on-demand transportation, mobility, independence, and convenience, and emergency power. During the 1920s, cars had another benefit: \"[c]ouples finally had a way to head off on unchaperoned dates, plus they had a private space to snuggle up close at the end of the night.\"Similarly the costs to society of car use may include; maintaining roads, land use, air pollution, noise pollution, road congestion, public health, health care, and of disposing of the vehicle at the end of its life; and can be balanced against the value of the benefits to society that car use generates. Societal benefits may include: economy benefits, such as job and wealth creation, of car production and maintenance, transportation provision, society wellbeing derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability of humans to move flexibly from place to place has far-reaching implications for the nature of societies.\n\n\n== Environmental effects ==\n\nCars are a major cause of urban air pollution, with all types of cars producing dust from brakes, tyres, and road wear, although these may be limited by vehicle emission standards. While there are different ways to power cars, most rely on petrol or diesel, and they consume almost a quarter of world oil production as of 2019. Both petrol and diesel cars pollute more than electric cars. Cars and vans caused 8% of direct carbon dioxide emissions in 2021. As of 2021, due to greenhouse gases emitted during battery production, electric cars must be driven tens of thousands of kilometers before their lifecycle carbon emissions are less than fossil fuel cars; however this varies considerably and is expected to improve in future due to lower carbon electricity, and longer lasting batteries produced in larger factories. Many governments use fiscal policies, such as road tax, to discourage the purchase and use of more polluting cars; and many cities are doing the same with low-emission zones. Fuel taxes may act as an incentive for the production of more efficient, hence less polluting, car designs (e.g., hybrid vehicles) and the development of alternative fuels. High fuel taxes or cultural change may provide a strong incentive for consumers to purchase lighter, smaller, more fuel-efficient cars, or to not drive.The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 millionkm (1.2 millionmiles) if driven a lot. According to the International Energy Agency the average rated fuel consumption of new light-duty vehicles fell by only 0.9% between 2017 and 2019, far smaller than the 1.8% annual average reduction between 2010 and 2015. Given slow progress to date, the IEA estimates fuel consumption will have to decrease by 4.3% per year on average from 2019 to 2030. The increase in sales of SUVs is bad for fuel economy. Many cities in Europe have banned older fossil fuel cars and all fossil fuel vehicles will be banned in Amsterdam from 2030. Many Chinese cities limit licensing of fossil fuel cars, and many countries plan to stop selling them between 2025 and 2050.The manufacture of vehicles is resource intensive, and many manufacturers now report on the environmental performance of their factories, including energy usage, waste and water consumption. Manufacturing each kWh of battery emits a similar amount of carbon as burning through one full tank of petrol. The growth in popularity of the car allowed cities to sprawl, therefore encouraging more travel by car, resulting in inactivity and obesity, which in turn can lead to increased risk of a variety of diseases.Animals and plants are often negatively affected by cars via habitat destruction and pollution. Over the lifetime of the average car, the \"loss of habitat potential\" may be over 50,000 square metres (540,000 sq ft) based on primary production correlations. Animals are also killed every year on roads by cars, referred to as roadkill. More recent road developments are including significant environmental mitigation in their designs, such as green bridges (designed to allow wildlife crossings) and creating wildlife corridors.\nGrowth in the popularity of cars and commuting has led to traffic congestion. Moscow, Istanbul, Bogot\u00e1, Mexico City and S\u00e3o Paulo were the world's most congested cities in 2018 according to INRIX, a data analytics company.\n\n\n== Social issues ==\nMass production of personal motor vehicles in the United States and other developed countries with extensive territories such as Australia, Argentina, and France vastly increased individual and group mobility and greatly increased and expanded economic development in urban, suburban, exurban and rural areas.In the United States, the transport divide and car dependency resulting from domination of car-based transport systems presents barriers to employment in low-income neighbourhoods, with many low-income individuals and families forced to run cars they cannot afford in order to maintain their income. The historic commitment to a car-based transport system continued during the presidency of Joe Biden. Dependency on automobiles by African Americans may result in exposure to the hazards of driving while black and other types of racial discrimination related to buying, financing and insuring them.\n\n\n== Emerging car technologies ==\nAlthough intensive development of conventional battery electric vehicles is continuing into the 2020s, other car propulsion technologies that are under development include wireless charging, hydrogen cars, and hydrogen/electric hybrids. Research into alternative forms of power includes using ammonia instead of hydrogen in fuel cells.New materials which may replace steel car bodies include aluminium, fiberglass, carbon fiber, biocomposites, and carbon nanotubes. Telematics technology is allowing more and more people to share cars, on a pay-as-you-go basis, through car share and carpool schemes. Communication is also evolving due to connected car systems.\n\n\n=== Autonomous car ===\n\nFully autonomous vehicles, also known as driverless cars, already exist as robotaxis but have a long way to go before they are in general use.\n\n\n=== Open source development ===\n\nThere have been several projects aiming to develop a car on the principles of open design, an approach to designing in which the plans for the machinery and systems are publicly shared, often without monetary compensation. None of the projects have succeeded in developing a car as a whole including both hardware and software, and no mass production ready open-source based designs have been introduced. Some car hacking through on-board diagnostics (OBD) has been done so far.\n\n\n=== Car sharing ===\nCar-share arrangements and carpooling are also increasingly popular, in the US and Europe. For example, in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007. Services like car sharing offer residents to \"share\" a vehicle rather than own a car in already congested neighbourhoods.\n\n\n== Industry ==\n\nThe automotive industry designs, develops, manufactures, markets, and sells the world's motor vehicles, more than three-quarters of which are cars. In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year.The automotive industry in China produces by far the most (20 million in 2020), followed by Japan (seven million), then Germany, South Korea and India. The largest market is China, followed by the US.\nAround the world, there are about a billion cars on the road; they burn over a trillion litres (0.26\u00d710^12 US gal; 0.22\u00d710^12 imp gal) of petrol and diesel fuel yearly, consuming about 50 exajoules (14,000 TWh) of energy. The numbers of cars are increasing rapidly in China and India. In the opinion of some, urban transport systems based around the car have proved unsustainable, consuming excessive energy, affecting the health of populations, and delivering a declining level of service despite increasing investment. Many of these negative effects fall disproportionately on those social groups who are also least likely to own and drive cars. The sustainable transport movement focuses on solutions to these problems. The car industry is also facing increasing competition from the public transport sector, as some people re-evaluate their private vehicle usage.\n\n\n== Alternatives ==\n\nEstablished alternatives for some aspects of car use include public transport such as busses, trolleybusses, trains, subways, tramways, light rail, cycling, and walking. Bicycle sharing systems have been established in China and many European cities, including Copenhagen and Amsterdam. Similar programs have been developed in large US cities. Additional individual modes of transport, such as personal rapid transit could serve as an alternative to cars if they prove to be socially accepted.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nHalberstam, David (1986). The Reckoning. New York: Morrow. ISBN 0-688-04838-2.\nKay, Jane Holtz (1997). Asphalt nation : how the automobile took over America, and how we can take it back. New York: Crown. ISBN 0-517-58702-5.\nWilliams, Heathcote (1991). Autogeddon. New York: Arcade. ISBN 1-55970-176-5.\nSachs, Wolfgang (1992). For love of the automobile: looking back into the history of our desires. Berkeley: University of California Press. ISBN 0-520-06878-5.\nMargolius, Ivan (2020). \"What is an automobile?\". The Automobile. 37 (11): 48\u201352. ISSN 0955-1328.\nCole, John; Cole, Francis (213). A Geography of the European Union. London: Routledge. p. 110. ISBN 9781317835585. \u2013 Number of cars in use (in millions) in various European countries in 1973 and 1992\nLatin America: Economic Growth Trends. US: Agency for International Development, Office of Statistics and Reports. 1972. p. 11. \u2013 Number of motor vehicles registered in Latin America in 1970\nWorld Motor Vehicle Production and Registration. US: Business and Defense Services Administration, Transportation Equipment Division. p. 3. \u2013 Number of registered passenger cars in various countries in 1959-60 and 1969-70\n\n\n== External links ==\n\n Media related to Automobiles at Wikimedia Commons\nF\u00e9d\u00e9ration Internationale de l'Automobile\nForum for the Automobile and Society\nTransportation Statistics Annual Report 1996: Transportation and the Environment by Fletcher, Wendell; Sedor, Joanne; p. 219 (contains figures on vehicle registrations in various countries in 1970 and 1992)"}, {"id": 55, "title": "Budget", "content": "A budget is a calculation plan, usually but not always financial, for a defined period, often one year or a month. A budget may include anticipated sales volumes and revenues, resource quantities including time, costs and expenses, environmental impacts such as greenhouse gas emissions, other impacts, assets, liabilities and cash flows. Companies, governments, families, and other organizations use budgets to express strategic plans of activities in measurable terms.A budget expresses intended expenditures along with proposals for how to meet them with resources. A budget may express a surplus, providing resources for use at a future time, or a deficit in which expenditures exceed income or other resources.\n\n\n== Government ==\n\nThe budget of a government is a summary or plan of the anticipated resources (often but not always from taxes) and expenditures of that government. There are three types of government budgets: the operating or current budget, the capital or investment budget, and the cash or cash flow budget.\n\n\n== By country ==\n\n\n=== United States ===\n\nThe federal budget is prepared by the Office of Management and Budget, and submitted to Congress for consideration. Invariably, Congress makes many and substantial changes. Nearly all American states are required to have balanced budgets, but the federal government is allowed to run deficits.\n\n\n=== India ===\n\nThe budget is prepared by the Budget Division  Department of Economic Affairs of the Ministry of Finance annually. The Finance Minister is the head of the budget making committee. The present Indian Finance minister is Nirmala Sitharaman. The Budget includes supplementary excess grants and when a proclamation by the President as to failure of Constitutional machinery is in operation in relation to a State or a Union Territory, preparation of the Budget of such State.The first budget of India was submitted on 18 February 1860 by James Wilson. \nP C Mahalanobis is known as the father of Indian budget.\n\n\n=== Iran ===\n\n2022\u201323 Iranian national budget is the latest one. Documents related to budget program are not released.\n\n\n=== Philippines ===\nThe Philippine budget is considered the most complicated in the world, incorporating multiple approaches in one single budget system: line-item (budget execution), performance (budget accountability), and zero-based budgeting. The Department of Budget and Management (DBM) prepares the National Expenditure Program and forwards it to the Committee on Appropriations of the House of Representatives to come up with a General Appropriations Bill (GAB). The GAB will go through budget deliberations and voting; the same process occurs when the GAB is transmitted to the Philippine Senate.\nAfter both houses of Congress approves the GAB, the President signs the bill into a General Appropriations Act (GAA); also, the President may opt to veto the GAB and have it returned to the legislative branch or leave the bill unsigned for 30 days and lapse into law. There are two types of budget bill veto: the line-item veto and the veto of the whole budget.\n\n\n== Personal ==\n\nA personal budget or home budget is a finance plan that allocates future personal income towards expenses, savings and debt repayment. Past spending and personal debt are considered when creating a personal budget. There are several methods and tools available for creating, using, and adjusting a personal budget. For example, jobs are an income source, while bills and rent payments are expenses. A third category (other than income and expenses) may be assets (such as property, investments, or other savings or value) representing a potential reserve for funds in case of budget shortfalls.\n\n\n== Corporate budget ==\n\nThe budget of a business, division, or corporation\n\nis a financial forecast for the near-term future, aggregating the expected revenues and expenses of the various departments \u2013 operations, human resources, IT, etc.\nIt is thus a key element in integrated business planning, with measurable targets correspondingly devolved to departmental managers (and becoming KPIs);\nbudgets can then also refer to non-cash resources, such as staff or time.The  budgeting process typically requires considerable effort,\n \noften involving dozens of staff; final sign off resides with both the financial director and operations director.\nThe budget is typically compiled on an annual basis - although, e.g. in mining,\n \nthis may be quarterly - while the monitoring is ongoing;\nsee Financial risk management \u00a7 Corporate finance.\nHere, if the actual figures delivered come close to those budgeted, this suggests that managers understand their business and have been successful in delivering. \nOn the other hand, if the figures diverge this sends an \"out of control\" signal; \nadditionally, the share price could suffer where these figures have been communicated to analysts. \nCriticism is sometimes directed at the nature of budgeting, and its impact on the organization.\n\nAdditional to the cost in time and resources, two phenomena are identified as problematic: \nIt is suggested that managers will often \"game the system\" in specifying targets that are easily attainable, and / or in asking for more resources than required, such that the required resources  will be budgeted as a compromise.\nA second observation is that managers' thinking may emphasize short term, operational thinking at the expense of a long term and strategic perspective;\nfor the relationship with strategy, see Strategic planning \u00a7 Strategic planning vs. financial planning.\nProfessionals employed in this area are often designated \"Budget Analyst\", \na specialized financial analyst role. \nThis usually sits within the company's financial management area in general, sometimes, specifically, in \"FP&A\" (Financial planning and analysis).\n\n\n== Types of budgets ==\nSale budget \u2013 an estimate of future sales, often broken down into both units. It is used to create company and sales goals.\nProduction budget \u2013 an estimate of the number of units that must be manufactured to meet the sales goals. The production budget also estimates the various costs involved with manufacturing those units, including labor and material. Created by product oriented companies.\nCapital budget \u2013 used to determine whether an organization's long-term investments such as new machinery, replacement machinery, new plants, new products, and research development projects are worth pursuing.\nCash flow/cash budget \u2013 a prediction of future cash receipts and expenditures for a particular time period. It usually covers a period in the short-term future. The cash flow budget helps the business to determine when income will be sufficient to cover expenses and when the company will need to seek outside financing.\nConditional budgeting is a budgeting approach designed for companies with fluctuating income, high fixed costs, or income depending on sunk costs, as well as NPOs and NGOs.\nMarketing budget \u2013 an estimate of the funds needed for promotion, advertising, and public relations in order to market the product or service.\nProject budget \u2013 a prediction of the costs associated with a particular company project. These costs include labour, materials, and other related expenses. The project budget is often broken down into specific tasks, with task budgets assigned to each. A cost estimate is used to establish a project budget.\nRevenue budget \u2013 consists of revenue receipts of government and the expenditure met from these revenues. Revenues are made up of taxes and other duties that the government levies. Various countries and unions have created four types of tax jurisdictions: interstate, state, local and tax jurisdictions with a special status (Free-trade zones). Each of them provides a money flow to the corresponding revenue budget levels.\nExpenditure budget \u2013 includes spending data items.\nFlexibility budget \u2013 it is established for fixed cost and variable rate is determined per activity measure for variable cost.\nAppropriation budget \u2013 a maximum amount is established for certain expenditure based on management judgement.\nPerformance budget \u2013 it is mostly used by organization and ministries involved in the development activities. This process of budget takes into account the end results.\nZero based budget \u2013 A budget type where every item added to the budget needs approval and no items are carried forward from the prior years budget. This type of budget has a clear advantage when the limited resources are to be allocated carefully and objectively. Zero based budgeting takes more time to create as all pieces of the budget need to be reviewed by management.\nPersonal budget \u2013 A budget type focusing on expenses for self or for home, usually involves an income to budget.\n\n\n== References ==\n\n\n== External links ==\n The dictionary definition of budget at Wiktionary\n Media related to Budget at Wikimedia Commons\n Quotations related to Budget at Wikiquote\nOrigin of the word"}, {"id": 56, "title": "Car", "content": "A car, or an automobile, is a motor vehicle with wheels. Most definitions of cars state that they run primarily on roads, seat one to eight people, have four wheels, and mainly transport people, not cargo. French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while French-born Swiss inventor Fran\u00e7ois Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.\nThe modern car\u2014a practical, marketable automobile for everyday use\u2014was invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen. Commercial cars became widely available during the 20th century. One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced horse-drawn carriages. In Europe and other parts of the world, demand for automobiles did not increase until after World War II. The car is considered an essential part of the developed economy.\nCars have controls for driving, parking, passenger comfort, and a variety of lamps. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex. These include rear-reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the early 2020s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than petrol-driven cars before 2025. The transition from fossil fuels to electric cars features prominently in most climate change mitigation scenarios, such as Project Drawdown's 100 actionable solutions for climate change.There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance. The costs to society include maintaining roads, land use, road congestion, air pollution, noise pollution, public health, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide. Personal benefits include on-demand transportation, mobility, independence, and convenience. Societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies. There are around one billion cars in use worldwide. Car usage is increasing rapidly, especially in China, India, and other newly industrialized countries.\n\n\n== Etymology ==\nThe English word car is believed to originate from Latin carrus/carrum \"wheeled vehicle\" or (via Old North French) Middle English carre \"two-wheeled cart\", both of which in turn derive from Gaulish karros \"chariot\". It originally referred to any wheeled horse-drawn vehicle, such as a cart, carriage, or wagon.\"Motor car\", attested from 1895, is the usual formal term in British English. \"Autocar\", a variant likewise attested from 1895 and literally meaning \"self-propelled car\", is now considered archaic. \"Horseless carriage\" is attested from 1895.\"Automobile\", a classical compound derived from Ancient Greek aut\u00f3s (\u03b1\u1f50\u03c4\u03cc\u03c2) \"self\" and Latin mobilis \"movable\", entered English from French and was first adopted by the Automobile Club of Great Britain in 1897. It fell out of favour in Britain and is now used chiefly in North America, where the abbreviated form \"auto\" commonly appears as an adjective in compound formations like \"auto industry\" and \"auto mechanic\".\n\n\n== History ==\nThe first steam-powered vehicle was designed by Ferdinand Verbiest, a Flemish member of a Jesuit mission in China around 1672. It was a 65-centimetre-long (26 in) scale-model toy for the Kangxi Emperor that was unable to carry a driver or a passenger. It is not known with certainty if Verbiest's model was successfully built or run.Nicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle in about 1769; he created a steam-powered tricycle. He also constructed two steam tractors for the French Army, one of which is preserved in the French National Conservatory of Arts and Crafts. His inventions were limited by problems with water supply and maintaining steam pressure. In 1801, Richard Trevithick built and demonstrated his Puffing Devil road locomotive, believed by many to be the first demonstration of a steam-powered road vehicle. It was unable to maintain sufficient steam pressure for long periods and was of little practical use.\nThe development of external combustion (steam) engines is detailed as part of the history of the car but often treated separately from the development of true cars. A variety of steam-powered road vehicles were used during the first part of the 19th century, including steam cars, steam buses, phaetons, and steam rollers. In the United Kingdom, sentiment against them led to the Locomotive Acts of 1865.\nIn 1807, Nic\u00e9phore Ni\u00e9pce and his brother Claude created what was probably the world's first internal combustion engine (which they called a Pyr\u00e9olophore), but installed it in a boat on the river Saone in France. Coincidentally, in 1807, the Swiss inventor Fran\u00e7ois Isaac de Rivaz designed his own \"de Rivaz internal combustion engine\", and used it to develop the world's first vehicle to be powered by such an engine. The Ni\u00e9pces' Pyr\u00e9olophore was fuelled by a mixture of Lycopodium powder (dried spores of the Lycopodium plant), finely crushed coal dust and resin that were mixed with oil, whereas de Rivaz used a mixture of hydrogen and oxygen. Neither design was successful, as was the case with others, such as Samuel Brown, Samuel Morey, and Etienne Lenoir, who each built vehicles (usually adapted carriages or carts) powered by internal combustion engines.In November 1881, French inventor Gustave Trouv\u00e9 demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity. Although several other German engineers (including Gottlieb Daimler, Wilhelm Maybach, and Siegfried Marcus) were working on cars at about the same time, the year 1886 is regarded as the birth year of the modern car\u2014a practical, marketable automobile for everyday use\u2014when the German Carl Benz patented his Benz Patent-Motorwagen; he is generally acknowledged as the inventor of the car.In 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a cheaper model. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz car to his line of products. Because France was more open to the early cars, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888, Bertha Benz, the wife of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\nIn 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor. During the last years of the 19th century, Benz was the largest car company in the world with 572 units produced in 1899 and, because of its size, Benz & Cie., became a joint-stock company. The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Pr\u00e4sident automobil.\nDaimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890, and sold their first car in 1892 under the brand name Daimler. It was a horse-drawn stagecoach built by another manufacturer, which they retrofitted with an engine of their design. By 1895, about 30 vehicles had been built by Daimler and Maybach, either at the Daimler works or in the Hotel Hermann, where they set up shop after disputes with their backers. Benz, Maybach, and the Daimler team seem to have been unaware of each other's early work. They never worked together; by the time of the merger of the two companies, Daimler and Maybach were no longer part of DMG. Daimler died in 1900 and later that year, Maybach designed an engine named Daimler-Mercedes that was placed in a specially ordered model built to specifications set by Emil Jellinek. This was a production of a small number of vehicles for Jellinek to race and market in his country. Two years later, in 1902, a new model DMG car was produced and the model was named Mercedes after the Maybach engine, which generated 35 hp. Maybach quit DMG shortly thereafter and opened a business of his own. Rights to the Daimler brand name were sold to other manufacturers.\nIn 1890, \u00c9mile Levassor and Armand Peugeot of France began producing vehicles with Daimler engines, and so laid the foundation of the automotive industry in France. In 1891, Auguste Doriot and his Peugeot colleague Louis Rigoulot completed the longest trip by a petrol-driven vehicle when their self-designed and built Daimler powered Peugeot Type 3 completed 2,100 kilometres (1,300 mi) from Valentigney to Paris and Brest and back again. They were attached to the first Paris\u2013Brest\u2013Paris bicycle race, but finished six days after the winning cyclist, Charles Terront.\nThe first design for an American car with a petrol internal combustion engine was made in 1877 by George Selden of Rochester, New York. Selden applied for a patent for a car in 1879, but the patent application expired because the vehicle was never built. After a delay of 16 years and a series of attachments to his application, on 5 November 1895, Selden was granted a US patent (U.S. Patent 549,160) for a two-stroke car engine, which hindered, more than encouraged, development of cars in the United States. His patent was challenged by Henry Ford and others, and overturned in 1911.\nIn 1893, the first running, petrol-driven American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts. The first public run of the Duryea Motor Wagon took place on 21 September 1893, on Taylor Street in Metro Center Springfield. Studebaker, subsidiary of a long-established wagon and coach manufacturer, started to build cars in 1897:\u200a66\u200a and commenced sales of electric vehicles in 1902 and petrol vehicles in 1904.In Britain, there had been several attempts to build steam cars with varying degrees of success, with Thomas Rickett even attempting a production run in 1860. Santler from Malvern is recognized by the Veteran Car Club of Great Britain as having made the first petrol-driven car in the country in 1894, followed by Frederick William Lanchester in 1895, but these were both one-offs. The first production vehicles in Great Britain came from the Daimler Company, a company founded by Harry J. Lawson in 1896, after purchasing the right to use the name of the engines. Lawson's company made its first car in 1897, and they bore the name Daimler.In 1892, German engineer Rudolf Diesel was granted a patent for a \"New Rational Combustion Engine\". In 1897, he built the first diesel engine. Steam-, electric-, and petrol-driven vehicles competed for a few decades, with petrol internal combustion engines achieving dominance in the 1910s. Although various pistonless rotary engine designs have attempted to compete with the conventional piston and crankshaft design, only Mazda's version of the Wankel engine has had more than very limited success.\nAll in all, it is estimated that over 100,000 patents created the modern automobile and motorcycle.\n\n\n== Mass production ==\n\nLarge-scale, production-line manufacturing of affordable cars was started by Ransom Olds in 1901 at his Oldsmobile factory in Lansing, Michigan, and based upon stationary assembly line techniques pioneered by Marc Isambard Brunel at the Portsmouth Block Mills, England, in 1802. The assembly line style of mass production and interchangeable parts had been pioneered in the US by Thomas Blanchard in 1821, at the Springfield Armory in Springfield, Massachusetts. This concept was greatly expanded by Henry Ford, beginning in 1913 with the world's first moving assembly line for cars at the Highland Park Ford Plant.\nAs a result, Ford's cars came off the line in 15-minute intervals, much faster than previous methods, increasing productivity eightfold, while using less manpower (from 12.5 manhours to 1 hour 33 minutes). It was so successful, paint became a bottleneck. Only Japan black would dry fast enough, forcing the company to drop the variety of colors available before 1913, until fast-drying Duco lacquer was developed in 1926. This is the source of Ford's apocryphal remark, \"any color as long as it's black\". In 1914, an assembly line worker could buy a Model T with four months' pay.Ford's complex safety procedures\u2014especially assigning each worker to a specific location instead of allowing them to roam about\u2014dramatically reduced the rate of injury. The combination of high wages and high efficiency is called \"Fordism\" and was copied by most major industries. The efficiency gains from the assembly line also coincided with the economic rise of the US. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.\nIn the automotive industry, its success was dominating, and quickly spread worldwide seeing the founding of Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany 1925; in 1921, Citro\u00ebn was the first native European manufacturer to adopt the production method. Soon, companies had to have assembly lines, or risk going broke; by 1930, 250 companies which did not, had disappeared.Development of automotive technology was rapid, due in part to the hundreds of small manufacturers competing to gain the world's attention. Key developments included electric ignition and the electric self-starter (both by Charles Kettering, for the Cadillac Motor Company in 1910\u20131911), independent suspension, and four-wheel brakes.\nSince the 1920s, nearly all cars have been mass-produced to meet market needs, so marketing plans often have heavily influenced car design. It was Alfred P. Sloan who established the idea of different makes of cars produced by one company, called the General Motors Companion Make Program, so that buyers could \"move up\" as their fortunes improved.\nReflecting the rapid pace of change, makes shared parts with one another so larger production volume resulted in lower costs for each price range. For example, in the 1930s, LaSalles, sold by Cadillac, used cheaper mechanical parts made by Oldsmobile; in the 1950s, Chevrolet shared bonnet, doors, roof, and windows with Pontiac; by the 1990s, corporate powertrains and shared platforms (with interchangeable brakes, suspension, and other parts) were common. Even so, only major makers could afford high costs, and even companies with decades of production, such as Apperson, Cole, Dorris, Haynes, or Premier, could not manage: of some two hundred American car makers in existence in 1920, only 43 survived in 1930, and with the Great Depression, by 1940, only 17 of those were left.In Europe, much the same would happen. Morris set up its production line at Cowley in 1924, and soon outsold Ford, while beginning in 1923 to follow Ford's practice of vertical integration, buying Hotchkiss (engines), Wrigley (gearboxes), and Osberton (radiators), for instance, as well as competitors, such as Wolseley: in 1925, Morris had 41 per cent of total British car production. Most British small-car assemblers, from Abbey to Xtra, had gone under. Citro\u00ebn did the same in France, coming to cars in 1919; between them and other cheap cars in reply such as Renault's 10CV and Peugeot's 5CV, they produced 550,000 cars in 1925, and Mors, Hurtu, and others could not compete. Germany's first mass-manufactured car, the Opel 4PS Laubfrosch (Tree Frog), came off the line at R\u00fcsselsheim in 1924, soon making Opel the top car builder in Germany, with 37.5 per cent of the market.In Japan, car production was very limited before World War II. Only a handful of companies were producing vehicles in limited numbers, and these were small, three-wheeled for commercial uses, like Daihatsu, or were the result of partnering with European companies, like Isuzu building the Wolseley A-9 in 1922. Mitsubishi was also partnered with Fiat and built the Mitsubishi Model A based on a Fiat vehicle. Toyota, Nissan, Suzuki, Mazda, and Honda began as companies producing non-automotive products before the war, switching to car production during the 1950s. Kiichiro Toyoda's decision to take Toyoda Loom Works into automobile manufacturing would create what would eventually become Toyota Motor Corporation, the largest automobile manufacturer in the world. Subaru, meanwhile, was formed from a conglomerate of six companies who banded together as Fuji Heavy Industries, as a result of having been broken up under keiretsu legislation.\n\n\n== Fuel and propulsion technologies ==\n\nThe transport sector is a major contributor to air pollution, noise pollution and climate change.Most cars in use in the early 2020s run on petrol burnt in an internal combustion engine (ICE). The International Organization of Motor Vehicle Manufacturers says that, in countries that mandate low sulphur motor spirit, petrol-fuelled cars built to late 2010s standards (such as Euro-6) emit very little local air pollution. Some cities ban older petrol-driven cars and some countries plan to ban sales in future. However, some environmental groups say this phase-out of fossil fuel vehicles must be brought forwards to limit climate change. Production of petrol-fuelled cars peaked in 2017.Other hydrocarbon fossil fuels also burnt by deflagration (rather than detonation) in ICE cars include diesel, autogas, and CNG. Removal of fossil fuel subsidies, concerns about oil dependence, tightening environmental laws and restrictions on greenhouse gas emissions are propelling work on alternative power systems for cars. This includes hybrid vehicles, plug-in electric vehicles and hydrogen vehicles. Out of all cars sold in 2021, nine per cent were electric, and by the end of that year there were more than 16 million electric cars on the world's roads. Despite rapid growth, less than two per cent of cars on the world's roads were fully electric and plug-in hybrid cars by the end of 2021. Cars for racing or speed records have sometimes employed jet or rocket engines, but these are impractical for common use.\nOil consumption has increased rapidly in the 20th and 21st centuries because there are more cars; the 1980s oil glut even fuelled the sales of low-economy vehicles in OECD countries. The BRIC countries are adding to this consumption.\nAs of 2023 few production cars use wheel hub motors.\n\n\n=== Batteries ===\n\nIn almost all hybrid (even mild hybrid) and pure electric cars regenerative braking recovers and returns to a battery some energy which would otherwise be wasted by friction brakes getting hot. Although all cars must have friction brakes (front disc brakes and either disc or drum rear brakes) for emergency stops, regenerative braking improves efficiency, particularly in city driving.\n\n\n== User interface ==\n\nCars are equipped with controls used for driving, passenger comfort, and safety, normally operated by a combination of the use of feet and hands, and occasionally by voice on 21st-century cars. These controls include a steering wheel, pedals for operating the brakes and controlling the car's speed (and, in a manual transmission car, a clutch pedal), a shift lever or stick for changing gears, and a number of buttons and dials for turning on lights, ventilation, and other functions. Modern cars' controls are now standardized, such as the location for the accelerator and brake, but this was not always the case. Controls are evolving in response to new technologies, for example, the electric car and the integration of mobile communications.\nSome of the original controls are no longer required. For example, all cars once had controls for the choke valve, clutch, ignition timing, and a crank instead of an electric starter. However, new controls have also been added to vehicles, making them more complex. These include air conditioning, navigation systems, and in-car entertainment. Another trend is the replacement of physical knobs and switches by secondary controls with touchscreen controls such as BMW's iDrive and Ford's MyFord Touch. Another change is that while early cars' pedals were physically linked to the brake mechanism and throttle, in the early 2020s, cars have increasingly replaced these physical linkages with electronic controls.\n\n\n== Electronics and interior ==\nCars are typically equipped with interior lighting which can be toggled manually or be set to light up automatically with doors open, an entertainment system which originated from car radios, sideways windows which can be lowered or raised electrically (manually on earlier cars), and one or multiple auxiliary power outlets for supplying portable appliances such as mobile phones, portable fridges, power inverters, and electrical air pumps from the on-board electrical system. More costly upper-class and luxury cars are equipped with features earlier such as massage seats and collision avoidance systems.Dedicated automotive fuses and circuit breakers prevent damage from electrical overload.\n\n\n== Lighting ==\n\nCars are typically fitted with multiple types of lights. These include headlights, which are used to illuminate the way ahead and make the car visible to other users, so that the vehicle can be used at night; in some jurisdictions, daytime running lights; red brake lights to indicate when the brakes are applied; amber turn signal lights to indicate the turn intentions of the driver; white-colored reverse lights to illuminate the area behind the car (and indicate that the driver will be or is reversing); and on some vehicles, additional lights (e.g., side marker lights) to increase the visibility of the car. Interior lights on the ceiling of the car are usually fitted for the driver and passengers. Some vehicles also have a boot light and, more rarely, an engine compartment light.\n\n\n== Weight ==\nDuring the late 20th and early 21st century, cars increased in weight due to batteries, modern steel safety cages, anti-lock brakes, airbags, and \"more-powerful\u2014if more efficient\u2014engines\" and, as of 2019, typically weigh between 1 and 3 tonnes (1.1 and 3.3 short tons; 0.98 and 2.95 long tons). Heavier cars are safer for the driver from a crash perspective, but more dangerous for other vehicles and road users. The weight of a car influences fuel consumption and performance, with more weight resulting in increased fuel consumption and decreased performance. The Wuling Hongguang Mini EV, a typical city car, weighs about 700 kilograms (1,500 lb). Heavier cars include SUVs and extended-length SUVs like the Suburban.\nSome places tax heavier cars more: as well as improving pedestrian safety this can encourage manufacturers to use materials such as recycled aluminium instead of steel. It has been suggested that one benefit of subsidizing charging infrastructure is that cars can use lighter batteries.\n\n\n== Seating and body style ==\n\nMost cars are designed to carry multiple occupants, often with four or five seats. Cars with five seats typically seat two passengers in the front and three in the rear. Full-size cars and large sport utility vehicles can often carry six, seven, or more occupants depending on the arrangement of the seats. On the other hand, sports cars are most often designed with only two seats. Utility vehicles like pickup trucks, combine seating with extra cargo or utility functionality. The differing needs for passenger capacity and their luggage or cargo space has resulted in the availability of a large variety of body styles to meet individual consumer requirements that include, among others, the sedan/saloon, hatchback, station wagon/estate, coupe, and minivan.\n\n\n== Safety ==\n\nTraffic collisions are the largest cause of injury-related deaths worldwide. Mary Ward became one of the first documented car fatalities in 1869 in Parsonstown, Ireland, and Henry Bliss one of the US's first pedestrian car casualties in 1899 in New York City. There are now standard tests for safety in new cars, such as the Euro and US NCAP tests, and insurance-industry-backed tests by the Insurance Institute for Highway Safety (IIHS).\n\n\n== Costs and benefits ==\n\nThe costs of car usage, which may include the cost of: acquiring the vehicle, repairs and auto maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance, are weighed against the cost of the alternatives, and the value of the benefits\u2014perceived and real\u2014of vehicle usage. The benefits may include on-demand transportation, mobility, independence, and convenience, and emergency power. During the 1920s, cars had another benefit: \"[c]ouples finally had a way to head off on unchaperoned dates, plus they had a private space to snuggle up close at the end of the night.\"Similarly the costs to society of car use may include; maintaining roads, land use, air pollution, noise pollution, road congestion, public health, health care, and of disposing of the vehicle at the end of its life; and can be balanced against the value of the benefits to society that car use generates. Societal benefits may include: economy benefits, such as job and wealth creation, of car production and maintenance, transportation provision, society wellbeing derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability of humans to move flexibly from place to place has far-reaching implications for the nature of societies.\n\n\n== Environmental effects ==\n\nCars are a major cause of urban air pollution, with all types of cars producing dust from brakes, tyres, and road wear, although these may be limited by vehicle emission standards. While there are different ways to power cars, most rely on petrol or diesel, and they consume almost a quarter of world oil production as of 2019. Both petrol and diesel cars pollute more than electric cars. Cars and vans caused 8% of direct carbon dioxide emissions in 2021. As of 2021, due to greenhouse gases emitted during battery production, electric cars must be driven tens of thousands of kilometers before their lifecycle carbon emissions are less than fossil fuel cars; however this varies considerably and is expected to improve in future due to lower carbon electricity, and longer lasting batteries produced in larger factories. Many governments use fiscal policies, such as road tax, to discourage the purchase and use of more polluting cars; and many cities are doing the same with low-emission zones. Fuel taxes may act as an incentive for the production of more efficient, hence less polluting, car designs (e.g., hybrid vehicles) and the development of alternative fuels. High fuel taxes or cultural change may provide a strong incentive for consumers to purchase lighter, smaller, more fuel-efficient cars, or to not drive.The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 millionkm (1.2 millionmiles) if driven a lot. According to the International Energy Agency the average rated fuel consumption of new light-duty vehicles fell by only 0.9% between 2017 and 2019, far smaller than the 1.8% annual average reduction between 2010 and 2015. Given slow progress to date, the IEA estimates fuel consumption will have to decrease by 4.3% per year on average from 2019 to 2030. The increase in sales of SUVs is bad for fuel economy. Many cities in Europe have banned older fossil fuel cars and all fossil fuel vehicles will be banned in Amsterdam from 2030. Many Chinese cities limit licensing of fossil fuel cars, and many countries plan to stop selling them between 2025 and 2050.The manufacture of vehicles is resource intensive, and many manufacturers now report on the environmental performance of their factories, including energy usage, waste and water consumption. Manufacturing each kWh of battery emits a similar amount of carbon as burning through one full tank of petrol. The growth in popularity of the car allowed cities to sprawl, therefore encouraging more travel by car, resulting in inactivity and obesity, which in turn can lead to increased risk of a variety of diseases.Animals and plants are often negatively affected by cars via habitat destruction and pollution. Over the lifetime of the average car, the \"loss of habitat potential\" may be over 50,000 square metres (540,000 sq ft) based on primary production correlations. Animals are also killed every year on roads by cars, referred to as roadkill. More recent road developments are including significant environmental mitigation in their designs, such as green bridges (designed to allow wildlife crossings) and creating wildlife corridors.\nGrowth in the popularity of cars and commuting has led to traffic congestion. Moscow, Istanbul, Bogot\u00e1, Mexico City and S\u00e3o Paulo were the world's most congested cities in 2018 according to INRIX, a data analytics company.\n\n\n== Social issues ==\nMass production of personal motor vehicles in the United States and other developed countries with extensive territories such as Australia, Argentina, and France vastly increased individual and group mobility and greatly increased and expanded economic development in urban, suburban, exurban and rural areas.In the United States, the transport divide and car dependency resulting from domination of car-based transport systems presents barriers to employment in low-income neighbourhoods, with many low-income individuals and families forced to run cars they cannot afford in order to maintain their income. The historic commitment to a car-based transport system continued during the presidency of Joe Biden. Dependency on automobiles by African Americans may result in exposure to the hazards of driving while black and other types of racial discrimination related to buying, financing and insuring them.\n\n\n== Emerging car technologies ==\nAlthough intensive development of conventional battery electric vehicles is continuing into the 2020s, other car propulsion technologies that are under development include wireless charging, hydrogen cars, and hydrogen/electric hybrids. Research into alternative forms of power includes using ammonia instead of hydrogen in fuel cells.New materials which may replace steel car bodies include aluminium, fiberglass, carbon fiber, biocomposites, and carbon nanotubes. Telematics technology is allowing more and more people to share cars, on a pay-as-you-go basis, through car share and carpool schemes. Communication is also evolving due to connected car systems.\n\n\n=== Autonomous car ===\n\nFully autonomous vehicles, also known as driverless cars, already exist as robotaxis but have a long way to go before they are in general use.\n\n\n=== Open source development ===\n\nThere have been several projects aiming to develop a car on the principles of open design, an approach to designing in which the plans for the machinery and systems are publicly shared, often without monetary compensation. None of the projects have succeeded in developing a car as a whole including both hardware and software, and no mass production ready open-source based designs have been introduced. Some car hacking through on-board diagnostics (OBD) has been done so far.\n\n\n=== Car sharing ===\nCar-share arrangements and carpooling are also increasingly popular, in the US and Europe. For example, in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007. Services like car sharing offer residents to \"share\" a vehicle rather than own a car in already congested neighbourhoods.\n\n\n== Industry ==\n\nThe automotive industry designs, develops, manufactures, markets, and sells the world's motor vehicles, more than three-quarters of which are cars. In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year.The automotive industry in China produces by far the most (20 million in 2020), followed by Japan (seven million), then Germany, South Korea and India. The largest market is China, followed by the US.\nAround the world, there are about a billion cars on the road; they burn over a trillion litres (0.26\u00d710^12 US gal; 0.22\u00d710^12 imp gal) of petrol and diesel fuel yearly, consuming about 50 exajoules (14,000 TWh) of energy. The numbers of cars are increasing rapidly in China and India. In the opinion of some, urban transport systems based around the car have proved unsustainable, consuming excessive energy, affecting the health of populations, and delivering a declining level of service despite increasing investment. Many of these negative effects fall disproportionately on those social groups who are also least likely to own and drive cars. The sustainable transport movement focuses on solutions to these problems. The car industry is also facing increasing competition from the public transport sector, as some people re-evaluate their private vehicle usage.\n\n\n== Alternatives ==\n\nEstablished alternatives for some aspects of car use include public transport such as busses, trolleybusses, trains, subways, tramways, light rail, cycling, and walking. Bicycle sharing systems have been established in China and many European cities, including Copenhagen and Amsterdam. Similar programs have been developed in large US cities. Additional individual modes of transport, such as personal rapid transit could serve as an alternative to cars if they prove to be socially accepted.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nHalberstam, David (1986). The Reckoning. New York: Morrow. ISBN 0-688-04838-2.\nKay, Jane Holtz (1997). Asphalt nation : how the automobile took over America, and how we can take it back. New York: Crown. ISBN 0-517-58702-5.\nWilliams, Heathcote (1991). Autogeddon. New York: Arcade. ISBN 1-55970-176-5.\nSachs, Wolfgang (1992). For love of the automobile: looking back into the history of our desires. Berkeley: University of California Press. ISBN 0-520-06878-5.\nMargolius, Ivan (2020). \"What is an automobile?\". The Automobile. 37 (11): 48\u201352. ISSN 0955-1328.\nCole, John; Cole, Francis (213). A Geography of the European Union. London: Routledge. p. 110. ISBN 9781317835585. \u2013 Number of cars in use (in millions) in various European countries in 1973 and 1992\nLatin America: Economic Growth Trends. US: Agency for International Development, Office of Statistics and Reports. 1972. p. 11. \u2013 Number of motor vehicles registered in Latin America in 1970\nWorld Motor Vehicle Production and Registration. US: Business and Defense Services Administration, Transportation Equipment Division. p. 3. \u2013 Number of registered passenger cars in various countries in 1959-60 and 1969-70\n\n\n== External links ==\n\n Media related to Automobiles at Wikimedia Commons\nF\u00e9d\u00e9ration Internationale de l'Automobile\nForum for the Automobile and Society\nTransportation Statistics Annual Report 1996: Transportation and the Environment by Fletcher, Wendell; Sedor, Joanne; p. 219 (contains figures on vehicle registrations in various countries in 1970 and 1992)"}, {"id": 57, "title": "President", "content": "A president is a leader of an organization, company, community, club, trade union, university or other group. The relationship between a president and a chief executive officer varies, depending on the structure of the specific organization. In a similar vein to a chief operating officer, the title of corporate president as a separate position (as opposed to being combined with a \"C-suite\" designation, such as \"president and chief executive officer\" or \"president and chief operating officer\") is also loosely defined; the president is usually the legally recognized highest rank of corporate officer, ranking above the various vice presidents (including senior vice president and executive vice president), but on its own generally considered subordinate, in practice, to the CEO. The powers of a president vary widely across organizations and such powers come from specific authorization in the bylaws like Robert's Rules of Order (e.g. the president can make an \"executive decision\" only if the bylaws allow for it).\n\n\n== History ==\nOriginally, the term president was used in the same way that foreman or overseer is used now (the term is still used in that sense today). It has now also come to mean \"chief officer\" in terms of administrative or executive duties.\n\n\n== Powers and authority ==\nThe powers of the president vary widely across organizations. In some organizations the president has the authority to hire staff and make financial decisions, while in others the president only makes recommendations to a board of directors, and still others the president has no executive powers and is mainly a spokesperson for the organization. The amount of power given to the president depends on the type of organization, its structure, and the rules it has created for itself.In addition to administrative or executive duties in organizations, a president has the duties of presiding over meetings. Such duties at meetings include:\n\ncalling the meeting to order\ndetermining if a quorum is present\nannouncing the items on the order of business or agenda as they come up\nrecognition of members to have the floor\nenforcing the rules of the group\nputting all questions (motions) to a vote\nadjourning the meetingWhile presiding, a president remains impartial and does not interrupt speakers if a speaker has the floor and is following the rules of the group. In committees or small boards, the president votes along with the other members. However, in assemblies or larger boards, the president should vote only when it can affect the result. At a meeting, the president only has one vote (i.e. the president cannot vote twice and cannot override the decision of the group unless the organisation has specifically given the president such authority).\n\n\n== Disciplinary procedures ==\nIf the president exceeds the given authority, engages in misconduct, or fails to perform the duties, the president may face disciplinary procedures. Such procedures may include censure, suspension, or removal from office. The rules of the particular organization would provide details on who can perform these disciplinary procedures and the extent that they can be done. Usually, whoever appointed or elected the president has the power to discipline this officer.\n\n\n== President-elect ==\n\nSome organizations may have a position of  president-elect in addition to the position of president. Generally the membership of the organization elects a president-elect and when the term of the president-elect is complete, that person automatically becomes president.\n\n\n== Immediate past president ==\nSome organizations may have a position of immediate past president in addition to the position of president. In those organizations, when the term of the president is complete, that person automatically fills the position of immediate past president. The organization can have such a position only if the bylaws provide it. The duties of such a position would also have to be provided in the bylaws.\n\n\n== References ==\n\n\n== Further reading ==\nBennett, Nathan; Stephen A. Miles (2006). Riding Shotgun: The Role of the COO. Stanford, California: Stanford University Press. ISBN 0-8047-5166-8.\nNational Association of Parliamentarians, Education Committee (1993). Spotlight on You the President. Independence, MO: National Association of Parliamentarians. ISBN 1-884048-15-3."}, {"id": 58, "title": "Association football", "content": "Association football, more commonly known as football or soccer, is a team sport played between two teams of 11 players each, who primarily use their feet to propel a ball around a rectangular field called a pitch. The objective of the game is to score more goals than the opposing team by moving the ball beyond the goal line into a rectangular-framed goal defended by the opposing team. Traditionally, the game has been played over two 45-minute halves, for a total match time of 90 minutes. With an estimated 250 million players active in over 200 countries and territories, it is the world's most popular sport.\nThe game of association football is played in accordance with the Laws of the Game, a set of rules that has been in effect since 1863 and maintained by the IFAB since 1886. The game is played with a football that is 68\u201370 cm (27\u201328 in) in circumference. The two teams compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. When the ball is in play, the players mainly use their feet, but may use any other part of their body, except for their hands or arms, to control, strike, or pass the ball. Only the goalkeepers may use their hands and arms, and only then within the penalty area. The team that has scored more goals at the end of the game is the winner. Depending on the format of the competition, an equal number of goals scored may result in a draw being declared, or the game goes into extra time or a penalty shoot-out.Internationally, association football is governed by FIFA. Under FIFA, there are six continental confederations: AFC, CAF, CONCACAF, CONMEBOL, OFC, and UEFA. Of these confederations, CONMEBOL is the oldest one, being founded in 1916. National associations (e.g. The FA or JFA) are responsible for managing the game in their own countries both professionally and at an amateur level, and coordinating competitions in accordance with the Laws of the Game. The most senior and prestigious international competitions are the FIFA World Cup and the FIFA Women's World Cup. The men's World Cup is the most-viewed sporting event in the world, surpassing the Olympic Games. The two most prestigious competitions in European club football are the UEFA Champions League and the UEFA Women's Champions League, which attract an extensive television audience throughout the world. Since 2009, the final of the men's tournament has been the most-watched annual sporting event in the world.\n\n\n== Name ==\n\nAssociation football is one of a family of football codes that emerged from various ball games played worldwide since antiquity. Within the English-speaking world, the sport is now usually called \"football\" in Great Britain and most of Ulster in the north of Ireland, whereas people usually call it \"soccer\" in regions and countries where other codes of football are prevalent, such as Australia, Canada, South Africa, most of Ireland (excluding Ulster), and the United States. A notable exception is New Zealand, where in the first two decades of the 21st century, under the influence of international television, \"football\" has been gaining prevalence, despite the dominance of other codes of football, namely rugby union and rugby league.The term soccer comes from Oxford \"-er\" slang, which was prevalent at the University of Oxford in England from about 1875, and is thought to have been borrowed from the slang of Rugby School. Initially spelt assoccer, it was later reduced to the modern spelling. This form of slang also gave rise to rugger for rugby football, fiver and tenner for five pound and ten pound notes, and the now-archaic footer that was also a name for association football. The word soccer arrived at its current form in 1895 and was first recorded in 1889 in the earlier form of socca.\n\n\n== History ==\n\nKicking ball games arose independently multiple times across multiple cultures. Phaininda and episkyros were Greek ball games. An image of an episkyros player depicted in low relief on a stele of c.\u2009375\u2013400 BCE in the National Archaeological Museum of Athens appears on the UEFA European Championship trophy. Athenaeus, writing in 228 CE, mentions the Roman ball game harpastum. Phaininda, episkyros and harpastum were played involving hands and violence. They all appear to have resembled rugby football, wrestling, and volleyball more than what is recognisable as modern football. As with pre-codified mob football, the antecedent of all modern football codes, these three games involved more handling the ball than kicking it.The Chinese competitive game cuju (\u8e74\u97a0, literally \"kick ball\"; also known as tsu chu) resembles association football. Cuju players could use any part of the body apart from hands and the intent was to kick a ball through an opening into a net. During the Han dynasty (202 BCE \u2013 220 CE), cuju games were standardised and rules were established. Other East Asian games included kemari in Japan and chuk-guk in Korea, both influenced by cuju. Kemari originated after the year 600 during the Asuka period. It was a ceremonial rather than a competitive game, and involved the kicking of a mari, a ball made of animal skin. In North America, pasuckuakohowog was a ball game played by the Algonquians; it was described as \"almost identical to the kind of folk football being played in Europe at the same time, in which the ball was kicked through goals\".The predecessors of association football in themselves do not have a classical history. Notwithstanding any similarities to other ball games played around the world, FIFA has described that no historical connection exists with any game played in antiquity outside Europe. The history of kicking ball in England dates back to at least the eighth century. The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England.\n\nThe Cambridge rules, first drawn up at the University of Cambridge in 1848, were particularly influential in the development of subsequent codes, including association football. The Cambridge rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857, which led to the formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules.\nThese ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemasons' Tavern in Great Queen Street, London. The only school to be represented on this occasion was Charterhouse. The Freemasons' Tavern was the setting for five more meetings of The FA between October and December 1863; the English FA eventually issued the first comprehensive set of rules named Laws of the Game on 8 December 1863, forming football. The laws included bans on running with the ball in hand and hacking (kicking an opponent in the shins), tripping and holding. Eleven clubs, under the charge of FA secretary Ebenezer Cobb Morley, ratified the original thirteen laws of the game. The sticking point was hacking, which a twelfth club at the meeting, Blackheath FC, had wanted to keep, resulting in them withdrawing from the FA. Other English rugby clubs followed this lead and did not join the FA, and instead in 1871, along with Blackheath, formed the Rugby Football Union. The FA rules included handling of the ball by \"marks\" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s, with the FA absorbing some of its rules until there was little difference between the games. On 19 December 1863, Barnes played neighbouring Richmond in the first ever match under the Laws of the Game; this 15-a-side clash ended in a goalless draw.\n\nThe world's oldest football competition is the FA Cup, which was founded by the footballer and cricketer Charles W. Alcock, and has been contested by English teams since 1872. The first official international football match also took place in 1872, between Scotland and England in Glasgow, again at the instigation of Alcock. England is also home to the world's first football league, which was founded in Birmingham in 1888 by Aston Villa director William McGregor. The original format contained 12 clubs from the Midlands and Northern England.Laws of the Game are determined by the International Football Association Board (IFAB). The board was formed in 1886 after a meeting in Manchester of the Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. FIFA, the international football body, was formed in Paris in 1904 and declared that they would adhere to the Laws of the Game of the Football Association. The growing popularity of the international game led to the admittance of FIFA representatives to the IFAB in 1913. The board consists of four representatives from FIFA and one representative from each of the four British associations.For most of the 20th century, Europe and South America were the dominant regions in association football. The FIFA World Cup, inaugurated in 1930, became the main stage for players of both continents to show their worth and the strength of their national teams. In the second half of the century, the European Cup and the Copa Libertadores were created, and the champions of these two club competitions would contest the Intercontinental Cup to prove which team was the best in the world.In the 21st century, South America has continued to produce some of the best footballers in the world, but its clubs have fallen behind the still dominant European clubs, which often sign the best players from Latin America and elsewhere. Meanwhile, football has improved in Africa, Asia and North America, and nowadays, these regions are at least on equal grounds with South America in club football, although countries in the Caribbean and Oceania regions (except Australia) have yet to make a mark in international football. When it comes to national teams, however, Europeans and South Americans continue to dominate the FIFA World Cup, as no team from any other region has managed to even reach the final.Football is played at a professional level all over the world. Millions of people regularly go to football stadiums to follow their favourite teams, while billions more watch the game on television or on the internet. A very large number of people also play football at an amateur level. According to a survey conducted by FIFA published in 2001, over 240 million people from more than 200 countries regularly play football. Football has the highest global television audience in sport.In many parts of the world, football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations. Ryszard Kapu\u015bci\u0144ski says that Europeans who are polite, modest, or humble fall easily into rage when playing or watching football games. The Ivory Coast national football team helped secure a truce to the nation's civil war in 2006 and it helped further reduce tensions between government and rebel forces in 2007 by playing a match in the rebel capital of Bouak\u00e9, an occasion that brought both armies together peacefully for the first time. By contrast, football is widely considered to have been the final proximate cause for the Football War in June 1969 between El Salvador and Honduras. The sport also exacerbated tensions at the beginning of the Croatian War of Independence of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade degenerated into rioting in May 1990.\n\n\n=== Women's association football ===\n\nWomen's association football has historically seen opposition, with national associations severely curbing its development and several outlawing it completely. Women may have been playing football for as long as the game has existed. Evidence shows that a similar ancient game (cuju, or tsu chu) was played by women during the Han dynasty (25\u2013220 CE), as female figures are depicted in frescoes of the period playing tsu chu. There are also reports of annual football matches played by women in Midlothian, Scotland, during the 1790s.\nAssociation football, the modern game, has documented early involvement of women. In 1863, football governing bodies introduced standardised rules to prohibit violence on the pitch, making it more socially acceptable for women to play. The first match recorded by the Scottish Football Association took place in 1892 in Glasgow. In England, the first recorded game of football between women took place in 1895. Women's football has traditionally been associated with charity games and physical exercise, particularly in the United Kingdom.Association football continued to be played by women since the time of the first recorded women's games in the late 19th century. The best-documented early European team was founded by activist Nettie Honeyball in England in 1894. It was named the British Ladies' Football Club. Honeyball is quoted as, \"I founded the association late last year [1894], with the fixed resolve of proving to the world that women are not the 'ornamental and useless' creatures men have pictured. I must confess, my convictions on all matters where the sexes are so widely divided are all on the side of emancipation, and I look forward to the time when ladies may sit in Parliament and have a voice in the direction of affairs, especially those which concern them most.\" Honeyball and those like her paved the way for women's football. However, the women's game was frowned upon by the British football associations and continued without their support. It has been suggested that this was motivated by a perceived threat to the \"masculinity\" of the game.Women's football became popular on a large scale at the time of the First World War, when female employment in heavy industry spurred the growth of the game, much as it had done for men 50 years earlier. The most successful team of the era was Dick, Kerr Ladies F.C. of Preston, England. The team played in one of the first women's international matches against a French XI team in 1920, and also made up most of the England team against a Scottish Ladies XI in the same year, winning 22\u20130.Despite being more popular than some men's football events, with one match seeing a 53,000 strong crowd in 1920, women's football in England suffered a blow in 1921 when The Football Association outlawed the playing of the game on association members' pitches, stating that \"the game of football is quite unsuitable for females and should not be encouraged.\" Players and football writers have argued that this ban was, in fact, due to envy of the large crowds that women's matches attracted, and because the FA had no control over the money made from the women's game. The FA ban led to the formation of the short-lived English Ladies Football Association and play moved to rugby grounds. Women's football also faced bans in several other countries, notably in Brazil from 1941 to 1979, in France from 1941 to 1970, and in Germany from 1955 to 1970.\nRestrictions began to be reduced in the 1960s and 1970s. The Italian women's football league was established in 1968. In December 1969, the Women's Football Association was formed in England, with the sport eventually becoming the most prominent team sport for women in the United Kingdom. Two unofficial women's World Cups were organised by the FIEFF in 1970 and in 1971.  Also in 1971, Union of European Football Associations (UEFA) members voted to officially recognise women's football, while The Football Association rescinded the ban that prohibited women from playing on association members' pitches in England.Women's football still faces many struggles, but its worldwide growth has seen major competitions being launched at both the national and international levels, mirroring the men's competitions. The FIFA Women's World Cup was inaugurated in 1991: the first tournament was held in China, featuring 12 teams from the respective six confederations.  The World Cup has been held every four years since; by the 2019 FIFA Women's World Cup in France, it had expanded to 24 national teams, and 1.12 billion viewers watched the competition. Women's football has been an Olympic event since 1996.North America is the dominant region in women's football, with the United States winning most FIFA Women's World Cups and Olympic tournaments. Europe and Asia come second and third in terms of international success, and the women's game has been improving in South America.\n\n\n== Gameplay ==\n\nAssociation football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a spherical ball of 68\u201370 cm (27\u201328 in) circumference, known as the football (or soccer ball). Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw. Each team is led by a captain who has only one official responsibility as mandated by the Laws of the Game: to represent their team in the coin toss before kick-off or penalty kicks.The primary law is that players other than goalkeepers may not deliberately handle the ball with their hands or arms during play, though they must use both their hands during a throw-in restart. Although players usually use their feet to move the ball around, they may use any part of their body (notably, \"heading\" with the forehead) other than their hands or arms. Within normal play, all players are free to play the ball in any direction and move throughout the pitch, though players may not pass to teammates who are in an offside position.During gameplay, players attempt to create goal-scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a teammate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee for an infringement of the rules. After a stoppage, play recommences with a specified restart.\nAt a professional level, most matches produce only a few goals. For example, the 2005\u201306 season of the English Premier League produced an average of 2.48 goals per match. The Laws of the Game do not specify any player positions other than goalkeeper, but a number of specialised roles have evolved. Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball to pass it to the forwards on their team. Players in these positions are referred to as outfield players, to distinguish them from the goalkeeper.\nThese positions are further subdivided according to the area of the field in which the player spends the most time. For example, there are central defenders and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time. The layout of a team's players is known as a formation. Defining the team's formation and tactics is usually the prerogative of the team's manager.\n\n\n== Laws ==\n\nThere are 17 laws in the official Laws of the Game, each containing a collection of stipulations and guidelines. The same laws are designed to apply to all levels of football for both sexes, although certain modifications for groups such as juniors, seniors and people with physical disabilities are permitted. The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. The Laws of the Game are published by FIFA, but are maintained by the IFAB. In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of association football. Within the United States, Major League Soccer used a distinct ruleset during the 1990s and the National Federation of State High School Associations and National Collegiate Athletic Association still use rulesets that are comparable to, but different from, the IFAB Laws.\n\n\n=== Players, equipment, and officials ===\n\nEach team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team, which is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws.The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. An athletic supporter and protective cup is highly recommended for male players by medical experts and professionals. Headgear is not a required piece of basic equipment, but players today may choose to wear it to protect themselves from head injury. Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials.A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is five in 90 minutes, with each team being allowed one more if the game should go into extra-time; the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match. IFAB recommends \"that a match should not continue if there are fewer than seven players in either team\". Any decision regarding points awarded for abandoned games is left to the individual football associations.\nA game is officiated by a referee, who has \"full authority to enforce the Laws of the Game in connection with the match to which he has been appointed\" (Law 5), and whose decisions are final. The referee is assisted by two assistant referees. In many high-level games there is also a fourth official who assists the referee and may replace another official should the need arise.Goal line technology is used to measure if the whole ball has crossed the goal-line thereby determining whether a goal has been scored or not; this was brought in to prevent controversy. Video assistant referees (VAR) have also been increasingly introduced in high-level matches to assist officials through video replays to correct clear and obvious mistakes. There are four types of calls that can be reviewed: mistaken identity in awarding a red or yellow card, goals and whether there was a violation during the buildup, direct red card decisions, and penalty decisions.\n\n\n=== Ball ===\n\nThe ball is spherical with a circumference of between 68 and 70 cm (27 and 28 in), a weight in the range of 410 to 450 g (14 to 16 oz), and a pressure between 0.6 and 1.1 standard atmospheres (8.5 and 15.6 pounds per square inch) at sea level. In the past the ball was made up of leather panels sewn together, with a latex bladder for pressurisation, but modern balls at all levels of the game are now synthetic.\n\n\n=== Pitch ===\n\nAs the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though use of imperial units remains popular in English-speaking countries with a relatively recent history of metrication (or only partial metrication), such as Britain.The length of the pitch, or field, for international adult matches is in the range of 100\u2013110 m (110\u2013120 yd) and the width is in the range of 64\u201375 m (70\u201380 yd). Fields for non-international matches may be 90\u2013120 m (100\u2013130 yd) in length and 45\u201390 m (50\u2013100 yd) in width, provided the pitch does not become square. In 2008, the IFAB initially approved a fixed size of 105 m (115 yd) long and 68 m (74 yd) wide as a standard pitch dimension for international matches; however, this decision was later put on hold and was never actually implemented.The longer boundary lines are touchlines, while the shorter boundaries (on which the goals are placed) are goal lines. A rectangular goal is positioned on each goal line, midway between the two touchlines. The inner edges of the vertical goal posts must be 7.32 m (24 ft) apart, and the lower edge of the horizontal crossbar supported by the goal posts must be 2.44 m (8 ft) above the ground. Nets are usually placed behind the goal, but are not required by the Laws.In front of the goal is the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5 m (18 yd) from the goalposts and extending 16.5 m (18 yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks.\n\n\n=== Duration and tie-breaking methods ===\n\n\n==== 90-minute ordinary time ====\nA standard adult football match consists of two halves of 45 minutes each. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute half-time break between halves. The end of the match is known as full-time. The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is called \"additional time\" in FIFA documents, but is most commonly referred to as stoppage time or injury time, while lost time can also be used as a synonym. The duration of stoppage time is at the sole discretion of the referee. Stoppage time does not fully compensate for the time in which the ball is out of play, and a 90-minute game typically involves about an hour of \"effective playing time\". The referee alone signals the end of the match. In matches where a fourth official is appointed, towards the end of the half, the referee signals how many minutes of stoppage time they intend to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee. Added time was introduced because of an incident which happened in 1891 during a match between Stoke and Aston Villa. Trailing 1\u20130 with two minutes remaining, Stoke were awarded a penalty kick. Villa's goalkeeper deliberately kicked the ball out of play; by the time it was recovered, the clock had run out and the game was over, leaving Stoke unable to attempt the penalty. The same law also states that the duration of either half is extended until a penalty kick to be taken or retaken is completed; thus, no game can end with an uncompleted penalty.\n\n\n==== Tie-breaking ====\n\nIn league competitions, games may end in a draw. In knockout competitions where a winner is required, various methods may be employed to break such a deadlock; some competitions may invoke replays. A game tied at the end of regulation time may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shoot-outs (known officially in the Laws of the Game as \"kicks from the penalty mark\") to determine which team will progress to the next stage of the tournament or be the champion. Goals scored during extra time periods count towards the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament, with goals scored in a penalty shoot-out not making up part of the final score.In competitions using two-legged matches, each team competes at home once, with an aggregate score from the two matches deciding which team progresses. Where aggregates are equal, the away goals rule may be used to determine the winners, in which case the winner is the team that scored the most goals in the leg they played away from home. If the result is still equal, extra time and potentially a penalty shoot-out are required.\n\n\n=== Ball in and out of play ===\nUnder the Laws, the two basic states of play during a game are ball in play and ball out of play. From the beginning of each playing period with a kick-off until the end of the playing period, the ball is in play at all times, except when either the ball leaves the field of play, or play is stopped by the referee. When the ball becomes out of play, play is restarted by one of eight restart methods depending on how it went out of play:\nKick-off: following a goal by the opposing team, or to begin each period of play.\nThrow-in: when the ball has crossed the touchline; awarded to the opposing team to that which last touched the ball.\nGoal kick: when the ball has wholly crossed the goal line without a goal having been scored and having last been touched by a player of the attacking team; awarded to defending team.\nCorner kick: when the ball has wholly crossed the goal line without a goal having been scored and having last been touched by a player of the defending team; awarded to attacking team.\nIndirect free kick: awarded to the opposing team following \"non-penal\" fouls, certain technical infringements, or when play is stopped to caution or dismiss an opponent without a specific foul having occurred. A goal may not be scored directly (without the ball first touching another player) from an indirect free kick.\nDirect free kick: awarded to fouled team following certain listed \"penal\" fouls. A goal may be scored directly from a direct free kick.\nPenalty kick: awarded to the fouled team following a foul usually punishable by a direct free kick but that has occurred within their opponent's penalty area.\nDropped-ball: occurs when the referee has stopped play for any other reason, such as a serious injury to a player, interference by an external party, or a ball becoming defective.\n\n\n=== Misconduct ===\n\n\n==== On-field ====\n\nA foul occurs when a player commits an offence listed in the Laws of the Game while the ball is in play. The offences that constitute a foul are listed in Law 12. Handling the ball deliberately, tripping an opponent, or pushing an opponent, are examples of \"penal fouls\", punishable by a direct free kick or penalty kick depending on where the offence occurred. Other fouls are punishable by an indirect free kick.The referee may punish a player's or substitute's misconduct by a caution (yellow card) or dismissal (red card). A second yellow card in the same game leads to a red card, which results in a dismissal. A player given a yellow card is said to have been \"booked\", the referee writing the player's name in their official notebook. If a player has been dismissed, no substitute can be brought on in their place and the player may not participate in further play. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of \"unsporting behaviour\" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute, substituted player, and to non-players such as managers and support staff.Rather than stopping play, the referee may allow play to continue if doing so will benefit the team against which an offence has been committed. This is known as \"playing an advantage\". The referee may \"call back\" play and penalise the original offence if the anticipated advantage does not ensue within \"a few seconds\". Even if an offence is not penalised due to advantage being played, the offender may still be sanctioned for misconduct at the next stoppage of play.The referee's decision in all on-pitch matters is considered final. The score of a match cannot be altered after the game, even if later evidence shows that decisions (including awards/non-awards of goals) were incorrect.\n\n\n==== Off-field ====\n\nAlong with the general administration of the sport, football associations and competition organisers also enforce good conduct in wider aspects of the game, dealing with issues such as comments to the press, clubs' financial management, doping, age fraud and match fixing. Most competitions enforce mandatory suspensions for players who are sent off in a game. Some on-field incidents, if considered very serious (such as allegations of racial abuse), may result in competitions deciding to impose heavier sanctions than those normally associated with a red card. Some associations allow for appeals against player suspensions incurred on-field if clubs feel a referee was incorrect or unduly harsh.Sanctions for such infractions may be levied on individuals or on clubs as a whole. Penalties may include fines, point deductions (in league competitions) or even expulsion from competitions. For example, the English Football League deduct 12 points from any team that enters financial administration. Among other administrative sanctions are penalties against game forfeiture. Teams that had forfeited a game or had been forfeited against would be awarded a technical loss or win.\n\n\n== Governing bodies ==\n\nThe recognised international governing body of football (and associated games, such as futsal and beach soccer) is FIFA. The FIFA headquarters are located in Z\u00fcrich, Switzerland. Six regional confederations are associated with FIFA; these are:\nAsia: Asian Football Confederation (AFC)\nAfrica: Confederation of African Football (CAF)\nEurope: Union of European Football Associations (UEFA)\nNorth/Central America & Caribbean: Confederation of North, Central American and Caribbean Association Football (CONCACAF)\nOceania: Oceania Football Confederation (OFC)\nSouth America: Confederaci\u00f3n Sudamericana de F\u00fatbol (South American Football Confederation; CONMEBOL)National associations (or national federations) oversee football within individual countries. These are generally synonymous with sovereign states (for example, the Cameroonian Football Federation in Cameroon), but also include a smaller number of associations responsible for sub-national entities or autonomous regions (for example, the Scottish Football Association in Scotland). 211 national associations are affiliated both with FIFA and with their respective continental confederations. Other national associations may be members of continental confederations but otherwise not participate in FIFA competitions.While FIFA is responsible for arranging competitions and most rules related to international competition, the actual Laws of the Game are set by the IFAB, where each of the UK Associations has one vote, while FIFA collectively has four votes.\n\n\n== International competitions ==\n\nInternational competitions in association football principally consist of two varieties: competitions involving representative national teams or those involving clubs based in multiple nations and national leagues. International football, without qualification, most often refers to the former. In the case of international club competition, it is the country of origin of the clubs involved, not the nationalities of their players, that renders the competition international in nature.\nThe major international competition in football is the World Cup, organised by FIFA. This competition has taken place every four years since 1930, with the exception of the 1942 and 1946 tournaments, which were cancelled because of World War II. As of 2022, over 200 national teams compete in qualifying tournaments within the scope of continental confederations for a place in the finals. The finals tournament, held every four years, involved 32 national teams (expanding to 48 teams for the 2026 tournament) competing over a four-week period. The World Cup is the most prestigious association football tournament as well as the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; the cumulative audience of all matches of the 2006 FIFA World Cup was estimated to be 26.29 billion with an estimated 715.1 million people watching the final match, one-ninth of the entire population of the planet. The 1958 World Cup saw the emergence of Pel\u00e9 as a global sporting star, a period that coincided with \"the explosive spread of television, which massively amplified his presence everywhere\". The current champions are Argentina, who won their third title at the 2022 tournament in Qatar. The FIFA Women's World Cup has been held every four years since 1991. Under the tournament's current format that was expanded in 2023, national teams vie for 31 slots in a three-year qualification phase, while the host nation's team enters automatically as the 32nd slot. The current champions are Spain, after winning their first title in the 2023 tournament.There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles when FIFA and the IOC had disagreed over the status of amateur players. Before the inception of the World Cup, the Olympics (especially during the 1920s) were the most prestigious international event. Originally, the tournament was for amateurs only. As professionalism spread around the world, the gap in quality between the World Cup and the Olympics widened. The countries that benefited most were the Soviet Bloc countries of Eastern Europe, where top athletes were state-sponsored while retaining their status as amateurs. Between 1948 and 1980, 23 out of 27 Olympic medals were won by Eastern Europe, with only Sweden (gold in 1948 and bronze in 1952), Denmark (bronze in 1948 and silver in 1960) and Japan (bronze in 1968) breaking their dominance. For the 1984 Los Angeles Games, the IOC allowed professional players to compete. Since 1992, male competitors must be under 23 years old, although since 1996, three players over the age of 23 have been allowed per squad. A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the women's Olympic tournament.\nAfter the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa Am\u00e9rica (CONMEBOL), the African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). These competitions are not strictly limited to members of the continental confederations, with guest teams from other continents sometimes invited to compete. The FIFA Confederations Cup was contested by the winners of all six continental championships, the current FIFA World Cup champions, and the country which was hosting the next World Cup. This was generally regarded as a warm-up tournament for the upcoming FIFA World Cup and did not carry the same prestige as the World Cup itself. The tournament was discontinued following the 2017 edition with its calendar slot replaced by an expanded FIFA Club World Cup. The UEFA Nations League and the CONCACAF Nations League were introduced in the late 2010s to replace international friendlies during the two-year cycle between major tournaments.The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example, the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup.\n\n\n== Domestic competitions ==\n\nThe governing bodies in each country operate league systems in a domestic season, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team is declared the champion. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division.The teams finishing at the top of a country's league may also be eligible to play in international club competitions in the following season. The main exceptions to this system occur in some Latin American leagues, which divide football championships into two sections named Apertura and Clausura (Spanish for Opening and Closing), awarding a champion for each. The majority of countries supplement the league system with one or more \"cup\" competitions organised on a knock-out basis. These also include the domestic cup, which may be open to all eligible teams in a country's league system\u2014both professional and amateur\u2014and is organised by the national federation.Some countries' top divisions feature highly paid star players; in smaller countries, lower divisions, and many women's clubs, players may be part-timers with a second job, or amateurs. The five top European leagues \u2013 Premier League (England), Bundesliga (Germany), La Liga (Spain), Serie A (Italy), and Ligue 1 (France) \u2013 attract most of the world's best players and, during the 2006\u201307 season, each of these leagues had a total wage cost in excess of \u20ac600 million. These leagues also generated a combined \u20ac17.2 billion in revenue in the 2021\u201322 season from television contracts, matchday tickets, sponsorships, and other sources.\n\n\n== See also ==\nList of association football films\nList of association football video games\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nF\u00e9d\u00e9ration Internationale de Football Association (FIFA) (in English, Arabic, French, German, and Spanish)\nInternational Football Association Board (IFAB) (in English, French, German, and Spanish)\nAssociation football at the Encyclop\u00e6dia Britannica\nAssociation football at Curlie"}, {"id": 59, "title": "Film", "content": "A film \u2013 also called a movie, motion picture, moving picture, picture, photoplay or (slang) flick \u2013 is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, feelings, beauty, or atmosphere through the use of moving images. These images are generally accompanied by sound and, more rarely, other sensory stimulations. The word \"cinema\", short for cinematography, is often used to refer to filmmaking and the film industry, and the art form that is the result of it.\n\n\n== Recording and transmission of the film ==\nThe moving images of a film are created by photographing actual scenes with a motion-picture camera, by photographing drawings or miniature models using traditional animation techniques, by means of CGI and computer animation, or by a combination of some or all of these techniques, and other visual effects.\nBefore the introduction of digital production, a series of still images were recorded on a strip of chemically sensitized celluloid (photographic film stock), usually at a rate of 24 frames per second. The images are transmitted through a movie projector at the same rate as they were recorded, with a Geneva drive ensuring that each frame remains still during its short projection time. A rotating shutter causes stroboscopic intervals of darkness, but the viewer does not notice the interruptions due to flicker fusion. The apparent motion on the screen is the result of the fact that the visual sense cannot discern the individual images at high speeds, so the impressions of the images blend with the dark intervals and are thus linked together to produce the illusion of one moving image. An analogous optical soundtrack (a graphic recording of the spoken words, music and other sounds) runs along a portion of the film exclusively reserved for it, and was not projected.\nContemporary films are usually fully digital  through the entire process of production, distribution, and exhibition.\n\n\n== Etymology and alternative terms ==\nThe name \"film\" originally referred to the thin layer of photochemical emulsion on the celluloid strip that used to be the actual medium for recording and displaying motion pictures.\nMany other terms exist for an individual motion-picture, including \"picture\", \"picture show\", \"moving picture\", \"photoplay\", and \"flick\". The most common term in the United States is \"movie\", while in Europe, \"film\" is preferred. Archaic terms include \"animated pictures\" and \"animated photography\".\n\"Flick\" is, in general a slang term, first recorded in 1926. It originates in the verb flicker, owing to the flickering appearance of early films.Common terms for the field, in general, include \"the big screen\", \"the silver screen\", \"the movies\", and \"cinema\"; the last of these is commonly used, as an overarching term, in scholarly texts and critical essays. In the early years, the word \"sheet\" was sometimes used instead of \"screen\".\n\n\n== History ==\n\n\n=== Precursors ===\nThe art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images include:\n\nshadowgraphy, probably used since prehistoric times\ncamera obscura, a natural phenomenon that has possibly been used as an artistic aid since prehistoric times\nshadow puppetry, possibly originated around 200 BCE in Central Asia, India, Indonesia or China\nThe magic lantern, developed in the 1650s. The multi-media phantasmagoria shows that magic lanterns were popular from 1790 throughout the first half of the 19th century and could feature mechanical slides, rear projection, mobile projectors, superimposition, dissolving views, live actors, smoke (sometimes to project images upon), odors, sounds and even electric shocks.\n\n\n=== Before celluloid ===\nThe stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the ph\u00e9nakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography.\nExperiments with early ph\u00e9nakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed ph\u00e9nakisticope projection systems in France from c.\u20091853 until the 1890s.\nPhotography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture colour and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the ph\u00e9nakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the \"St\u00e9r\u00e9oscope-fantascope, ou B\u00efoscope\", but he only marketed it very briefly, without success. One B\u00efoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found.\n\nBy the late 1850s the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as \u00c9tienne-Jules Marey, Ottomar Ansch\u00fctz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895.\n\nAnsch\u00fctz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1/1000 of a second in 1882. The quality of his pictures was generally regarded to be much higher than that of the chronophotography works Muybridge and \u00c9tienne-Jules Marey.\nIn 1886, Ansch\u00fctz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to the speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally. Nearly 34,000 people paid to see it at the Berlin Exhibition Park in summer 1892. Others saw it in London or at the 1893 Chicago World's Fair.On 25 November 1894, Ansch\u00fctz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin.\n\u00c9mile Reynaud already mentioned the possibility of projecting the images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Soci\u00e9t\u00e9 fran\u00e7aise de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Th\u00e9\u00e2tre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Mus\u00e9e Gr\u00e9vin in Paris.\n\n\n=== First motion pictures ===\nBy the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film.\nMovies were initially shown publicly to one person at a time through \"peep show\" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences.\nThe first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons, using films produced by their Eidoloscope company, by the Skladanowsky brothers and by the \u2013 arguably better known \u2013 French brothers Auguste and Louis Lumi\u00e8re with ten of their own productions. Private screenings had preceded these by several months, with Latham's slightly predating the others\u00b4s.\n\n\n=== Early evolution ===\nThe earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles. Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.\n\nThe rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.\n\n\n=== Sound ===\nIn the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen. The resulting sound films were initially distinguished from the usual silent \"moving pictures\" or \"movies\" by calling them \"talking pictures\" or \"talkies.\" The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as \"the old medium.\"The evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927).American film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933).As the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters.In 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters.Today, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development. However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences.\n\n\n=== Color ===\nA significant technological advancement in film was the introduction of \"natural color,\" where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring. Early color processes often produced colors that appeared far from \"natural\". Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually.The crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932. The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935. Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost. Consequently, the number of films made in color gradually increased year after year.\n\n\n=== 1950s: growing influence of television ===\nIn the early 1950s, the proliferation of black-and-white television started seriously depressing North American theater attendance. In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color. During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of \"star\" filmmakers such as Peter Bogdanovich and Martin Scorsese.\n\n\n=== 1960s and later ===\nThe decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century. Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and has become popular in the early 2010s.\n\n\n== Film theory ==\n\"Film theory\" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, B\u00e9la Bal\u00e1zs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. Andr\u00e9 Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.\n\n\n=== Language ===\nFilm is considered to have its own language. James Monaco wrote a classic text on film theory, titled \"How to Read a Film,\" that addresses this. Director Ingmar Bergman famously said, \"Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream.\" An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation. This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing. The \"Hollywood style\" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.\n\n\n=== Montage ===\n\nMontage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning.The concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience.As the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-sc\u00e8ne, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet.The French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and Fran\u00e7ois Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films.In contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema.\n\nRapid editing and fast-paced montages: With the advent of digital editing tools, filmmakers can now create rapid and intricate montages to convey information or emotions quickly. Films like Darren Aronofsky's Requiem for a Dream (2000) and Edgar Wright's Shaun of the Dead (2004) employ fast-paced editing techniques to create immersive and intense experiences for the audience.\nMusic video influence: The influence of music videos on film has led to the incorporation of stylized montage sequences, often accompanied by popular music. Films like Guardians of the Galaxy (2014) and Baby Driver (2017) use montage to create visually striking sequences that are both entertaining and narratively functional.\nSports and training montages: The sports and training montage has become a staple in modern cinema, often used to condense time and show a character's growth or development. Examples of this can be found in films like Rocky (1976), The Karate Kid (1984), and Million Dollar Baby (2004).\nCross-cutting and parallel action: Contemporary filmmakers often use montage to create tension and suspense by cross-cutting between parallel storylines. Christopher Nolan's Inception (2010) and Dunkirk (2017) employ complex cross-cutting techniques to build narrative momentum and heighten the audience's emotional engagement.\nThematic montage: Montage can also be used to convey thematic elements or motifs in a film. Wes Anderson's The Royal Tenenbaums (2001) employs montage to create a visual language that reflects the film's themes of family, nostalgia, and loss.As the medium of film continues to evolve, montage remains an integral aspect of visual storytelling, with filmmakers finding new and innovative ways to employ this powerful technique.\n\n\n=== Film criticism ===\n\nFilm criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media. Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate their opinions. Despite this, critics have an important impact on the audience response and attendance at films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film and the assessment of the director's and screenwriters' work that makes up the majority of most film reviews can still have an important impact on whether people decide to see a film. For prestige films such as most dramas and art films, the influence of reviews is important. Poor reviews from leading critics at major papers and magazines will often reduce audience interest and attendance.\nThe impact of a reviewer on a given film's box office performance is a matter of debate. Some observers claim that movie marketing in the 2000s is so intense, well-coordinated and well financed that reviewers cannot prevent a poorly written or filmed blockbuster from attaining market success. However, the cataclysmic failure of some heavily promoted films which were harshly reviewed, as well as the unexpected success of critically praised independent films indicates that extreme critical reactions can have considerable influence. Other observers note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires, as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result. Journalist film critics are sometimes called film reviewers. Critics who take a more academic approach to films, through publishing in film journals and writing books about films using film theory or film studies approaches, study how film and filming techniques work, and what effect they have on people. Rather than having their reviews published in newspapers or appearing on television, their articles are published in scholarly journals or up-market magazines. They also tend to be affiliated with colleges or universities as professors or instructors.\n\n\n== Industry ==\n\nThe making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the Lumi\u00e8res quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import, and screen additional product commercially. The Oberammergau Passion Play of 1898 was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. By 1917 Charlie Chaplin had a contract that called for an annual salary of one million dollars. From 1931 to 1956, film was also the only image storage and playback system for television programming until the introduction of videotape recorders.\nIn the United States, much of the film industry is centered around Hollywood, California. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world. Though the expense involved in making films has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish.\nProfit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, an example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as \"the Oscars\") are the most prominent film awards in the United States, providing recognition each year to films, based on their artistic merits. There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts. Revenue in the industry is sometimes volatile due to the reliance on blockbuster films released in movie theaters. The rise of alternative home entertainment has raised questions about the future of the cinema industry, and Hollywood employment has become less reliable, particularly for medium and low-budget films.\n\n\n== Associated fields ==\n\nDerivative academic fields of study may both interact with and develop independently of filmmaking, as in film theory and analysis. Fields of academic study have been created that are derivative or dependent on the existence of film, such as film criticism, film history, divisions of film propaganda in authoritarian governments, or psychological on subliminal effects (e.g., of a flashing soda can during a screening). These fields may further create derivative fields, such as a movie review section in a newspaper or a television guide. Sub-industries can spin off from film, such as popcorn makers, and film-related toys (e.g., Star Wars figures). Sub-industries of pre-existing industries may deal specifically with film, such as product placement and other advertising within films.\n\n\n== Terminology ==\nThe terminology used for describing motion pictures varies considerably between British and American English. In British usage, the name of the medium is \"film\". The word \"movie\" is understood but seldom used. Additionally, \"the pictures\" (plural) is used semi-frequently to refer to the place where movies are exhibited, while in American English this may be called \"the movies\", but it is becoming outdated. In other countries, the place where movies are exhibited may be called a cinema or movie theatre. By contrast, in the United States, \"movie\" is the predominant form. Although the words \"film\" and \"movie\" are sometimes used interchangeably, \"film\" is more often used when considering artistic, theoretical, or technical aspects. The term \"movies\" more often refers to entertainment or commercial aspects, as where to go for fun evening on a date. For example, a book titled \"How to Understand a Film\" would probably be about the aesthetics or theory of film, while a book entitled \"Let's Go to the Movies\" would probably be about the history of entertaining movies and blockbusters.\nFurther terminology is used to distinguish various forms and media used in the film industry. \"Motion pictures\" and \"moving pictures\" are frequently used terms for film and movie productions specifically intended for theatrical exhibition, such as, for instance, Star Wars. \"DVD\" and \"videotape\" are video formats that can reproduce a photochemical film. A reproduction based on such is called a \"transfer.\" After the advent of theatrical film as an industry, the television industry began using videotape as a recording medium. For many decades, tape was solely an analog medium onto which moving images could be either recorded or transferred. \"Film\" and \"filming\" refer to the photochemical medium that chemically records a visual image and the act of recording respectively. However, the act of shooting images with other visual media, such as with a digital camera, is still called \"filming\" and the resulting works often called \"films\" as interchangeable to \"movies,\" despite not being shot on film. \"Silent films\" need not be utterly silent, but are films and movies without an audible dialogue, including those that have a musical accompaniment. The word, \"Talkies,\" refers to the earliest sound films created to have audible dialogue recorded for playback along with the film, regardless of a musical accompaniment. \"Cinema\" either broadly encompasses both films and movies, or it is roughly synonymous with film and theatrical exhibition, and both are capitalized when referring to a category of art. The \"silver screen\" refers to the projection screen used to exhibit films and, by extension, is also used as a metonym for the entire film industry.\n\"Widescreen\" refers to a larger width to height in the frame, compared to earlier historic aspect ratios. A \"feature-length film\", or \"feature film\", is of a conventional full length, usually 60 minutes or more, and can commercially stand by itself without other films in a ticketed screening. A \"short\" is a film that is not as long as a feature-length film, often screened with other shorts, or preceding a feature-length film. An \"independent\" is a film made outside the conventional film industry.\nIn US usage, one talks of a \"screening\" or \"projection\" of a movie or video on a screen at a public or private \"theater.\" In British English, a \"film showing\" happens at a cinema (never a \"theatre\", which is a different medium and place altogether). A cinema usually refers to an arena designed specifically to exhibit films, where the screen is affixed to a wall, while a theater usually refers to a place where live, non-recorded action or combination thereof occurs from a podium or other type of stage, including the amphitheater. Theaters can still screen movies in them, though the theater would be retrofitted to do so. One might propose \"going to the cinema\" when referring to the activity, or sometimes \"to the pictures\" in British English, whereas the US expression is usually \"going to the movies.\" A cinema usually shows a mass-marketed movie using a front-projection screen process with either a film projector or, more recently, with a digital projector. But, cinemas may also show theatrical movies from their home video transfers that include Blu-ray Disc, DVD, and videocassette when they possess sufficient projection quality or based upon need, such as movies that exist only in their transferred state, which may be due to the loss or deterioration of the film master and prints from which the movie originally existed. Due to the advent of digital film production and distribution, physical film might be absent entirely. A \"double feature\" is a screening of two independently marketed, stand-alone feature films. A \"viewing\" is a watching of a film. \"Sales\" and \"at the box office\" refer to tickets sold at a theater, or more currently, rights sold for individual showings. A \"release\" is the distribution and often simultaneous screening of a film. A \"preview\" is a screening in advance of the main release.\nAny film may also have a \"sequel\", which portrays events following those in the film. Bride of Frankenstein is an early example. When there are more films than one with the same characters, story arcs, or subject themes, these movies become a \"series,\" such as the James Bond series. And, existing outside a specific story timeline usually, does not exclude a film from being part of a series. A film that portrays events occurring earlier in a timeline with those in another film, but is released after that film, is sometimes called a \"prequel,\" an example being Butch and Sundance: The Early Days.\nThe \"credits,\" or \"end credits,\" is a list that gives credit to the people involved in the production of a film. Films from before the 1970s usually start a film with credits, often ending with only a title card, saying \"The End\" or some equivalent, often an equivalent that depends on the language of the production. From then onward, a film's credits usually appear at the end of most films. However, films with credits that end a film often repeat some credits at or near the start of a film and therefore appear twice, such as that film's acting leads, while less frequently some appearing near or at the beginning only appear there, not at the end, which often happens to the director's credit. The credits appearing at or near the beginning of a film are usually called \"titles\" or \"beginning titles.\" A post-credits scene is a scene shown after the end of the credits. Ferris Bueller's Day Off has a post-credit scene in which Ferris tells the audience that the film is over and they should go home.\nA film's \"cast\" refers to a collection of the actors and actresses who appear, or \"star,\" in a film. A star is an actor or actress, often a popular one, and in many cases, a celebrity who plays a central character in a film. Occasionally the word can also be used to refer to the fame of other members of the crew, such as a director or other personality, such as Martin Scorsese. A \"crew\" is usually interpreted as the people involved in a film's physical construction outside cast participation, and it could include directors, film editors, photographers, grips, gaffers, set decorators, prop masters, and costume designers. A person can both be part of a film's cast and crew, such as Woody Allen, who directed and starred in Take the Money and Run.\nA \"film goer,\" \"movie goer,\" or \"film buff\" is a person who likes or often attends films and movies, and any of these, though more often the latter, could also see oneself as a student to films and movies or the filmic process. Intense interest in films, film theory, and film criticism, is known as cinephilia. A film enthusiast is known as a cinephile or cineaste.\n\n\n=== Preview ===\n\nA preview performance refers to a showing of a film to a select audience, usually for the purposes of corporate promotions, before the public film premiere itself. Previews are sometimes used to judge audience reaction, which if unexpectedly negative, may result in recutting or even refilming certain sections based on the audience response. One example of a film that was changed after a negative response from the test screening is 1982's First Blood. After the test audience responded very negatively to the death of protagonist John Rambo, a Vietnam veteran, at the end of the film, the company wrote and re-shot a new ending in which the character survives.\n\n\n=== Trailer and teaser ===\n\nTrailers or previews are advertisements for films that will be shown in 1 to 3 months at a cinema. Back in the early days of cinema, with theaters that had only one or two screens, only certain trailers were shown for the films that were going to be shown there. Later, when theaters added more screens or new theaters were built with a lot of screens, all different trailers were shown even if they were not going to play that film in that theater. Film studios realized that the more trailers that were shown (even if it was not going to be shown in that particular theater) the more patrons would go to a different theater to see the film when it came out. The term \"trailer\" comes from their having originally been shown at the end of a film program. That practice did not last long because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film (or the \"A film\" in a double feature program) begins. Film trailers are also common on DVDs and Blu-ray Discs, as well as on the Internet and mobile devices. Trailers are created to be engaging and interesting for viewers. As a result, in the Internet era, viewers often seek out trailers to watch them. Of the ten billion videos watched online annually in 2008, film trailers ranked third, after news and user-created videos. Teasers are a much shorter preview or advertisement that lasts only 10 to 30 seconds. Teasers are used to get patrons excited about a film coming out in the next six to twelve months. Teasers may be produced even before the film production is completed.\n\n\n== The role of film in culture ==\nFilms are cultural artifacts created by specific cultures, facilitating intercultural dialogue. It is considered to be an important art form that provides entertainment and historical value, often visually documenting a period of time. The visual basis of the medium gives it a universal power of communication, often stretched further through the use of dubbing or subtitles to translate the dialog into other languages. Just seeing a location in a film is linked to higher tourism to that location, demonstrating how powerful the suggestive nature of the medium can be.\n\n\n=== Education and propaganda ===\n\nFilm is used for a range of goals, including education and propaganda due its ability to effectively intercultural dialogue. When the purpose is primarily educational, a film is called an \"educational film\". Examples are recordings of academic lectures and experiments, or a film based on a classic novel. Film may be propaganda, in whole or in part, such as the films made by Leni Riefenstahl in Nazi Germany, US war film trailers during World War II, or artistic films made under Stalin by Sergei Eisenstein. They may also be works of political protest, as in the films of Andrzej Wajda, or more subtly, the films of Andrei Tarkovsky. The same film may be considered educational by some, and propaganda by others as the categorization of a film can be subjective.\n\n\n== Production ==\n\nAt its core, the means to produce a film depend on the content the filmmaker wishes to show, and the apparatus for displaying it: the zoetrope merely requires a series of images on a strip of paper. Film production can, therefore, take as little as one person with a camera (or even without a camera, as in Stan Brakhage's 1963 film Mothlight), or thousands of actors, extras, and crew members for a live-action, feature-length epic.\nThe necessary steps for almost any film can be boiled down to conception, planning, execution, revision, and distribution. The more involved the production, the more significant each of the steps becomes. In a typical production cycle of a Hollywood-style film, these main stages are defined as development, pre-production, production, post-production and distribution.\nThis production cycle usually takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution. The bigger the production, the more resources it takes, and the more important financing becomes; most feature films are artistic works from the creators' perspective (e.g., film director, cinematographer, screenwriter) and for-profit business entities for the production companies.\n\n\n=== Crew ===\n\nA film crew is a group of people hired by a film company, employed during the \"production\" or \"photography\" phase, for the purpose of producing a film or motion picture. Crew is distinguished from cast, who are the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as screenwriters and film editors. Communication between production and crew generally passes through the director and his/her staff of assistants. Medium-to-large crews are generally divided into departments with well-defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as \"craft services\") are usually not considered part of the crew.\n\n\n=== Technology ===\n\nFilm stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35 mm prints.\nOriginally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (162/3 frame/s) is generally cited as a standard silent speed, research indicates most films were shot between 16 frame/s and 23 frame/s and projected from 18 frame/s on up (often reels included instructions on how fast each scene should be shown). When synchronized sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second were chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality. The standard was set with Warner Bros.'s The Jazz Singer and their Vitaphone system in 1927. Improvements since the late 19th century include the mechanization of cameras \u2013 allowing them to record at a consistent speed, quiet camera design \u2013 allowing sound recorded on-set to be usable without requiring large \"blimps\" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures, many parts of the soundtrack are usually recorded simultaneously.\nAs a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most films on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters: three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher concern for nitrate and single-strip color films, due to their high decay rates; black-and-white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage.\nSome films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are preferred by some film-makers, especially because footage shot with digital cinema can be evaluated and edited with non-linear editing systems (NLE) without waiting for the film stock to be processed. The migration was gradual, and as of 2005, most major motion pictures were still shot on film.\n\n\n=== Independent ===\n\nIndependent filmmaking often takes place outside Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major film studio. Creative, business and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century. On the business side, the costs of big-budget studio films also lead to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987). A hopeful director is almost never given the opportunity to get a job on a big-budget studio film unless he or she has significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles.\nBefore the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to film production significantly. Both production and post-production costs have been significantly lowered; in the 2000s, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and a wide variety of professional and consumer-grade video editing software make film-making relatively affordable.\nSince the introduction of digital video DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot a film with a digital video camera and edit the film, create and edit the sound and music, and mix the final cut on a high-end home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video websites such as YouTube and Veoh has further changed the filmmaking landscape, enabling indie filmmakers to make their films available to the public.\n\n\n=== Open content film ===\n\nAn open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works rather than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside Hollywood and other major studio systems. For example, the film Balloon was based on the real event during the Cold War.\n\n\n=== Fan film ===\n\nA fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the most notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures.\n\n\n== Distribution ==\n\nFilm distribution is the process through which a film is made available for viewing by an audience. This is normally the task of a professional film distributor, who would determine the marketing strategy of the film, the media by which a film is to be exhibited or made available for viewing, and may set the release date and other matters. The film may be exhibited directly to the public either through a movie theater (historically the main way films were distributed) or television for personal home viewing (including on DVD-Video or Blu-ray Disc, video-on-demand, online downloading, television programs through broadcast syndication etc.). Other ways of distributing a film include rental or personal purchase of the film in a variety of media and formats, such as VHS tape or DVD, or Internet downloading or streaming using a computer.\n\n\n== Animation ==\n\nAnimation is a technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the phi phenomenon). Generating such a film is very labor-intensive and tedious, though the development of computer animation has greatly sped up the process. Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and films comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry.\nLimited animation is a way of increasing production and decreasing costs of animation by using \"short cuts\" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera in the United States, and by Osamu Tezuka in Japan, and adapted by other studios as cartoons moved from movie theaters to television. Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Camera-less animation, made famous by film-makers like Norman McLaren, Len Lye, and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector.\n\n\n== See also ==\nDocufiction (hybrid genre)\nFilmophile\nLists\nBibliography of film by genre\nGlossary of motion picture terms\nIndex of video-related articles\nList of film awards\nList of film festivals\nList of film periodicals\nList of years in film\nLists of films\nList of books on films\nOutline of film\nLost film\nThe Movies, a simulation game about the film industry, taking place at the dawn of cinema\nPlatforms\nTelevision film\nWeb film\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nBurton, Gideon O., and Randy Astle, jt. eds. (2007). \"Mormons and Film\", entire special issue, B.Y.U. Studies (Brigham Young University), vol. 46 (2007), no. 2. 336 p, ill. ISSN 0007-0106.\nHickenlooper, George (1991). Reel [sic] Conversations: Candid Interviews with Film's Foremost Directors and Critics, in series, Citadel Press Book[s]. New York: Carol Publishing Group. xii, 370 p. ISBN 0-8065-1237-7.\nThomson, David (2002). The New Biographical Dictionary of Film (4th ed.). New York: A.A. Knopf. ISBN 0-375-41128-3.\nJeffrey Zacks (2014). Flicker: Your Brain on Movies. Oxford University Press. ISBN 978-0-19-998287-5.\n\n\n== External links ==\n\nAllmovie \u2013 Information on films: actors, directors, biographies, reviews, cast and production credits, box office sales, and other movie data.\nFilm Site \u2013 Reviews of classic films\nMovies at Curlie\nRottentomatoes.com \u2013 Movie reviews, previews, forums, photos, cast info, and more.\nIMDb: The Internet Movie Database \u2013 Information on current and historical films and cast listings (archived 22 January 1997)"}, {"id": 60, "title": "Personal finance", "content": "Personal finance is the financial management which an individual or a family unit performs to budget, save, and spend monetary resources over time, taking into account various financial risks and future life events.\nWhen planning personal finances, the individual would consider the suitability to their needs of a range of banking products (checking, savings accounts, credit cards and consumer loans) or investment in private equity, (companies' shares, bonds, mutual funds) and insurance (life insurance, health insurance, disability insurance) products or participation and monitoring of and- or employer-sponsored retirement plans, social security benefits, and income tax management.\n\n\n== History ==\nBefore a specialty in personal finance was developed, various disciplines which are closely related to it, such as family economics, and consumer economics, were taught in various colleges as part of home economics for over 100 years.The earliest known research in personal finance was done in 1920 by Hazel Kyrk. Her dissertation at University of Chicago laid the foundation of consumer economics and family economics. Margaret Reid, a professor of Home Economics at the same university, is recognized as one of the pioneers in the study of consumer behavior and Household behavior.In 1947, Herbert A. Simon, a Nobel laureate, suggested that a decision-maker did not always make the best financial decision because of limited educational resources and personal inclinations. In 2009, Dan Ariely suggested the 2008 financial crisis showed that human beings do not always make rational financial decisions, and the market is not necessarily automated and corrective of any imbalances in the economy.Research into personal finance is based on several theories, such as social exchange theory and andragogy (adult learning theory). Professional bodies such as American Association of Family and Consumer Sciences and the American Council on Consumer Interests started to play an important role in developing this field from the 1950s to the 1970s. The establishment of the Association for Financial Counseling and Planning Education (AFCPE) in 1984 at Iowa State University and the Academy of Financial Services (AFS) in 1985 marked an important milestone in personal finance history. Attendances of the two societies mainly come from faculty and graduates from business and home economics colleges. AFCPE started to offered several certifications for professionals in this field, such as Accredited Financial Counselor (AFC) and Certified Housing Counselor (CHC). Meanwhile, AFS cooperates with Certified Financial Planner (CFP Board).Before 1990, the study of personal finance received little attention from mainstream economists and business faculties. However, several American universities such as Brigham Young University, Iowa State University, and San Francisco State University started to offer financial educational programs in both undergraduate and graduate programs since the 1990s. These institutions published several works in journals such as The Journal of Financial Counseling and Planning and the Journal of Personal Finance. \nAs the concerns about consumers' financial capability increased during the early 2000s, various education programs emerged, catering to a broad audience or a specific group of people, such as youth and women. The educational programs are frequently known as \"financial literacy\". However, there was no standardized curriculum for personal finance education until after the 2008 financial crisis. The United States President's Advisory Council on Financial Capability was set up in 2008 to encourage financial literacy among the American people. It also stressed the importance of developing a standard in financial education.\n\n\n== Personal finance principles ==\nIndividual situations vary significantly when it comes to income, wealth, and consumption requirements. Moreover, tax and financial regulations vary between countries, and market conditions change both geographically and over time. This means that advice for one person might not be appropriate for another. A financial advisor can offer personalized advice in complicated situations and for high-wealth individuals. Still, University of Chicago professor Harold Pollack and personal finance writer Helaine Olen argue that in the United States, good personal finance advice boils down to a few simple points:\nPay off credit card balances every month in full\nSave 20% of income\nCreate an emergency fund that can last at least 6 months\nMaximize contributions to tax-advantaged funds such as a 401(k) retirement funds, individual retirement accounts, and 529 education savings plans\nWhen investing savings:\nAvoid trading individual securities\nLook for low-cost, diversified mutual funds that balance risk vs. reward appropriately to an individual's target retirement year\nIf using a financial advisor, require them to commit to a fiduciary duty to act in an individual's best interest\n\n\n== Personal financial planning process ==\nThe key component of personal finance is financial planning, a dynamic process requiring regular monitoring and re-evaluation. In general, it involves five steps:\nAssessment: A person's financial situation is assessed by compiling simplified versions of financial statements, including balance sheets and income statements. A personal balance sheet lists the values of personal assets (e.g., car, house, clothes, stocks, bank account, cryptocurrencies), along with personal liabilities (e.g., credit card debt, bank loan, mortgage). A personal income statement lists personal income and expenses.\nGoal setting: Multiple goals are expected, including short- and long-term goals. For example, a long-term goal would be to \"retire at age 65 with a personal net worth of $1,000,000\", while a short-term goal would be to \"save up for a new computer in the next month.\" Setting financial goals helps to direct financial planning. Goal setting is done to meet specific financial requirements.\nPlan creation: The financial plan details how to accomplish the goals. It could include, for example, reducing unnecessary expenses, increasing employment income, or investing in the stock market.\nExecution: Execution of a financial plan often requires discipline and perseverance. Many people obtain assistance from professionals such as accountants, financial planners, investment advisers, and lawyers.\nMonitoring and reassessment: The financial plan is monitored for possible adjustments or reassessments as time passes.Typical goals that most adults and young adults have are paying off credit card/student loan/housing/car loan debt, investing for retirement, investing for college costs for children, and paying medical expenses.Need for Personal Finance\nThere is a great need for people to understand and take control of their finances. These are some of the overarching reasons for it;\n1. No formal education for personal finance: Most countries have a formal education across most disciplines or areas of study.\n\nTheir pursuit translates to earning tangible outcomes in the form of money.\nEven when we realize the above to be a primary objective, there is no formal education at an elementary level in schools or colleges to learn money management or personal finance.\nHence, it is essential to understand this gap or disconnect in the education system where there is no formal way of equipping individuals to manage their own money.This illustrates the need to learn personal finance from an early stage, to differentiate between needs vs. wants and plan accordingly.\n2. Shortened employable age: Over the years, with the advent of automation  and changing needs; it has been witnessed across the globe that several jobs that require manual intervention or that are mechanical are increasingly becoming redundant.\n\nSeveral employment opportunities are shifting from countries with higher labor costs to countries with lower labor costs keeping margins low for companies.\nIn economies with a considerably large younger population entering the workforce who are more equipped with the latest technologies, several employees in the middle management who have not up-skilled are easily replaceable with new and fresh talent that is cheaper and more valuable to the organizations.\nCyclical nature of several industries like automobile, chemicals, construction; consumption and demand is driven by the health of the countries economy. It has been observed that when economies stagnate, are in recession, and in war - specific industries suffer more than others. This results in companies rationalizing their workforce. An individual can lose their job quickly and remain unemployed for a considerable time. All these reasons bring to the realization that the legal employable age of 60 is slowly and gradually becoming shorter.These are some of the reasons why individuals should start planning for their retirement and systematically build on their retirement corpus, hence the need for personal finance.\n3. Increased life expectancy: With the developments in healthcare, people today live till a much older age than their forefathers. The average life expectancy has changed, and people, even in developing economies, live much longer. The average life expectancy has gradually shifted from 60 to 81 and upwards. Increased life expectancy coupled with a shorter employable age reinforces the need for a large enough retirement corpus and the importance of personal finance.\n4. Rising medical expenses: Medical expenses including cost of prescription medicine, hospital admission care and charges, nursing care, specialized care, geriatric care have all seen an exponential rise over the years. Many of these medical expenses are not covered through insurance policies that might either be private/individual insurance coverage or through federal or national insurance coverage.\n\nIn developed markets like the US, insurance coverage is provided by either the employers, private insurers or through federal government (Medicare, primarily for senior citizens or Medicaid, primarily for individuals of lower income levels). However, with the rising US fiscal deficit and large proportion of the senior population, it needs to be seen whether the extent of the Medicare program is sustainable in the long run, therapy exclusions in the coverage, co-pay, deductibles - several cost elements are to be borne by individuals continually.\nIn other developed markets like the EU, most medical care is nationally reimbursed. This leads to the national healthcare budgets being very tightly controlled. Many newer, expensive therapies are frequently excluded from national formularies. This means that patients may not have access through the government policy and would have to pay out of pocket to avail of these medicines\nIn developing countries like India, China, most of the expenses are out of pocket as there is no overarching government social security system covering medical expenses.These reasons illustrate the need to have medical, accidental, critical illness, life coverage insurance for oneself and one's family as well as the need for emergency corpus; translating the immense need for personal finance.\n\n\n== Areas of focus ==\nCritical areas of personal financial planning, as suggested by the Financial Planning Standards Board, are:\nFinancial position: is concerned with understanding the personal resources available by examining net worth and household cash flow.  Net worth is a person's balance sheet, calculated by adding up all assets under that person's control, minus all household liabilities, at one point.  Household cash flow totals all the expected income sources within a year minus all expected expenses within the same year.  From this analysis, the financial planner can determine to what degree and when the personal goals can be accomplished.\nAdequate protection: or insurance, the analysis of how to protect a household from unforeseen risks.  These risks can be divided into liability, property, death, disability, health, and long-term care.  Some wagers may be self-insurable, while most require an insurance contract.  Determining how much insurance to get, at the most cost-effective terms, requires knowledge of the market for personal insurance.  Business owners, professionals, athletes, and entertainers need specialized insurance professionals to protect themselves adequately.  Since insurance also enjoys some tax benefits, utilizing insurance investment products may be critical to the overall investment planning.\nTax planning: typically, the income tax is the single largest expense in a household. Managing taxes is not a question of whether or not taxes will be paid but when and how much.  The government gives many incentives in the form of tax deductions and credits, which can be used to reduce the lifetime tax burden.  Most modern governments use a progressive tax.  As one's income grows, a higher marginal rate of tax must be paid.  Understanding how to take advantage of the myriad tax breaks when planning one's finances can significantly impact.\nInvestment and accumulation goals: planning how to accumulate enough money for large purchases and life events is what most people consider financial planning. Significant reasons to get assets include purchasing a house or car, starting a business, paying for education expenses, and saving for retirement.\nAchieving these goals requires projecting their costs and when to withdraw funds.  A significant risk to the household in achieving their accumulation goal is the rate of price increases over time, or inflation.  Using net present value calculators, the financial planner will suggest a combination of asset earmarking and regular savings to be invested in various investments.  To overcome the rate of inflation, the investment portfolio has to get a higher rate of return, which typically will subject the portfolio to several risks.  Managing these portfolio risks is often accomplished using asset allocation, which seeks to diversify investment risk and opportunity.  This asset allocation will prescribe a percentage allocation for stocks, bonds, cash, and alternative investments.  The budget should also consider every investor's risk profile since risk attitudes vary from person to person.\nDepreciating Assets- One thing to consider with personal finance and net worth goals is depreciating assets. A depreciating asset is an asset that loses value over time or with use. A few examples would be the vehicle a person owns, boats, and capitalizes expenses. They add value to a person's life, but unlike other assets, they do not make money and should be a class of their own. In the business world, these are depreciated over time for tax and bookkeeping purposes because their useful life runs out. This is known as accumulated depreciation, and the asset will eventually need to be replaced.\nRetirement planning is understanding how much it costs to live at retirement and developing a plan to distribute assets to meet any income shortfall. Methods for retirement plans include taking advantage of government-allowed structures to manage tax liability, including individual (IRA) structures or employer-sponsored retirement plans.\nEstate planning involves planning to disposition one's assets after death.  Typically, a tax is due to the state or federal government when one dies.  Avoiding these taxes means more of one's assets will be distributed to their heirs.  One can leave their assets to family, friends, or charitable groups.\nDelayed gratification: Delayed gratification, or deferred gratification, is the ability to resist the temptation for an immediate reward and wait for a later reward. This is thought to be an important consideration in the creation of personal wealth.\nCash Management: It is the soul of financial planning, whether a person is an employee or planning for retirement. It is a must for every financial planner to know how much they spend before their retirement so that they can save a significant amount. This analysis is a wake-up call as many of us know our income, but very few track their expenses.\nRevisiting Written Financial Plan Regularly: Make monitoring a financial plan regularly a habit. An annual financial planning review with a professional keeps people well-positioned and informed about the required changes, if any, in personal needs or life circumstances. It would be best to be prepared for all the sudden curve balls life throws.\nEducation Planning: With the growing interest in students' loans, having a proper financial plan in place is crucial. Parents often want to save for their kids but make the wrong decisions, adversely affecting the savings. We often observe that many parents give their kids expensive gifts or unintentionally endanger the opportunity to obtain the much-needed grant. Instead, one should make their kids prepare for the future and support them financially in their education.\nReal Estate Planning:  Shelter is a basic human need, and as such, it is imperative that one understands how to obtain a place to live and at the same time maintain their financial security.  Housing can be very complicated, with decisions regarding buying or renting, mortgages, insurance, taxes, utilities, maintenance, etc.  Apartment or house?  That question is crucial for any individual as each option has pros and cons.\nBuy or Rent: If a person chooses to buy a house, they can make a financial investment in a home and improve their credit score and history. Home ownership can make life more stable. The price of the house, including the down payment, monthly mortgage payment, repair and maintenance costs, HOA fees, utilities, insurance, property taxes, and other costs, are considerations. If renting a home is chosen, there is no need to worry about maintenance and no real estate taxes. Moving to a different location can also be easier. Expenses for renters may include electricity, water, internet, parking, and pet fees.\nMortgages: When purchasing a home/real estate, it is essential to understand the options. Most people either go with a 15- or 30-year plan. The payment rate can be a fixed plan, a constant payment of the same amount over a certain period. The other is an ARM mortgage (Adjustable-Rate Mortgage). This rate can be adjusted and agreed upon to be changed in the given plan depending on mortgage rate fluctuations. Mortgage plans depend on a person's situation, and it is essential for potential borrowers to assess their credit score and financial status when contemplating plans.\nLocation / Wants and Needs: When choosing a new home, it is essential to consider the location, along with the qualities that are desired and needed in a home. These variables can increase or decrease the price of an estate. Location-related considerations include a city or rural location, length of commute, the importance of quality public schools, level of safety, the amount of land, included amenities, proximity to family. Examples of variables that would affect the value of an estate include but are not limited to, the quality of school systems in that area, proximity to the community, shopping and entertainment/recreation, safety levels and crime rates of the neighborhood, amenities, and land size and surrounding developments. It is essential to keep all of this in mind when thinking about the future value of a home.\n\n\n== Education and tools ==\n\nAccording to a survey done by Harris Interactive, 99% of the adults agreed that personal finance should be taught in schools. Financial authorities and the American federal government had offered free educational materials online to the public. However, a Bank of America poll found that 42% of adults were discouraged. In comparison, 28% of adults thought that personal finance is difficult because of the vast amount of online information. As of 2015, 17 out of 50 states in the United States require high school students to study personal finance before graduation. The effectiveness of financial education on general audience is controversial. For example, a study by Bell, Gorin, and Hogarth (2009) stated that financial education graduates were more likely to use a formal spending plan. Financially educated high school students are more likely to have a savings account with regular savings, fewer overdrafts, and more likely to pay off their credit card balances. However, another study done by Cole and Shastry (Harvard Business School, 2009) found that there were no differences in saving behaviors of people in American states with financial literacy mandate enforced and the states without a literacy mandate.Kiplinger publishes magazines on personal finance.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nGraham, Benjamin; Jason Zweig (8 July 2003) [1949]. The Intelligent Investor. Warren E. Buffett (collaborator) (2003 ed.). HarperCollins. front cover. ISBN 0-06-055566-1.\nKwok, H., Milevsky, M., and Robinson, C. (1994) Asset Allocation, Life Expectancy, and Shortfall, Financial Services Review, 1994, vol 3(2), pg. 109\u2013126.\nRich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not!, by Robert Kiyosaki and Sharon Lechter. Warner Business Books, 2000. ISBN 0-446-67745-0\nStanley, Thomas J. and Danko, W.D. (1998). The Millionaire Next Door. Gallery Books. ISBN 978-0-671-01520-6. LCCN 98046515.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nOpdyke, J.D. (2010). The Wall Street Journal. Complete Personal Finance Guidebook. The Wall Street Journal Guidebooks. Crown Publishing Group. p. 256. ISBN 978-0-307-49887-8.\nClason, George (2015). The Richest Man in Babylon: Original 1926 Edition. CreateSpace Independent Publishing Platform. ISBN 978-1-508-52435-9.\nBogle, John Bogle (2007). The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns. John Wiley and Sons. pp. 216. ISBN 9780470102107.\nDominguez, J.R. and Robin, Vicki (1993). Your Money Or Your Life: Transforming Your Relationship with Money and Achieving Financial Independence. Penguin Books. ISBN 978-0-140-16715-3. LCCN 92003027.{{cite book}}:  CS1 maint: multiple names: authors list (link)\nBach, D. (2009). The Automatic Millionaire: Canadian Edition: A Powerful One-Step Plan to Live and Finish Rich. Doubleday Canada. ISBN 978-0-307-37209-3.\nRamsey, Dave (2003). The Total Money Makeover: A Proven Plan for Financial Fitness. Thomas Nelson. ISBN 978-1-418-52999-4.\n\n\n== External links ==\n Media related to Personal finance at Wikimedia Commons"}, {"id": 61, "title": "Solar power", "content": "Solar power, also known as solar electricity, is the conversion of energy from sunlight into electricity, either directly using photovoltaics (PV) or indirectly using concentrated solar power. Photovoltaic cells convert light into an electric current using the photovoltaic effect. Concentrated solar power systems use lenses or mirrors and solar tracking systems to focus a large area of sunlight to a hot spot, often to drive a steam turbine.\nPhotovoltaics were initially solely used as a source of electricity for small and medium-sized applications, from the calculator powered by a single solar cell to remote homes powered by an off-grid rooftop PV system. Commercial concentrated solar power plants were first developed in the 1980s. Since then, as the cost of solar electricity has fallen,  grid-connected solar PV systems' capacity and production have grown more or less exponentially, doubling about every three years. Millions of installations and gigawatt-scale photovoltaic power stations continue to be built, with half of new generation capacity being solar in 2021.In 2022 solar generated 4.5% of the world's electricity, compared to 1% in 2015 when the Paris Agreement to limit climate change was signed. Along with onshore wind, in most countries the cheapest levelised cost of electricity for new installations is utility-scale solar.Almost half the solar power installed in 2022 was rooftop. Low-carbon power has been recommended as part of a plan to limit climate change. The International Energy Agency said in 2022 that more effort was needed for grid integration and the mitigation of policy, regulation and financing challenges.\n\n\n== Potential ==\nGeography affects solar energy potential because different locations receive different amounts of solar radiation. In particular, with some variations, areas that are closer to the equator generally receive higher amounts of solar radiation. However, the use of photovoltaics that can follow the position of the Sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation affects the potential of solar energy, because during the night there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can affect the potential of solar panels because clouds block incoming light from the Sun and reduce the light available for solar cells.\nBesides, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is otherwise unused and suitable for solar panels. Roofs are a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are not being used for businesses, where solar plants can be established.\n\n\n== Technologies ==\nSolar power plants use one of two technologies:\n\nPhotovoltaic (PV) systems use solar panels, either on rooftops or in ground-mounted solar farms, converting sunlight directly into electric power.\nConcentrated solar power (CSP) uses mirrors or lenses to concentrate sunlight to extreme heat to eventually make steam, which is converted into electricity by a turbine.\n\n\n=== Photovoltaic cells ===\n\nA solar cell, or photovoltaic cell, is a device that converts light into electric current using the photovoltaic effect. The first solar cell was constructed by Charles Fritts in the 1880s. The German industrialist Ernst Werner von Siemens was among those who recognized the importance of this discovery. In 1931, the German engineer Bruno Lange developed a photo cell using silver selenide in place of copper oxide, although the prototype selenium cells converted less than 1% of incident light into electricity. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the silicon solar cell in 1954. These early solar cells cost US$286/watt and reached efficiencies of 4.5\u20136%. In 1957, Mohamed M. Atalla developed the process of silicon surface passivation by thermal oxidation at Bell Labs. The surface passivation process has since been critical to solar cell efficiency.As of 2022 over 90% of the market is crystalline silicon. The array of a photovoltaic system, or PV system, produces direct current (DC) power which fluctuates with the sunlight's intensity. For practical use this usually requires conversion to alternating current (AC), through the use of inverters. Multiple solar cells are connected inside panels. Panels are wired together to form arrays, then tied to an inverter, which produces power at the desired voltage, and for AC, the desired frequency/phase.Many residential PV systems are connected to the grid wherever available, especially in developed countries with large markets. \nIn these grid-connected PV systems, use of energy storage is optional. \nIn certain applications such as satellites, lighthouses, or in developing countries, batteries or additional power generators are often added as back-ups. Such stand-alone power systems permit operations at night and at other times of limited sunlight.\n\n\n==== Thin-film solar ====\n\nA thin-film solar cell is a second generation solar cell that is made by depositing one or more thin layers, or thin film (TF) of photovoltaic material on a substrate, such as glass, plastic or metal. Thin-film solar cells are commercially used in several technologies, including cadmium telluride (CdTe), copper indium gallium diselenide (CIGS), and amorphous thin-film silicon (a-Si, TF-Si).\n\n\n==== Perovskite solar cells ====\n\n\n=== Concentrated solar power ===\n\nConcentrated solar power (CSP), also called \"concentrated solar thermal\", uses lenses or mirrors and tracking systems to concentrate sunlight, then use the resulting heat to generate electricity from conventional steam-driven turbines.A wide range of concentrating technologies exists among the best known are the parabolic trough, the compact linear Fresnel reflector, the dish Stirling and the solar power tower. Various techniques are used to track the sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight and is then used for power generation or energy storage. Thermal storage efficiently allows overnight electricity generation, thus complementing PV. CSP generates a very small share of solar power and in 2022 the IEA said that CSP should be better paid for its storage.As of 2021 the levelized cost of electricity from CSP is over twice that of PV. However, their very high temperatures may prove useful to help decarbonize industries (perhaps via hydrogen) which need to be hotter than electricity can provide.\n\n\n=== Hybrid systems ===\n\nA hybrid system combines solar with energy storage and/or one or more other forms of generation. Hydro, wind and batteries are commonly combined with solar. The combined generation may enable the system to vary power output with demand, or at least smooth the solar power fluctuation. There is a lot of hydro worldwide, and adding solar panels on or around existing hydro reservoirs is particularly useful, because hydro is usually more flexible than wind and cheaper at scale than batteries, and existing power lines can sometimes be used.\n\n\n== Development and deployment ==\n\n\n=== Early days ===\nThe early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce, such as experiments by Augustin Mouchot. Charles Fritts installed the world's first rooftop photovoltaic solar array, using 1%-efficient selenium cells, on a New York City roof in 1884. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum. Bell Telephone Laboratories\u2019 1950s research used silicon wafers with a very thin coating of boron. The \u201cBell Solar Battery\u201d was described as 6% efficient, with a square yard of the panels generating 50 watts. The first satellite with solar panels was launched in 1957.By the 1970s, solar power was being used on satellites, but the cost of solar power was considered to be unrealistic for conventional applications. In 1974 it was estimated that only six private homes in all of North America were entirely heated or cooled by functional solar power systems. However, the 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies.Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the United States (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer ISE). Between 1970 and 1983 installations of photovoltaic systems grew rapidly. In the United States, President Jimmy Carter set a target of producing 20% of U.S. energy from solar by the year 2000, but his successor, Ronald Reagan, removed the funding for research into renewables. Falling oil prices in the early 1980s moderated the growth of photovoltaics from 1984 to 1996.\n\n\n=== Mid-1990s to 2010 ===\nIn the mid-1990s development of both, residential and commercial rooftop solar as well as utility-scale photovoltaic power stations began to accelerate again due to supply issues with oil and natural gas, global warming concerns, and the improving economic position of PV relative to other energy technologies. In the early 2000s, the adoption of feed-in tariffs\u2014a policy mechanism, that gives renewables priority on the grid and defines a fixed price for the generated electricity\u2014led to a high level of investment security and to a soaring number of PV deployments in Europe.\n\n\n=== 2010s ===\nFor several years, worldwide growth of solar PV was driven by European deployment, but it has since shifted to Asia, especially China and Japan, and to a growing number of countries and regions all over the world. The largest manufacturers of solar equipment were based in China. Although concentrated solar power capacity grew more than tenfold, it remained a tiny proportion of the total,:\u200a51\u200a because the cost of utility-scale solar PV fell by 85% between 2010 and 2020, while CSP costs have only fallen 68% in the same timeframe.\n\n\n=== 2020s ===\nDespite the rising cost of materials, such as polysilicon, during the 2021\u20132022 global energy crisis, utility scale solar was still the cheapest energy source in many countries due to the rising costs of other energy sources, such as natural gas. In 2022, global solar generation capacity exceeded 1 TW for the first time. However, fossil-fuel subsidies have slowed the growth of solar generation capacity.\n\n\n=== Current status ===\nAbout half of installed capacity is utility scale.\n\n\n=== Forecasts ===\nMost new renewable capacity between 2022 and 2027 is forecast to be solar, surpassing coal as the largest source of installed power capacity.:\u200a26\u200a Utility scale is forecast to become the largest source of electricity in all regions except sub-Saharan Africa by 2050.According to a 2021 study, global electricity generation potential of rooftop solar panels is estimated at 27 PWh per year at costs ranging from $40 (Asia) to $240 per MWh (US, Europe). Its practical realization will however depend on the availability and cost of scalable electricity storage solutions.\n\n\n=== Photovoltaic power stations ===\n\n\n=== Concentrating solar power stations ===\n\nCommercial concentrating solar power (CSP) plants, also called \"solar thermal power stations\", were first developed in the 1980s. The 377  MW Ivanpah Solar Power Facility, located in California's Mojave Desert, is the world's largest solar thermal power plant project. Other large CSP plants include the Solnova Solar Power Station (150 MW), the Andasol solar power station (150 MW), and Extresol Solar Power Station (150 MW), all in Spain. The principal advantage of CSP is the ability to efficiently add thermal storage, allowing the dispatching of electricity over up to a 24-hour period. Since peak electricity demand typically occurs at about 5 pm, many CSP power plants use 3 to 5 hours of thermal storage.\n\n\n== Economics ==\n\n\n=== Cost per watt ===\nThe typical cost factors for solar power include the costs of the modules, the frame to hold them, wiring, inverters, labour cost, any land that might be required, the grid connection, maintenance and the solar insolation that location will receive.\nPhotovoltaic systems use no fuel, and modules typically last 25 to 40 years. Thus upfront capital and financing costs make up 80 to 90% of the cost of solar power.:\u200a165\u200aSome countries are considering price caps, whereas others prefer contracts for difference.The large magnitude of solar energy available makes it a highly appealing source of electricity. In 2020, solar energy was the cheapest source of electricity. In Saudi Arabia, a power purchase agreement (PPA) was signed in April 2021 for a new solar power plant in Al-Faisaliah. The project has recorded the world's lowest cost for solar PV electricity production of USD 1.04 cents/ kWh.\n\n\n=== Installation prices ===\nExpenses of high-power band solar modules has greatly decreased over time. Beginning in 1982, the cost per kW was approximately 27,000 American dollars, and in 2006 the cost dropped to approximately 4,000 American dollars per kW. The PV system in 1992 cost approximately 16,000 American dollars per kW and it dropped to approximately 6,000 American dollars per kW in 2008.In 2021 in the US, residential solar cost from 2 to 4 dollars/watt (but solar shingles cost much more) and utility solar costs were around $1/watt.\n\n\n=== Productivity by location ===\n\nThe productivity of solar power in a region depends on solar irradiance, which varies through the day and year and is influenced by latitude and climate. PV system output power also depends on ambient temperature, wind speed, solar spectrum, the local soiling conditions, and other factors.\nOnshore wind power tends to be the cheapest source of electricity in Northern Eurasia, Canada, some parts of the United States, and Patagonia in Argentina: whereas in other parts of the world mostly solar power (or less often a combination of wind, solar and other low carbon energy) is thought to be best.:\u200a8\u200a Modelling by Exeter University suggests that by 2030 solar will be cheapest in all countries except for some in north-east Europe.The locations with highest annual solar irradiance lie in the arid tropics and subtropics. Deserts lying in low latitudes usually have few clouds and can receive sunshine for more than ten hours a day. These hot deserts form the Global Sun Belt circling the world. This belt consists of extensive swathes of land in Northern Africa, Southern Africa, Southwest Asia, Middle East, and Australia, as well as the much smaller deserts of North and South America.So solar is (or is predicted to become) the cheapest source of energy in all of Central America, Africa, the Middle East, India, South-east Asia, Australia, and several other places.:\u200a8\u200aDifferent measurements of solar irradiance (direct normal irradiance, global horizontal irradiance) are mapped below:\n\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Self-consumption ===\nIn cases of self-consumption of solar energy, the payback time is calculated based on how much electricity is not purchased from the grid. However, in many cases, the patterns of generation and consumption do not coincide, and some or all of the energy is fed back into the grid. The electricity is sold, and at other times when energy is taken from the grid, electricity is bought. The relative costs and prices obtained affect the economics. In many markets, the price paid for sold PV electricity is significantly lower than the price of bought electricity, which incentivizes self-consumption. Moreover, separate self-consumption incentives have been used in e.g., Germany and Italy. Grid interaction regulation has also included limitations of grid feed-in in some regions in Germany with high amounts of installed PV capacity. By increasing self-consumption, the grid feed-in can be limited without curtailment, which wastes electricity.A good match between generation and consumption is key for high self-consumption. The match can be improved with batteries or controllable electricity consumption. However, batteries are expensive, and profitability may require the provision of other services from them besides self-consumption increase, for example avoiding power outages. Hot water storage tanks with electric heating with heat pumps or resistance heaters can provide low-cost storage for self-consumption of solar power. Shiftable loads, such as dishwashers, tumble dryers and washing machines, can provide controllable consumption with only a limited effect on the users, but their effect on self-consumption of solar power may be limited.\n\n\n=== Energy pricing, incentives and taxes ===\n\nThe original political purpose of incentive policies for PV was to facilitate an initial small-scale deployment to begin to grow the industry, even where the cost of PV was significantly above grid parity, to allow the industry to achieve the economies of scale necessary to reach grid parity. Since reaching grid parity some policies are implemented to promote national energy independence, high tech job creation and reduction of CO2 emissions.Financial incentives for photovoltaics differ across countries, including Australia, China, Germany, India, Japan, and the United States and even across states within the US.\n\n\n==== Net metering ====\nIn net metering the price of the electricity produced is the same as the price supplied to the consumer, and the consumer is billed on the difference between production and consumption. Net metering can usually be done with no changes to standard electricity meters, which accurately measure power in both directions and automatically report the difference, and because it allows homeowners and businesses to generate electricity at a different time from consumption, effectively using the grid as a giant storage battery. With net metering, deficits are billed each month while surpluses are rolled over to the following month. Best practices call for perpetual roll over of kWh credits. Excess credits upon termination of service are either lost or paid for at a rate ranging from wholesale to retail rate or above, as can be excess annual credits.\n\n\n==== Community solar ====\nA community solar project is a solar power installation that accepts capital from and provides output credit and tax benefits to multiple customers, including individuals, businesses, nonprofits, and other investors. Participants typically invest in or subscribe to a certain kW capacity or kWh generation of remote electrical production.\n\n\n==== Taxes ====\nIn some countries tariffs (import taxes) are imposed on imported solar panels.\n\n\n== Grid integration ==\n\n\n=== Variability ===\nThe overwhelming majority of electricity produced worldwide is used immediately because traditional generators can adapt to demand and storage is usually more expensive. Both solar power and wind power are sources of variable renewable power, meaning that all available output must be used locally, carried on transmission lines to be used elsewhere, or stored (e.g., in a battery). Since solar energy is not available at night, storing it so as to have continuous electricity availability is potentially an important issue, particularly in off-grid applications and for future 100% renewable energy scenarios.Solar electricity is inherently variable but somewhat predictable by time of day, location, and seasons (see solar power forecasting). Solar is intermittent due to the day/night cycles and variable weather conditions. The challenge of integrating solar power in any given electric utility varies significantly. In places with hot summers and mild winters, solar is well matched to daytime cooling demands.\n\n\n=== Energy storage ===\nConcentrated solar power plants may use thermal storage to store solar energy, such as in high-temperature molten salts. These salts are an effective storage medium because they are low-cost, have a high specific heat capacity, and can deliver heat at temperatures compatible with conventional power systems. This method of energy storage is used, for example, by the Solar Two power station, allowing it to store 1.44 TJ in its 68 m3 storage tank, enough to provide full output for close to 39 hours, with an efficiency of about 99%.In stand alone PV systems batteries are traditionally used to store excess electricity. With grid-connected photovoltaic power systems, excess electricity can be sent to the electrical grid. Net metering and feed-in tariff programs give these systems a credit for the electricity they produce. This credit offsets electricity provided from the grid when the system cannot meet demand, effectively trading with the grid instead of storing excess electricity. \nWhen wind and solar are a small fraction of the grid power, other generation techniques can adjust their output appropriately, but as these forms of variable power grow, additional balance on the grid is needed. As prices are rapidly declining, PV systems increasingly use rechargeable batteries to store a surplus to be used later at night. Batteries used for grid-storage can stabilize the electrical grid by leveling out peak loads for a few hours. In the future, less expensive batteries could play an important role on the electrical grid, as they can charge during periods when generation exceeds demand and feed their stored energy into the grid when demand is higher than generation.\nCommon battery technologies used in today's home PV systems include nickel-cadmium, lead-acid, nickel metal hydride, and lithium-ion.Lithium-ion batteries have the potential to replace lead-acid batteries in the near future, as they are being intensively developed and lower prices are expected due to economies of scale provided by large production facilities such as the Gigafactory 1. In addition, the Li-ion batteries of plug-in electric cars may serve as future storage devices in a vehicle-to-grid system. Since most vehicles are parked an average of 95% of the time, their batteries could be used to let electricity flow from the car to the power lines and back. Other rechargeable batteries used for distributed PV systems include, sodium\u2013sulfur and vanadium redox batteries, two prominent types of a molten salt and a flow battery, respectively.\n\n\n=== Other technologies ===\nSolar power plants, while they can be curtailed, usually simply output as much power as possible. Therefore in an electricity system without sufficient grid energy storage, generation from other sources (coal, biomass, natural gas, nuclear, hydroelectricity) generally go up and down in reaction to the rise and fall of solar electricity and variations in demand (see load following power plant). \nConventional hydroelectric dams work very well in conjunction with solar power; water can be held back or released from a reservoir as required. Where suitable geography is not available, pumped-storage hydroelectricity can use solar power to pump water to a high reservoir on sunny days, then the energy is recovered at night and in bad weather by releasing water via a hydroelectric plant to a low reservoir where the cycle can begin again.While hydroelectric and natural gas plants can quickly respond to changes in load; coal, biomass and nuclear plants usually take considerable time to respond to load and can only be scheduled to follow the predictable variation. Depending on local circumstances, beyond about 20\u201340% of total generation, grid-connected intermittent sources like solar tend to require investment in some combination of grid interconnections, energy storage or demand side management. In countries with high solar generation, such as Australia, electricity prices may become negative in the middle of the day when solar generation is high, thus incentivizing new battery storage.The combination of wind and solar PV has the advantage that the two sources complement each other because the peak operating times for each system occur at different times of the day and year. The power generation of such solar hybrid power systems is therefore more constant and fluctuates less than each of the two component subsystems. Solar power is seasonal, particularly in northern/southern climates, away from the equator, suggesting a need for long term seasonal storage in a medium such as hydrogen or pumped hydroelectric.\n\n\n== Environmental effects ==\n\nSolar power is cleaner than electricity from fossil fuels, so can be good for the environment when it replaces that. Solar power does not lead to any harmful emissions during operation, but the production of the panels leads to some amount of pollution. A 2021 study estimated the carbon footprint of manufacturing monocrystalline panels at 515 g CO2/kWp in the US and 740 g CO2/kWp in China, but this is expected to fall as manufacturers use more clean electricity and recycled materials. Solar power carries an upfront cost to the environment via production with a carbon payback time of several years as of 2022, but offers clean energy for the remainder of their 30-year lifetime.The life-cycle greenhouse-gas emissions of solar farms are less than 50 gram (g) per kilowatt-hour (kWh), but with battery storage could be up to 150 g/kWh. In contrast, a combined cycle gas-fired power plant without carbon capture and storage emits around 500 g/kWh, and a coal-fired power plant about 1000 g/kWh. Similar to all energy sources where their total life cycle emissions are mostly from construction, the switch to low carbon power in the manufacturing and transportation of solar devices would further reduce carbon emissions.Lifecycle surface power density of solar power varies a lot but averages about 7 W/m2, compared to about 240 for nuclear power and 480 for gas. However when the land required for gas extraction and processing is accounted for gas power is estimated to have not much higher power density than solar. PV requires much larger amounts of land surface to produce the same nominal amount of energy as sources with higher surface power density and capacity factor. According to a 2021 study, obtaining 25 to 80% of electricity from solar farms in their own territory by 2050 would require the panels to cover land ranging from 0.5 to 2.8% of the European Union, 0.3 to 1.4% in India, and 1.2 to 5.2% in Japan and South Korea. Occupation of such large areas for PV farms could drive residential opposition as well as lead to deforestation, removal of vegetation and conversion of farm land. However some countries, such as South Korea and Japan, use land for agriculture under PV, or floating solar, together with other low-carbon power sources. Worldwide land use has minimal ecological impact. Land use can be reduced to the level of gas power by installing on buildings and other built up areas.Harmful materials are used in the production of solar panels, but in generally in small amounts.  As of 2022 the environmental impact of perovskite is hard to estimate, but there is some concern that lead may become a problem. A 2021 International Energy Agency study projects the demand for copper will double by 2040. The study cautions that supply needs to increase rapidly to match demand from large-scale deployment of solar and required grid upgrades. More tellurium and indium may also be needed and recycling may help.As solar panels are sometimes replaced with more efficient panels, the second-hand panels are sometimes reused in developing countries, for example in Africa. Several countries have specific regulations for the recycling of solar panels. Although maintenance cost is already low compared to other energy sources, some academics have called for solar power systems to be designed to be more repairable.A very small proportion of solar power is concentrated solar power. Concentrated solar power may use much more water than gas-fired power. This can be a problem, as this type of solar power needs strong sunlight so is often built in deserts.\n\n\n== Politics ==\nSolar production cannot be cut off by geopolitics once installed, unlike oil and gas, which contributes to energy security.As of 2022 over 40% of global polysilicon manufacturing capacity is in Xinjiang in China, which raises concerns about human rights violations (Xinjiang internment camps) as well as supply chain dependency.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== Further reading ==\nSivaram, Varun (2018). Taming the Sun: Innovation to Harness Solar Energy and Power the Planet. Cambridge, MA: MIT Press. ISBN 978-0-262-03768-6.\n\n\n== External links ==\nSolar energy and the environment at U.S. Energy Information Administration"}, {"id": 62, "title": "Franklin D Roosevelt", "content": "Franklin Delano Roosevelt (January 30, 1882 \u2013 April 12, 1945), commonly known by his initials FDR, was an American politician and statesman who served as the 32nd president of the United States from 1933 until his death in 1945. He was a member of the Democratic Party and is the only U.S. president to have served more than two terms in office. During his third and fourth terms he was preoccupied with World War II.\nA member of the prominent Roosevelt family, after attending university, Roosevelt began to practice law in New York City. He was elected a member of the New York State Senate from 1911 to 1913 and was then the assistant secretary of the Navy under President Woodrow Wilson during World War I. Roosevelt was James M. Cox's running mate on the Democratic Party's ticket in the 1920 U.S. presidential election, but Cox lost to Republican nominee Warren G. Harding. In 1921, Roosevelt contracted a paralytic illness that permanently paralyzed his legs. Partly through the encouragement of his wife, Eleanor Roosevelt, he returned to public office as governor of New York from 1929 to 1933, during which he promoted programs to combat the Great Depression besetting the U.S. In the 1932 presidential election, Roosevelt defeated Republican president Herbert Hoover in a landslide.\nDuring his first 100 days as president, Roosevelt spearheaded unprecedented federal legislation and directed the federal government during most of the Great Depression, implementing the New Deal in response to the most significant economic crisis in American history. He also built the New Deal coalition, realigning American politics into the Fifth Party System and defining American liberalism throughout the middle third of the 20th century. He created numerous programs to provide relief to the unemployed and farmers while seeking economic recovery with the National Recovery Administration and other programs. He also instituted major regulatory reforms related to finance, communications, and labor, and presided over the end of Prohibition. In 1936, Roosevelt won a landslide reelection with the economy having improved from 1933, but the economy relapsed into a deep recession in 1937 and 1938. He was unable to expand the Supreme Court in 1937, the same year the conservative coalition was formed to block the implementation of further New Deal programs and reforms. Major surviving programs and legislation implemented under Roosevelt include the Securities and Exchange Commission, the National Labor Relations Act, the Federal Deposit Insurance Corporation, and Social Security. In 1940, he ran successfully for reelection, becoming the only American president to serve for more than two terms.\nWith World War II looming after 1938 in addition to the Japanese invasion of China and the aggression of Nazi Germany, Roosevelt gave strong diplomatic and financial support to China as well as the United Kingdom and the Soviet Union while the U.S. remained officially neutral. Following the Japanese attack on Pearl Harbor on December 7, 1941, he obtained a declaration of war on Japan the next day and on Germany and Italy a few days later. He worked closely with other national leaders in leading the Allies against the Axis powers. Roosevelt supervised the mobilization of the American economy to support the war effort and implemented a Europe first strategy. He also initiated the development of the world's first atomic bomb and worked with the other Allied leaders to lay the groundwork for the United Nations and other post-war institutions. Roosevelt won reelection in 1944 but died in 1945 after his physical health seriously and steadily declined during the war years. Since then, several of his actions have come under substantial criticism, including his ordering of the internment of Japanese Americans in concentration camps. Nonetheless, historical rankings consistently place him as one of the greatest American presidents.\n\n\n== Early life and marriage ==\n\n\n=== Childhood ===\nFranklin Delano Roosevelt was born on January 30, 1882, in the Hudson Valley town of Hyde Park, New York, to businessman James Roosevelt I and his second wife, Sara Ann Delano. His parents, who were sixth cousins, both came from wealthy, established New York families, the Roosevelts, the Aspinwalls and the Delanos, respectively. Roosevelt's paternal ancestor migrated to New Amsterdam in the 17th century, and the Roosevelts succeeded as merchants and landowners. The Delano family patriarch, Philip Delano, traveled to the New World on the Fortune in 1621, and the Delanos thrived as merchants and shipbuilders in Massachusetts. Franklin had a half-brother, James Roosevelt \"Rosy\" Roosevelt, from his father's previous marriage.Roosevelt's father, James, graduated from Harvard Law School in 1851 but chose not to practice law after receiving an inheritance from his grandfather. James Roosevelt, a prominent Bourbon Democrat, once took Franklin to meet President Grover Cleveland, who said to him: \"My little man, I am making a strange wish for you. It is that you may never be President of the United States.\" Franklin's mother, the dominant influence in his early years, once declared, \"My son Franklin is a Delano, not a Roosevelt at all.\" James, who was 54 when Franklin was born, was considered by some as a remote father, though biographer James MacGregor Burns indicates James interacted with his son more than was typical at the time.\n\n\n=== Education and early career ===\n\nAs a child, Roosevelt learned to ride, shoot, and sail; he also learned to play polo, tennis, and golf. Frequent trips to Europe\u2014beginning at age two and from age seven to fifteen\u2014helped Roosevelt become conversant in German and French. Except for attending public school in Germany at age nine, Roosevelt was home-schooled by tutors until age 14. He then attended Groton School, an Episcopal boarding school in Groton, Massachusetts. He was not among the more popular Groton students, who were better athletes and had rebellious streaks. Its headmaster, Endicott Peabody, preached the duty of Christians to help the less fortunate and urged his students to enter public service. Peabody remained a strong influence throughout Roosevelt's life, officiating at his wedding and visiting him as president.Like most of his Groton classmates, Roosevelt went to Harvard College. He was a member of the Alpha Delta Phi fraternity and the Fly Club, and served as a school cheerleader. Roosevelt was relatively undistinguished as a student or athlete, but he became editor-in-chief of The Harvard Crimson daily newspaper, a position that required ambition, energy, and the ability to manage others. He later said, \"I took economics courses in college for four years, and everything I was taught was wrong.\"Roosevelt's father died in 1900, causing great distress for him. The following year, Roosevelt's fifth cousin Theodore Roosevelt became President of the United States. Theodore's vigorous leadership style and reforming zeal made him Franklin's role model and hero. He graduated from Harvard in three years in 1903 with an A.B. in history. He remained there for a fourth year, taking graduate courses and becoming an editor of the Harvard Crimson.Roosevelt entered Columbia Law School in 1904 but dropped out in 1907 after passing the New York Bar Examination. In 1908, he took a job with the prestigious law firm of Carter Ledyard & Milburn, working in the firm's admiralty law division.\n\n\n=== Marriage, family, and marital affairs ===\nDuring his second year of college, he met and proposed to Boston heiress Alice Sohier, who turned him down. Franklin then began courting his child-acquaintance and fifth cousin once removed, Eleanor Roosevelt, a niece of Theodore Roosevelt. In 1903 Franklin proposed to Eleanor, and after resistance from his mother, they were married on March 17, 1905. Eleanor's father, Elliott, was deceased, and her uncle Theodore, then the president, gave away the bride. The young couple moved into Springwood, and Franklin and Sara Roosevelt also provided a townhouse for the couple in New York City, where Sara built a house alongside for herself. Eleanor never felt at home in the houses at Hyde Park or New York, but she loved the family's vacation home on Campobello Island, which Sara also gave the couple.\nBurns indicates young Roosevelt was self-assured and at ease in the upper class, while Eleanor was then shy and disliked social life, and initially stayed home to raise their children. As his father had, Franklin left the raising of the children to his wife, and Eleanor delegated it to caregivers. She later said she knew \"absolutely nothing about handling or feeding a baby.\" Although Eleanor thought sex was \"an ordeal to be endured\", she and Franklin had six children. Anna, James, and Elliott were born in 1906, 1907, and 1910, respectively. The couple's second son, Franklin, died in infancy in 1909. Another son, also named Franklin, was born in 1914, and the youngest child, John, was born in 1916.Roosevelt had several extra-marital affairs, including with Eleanor's social secretary Lucy Mercer, soon after she was hired in 1914, and discovered by Eleanor in 1918. Franklin contemplated divorcing Eleanor, but Sara objected, and Mercer would not marry a divorced man with five children. Franklin and Eleanor remained married, and Franklin promised never to see Mercer again. Eleanor never forgave him, and their marriage became more of a political partnership. Eleanor soon established a separate home in Hyde Park at Val-Kill, and devoted herself to social and political causes independent of her husband. The emotional break in their marriage was so severe that when Franklin asked Eleanor in 1942\u2014in light of his failing health\u2014to come back home and live with him again, she refused. He was not always aware of when she visited the White House and for some time she could not easily reach him on the telephone without his secretary's help; Franklin, in turn, did not visit Eleanor's New York City apartment until late 1944.Franklin broke his promise to Eleanor as he and Lucy Mercer Rutherfurd maintained a formal correspondence, and began seeing each other again in 1941 or earlier. Roosevelt's son Elliott claimed that his father had a 20-year affair with his private secretary, Marguerite \"Missy\" LeHand. Another son, James, stated that \"there is a real possibility that a romantic relationship existed\" between his father and Crown Princess M\u00e4rtha of Norway, who resided in the White House during part of World War II. Aides began to refer to her at the time as \"the president's girlfriend\", and gossip linking the two romantically appeared in the newspapers.\n\n\n== Early political career (1910\u20131920) ==\n\n\n=== New York state senator (1910\u20131913) ===\nRoosevelt cared little for the practice of law and told friends he planned to enter politics. Despite his admiration for cousin Theodore, Franklin shared his father's bond with the Democratic Party, and in preparation for the 1910 elections, the party recruited Roosevelt to run for a seat in the New York State Assembly. Roosevelt was a compelling recruit for the party. He had the personality and energy for campaigning, and he had the money to pay for his own campaign. But Roosevelt's campaign for the state assembly ended after the Democratic incumbent, Lewis Stuyvesant Chanler, chose to seek re-election. Rather than putting his political hopes on hold, Roosevelt ran for a seat in the state senate. The senate district, located in Dutchess, Columbia, and Putnam, was strongly Republican. Roosevelt feared that opposition from Theodore could end his campaign, but Theodore encouraged his candidacy despite their party differences. Acting as his own campaign manager, Roosevelt traveled throughout the senate district via automobile at a time when few could afford a car. Due to his aggressive campaign, his name recognition in the Hudson Valley, and the Democratic landslide in the 1910 United States elections, Roosevelt won a surprising victory.Despite short legislative sessions, Roosevelt treated his new position as a full-time career. Taking his seat on January 1, 1911, Roosevelt soon became the leader of a group of \"Insurgents\" in opposition to the Tammany Hall machine that dominated the state Democratic Party. In the 1911 U.S. Senate election, which was determined in a joint session of the New York state legislature, Roosevelt and nineteen other Democrats caused a prolonged deadlock by opposing a series of Tammany-backed candidates. Tammany threw its backing behind James A. O'Gorman, a highly regarded judge whom Roosevelt found acceptable, and O'Gorman won the election in late March. Roosevelt in the process became a popular figure among New York Democrats. News articles and cartoons depicted \"the second coming of a Roosevelt\", sending \"cold shivers down the spine of Tammany\".Roosevelt opposed Tammany Hall by supporting New Jersey Governor Woodrow Wilson's successful bid for the 1912 Democratic nomination. The election became a three-way contest when Theodore Roosevelt left the Republican Party to launch a third party campaign against Wilson and sitting Republican President William Howard Taft. Franklin's decision to back Wilson over his cousin in the general election alienated some of his family, except Theodore. Roosevelt overcame a bout of typhoid fever, and with help from journalist Louis McHenry Howe, he was re-elected in the 1912 elections. After the election, he served as chairman of the Agriculture Committee, and his success with farm and labor bills was a precursor to his New Deal policies years later. He had then become more consistently progressive, in support of labor and social welfare programs.\n\n\n=== Assistant Secretary of the Navy (1913\u20131919) ===\nRoosevelt's support of Wilson led to his appointment in March 1913 as Assistant Secretary of the Navy, the second-ranking official in the Navy Department after Secretary Josephus Daniels who paid it little attention. Roosevelt had an affection for the Navy, was well-read on the subject, and was a most ardent supporter of a large, efficient force. With Wilson's support, Daniels and Roosevelt instituted a merit-based promotion system and made other reforms to extend civilian control over the autonomous departments of the Navy. Roosevelt oversaw the Navy's civilian employees and earned the respect of union leaders for his fairness in resolving disputes. No strikes occurred during his seven-plus years in the office, as he gained valuable experience in labor issues, wartime management, naval issues, and logistics.In 1914, Roosevelt ran for the seat of retiring Republican Senator Elihu Root of New York. Though he had the backing of Treasury Secretary William Gibbs McAdoo and Governor Martin H. Glynn, he faced a formidable opponent in Tammany-Hall's James W. Gerard. He also was without Wilson's support, as the president needed Tammany's forces for his legislation and 1916 re-election. Roosevelt was soundly defeated in the Democratic primary by Gerard, who in turn lost the general election to Republican James Wolcott Wadsworth Jr. He learned that federal patronage alone, without White House support, could not defeat a strong local organization. After the election, he and Tammany Hall boss Charles Francis Murphy sought accommodation and became allies.Roosevelt refocused on the Navy Department, as World War I broke out in Europe in August 1914. Though he remained publicly supportive of Wilson, Roosevelt sympathized with the Preparedness Movement, whose leaders strongly favored the Allied Powers and called for a military build-up. The Wilson administration initiated an expansion of the Navy after the sinking of the RMS Lusitania by a German submarine, and Roosevelt helped establish the United States Navy Reserve and the Council of National Defense. In April 1917, after Germany declared it would engage in unrestricted submarine warfare and attacked several U.S. ships, Congress approved Wilson's call for a declaration of war on Germany.Roosevelt requested that he be allowed to serve as a naval officer, but Wilson insisted that he continue to serve as Assistant Secretary. For the next year, Roosevelt remained in Washington to coordinate the deployment of naval vessels and personnel, as the Navy expanded fourfold. In the summer of 1918, Roosevelt traveled to Europe to inspect naval installations and meet with French and British officials. In September, he returned to the United States on board the USS Leviathan. On the 11-day voyage, the pandemic influenza virus struck and killed many on board. Roosevelt became very ill with influenza and complicating pneumonia, but recovered by the time the ship landed in New York. After Germany signed an armistice in November 1918, Daniels and Roosevelt supervised the demobilization of the Navy. Against the advice of older officers such as Admiral William Benson\u2014who claimed he could not \"conceive of any use the fleet will ever have for aviation\"\u2014Roosevelt personally ordered the preservation of the Navy's Aviation Division. With the Wilson administration near an end, Roosevelt planned his next run for office. He approached Herbert Hoover about running for the 1920 Democratic presidential nomination, with Roosevelt as his running mate.\n\n\n=== Campaign for vice president (1920) ===\nRoosevelt's plan for Hoover to run for the nomination fell through after Hoover publicly declared himself to be a Republican, but Roosevelt decided to seek the 1920 vice presidential nomination. After Governor James M. Cox of Ohio won the party's presidential nomination at the 1920 Democratic National Convention, he chose Roosevelt as his running mate, and the convention nominated him by acclamation. Although his nomination surprised most people, he balanced the ticket as a moderate, a Wilsonian, and a prohibitionist with a famous name. Roosevelt, then 38, resigned as Assistant Secretary after the Democratic convention and campaigned across the nation for the party ticket.During the campaign, Cox and Roosevelt defended the Wilson administration and the League of Nations, both of which were unpopular in 1920. Roosevelt personally supported U.S. membership in the League of Nations, but, unlike Wilson, he favored compromising with Senator Henry Cabot Lodge and other \"Reservationists\". The Cox\u2013Roosevelt ticket was defeated by Republicans Warren G. Harding and Calvin Coolidge in the presidential election by a wide margin, and the Republican ticket carried every state outside of the South. Roosevelt accepted the loss without issue and later reflected that the relationships and goodwill that he built in the 1920 campaign proved to be a major asset in his 1932 campaign. The 1920 election also saw the first public participation of Eleanor Roosevelt who, with the support of Louis Howe, established herself as a valuable political player.\n\n\n== Paralytic illness and political comeback (1921\u20131928) ==\n\nAfter the election, Roosevelt returned to New York City, where he practiced law and served as a vice president of the Fidelity and Deposit Company. He also sought to build support for a political comeback in the 1922 elections, but his career was derailed by illness. While the Roosevelts were vacationing at Campobello Island in August 1921, he fell ill. His main symptoms were fever; symmetric, ascending paralysis; facial paralysis; bowel and bladder dysfunction; numbness and hyperesthesia; and a descending pattern of recovery. Roosevelt was left permanently paralyzed from the waist down and was diagnosed with polio. Historians have noted a 2003 study strongly favoring a diagnosis of Guillain\u2013Barr\u00e9 syndrome, but have continued to describe his paralysis according to the initial diagnosis.Though his mother favored his retirement from public life, Roosevelt, his wife, and Roosevelt's close friend and adviser, Louis Howe, were all determined that he continue his political career. He convinced many people that he was improving, which he believed to be essential prior to running for public office again. He laboriously taught himself to walk short distances while wearing iron braces on his hips and legs, by swiveling his torso while supporting himself with a cane. He was careful never to be seen using his wheelchair in public, and great care was taken to prevent any portrayal in the press that would highlight his disability. However, his disability was well known before and during his presidency and became a major part of his image. He usually appeared in public standing upright, supported on one side by an aide or one of his sons.Beginning in 1925, Roosevelt spent most of his time in the Southern United States, at first on his houseboat, the Larooco. Intrigued by the potential benefits of hydrotherapy, he established a rehabilitation center at Warm Springs, Georgia, in 1926. To create the rehabilitation center, he assembled a staff of physical therapists and used most of his inheritance to purchase the Merriweather Inn. In 1938, he founded the National Foundation for Infantile Paralysis, leading to the development of polio vaccines.Roosevelt maintained contacts with the Democratic Party during the 1920s, and he remained active in New York politics while also establishing contacts in the South, particularly in Georgia. He issued an open letter endorsing Al Smith's successful campaign in New York's 1922 gubernatorial election, which both aided Smith and showed Roosevelt's continuing relevance as a political figure. Roosevelt and Smith came from different backgrounds and never fully trusted one another, but Roosevelt supported Smith's progressive policies, while Smith was happy to have the backing of the prominent and well-respected Roosevelt.Roosevelt gave presidential nominating speeches for Smith at the 1924 and 1928 Democratic National Conventions; the speech at the 1924 convention marked a return to public life following his illness and convalescence. That year, the Democrats were badly divided between an urban wing, led by Smith, and a conservative, rural wing, led by William Gibbs McAdoo. On the 101st ballot, the nomination went to John W. Davis, a compromise candidate who suffered a landslide defeat in the 1924 presidential election. Like many others throughout the United States, Roosevelt did not abstain from alcohol during the Prohibition era, but publicly he sought to find a compromise on Prohibition acceptable to both wings of the party.In 1925, Smith appointed Roosevelt to the Taconic State Park Commission, and his fellow commissioners chose him as chairman. In this role, he came into conflict with Robert Moses, a Smith prot\u00e9g\u00e9, who was the primary force behind the Long Island State Park Commission and the New York State Council of Parks. Roosevelt accused Moses of using the name recognition of prominent individuals including Roosevelt to win political support for state parks, but then diverting funds to the ones Moses favored on Long Island, while Moses worked to block the appointment of Howe to a salaried position as the Taconic commission's secretary. Roosevelt served on the commission until the end of 1928, and his contentious relationship with Moses continued as their careers progressed.Peace was the catchword of the 1920s, and in 1923 Edward Bok established the $100,000 American Peace Award for the best plan to bring peace to the world. Roosevelt had leisure time and interest, and he drafted a plan for the contest. He never submitted it because his wife Eleanor Roosevelt was selected as a judge for the prize. His plan called for a new world organization that would replace the League of Nations. Although Roosevelt had been the vice-presidential candidate on the Democratic ticket of 1920 that supported the League of Nations, by 1924 he was ready to scrap it. His draft of a \"Society of Nations\" accepted the reservations proposed by Henry Cabot Lodge in the 1919 Senate debate. The new Society would not become involved in the Western Hemisphere, where the Monroe doctrine held sway. It would not have any control over military forces. Although Roosevelt's plan was never made public, he thought about the problem a great deal and incorporated some of his 1924 ideas into the design for the United Nations in 1944\u20131945.\n\n\n== Governor of New York (1929\u20131932) ==\n\nSmith, the Democratic presidential nominee in the 1928 election, asked Roosevelt to run for governor of New York in the 1928 state election. Roosevelt initially resisted, as he was reluctant to leave Warm Springs and feared a Republican landslide in 1928. Party leaders eventually convinced him only he could defeat the Republican gubernatorial nominee, New York Attorney General Albert Ottinger. He won the party's gubernatorial nomination by acclamation and again turned to Howe to lead his campaign. Roosevelt was also joined on the campaign trail by associates Samuel Rosenman, Frances Perkins, and James Farley. While Smith lost the presidency in a landslide, and was defeated in his home state, Roosevelt was elected governor by a one-percent margin, and became a contender in the next presidential election.Roosevelt proposed the construction of hydroelectric power plants and addressed the ongoing farm crisis of the 1920s. Relations between Roosevelt and Smith suffered after he chose not to retain key Smith appointees like Moses. He and his wife Eleanor established an understanding for the rest of his career; she would dutifully serve as the governor's wife but would also be free to pursue her own agenda and interests. He also began holding \"fireside chats\", in which he directly addressed his constituents via radio, often pressuring the New York State Legislature to advance his agenda.In October 1929, the Wall Street Crash occurred, and with it came the Great Depression in the United States. Roosevelt saw the seriousness of the situation and established a state employment commission. He also became the first governor to publicly endorse the idea of unemployment insurance.When Roosevelt began his run for a second term in May 1930, he reiterated his doctrine from the campaign two years before: \"that progressive government by its very terms must be a living and growing thing, that the battle for it is never-ending and that if we let up for one single moment or one single year, not merely do we stand still but we fall back in the march of civilization.\" He ran on a platform that called for aid to farmers, full employment, unemployment insurance, and old-age pensions. He was elected to a second term by a 14% margin.Roosevelt proposed an economic relief package and the establishment of the Temporary Emergency Relief Administration to distribute those funds. Led first by Jesse I. Straus and then by Harry Hopkins, the agency assisted well over one-third of New York's population between 1932 and 1938. Roosevelt also began an investigation into corruption in New York City among the judiciary, the police force, and organized crime, prompting the creation of the Seabury Commission. The Seabury investigations exposed an extortion ring, led many public officials to be removed from office, and made the decline of Tammany Hall inevitable.Roosevelt supported reforestation with the Hewitt Amendment in 1931, which gave birth to New York's State Forest system.\n\n\n== 1932 presidential election ==\n\nAs the 1932 presidential election approached, Roosevelt turned his attention to national politics, established a campaign team led by Howe and Farley, and a \"brain trust\" of policy advisers, primarily composed of Columbia University and Harvard University professors. There were some who were not so sanguine about his chances, such as Walter Lippmann, the dean of political commentators, who observed of Roosevelt: \"He is a pleasant man who, without any important qualifications for the office, would very much like to be president.\"However, Roosevelt's efforts as governor to address the effects of the depression in his own state established him as the front-runner for the 1932 Democratic presidential nomination. Roosevelt rallied the progressive supporters of the Wilson administration while also appealing to many conservatives, establishing himself as the leading candidate in the South and West. The chief opposition to Roosevelt's candidacy came from Northeastern conservatives, Speaker of the House John Nance Garner of Texas and Al Smith, the 1928 Democratic presidential nominee.Roosevelt entered the convention with a delegate lead due to his success in the 1932 Democratic primaries, but most delegates entered the convention unbound to any particular candidate. On the first presidential ballot of the convention, Roosevelt received the votes of more than half but less than two-thirds of the delegates, with Smith finishing in a distant second place. Roosevelt then promised the vice-presidential nomination to Garner, who controlled the votes of Texas and California; Garner threw his support behind Roosevelt after the third ballot, and Roosevelt clinched the nomination on the fourth ballot. Roosevelt flew in from New York to Chicago after learning that he had won the nomination, becoming the first major-party presidential nominee to accept the nomination in person. His appearance was essential, to show himself as vigorous, despite the ravaging disease that disabled him physically.In his acceptance speech, Roosevelt declared, \"I pledge you, I pledge myself to a new deal for the American people... This is more than a political campaign. It is a call to arms.\" Roosevelt promised securities regulation, tariff reduction, farm relief, government-funded public works, and other government actions to address the Great Depression. Reflecting changing public opinion, the Democratic platform included a call for the repeal of Prohibition; Roosevelt himself had not taken a public stand on the issue prior to the convention but promised to uphold the party platform. Otherwise, Roosevelt's primary campaign strategy was one of caution, intent upon avoiding mistakes that would distract from Hoover's failings on the economy. His statements attacked the incumbent and included no other specific policies or programs.After the convention, Roosevelt won endorsements from several progressive Republicans, including George W. Norris, Hiram Johnson, and Robert La Follette Jr. He also reconciled with the party's conservative wing, and even Al Smith was persuaded to support the Democratic ticket. Hoover's handling of the Bonus Army further damaged the incumbent's popularity, as newspapers across the country criticized the use of force to disperse assembled veterans.\nRoosevelt won 57% of the popular vote and carried all but six states. Historians and political scientists consider the 1932\u201336 elections to be a political realignment. Roosevelt's victory was enabled by the creation of the New Deal coalition, small farmers, the Southern whites, Catholics, big city political machines, labor unions, northern African Americans (southern ones were still disfranchised), Jews, intellectuals, and political liberals. The creation of the New Deal coalition transformed American politics and started what political scientists call the \"New Deal Party System\" or the Fifth Party System. Between the Civil War and 1929, Democrats had rarely controlled both houses of Congress and had won just four of seventeen presidential elections; from 1932 to 1979, Democrats won eight of twelve presidential elections and generally controlled both houses of Congress.\n\n\n=== Transition and assassination attempt ===\n\nRoosevelt was elected in November 1932 but like his predecessors did not take office until the following March. After the election, President Hoover sought to convince Roosevelt to renounce much of his campaign platform and to endorse the Hoover administration's policies. Roosevelt refused Hoover's request to develop a joint program to stop the economic decline, claiming that it would tie his hands and that Hoover had the power to act.During the transition, Roosevelt chose Howe as his chief of staff, and Farley as Postmaster General. Frances Perkins, as Secretary of Labor, became the first woman appointed to a cabinet position. William H. Woodin, a Republican industrialist close to Roosevelt, was the choice for Secretary of the Treasury, while Roosevelt chose Senator Cordell Hull of Tennessee as Secretary of State. Harold L. Ickes and Henry A. Wallace, two progressive Republicans, were selected for the roles of Secretary of the Interior and Secretary of Agriculture, respectively.In February 1933, Roosevelt escaped an assassination attempt by Giuseppe Zangara, who expressed a \"hate for all rulers.\" As he was attempting to shoot Roosevelt, Zangara was struck by a woman with her purse; he instead mortally wounded Chicago Mayor Anton Cermak, who was sitting alongside Roosevelt.\n\n\n== Presidency (1933\u20131945) ==\nAs president, Roosevelt appointed powerful men to top positions but made all the major decisions, regardless of delays, inefficiency, or resentment. Analyzing the president's administrative style, Burns concludes:\n\nThe president stayed in charge of his administration...by drawing fully on his formal and informal powers as Chief Executive; by raising goals, creating momentum, inspiring a personal loyalty, getting the best out of people...by deliberately fostering among his aides a sense of competition and a clash of wills that led to disarray, heartbreak, and anger but also set off pulses of executive energy and sparks of creativity...by handing out one job to several men and several jobs to one man, thus strengthening his own position as a court of appeals, as a depository of information, and as a tool of co-ordination; by ignoring or bypassing collective decision-making agencies, such as the Cabinet...and always by persuading, flattering, juggling, improvising, reshuffling, harmonizing, conciliating, manipulating.\n\n\n=== First and second terms (1933\u20131941) ===\n\nWhen Roosevelt was inaugurated on March 4, 1933, the U.S. was at the nadir of the worst depression in its history. A quarter of the workforce was unemployed, and farmers were in deep trouble as prices had fallen by 60%. Industrial production had fallen by more than half since 1929. Two million people were homeless. By the evening of March 4, 32 of the 48 states\u2014as well as the District of Columbia\u2014had closed their banks.Historians categorized Roosevelt's program as \"relief, recovery, and reform.\" Relief was urgently needed by tens of millions of unemployed. Recovery meant boosting the economy back to normal, and reform was required of the financial and banking systems. Through Roosevelt's series of 30 \"fireside chats\", he presented his proposals directly to the American public as a series of radio addresses. Energized by his own victory over paralytic illness, he used persistent optimism and activism to renew the national spirit.\n\n\n==== First New Deal (1933\u20131934) ====\n\nOn his second day in office, Roosevelt declared a four-day national \"bank holiday\", to end the run by depositors seeking to withdraw funds. He called for a special session of Congress on March 9, when Congress passed, almost sight unseen, the Emergency Banking Act. The act, first developed by the Hoover administration and Wall Street bankers, gave the president the power to determine the opening and closing of banks and authorized the Federal Reserve Banks to issue banknotes. The \"first 100 Days\" of the 73rd United States Congress saw an unprecedented amount of legislation and set a benchmark against which future presidents have been compared. When the banks reopened on Monday, March 15, stock prices rose by 15 percent and in the following weeks over $1 billion was returned to bank vaults, ending the bank panic. On March 22, Roosevelt signed the Cullen\u2013Harrison Act, which brought Prohibition to a close.\nRoosevelt saw the establishment of a number of agencies and measures designed to provide relief for the unemployed and others. The Federal Emergency Relief Administration (FERA), under the leadership of Harry Hopkins, distributed relief to state governments. The Public Works Administration (PWA), under Secretary of the Interior Harold Ickes, oversaw the construction of large-scale public works such as dams, bridges, and schools. The most popular of all New Deal agencies\u2014and Roosevelt's favorite\u2014was the Civilian Conservation Corps (CCC), which hired 250,000 unemployed men to work in rural projects. Roosevelt also expanded Hoover's Reconstruction Finance Corporation, which financed railroads and industry. Congress gave the Federal Trade Commission (FTC) broad regulatory powers and provided mortgage relief to millions of farmers and homeowners. Roosevelt also set up the Agricultural Adjustment Administration (AAA) to increase commodity prices, by paying farmers to leave land uncultivated and cut herds. In many instances, crops were plowed under and livestock killed, while many Americans died of hunger and were ill-clothed; critics labeled such policies \"utterly idiotic.\" On the positive side, nothing did more to rescue the farm family from isolation than the Rural Electrification Administration (REA), which brought electricity for the first time to millions of rural homes and with it such conveniences as radios and washing machines.\"Reform of the economy was the goal of the National Industrial Recovery Act (NIRA) of 1933. It sought to end cutthroat competition by forcing industries to establish rules such as minimum prices, agreements not to compete, and production restrictions. Industry leaders negotiated the rules with NIRA officials, who suspended antitrust laws in return for better wages. The Supreme Court in May 1935 declared NIRA unconstitutional by a unanimous decision, to Roosevelt's chagrin. He reformed financial regulations with the Glass\u2013Steagall Act, creating the Federal Deposit Insurance Corporation (FDIC) to underwrite savings deposits. The act also limited affiliations between commercial banks and securities firms. In 1934, the Securities and Exchange Commission was created to regulate the trading of securities, while the Federal Communications Commission (FCC) was established to regulate telecommunications.Recovery was sought through federal spending, as the NIRA included $3.3  billion (equivalent to $74.6 billion in 2022) of spending through the Public Works Administration. Roosevelt worked with Senator Norris to create the largest government-owned industrial enterprise in American history\u2014the Tennessee Valley Authority (TVA)\u2014which built dams and power stations, controlled floods, and modernized agriculture and home conditions in the poverty-stricken Tennessee Valley. However, natives criticized the TVA for displacing thousands of people for these projects. The Soil Conservation Service trained farmers in the proper methods of cultivation, and with the TVA, Roosevelt became the father of soil conservation. Executive Order 6102 declared that all privately held gold of American citizens was to be sold to the U.S. Treasury and the price raised from $20 to $35 per ounce. The goal was to counter the deflation which was paralyzing the economy.Roosevelt tried to keep his campaign promise by cutting the federal budget. This included a reduction in military spending from $752  million in 1932 to $531  million in 1934 and a 40% cut in spending on veterans benefits. 500,000 veterans and widows were removed from the pension rolls, and benefits were reduced for the remainder. Federal salaries were cut and spending on research and education was reduced. The veterans were well organized and strongly protested, so most benefits were restored or increased by 1934. Veterans groups such as the American Legion and the Veterans of Foreign Wars won their campaign to transform their benefits from payments due in 1945 to immediate cash when Congress overrode the President's veto and passed the Bonus Act in January 1936. It pumped sums equal to 2% of the GDP into the consumer economy and had a major stimulus effect.\n\n\n==== Second New Deal (1935\u20131936) ====\n\nRoosevelt expected that his party would lose seats in the 1934 Congressional elections, as the president's party had done in most previous midterm elections. Unexpectedly the Democrats picked up seats in both houses of Congress. Empowered by the public's vote of confidence, the first item on Roosevelt's agenda in the 74th Congress was the creation of a social insurance program. The Social Security Act established Social Security and promised economic security for the elderly, the poor, and the sick. Roosevelt insisted that it should be funded by payroll taxes rather than from the general fund, saying, \"We put those payroll contributions there so as to give the contributors a legal, moral, and political right to collect their pensions and unemployment benefits. With those taxes in there, no damn politician can ever scrap my social security program.\" Compared with the social security systems in western European countries, the Social Security Act of 1935 was rather conservative. But for the first time, the federal government took responsibility for the economic security of the aged, the temporarily unemployed, dependent children, and disabled people. Against Roosevelt's original intention for universal coverage, the act excluded farmers, domestic workers, and other groups, which made up about forty percent of the labor force.Roosevelt consolidated the various relief organizations, though some, like the PWA, continued to exist. After winning Congressional authorization for further funding of relief efforts, he established the Works Progress Administration (WPA). Under the leadership of Harry Hopkins, the WPA employed over three million people in its first year of operations. It undertook numerous massive construction projects in cooperation with local governments. It also set up the National Youth Administration and arts organizations.\nThe National Labor Relations Act guaranteed workers the right to collective bargaining through unions of their own choice. The act also established the National Labor Relations Board (NLRB) to facilitate wage agreements and suppress repeated labor disturbances. The act did not compel employers to reach an agreement with their employees, but it opened possibilities for American labor. The result was a tremendous growth of membership in the labor unions, especially in the mass-production sector. When the Flint sit-down strike threatened the production of General Motors, Roosevelt broke with the precedent set by many former presidents and refused to intervene; the strike ultimately led to the unionization of both General Motors and its rivals in the American automobile industry.While the First New Deal of 1933 had broad support from most sectors, the Second New Deal challenged the business community. Conservative Democrats, led by Al Smith, fought back with the American Liberty League, savagely attacking Roosevelt and equating him with socialism. But Smith overplayed his hand, and his boisterous rhetoric let Roosevelt isolate his opponents and identify them with the wealthy vested interests that opposed the New Deal, strengthening Roosevelt for the 1936 landslide. By contrast, labor unions, energized by labor legislation, signed up millions of new members and became a major backer of Roosevelt's re-elections in 1936, 1940, and 1944.Burns suggests that Roosevelt's policy decisions were guided more by pragmatism than ideology and that he \"was like the general of a guerrilla army whose columns, fighting blindly in the mountains through dense ravines and thickets, suddenly converge, half by plan and half by coincidence, and debouch into the plain below.\" Roosevelt argued that such apparently haphazard methodology was necessary. \"The country needs and, unless I mistake its temper, the country demands bold, persistent experimentation,\" he wrote. \"It is common sense to take a method and try it; if it fails, admit it frankly and try another. But above all, try something.\"\n\n\n==== Election of 1936 ====\n\nEight million workers remained unemployed in 1936, and though economic conditions had improved since 1932, they remained sluggish. By 1936, Roosevelt had lost the backing he once held in the business community because of his support for the NLRB and the Social Security Act. The Republicans had few alternative candidates and nominated Kansas Governor Alf Landon, a little-known bland candidate whose chances were damaged by the public re-emergence of the still-unpopular Herbert Hoover. While Roosevelt campaigned on his New Deal programs and continued to attack Hoover, Landon sought to win voters who approved of the goals of the New Deal but disagreed with its implementation.An attempt by Louisiana Senator Huey Long to organize a left-wing third party collapsed after Long's assassination in 1935. The remnants, helped by Father Charles Coughlin, supported William Lemke of the newly formed Union Party. Roosevelt won re-nomination with little opposition at the 1936 Democratic National Convention, while his allies overcame Southern resistance to permanently abolish the long-established rule that had required Democratic presidential candidates to win the votes of two-thirds of the delegates rather than a simple majority.In the election against Landon and a third-party candidate, Roosevelt won 60.8% of the vote and carried every state except Maine and Vermont. The Democratic ticket won the highest proportion of the popular vote. Democrats also expanded their majorities in Congress, winning control of over three-quarters of the seats in each house. The election also saw the consolidation of the New Deal coalition; while the Democrats lost some of their traditional allies in big business, they were replaced by groups such as organized labor and African Americans, the latter of whom voted Democratic for the first time since the Civil War. Roosevelt lost high-income voters, especially businessmen and professionals, but made major gains among the poor and minorities. He won 86 percent of the Jewish vote, 81 percent of Catholics, 80 percent of union members, 76 percent of Southerners, 76 percent of blacks in northern cities, and 75 percent of people on relief. Roosevelt carried 102 of the country's 106 cities with a population of 100,000 or more.\n\n\n==== Supreme Court fight and second term legislation ====\n\nThe Supreme Court became Roosevelt's primary domestic focus during his second term after the court overturned many of his programs, including NIRA. The more conservative members of the court upheld the principles of the Lochner era, which saw numerous economic regulations struck down on the basis of freedom of contract. Roosevelt proposed the Judicial Procedures Reform Bill of 1937, which would have allowed him to appoint an additional Justice for each incumbent Justice over the age of 70; in 1937, there were six Supreme Court Justices over the age of 70. The size of the Court had been set at nine since the passage of the Judiciary Act of 1869, and Congress had altered the number of Justices six other times throughout U.S. history. Roosevelt's \"court packing\" plan ran into intense political opposition from his own party, led by Vice President Garner since it upset the separation of powers. A bipartisan coalition of liberals and conservatives of both parties opposed the bill, and Chief Justice Charles Evans Hughes broke with precedent by publicly advocating the defeat of the bill. Any chance of passing the bill ended with the death of Senate Majority Leader Joseph Taylor Robinson in July 1937.Starting with the 1937 case of West Coast Hotel Co. v. Parrish, the court began to take a more favorable view of economic regulations. Historians have described this as, \"the switch in time that saved nine.\" That same year, Roosevelt appointed a Supreme Court Justice for the first time, and by 1941, seven of the nine Justices had been appointed by Roosevelt. After Parrish, the Court shifted its focus from judicial review of economic regulations to the protection of civil liberties. Four of Roosevelt's Supreme Court appointees, Felix Frankfurter, Robert H. Jackson,\nHugo Black, and William O. Douglas, were particularly influential in reshaping the jurisprudence of the Court.With Roosevelt's influence on the wane following the failure of the Judicial Procedures Reform Bill of 1937, conservative Democrats joined with Republicans to block the implementation of further New Deal programs. Roosevelt did manage to pass some legislation, including the Housing Act of 1937, a second Agricultural Adjustment Act, and the Fair Labor Standards Act (FLSA) of 1938, which was the last major piece of New Deal legislation. The FLSA outlawed child labor, established a federal minimum wage, and required overtime pay for certain employees who work in excess of forty-hours per week. He also won passage of the Reorganization Act of 1939 and subsequently created the Executive Office of the President, making it \"the nerve center of the federal administrative system.\" When the economy began to deteriorate again in mid-1937, during the onset of the recession of 1937\u20131938, Roosevelt launched a rhetorical campaign against big business and monopoly power in the United States, alleging that the recession was the result of a capital strike and even ordering the Federal Bureau of Investigation to look for a criminal conspiracy (of which they found none). He then asked Congress for $5 billion (equivalent to $101.78 billion in 2022) in relief and public works funding. This managed to eventually create as many as 3.3 million WPA jobs by 1938. Projects accomplished under the WPA ranged from new federal courthouses and post offices to facilities and infrastructure for national parks, bridges, and other infrastructure across the country, and architectural surveys and archaeological excavations\u2014investments to construct facilities and preserve important resources. Beyond this, however, Roosevelt recommended to a special congressional session only a permanent national farm act, administrative reorganization, and regional planning measures, all of which were leftovers from a regular session. According to Burns, this attempt illustrated Roosevelt's inability to settle on a basic economic program.Determined to overcome the opposition of conservative Democrats in Congress, Roosevelt became involved in the 1938 Democratic primaries, actively campaigning for challengers who were more supportive of New Deal reform. Roosevelt failed badly, managing to defeat only one of the ten targeted, a conservative Democrat from New York City. In the November 1938 elections, Democrats lost six Senate seats and 71 House seats, with losses concentrated among pro-New Deal Democrats. When Congress reconvened in 1939, Republicans under Senator Robert Taft formed a Conservative coalition with Southern Democrats, virtually ending Roosevelt's ability to enact his domestic proposals. Despite their opposition to Roosevelt's domestic policies, many of these conservative Congressmen would provide crucial support for Roosevelt's foreign policy before and during World War II.\n\n\n==== Conservation and the environment ====\nRoosevelt had a lifelong interest in the environment and conservation starting with his youthful interest in forestry on his family estate. Although he was never an outdoorsman or sportsman on Theodore Roosevelt's scale, his growth of the national systems was comparable. When Franklin was Governor of New York, the Temporary Emergency Relief Administration was essentially a state-level predecessor of the federal Civilian Conservation Corps, with 10,000 or more men building fire trails, combating soil erosion and planting tree seedlings in marginal farmland in the state of New York. As President, Roosevelt was active in expanding, funding, and promoting the National Park and National Forest systems. Their popularity soared, from three million visitors a year at the start of the decade to 15.5 million in 1939. The Civilian Conservation Corps enrolled 3.4 million young men and built 13,000 miles (21,000 kilometres) of trails, planted two billion trees, and upgraded 125,000 miles (201,000 kilometres) of dirt roads. Every state had its own state parks, and Roosevelt made sure that WPA and CCC projects were set up to upgrade them as well as the national systems.\n\n\n==== GNP and unemployment rates ====\n\nGovernment spending increased from 8.0% of the gross national product (GNP) under Hoover in 1932 to 10.2% in 1936. The national debt as a percentage of the GNP had more than doubled under Hoover from 16% to 40% of the GNP in early 1933. It held steady at close to 40% as late as fall 1941, then grew rapidly during the war. The GNP was 34% higher in 1936 than in 1932 and 58% higher in 1940 on the eve of war. That is, the economy grew 58% from 1932 to 1940 in eight years of peacetime, and then grew 56% from 1940 to 1945 in five years of wartime. Unemployment fell dramatically during Roosevelt's first term. It increased in 1938 (\"a depression within a depression\") but continually declined after 1938. Total employment during Roosevelt's term expanded by 18.31 million jobs, with an average annual increase in jobs during his administration of 5.3%.\n\n\n==== Foreign policy (1933\u20131941) ====\n\nThe main foreign policy initiative of Roosevelt's first term was the Good Neighbor Policy, which was a re-evaluation of U.S. policy toward Latin America. The United States frequently intervened in Latin America following the promulgation of the Monroe Doctrine in 1823, and the United States occupied several Latin American nations in the Banana Wars that occurred following the Spanish\u2013American War of 1898. After Roosevelt took office, he withdrew U.S. forces from Haiti and reached new treaties with Cuba and Panama, ending their status as U.S. protectorates. In December 1933, Roosevelt signed the Montevideo Convention on the Rights and Duties of States, renouncing the right to intervene unilaterally in the affairs of Latin American countries. Roosevelt also normalized relations with the Soviet Union, which the United States had refused to recognize since the 1920s. He hoped to renegotiate the Russian debt from World War I and open trade relations, but no progress was made on either issue and \"both nations were soon disillusioned by the accord.\"The rejection of the Treaty of Versailles in 1919\u20131920 marked the dominance of isolationism in American foreign policy. Despite Roosevelt's Wilsonian background, he and Secretary of State Cordell Hull acted with great care not to provoke isolationist sentiment. The isolationist movement was bolstered in the early to mid-1930s by Senator Gerald Nye and others who succeeded in their effort to stop the \"merchants of death\" in the U.S. from selling arms abroad. This effort took the form of the Neutrality Acts; the president was refused a provision he requested giving him the discretion to allow the sale of arms to victims of aggression. He largely acquiesced to Congress's non-interventionist policies in the early-to-mid 1930s. In the interim, Fascist Italy under Benito Mussolini proceeded to overcome Ethiopia, and the Italians joined Nazi Germany under Adolf Hitler in supporting General Francisco Franco and the Nationalist cause in the Spanish Civil War. As that conflict drew to a close in early 1939, Roosevelt expressed regret in not aiding the Spanish Republicans. When Japan invaded China in 1937, isolationism limited Roosevelt's ability to aid China, despite atrocities like the Nanking Massacre and the USS Panay incident.\nGermany annexed Austria in 1938, and soon turned its attention to its eastern neighbors. Roosevelt made it clear that, in the event of German aggression against Czechoslovakia, the U.S. would remain neutral. After completion of the Munich Agreement and the execution of Kristallnacht, American public opinion turned against Germany, and Roosevelt began preparing for a possible war with Germany. Relying on an interventionist political coalition of Southern Democrats and business-oriented Republicans, Roosevelt oversaw the expansion of U.S. airpower and war production capacity.When World War II began in September 1939 with Germany's invasion of Poland and Britain and France's subsequent declaration of war upon Germany, Roosevelt sought ways to assist Britain and France militarily. Isolationist leaders like Charles Lindbergh and Senator William Borah successfully mobilized opposition to Roosevelt's proposed repeal of the Neutrality Act, but Roosevelt won Congressional approval of the sale of arms on a cash-and-carry basis. He also began a regular secret correspondence with Britain's First Lord of the Admiralty, Winston Churchill, in September 1939\u2014the first of 1,700 letters and telegrams between them. Roosevelt forged a close personal relationship with Churchill, who became Prime Minister of the United Kingdom in May 1940.The Fall of France in June 1940 shocked the American public, and isolationist sentiment declined. In July 1940, Roosevelt appointed two interventionist Republican leaders, Henry L. Stimson and Frank Knox, as Secretaries of War and the Navy, respectively. Both parties gave support to his plans for a rapid build-up of the American military, but the isolationists warned that Roosevelt would get the nation into an unnecessary war with Germany. In July 1940, a group of Congressmen introduced a bill that would authorize the nation's first peacetime draft, and with the support of the Roosevelt administration, the Selective Training and Service Act of 1940 passed in September. The size of the army increased from 189,000 men at the end of 1939 to 1.4 million men in mid-1941. In September 1940, Roosevelt openly defied the Neutrality Acts by reaching the Destroyers for Bases Agreement, which, in exchange for military base rights in the British Caribbean Islands, gave 50 WWI American destroyers to Britain.\n\n\n==== Good Neighbor Policy ====\nWhile working under President Wilson, Roosevelt had perpetuated ideas of American racial superiority by believing that the people of Latin American were uncapable of self-government. However, by 1928 he had switched his point of view, becoming an advocate for cooperation. In an effort to denounce past U.S. interventionism and subdue any subsequent fears of Latin Americans, Roosevelt announced on March 4, 1933, during his inaugural address, \"In the field of World policy, I would dedicate this nation to the policy of the good neighbor, the neighbor who resolutely respects himself and, because he does so, respects the rights of others, the neighbor who respects his obligations and respects the sanctity of his agreements in and with a World of neighbors.\"In order to create a friendly relationship between the United States and Central as well as South American countries, Roosevelt sought to abstain from asserting military force in the region. This position was affirmed by Cordell Hull, Roosevelt's Secretary of State at a conference of American states in Montevideo in December 1933. Hull said: \"No country has the right to intervene in the internal or external affairs of another.\" Roosevelt then confirmed the policy in December of the same year: \"The definite policy of the United States from now on is one opposed to armed intervention.\" The fact that the policy was even put into place meant that the U.S. now recognised the maturity of Latin American countries and as a result were now more open to working together, especially when it comes to maintaining the peace. The policy, in the end, was yet another way for the U.S. to assert its own superiority.\n\n\n==== Election of 1940 ====\n\nIn the months prior to the July 1940 Democratic National Convention, there was much speculation as to whether Roosevelt would run for an unprecedented third term. The president was silent, and even his closest advisors were in the dark. The two-term tradition, although not yet enshrined in the Constitution, had been established by George Washington when he refused to run for a third term in the 1796 presidential election. Roosevelt refused to give a definitive statement as to his willingness to be a candidate again, and he even indicated to some ambitious Democrats, such as James Farley, that he would not run for a third term and that they could seek the Democratic nomination. Farley and Vice President John Garner were not pleased with Roosevelt when he ultimately made the decision to break from Washington's precedent. As Germany swept through Western Europe and menaced Britain in mid-1940, Roosevelt decided that only he had the necessary experience and skills to see the nation safely through the Nazi threat. He was aided by the party's political bosses, who feared that no Democrat except Roosevelt could defeat Wendell Willkie, the popular Republican nominee.\nAt the July 1940 Democratic Convention in Chicago, Roosevelt easily swept aside challenges from Farley and Vice President Garner, who had turned against Roosevelt in his second term because of his liberal economic and social policies. To replace Garner on the ticket, Roosevelt turned to Secretary of Agriculture Henry Wallace of Iowa, a former Republican who strongly supported the New Deal and was popular in farm states. The choice was strenuously opposed by many of the party's conservatives, who felt Wallace was too radical and \"eccentric\" in his private life to be an effective running mate. But Roosevelt insisted that without Wallace on the ticket he would decline re-nomination, and Wallace won the vice-presidential nomination, defeating Speaker of the House William B. Bankhead and other candidates.A late August poll taken by Gallup found the race to be essentially tied, but Roosevelt's popularity surged in September following the announcement of the Destroyers for Bases Agreement. Willkie supported much of the New Deal as well as rearmament and aid to Britain but warned that Roosevelt would drag the country into another European war. Responding to Willkie's attacks, Roosevelt promised to keep the country out of the war. Over its last month, the campaign degenerated into a series of outrageous accusations and mud-slinging, if not by the two candidates themselves then by their respective parties. Roosevelt won the 1940 election with 55% of the popular vote, 38 of the 48 states, and almost 85% of the electoral vote.\n\n\n=== Third and fourth terms (1941\u20131945) ===\n\nWorld War II dominated Roosevelt's attention, with far more time devoted to world affairs than ever before. Domestic politics and relations with Congress were largely shaped by his efforts to achieve total mobilization of the nation's economic, financial, and institutional resources for the war effort. Even relationships with Latin America and Canada were structured by wartime demands. Roosevelt maintained close personal control of all major diplomatic and military decisions, working closely with his generals and admirals, the war and Navy departments, the British, and even the Soviet Union. His key advisors on diplomacy were Harry Hopkins (who was based in the White House), Sumner Welles (based in the State Department), and Henry Morgenthau Jr. at Treasury. In military affairs, Roosevelt worked most closely with Secretary Henry L. Stimson at the War Department, Army Chief of Staff George Marshall, and Admiral William D. Leahy.\n\n\n==== Lead-up to the war ====\n\nBy late 1940, re-armament was in high gear, partly to expand and re-equip the Army and Navy and partly to become the \"Arsenal of Democracy\" for Britain and other countries. With his Four Freedoms speech in January 1941, Roosevelt laid out the case for an Allied battle for basic rights throughout the world. Assisted by Willkie, Roosevelt won Congressional approval of the Lend-Lease program, which directed massive military and economic aid to Britain, and China. In sharp contrast to the loans of World War I, there would be no repayment after the war. As Roosevelt took a firmer stance against Japan, Germany, and Italy, American isolationists such as Charles Lindbergh and the America First Committee vehemently attacked Roosevelt as an irresponsible warmonger. When Germany invaded the Soviet Union in June 1941, Roosevelt agreed to extend Lend-Lease to the Soviets. Thus, Roosevelt had committed the U.S. to the Allied side with a policy of \"all aid short of war.\" By July 1941, Roosevelt authorized the creation of the Office of the Coordinator of Inter-American Affairs (OCIAA) to counter perceived propaganda efforts in Latin America by Germany and Italy.In August 1941, Roosevelt and Churchill conducted a highly secret bilateral meeting in which they drafted the Atlantic Charter, conceptually outlining global wartime and postwar goals. This would be the first of several wartime conferences; Churchill and Roosevelt would meet ten more times in person. Though Churchill pressed for an American declaration of war against Germany, Roosevelt believed that Congress would reject any attempt to bring the United States into the war. In September, a German submarine fired on the U.S. destroyer Greer, and Roosevelt declared that the U.S. Navy would assume an escort role for Allied convoys in the Atlantic as far east as Great Britain and would fire upon German ships or submarines (U-boats) of the Kriegsmarine if they entered the U.S. Navy zone. According to historian George Donelson Moss, Roosevelt \"misled\" Americans by reporting the Greer incident as if it would have been an unprovoked German attack on a peaceful American ship. This \"shoot on sight\" policy effectively declared naval war on Germany and was favored by Americans by a margin of 2-to-1.\n\n\n==== Pearl Harbor and declarations of war ====\n\nAfter the German invasion of Poland, the primary concern of both Roosevelt and his top military staff was on the war in Europe, but Japan also presented foreign policy challenges. Relations with Japan had continually deteriorated since its invasion of Manchuria in 1931, and they had further worsened with Roosevelt's support of China. With the war in Europe occupying the attention of the major colonial powers, Japanese leaders eyed vulnerable colonies such as the Dutch East Indies, French Indochina, and British Malaya. After Roosevelt announced a $100 million loan (equivalent to $2.1 billion in 2022) to China in reaction to Japan's occupation of northern French Indochina, Japan signed the Tripartite Pact with Germany and Italy. The pact bound each country to defend the others against attack, and Germany, Japan, and Italy became known as the Axis powers. Overcoming those who favored invading the Soviet Union, the Japanese Army high command successfully advocated for the conquest of Southeast Asia to ensure continued access to raw materials. In July 1941, after Japan occupied the remainder of French Indochina, Roosevelt cut off the sale of oil to Japan, depriving Japan of more than 95 percent of its oil supply. He also placed the Philippine military under American command and reinstated General Douglas MacArthur into active duty to command U.S. forces in the Philippines.\n\nThe Japanese were incensed by the embargo and Japanese leaders became determined to attack the United States unless it lifted the embargo. The Roosevelt administration was unwilling to reverse the policy, and Secretary of State Hull blocked a potential summit between Roosevelt and Prime Minister Fumimaro Konoe. After diplomatic efforts to end the embargo failed, the Privy Council of Japan authorized a strike against the United States. The Japanese believed that the destruction of the United States Asiatic Fleet (stationed in the Philippines) and the United States Pacific Fleet (stationed at Pearl Harbor in Hawaii) was vital to the conquest of Southeast Asia. On the morning of December 7, 1941, the Japanese struck the U.S. naval base at Pearl Harbor with a surprise attack, knocking out the main American battleship fleet and killing 2,403 American servicemen and civilians. At the same time, separate Japanese task forces attacked Thailand, British Hong Kong, the Philippines, and other targets. Roosevelt called for war in his \"Infamy Speech\" to Congress, in which he said: \"Yesterday, December 7, 1941\u2014a date which will live in infamy\u2014the United States of America was suddenly and deliberately attacked by naval and air forces of the Empire of Japan.\" In a nearly unanimous vote, Congress declared war on Japan. After the Japanese attack at Pearl Harbor, antiwar sentiment in the United States largely evaporated overnight. On December 11, 1941, Hitler and Mussolini declared war on the United States, which responded in kind.\n\nA majority of scholars have rejected the conspiracy theories that Roosevelt, or any other high government officials, knew in advance about the Japanese attack on Pearl Harbor. The Japanese had kept their secrets closely guarded. Senior American officials were aware that war was imminent, but they did not expect an attack on Pearl Harbor. Roosevelt had expected that the Japanese would attack either the Dutch East Indies or Thailand. \n\n\n==== War plans ====\nIn late December 1941, Churchill and Roosevelt met at the Arcadia Conference, which established a joint strategy between the U.S. and Britain.\nBoth agreed on a Europe first strategy that prioritized the defeat of Germany before Japan. The U.S. and Britain established the Combined Chiefs of Staff to coordinate military policy and the Combined Munitions Assignments Board to coordinate the allocation of supplies. An agreement was also reached to establish a centralized command in the Pacific theater called ABDA, named for the American, British, Dutch, and Australian forces in the theater. On January 1, 1942, the United States, Britain, China, the Soviet Union, and twenty-two other countries (the Allied Powers) issued the Declaration by United Nations, in which each nation pledged to defeat the Axis powers.In 1942, Roosevelt formed a new body, the Joint Chiefs of Staff, which made the final decisions on American military strategy. Admiral Ernest J. King as Chief of Naval Operations commanded the Navy and Marines, while General George C. Marshall led the Army and was in nominal control of the Air Force, which in practice was commanded by General Hap Arnold. The Joint Chiefs were chaired by Admiral William D. Leahy, the most senior officer in the military. Roosevelt avoided micromanaging the war and let his top military officers make most decisions. Roosevelt's civilian appointees handled the draft and procurement of men and equipment, but no civilians\u2014not even the secretaries of War or Navy\u2014had a voice in strategy. Roosevelt avoided the State Department and conducted high-level diplomacy through his aides, especially Harry Hopkins, whose influence was bolstered by his control of the Lend Lease funds.\n\n\n==== Nuclear program ====\n\nIn August 1939, Leo Szilard and Albert Einstein sent the Einstein\u2013Szil\u00e1rd letter to Roosevelt, warning of the possibility of a German project to develop nuclear weapons. Szilard realized that the recently discovered process of nuclear fission could be used to create a nuclear chain reaction that could be used as a weapon of mass destruction. Roosevelt feared the consequences of allowing Germany to have sole possession of the technology and authorized preliminary research into nuclear weapons. After the attack on Pearl Harbor, the Roosevelt administration secured the funds needed to continue research and selected General Leslie Groves to oversee the Manhattan Project, which was charged with developing the first nuclear weapons. Roosevelt and Churchill agreed to jointly pursue the project, and Roosevelt helped ensure that American scientists cooperated with their British counterparts.\n\n\n==== Wartime conferences ====\n\nRoosevelt coined the term \"Four Policemen\" to refer to the \"Big Four\" Allied powers of World War II, the United States, the United Kingdom, the Soviet Union, and China. The \"Big Three\" of Roosevelt, Winston Churchill, and Soviet leader Joseph Stalin, together with Chinese Generalissimo Chiang Kai-shek, cooperated informally on a plan in which American and British troops concentrated in the West; Soviet troops fought on the Eastern front; and Chinese, British and American troops fought in Asia and the Pacific. The United States also continued to send aid via the Lend-Lease program to the Soviet Union and other countries. The Allies formulated strategy in a series of high-profile conferences as well as by contact through diplomatic and military channels. Beginning in May 1942, the Soviets urged an Anglo-American invasion of German-occupied France in order to divert troops from the Eastern front. Concerned that their forces were not yet ready for an invasion of France, Churchill and Roosevelt decided to delay such an invasion until at least 1943 and instead focus on a landing in North Africa, known as Operation Torch.In November 1943, Roosevelt, Churchill, and Stalin met to discuss strategy and post-war plans at the Tehran Conference, where Roosevelt met Stalin for the first time. At the conference, Britain and the United States committed to opening a second front against Germany in 1944, while Stalin committed to entering the war against Japan at an unspecified date. Subsequent conferences at Bretton Woods and Dumbarton Oaks established the framework for the post-war international monetary system and the United Nations, an intergovernmental organization similar to the failed League of Nations. Taking up the Wilsonian mantle, Roosevelt pushed as his highest postwar priority the establishment of the United Nations. Roosevelt expected it would be controlled by Washington, Moscow, London and Beijing, and would resolve all major world problems.\nRoosevelt, Churchill, and Stalin met for a second time at the February 1945 Yalta Conference in Crimea. With the end of the war in Europe approaching, Roosevelt's primary focus was on convincing Stalin to enter the war against Japan; the Joint Chiefs had estimated that an American invasion of Japan would cause as many as one million American casualties. In return for the Soviet Union's entrance into the war against Japan, the Soviet Union was promised control of Asian territories such as Sakhalin Island. The three leaders agreed to hold a conference in 1945 to establish the United Nations, and they also agreed on the structure of the United Nations Security Council, which would be charged with ensuring international peace and security. Roosevelt did not push for the immediate evacuation of Soviet soldiers from Poland, but he won the issuance of the Declaration on Liberated Europe, which promised free elections in countries that had been occupied by Germany. Germany itself would not be dismembered but would be jointly occupied by the United States, France, Britain, and the Soviet Union. Against Soviet pressure, Roosevelt and Churchill refused to consent to impose huge reparations and deindustrialization on Germany after the war. Roosevelt's role in the Yalta Conference has been controversial; critics charge that he naively trusted the Soviet Union to allow free elections in Eastern Europe, while supporters argue that there was little more that Roosevelt could have done for the Eastern European countries given the Soviet occupation and the need for cooperation with the Soviet Union during and after the war.\n\n\n==== Course of the war ====\n\nThe Allies invaded French North Africa in November 1942, securing the surrender of Vichy French forces within days of landing. At the January 1943 Casablanca Conference, the Allies agreed to defeat Axis forces in North Africa and then launch an invasion of Sicily, with an attack on France to take place in 1944. At the conference, Roosevelt also announced that he would only accept the unconditional surrender of Germany, Japan, and Italy. In February 1943, the Soviet Union won a major victory at the Battle of Stalingrad, and in May 1943, the Allies secured the surrender of over 250,000 German and Italian soldiers in North Africa, ending the North African Campaign. The Allies launched an invasion of Sicily in July 1943, capturing the island by the end of the following month. In September 1943, the Allies secured an armistice from Italian Prime Minister Pietro Badoglio, but Germany quickly restored Mussolini to power. The Allied invasion of mainland Italy commenced in September 1943, but the Italian Campaign continued until 1945 as German and Italian troops resisted the Allied advance.\nTo command the invasion of France, Roosevelt chose General Dwight D. Eisenhower, who had successfully commanded a multinational coalition in North Africa and Sicily. Eisenhower chose to launch Operation Overlord on June 6, 1944. Supported by 12,000 aircraft and the largest naval force ever assembled, the Allies successfully established a beachhead in Normandy and then advanced further into France. Though reluctant to back an unelected government, Roosevelt recognized Charles de Gaulle's Provisional Government of the French Republic as the de facto government of France in July 1944. After most of France had been liberated from German occupation, Roosevelt granted formal recognition to de Gaulle's government in October 1944. Over the following months, the Allies liberated more territory from Nazi occupation and began the invasion of Germany. By April 1945, Nazi resistance was crumbling in the face of advances by both the Western Allies and the Soviet Union.In the opening weeks of the war, Japan conquered the Philippines and the British and Dutch colonies in Southeast Asia. The Japanese advance reached its maximum extent by June 1942, when the U.S. Navy scored a decisive victory at the Battle of Midway. American and Australian forces then began a slow and costly strategy called island hopping or leapfrogging through the Pacific Islands, with the objective of gaining bases from which strategic airpower could be brought to bear on Japan and from which Japan could ultimately be invaded. In contrast to Hitler, Roosevelt took no direct part in the tactical naval operations, though he approved strategic decisions. Roosevelt gave way in part to insistent demands from the public and Congress that more effort be devoted against Japan, but he always insisted on Germany first. The strength of the Japanese navy was decimated in the Battle of Leyte Gulf, and by April 1945 the Allies had re-captured much of their lost territory in the Pacific.\n\n\n==== Home front ====\n\nThe home front was subject to dynamic social changes throughout the war, though domestic issues were no longer Roosevelt's most urgent policy concern. The military buildup spurred economic growth. Unemployment fell in half from 7.7 million in spring 1940 to 3.4 million in fall 1941 and fell in half again to 1.5 million in fall 1942, out of a labor force of 54 million. There was a growing labor shortage, accelerating the second wave of the Great Migration of African Americans, farmers and rural populations to manufacturing centers. African Americans from the South went to California and other West Coast states for new jobs in the defense industry. To pay for increased government spending, in 1941 Roosevelt proposed that Congress enact an income tax rate of 99.5% on all income over $100,000; when the proposal failed, he issued an executive order imposing an income tax of 100% on income over $25,000, which Congress rescinded. The Revenue Act of 1942 instituted top tax rates as high as 94% (after accounting for the excess profits tax), greatly increased the tax base, and instituted the first federal withholding tax. In 1944, Roosevelt requested that Congress enact legislation which would tax all \"unreasonable\" profits, both corporate and individual, and thereby support his declared need for over $10 billion in revenue for the war and other government measures. Congress overrode Roosevelt's veto to pass a smaller revenue bill raising $2 billion.In 1942, with the United States now in the conflict, war production increased dramatically but fell short of the goals established by the president, due in part to manpower shortages. The effort was also hindered by numerous strikes, especially among union workers in the coal mining and railroad industries, which lasted well into 1944. Nonetheless, between 1941 and 1945, the United States produced 2.4 million trucks, 300,000 military aircraft, 88,400 tanks, and 40 billion rounds of ammunition. The production capacity of the United States dwarfed that of other countries; for example, in 1944, the United States produced more military aircraft than the combined production of Germany, Japan, Britain, and the Soviet Union. The White House became the ultimate site for labor mediation, conciliation or arbitration. One particular battle royale occurred between Vice President Wallace, who headed the Board of Economic Warfare, and Jesse H. Jones, in charge of the Reconstruction Finance Corporation; both agencies assumed responsibility for the acquisition of rubber supplies and came to loggerheads over funding. Roosevelt resolved the dispute by dissolving both agencies. In 1943, Roosevelt established the Office of War Mobilization to oversee the home front; the agency was led by James F. Byrnes, who came to be known as the \"assistant president\" due to his influence.\nRoosevelt's 1944 State of the Union Address advocated that Americans should think of basic economic rights as a Second Bill of Rights. He stated that all Americans should have the right to \"adequate medical care\", \"a good education\", \"a decent home\", and a \"useful and remunerative job\". In the most ambitious domestic proposal of his third term, Roosevelt proposed the G.I. Bill, which would create a massive benefits program for returning soldiers. Benefits included post-secondary education, medical care, unemployment insurance, job counseling, and low-cost loans for homes and businesses. The G.I. Bill passed unanimously in both houses of Congress and was signed into law in June 1944. Of the fifteen million Americans who served in World War II, more than half benefitted from the educational opportunities provided for in the G.I. Bill.\n\n\n==== Declining health ====\nRoosevelt, a chain-smoker throughout his entire adult life, had been in declining physical health since at least 1940. In March 1944, shortly after his 62nd birthday, he underwent testing at Bethesda Hospital and was found to have high blood pressure, atherosclerosis, coronary artery disease causing angina pectoris, and congestive heart failure.Hospital physicians and two outside specialists ordered Roosevelt to rest. His personal physician, Admiral Ross McIntire, created a daily schedule that banned business guests for lunch and incorporated two hours of rest each day. During the 1944 re-election campaign, McIntire denied several times that Roosevelt's health was poor; on October 12, for example, he announced that \"The President's health is perfectly OK. There are absolutely no organic difficulties at all.\" Roosevelt realized that his declining health could eventually make it impossible for him to continue as president, and in 1945 he told a confidant that he might resign from the presidency following the end of the war.\n\n\n==== Election of 1944 ====\n\nWhile some Democrats had opposed Roosevelt's nomination in 1940, the president faced little difficulty in securing his re-nomination at the 1944 Democratic National Convention. Roosevelt made it clear before the convention that he was seeking another term, and on the lone presidential ballot of the convention, Roosevelt won the vast majority of delegates, although a minority of Southern Democrats voted for Harry F. Byrd. Party leaders prevailed upon Roosevelt to drop Vice President Wallace from the ticket, believing him to be an electoral liability and a poor potential successor in case of Roosevelt's death. Roosevelt preferred Byrnes as Wallace's replacement but was convinced to support Senator Harry S. Truman of Missouri, who had earned renown for his investigation of war production inefficiency and was acceptable to the various factions of the party. On the second vice presidential ballot of the convention, Truman defeated Wallace to win the nomination.The Republicans nominated Thomas E. Dewey, the governor of New York, who had a reputation as a liberal in his party. They accused the Roosevelt administration of domestic corruption and bureaucratic inefficiency, but Dewey's most effective gambit was to raise discreetly the age issue. He assailed the President as a \"tired old man\" with \"tired old men\" in his cabinet, pointedly suggesting that the President's lack of vigor had produced a less than vigorous economic recovery. Roosevelt, as most observers could see from his weight loss and haggard appearance, was a tired man in 1944. But upon entering the campaign in earnest in late September 1944, Roosevelt displayed enough passion and fight to allay most concerns and to deflect Republican attacks. With the war still raging, he urged voters not to \"change horses in mid-stream.\" Labor unions, which had grown rapidly in the war, fully supported Roosevelt. Roosevelt and Truman won the 1944 election by a comfortable margin, defeating Dewey and his running mate John W. Bricker with 53.4% of the popular vote and 432 out of the 531 electoral votes. The president campaigned in favor of a strong United Nations, so his victory symbolized support for the nation's future participation in the international community.\n\n\n==== Final months and death ====\n\nWhen Roosevelt returned to the United States from the Yalta Conference, many were shocked to see how old, thin and frail he looked. He spoke while seated in the well of the House, an unprecedented concession to his physical incapacity. During March 1945, he sent strongly worded messages to Stalin accusing him of breaking his Yalta commitments over Poland, Germany, prisoners of war and other issues. When Stalin accused the western Allies of plotting behind his back a separate peace with Hitler, Roosevelt replied: \"I cannot avoid a feeling of bitter resentment towards your informers, whoever they are, for such vile misrepresentations of my actions or those of my trusted subordinates.\" On March 29, 1945, Roosevelt went to the Little White House at Warm Springs, Georgia, to rest before his anticipated appearance at the founding conference of the United Nations.In the afternoon of April 12, 1945, in Warm Springs, Georgia, while sitting for a portrait by Elizabeth Shoumatoff, Roosevelt said: \"I have a terrific headache.\" He then slumped forward in his chair, unconscious, and was carried into his bedroom. The president's attending cardiologist, Howard Bruenn, diagnosed the medical emergency as a massive intracerebral hemorrhage. At 3:35 p.m. that day, Roosevelt died at the age of 63.The following morning, Roosevelt's body was placed in a flag-draped coffin and loaded onto the presidential train for the trip back to Washington. Along the route, thousands flocked to the tracks to pay their respects. After a White House funeral on April 14, Roosevelt was transported by train from Washington, D.C., to his place of birth at Hyde Park. On April 15 he was buried, per his wish, in the rose garden of his Springwood estate.Roosevelt's declining physical health had been kept secret from the public. His death was met with shock and grief across the world. Germany surrendered during the 30-day mourning period, but Harry Truman (who had succeeded Roosevelt as president) ordered flags to remain at half-staff; he also dedicated Victory in Europe Day and its celebrations to Roosevelt's memory. World War II finally ended with the signed surrender of Japan in September.Coincidentally, on April 12, 1945, a devastating tornado outbreak occurred in the United States, which killed 128 people and injured over a thousand others. The tornado outbreak included the fourth deadliest tornado in Oklahoma history, which leveled a third of the town of Antlers. Roosevelt's death overshadowed what would have \"commanded national media attention\" for a while. Tornado expert Thomas P. Grazulis said that, \"even nearby newspapers had more information on the death of the President than on the tornado\".\n\n\n== Civil rights, repatriation, internment, and the Jews ==\n\nRoosevelt was viewed as a hero by many African Americans, Catholics, and Jews, and he was highly successful in attracting large majorities of these voters into his New Deal coalition. From his first term until 1939, the Mexican Repatriation started by President Herbert Hoover continued under Roosevelt, which scholars today contend was a form of ethnic cleansing towards Mexican Americans. Roosevelt ended federal involvement in the deportations. After 1934, the number of deportations fell by approximately 50 percent. However, Roosevelt did not attempt to suppress the deportations on a local or state level. Mexican Americans were the only group explicitly excluded from New Deal benefits. The deprival of due process for Mexican Americans is cited as a precedent for Roosevelt's internment of Japanese Americans in concentration camps during World War II. Roosevelt won strong support from Chinese Americans and Filipino Americans, but not Japanese Americans, as he presided over their internment during the war. African Americans and Native Americans fared well in two New Deal relief programs, the Civilian Conservation Corps and the Indian Reorganization Act, respectively. Sitkoff reports that the WPA \"provided an economic floor for the whole black community in the 1930s, rivaling both agriculture and domestic service as the chief source\" of income.\n\n\n=== Lynching ===\nRoosevelt stopped short of joining NAACP leaders in pushing for federal anti-lynching legislation, as he believed that such legislation was unlikely to pass and that his support for it would alienate Southern congressmen. He did, however, appoint a \"Black Cabinet\" of African American advisers to advise on race relations and African American issues, and he publicly denounced lynching as \"murder\". First Lady Eleanor Roosevelt vocally supported efforts designed to aid the African American community, including the Fair Labor Standards Act, which helped boost wages for nonwhite workers in the South. In 1941, Roosevelt established the Fair Employment Practices Committee (FEPC) to implement Executive Order 8802, which prohibited racial and religious discrimination in employment among defense contractors. The FEPC was the first national program directed against employment discrimination, and it played a major role in opening up new employment opportunities to non-white workers. During World War II, the proportion of African American men employed in manufacturing positions rose significantly. In response to Roosevelt's policies, African Americans increasingly defected from the Republican Party during the 1930s and 1940s, becoming an important Democratic voting bloc in several Northern states.\n\n\n=== Japanese-Americans ===\nThe attack on Pearl Harbor raised concerns in the public regarding the possibility of sabotage by Japanese Americans. This suspicion was fed by long-standing racism against Japanese immigrants, as well as the findings of the Roberts Commission, which concluded that the attack on Pearl Harbor had been assisted by Japanese spies. On February 19, 1942, President Roosevelt signed Executive Order 9066, which relocated 110,000 Japanese-American citizens and immigrants, most of whom lived on the Pacific Coast. They were forced to liquidate their properties and businesses and interned in hastily built camps in interior, harsh locations. Distracted by other issues, Roosevelt had delegated the decision for internment to Secretary of War Stimson, who in turn relied on the judgment of Assistant Secretary of War John J. McCloy. The Supreme Court upheld the constitutionality of the executive order in the 1944 case of Korematsu v. United States. Many German and Italian citizens were also arrested or placed into internment camps.\n\n\n=== Jews ===\nThere is controversy among historians about Roosevelt's attitude to Jews and the Holocaust. Arthur M. Schlesinger Jr. says Roosevelt \"did what he could do\" to help Jews; David Wyman says Roosevelt's record on Jewish refugees and their rescue is \"very poor\" and one of the worst failures of his presidency. In 1923, as a member of the Harvard board of directors, Roosevelt decided there were too many Jewish students at Harvard University and helped institute a quota to limit the number of Jews admitted to Harvard. After Kristallnacht in 1938, Roosevelt had his ambassador to Germany recalled back to Washington. He did not loosen immigration quotas but did allow German Jews already in the U.S. on visas to stay indefinitely. According to Rafael Medoff, the U.S. president could have saved 190,000 Jewish lives by telling his State Department to fill immigration quotas to the legal limit, but his administration discouraged and disqualified Jewish refugees based on its prohibitive requirements that left less than 25% of the quotas filled.Hitler chose to implement the \"Final Solution\"\u2014the extermination of the European Jewish population\u2014by January 1942, and American officials learned of the scale of the Nazi extermination campaign in the following months. Against the objections of the State Department, Roosevelt convinced the other Allied leaders to jointly issue the Joint Declaration by Members of the United Nations, which condemned the ongoing Holocaust and warned to try its perpetrators as war criminals. In 1943, Roosevelt told U.S. government officials that there should be limits on Jews in various professions to \"eliminate the specific and understandable complaints which the Germans bore towards the Jews in Germany.\" The same year, Roosevelt was personally briefed by Polish Home Army intelligence agent Jan Karski who was an eyewitness of the Holocaust; pleading for action, Karski told him that 1.8 million Jews had already been exterminated. Karski recalled that in response, Roosevelt \"did not ask one question about the Jews.\" In January 1944, Roosevelt established the War Refugee Board to aid Jews and other victims of Axis atrocities. Aside from these actions, Roosevelt believed that the best way to help the persecuted populations of Europe was to end the war as quickly as possible. Top military leaders and War Department leaders rejected any campaign to bomb the extermination camps or the rail lines leading to the camps, fearing it would be a diversion from the war effort. According to biographer Jean Edward Smith, there is no evidence that anyone ever proposed such a campaign to Roosevelt.\n\n\n== Legacy ==\n\n\n=== Historical reputation ===\nRoosevelt is widely considered to be one of the most important figures in the history of the United States, as well as one of the most influential figures of the 20th century. Historians and political scientists consistently rank Roosevelt, George Washington, and Abraham Lincoln as the three greatest presidents, although the order varies. Reflecting on Roosevelt's presidency, \"which brought the United States through the Great Depression and World War II to a prosperous future\", biographer Jean Edward Smith said in 2007, \"He lifted himself from a wheelchair to lift the nation from its knees.\"His commitment to the working class and unemployed in need of relief in the nation's longest recession made him a favorite of the blue collar workers, labor unions, and ethnic minorities. The rapid expansion of government programs that occurred during Roosevelt's term redefined the role of the government in the United States, and Roosevelt's advocacy of government social programs was instrumental in redefining liberalism for coming generations. Roosevelt firmly established the United States' leadership role on the world stage, with his role in shaping and financing World War II. His isolationist critics faded away, and even the Republicans joined in his overall policies. He also created a new understanding of the presidency, permanently increasing the power of the president at the expense of Congress.His Second Bill of Rights became, according to historian Joshua Zeitz, \"the basis of the Democratic Party's aspirations for the better part of four decades.\" After his death, his widow, Eleanor, continued to be a forceful presence in U.S. and world politics, serving as delegate to the conference which established the United Nations and championing civil rights and liberalism generally. Some junior New Dealers played leading roles in the presidencies of Truman, John Kennedy, and Lyndon Johnson. Kennedy came from a Roosevelt-hating family. Historian William Leuchtenburg says that before 1960, \"Kennedy showed a conspicuous lack of inclination to identify himself as a New Deal liberal.\" He adds, as president, \"Kennedy never wholly embraced the Roosevelt tradition and at times he deliberately severed himself from it.\" By contrast, young Lyndon Johnson had been an enthusiastic New Dealer and a favorite of Roosevelt. Johnson modelled his presidency on Roosevelt's and relied heavily on New Deal lawyer Abe Fortas, as well as James H. Rowe, Anna M. Rosenberg, Thomas Gardiner Corcoran, and Benjamin V. Cohen.During his presidency, and continuing to a lesser extent afterwards, there has been much criticism of Roosevelt, some of it intense. Critics have questioned not only his policies, positions, and the consolidation of power that occurred due to his responses to the crises of the Depression and World War II but also his breaking with tradition by running for a third term as president. Long after his death, new lines of attack criticized Roosevelt's policies regarding helping the Jews of Europe, incarcerating the Japanese on the West Coast, and opposing anti-lynching legislation.Roosevelt was criticized by conservatives for his economic policies, especially the shift in tone from individualism to collectivism with the expansion of the welfare state and regulation of the economy. Those criticisms continued decades after his death. One factor in the revisiting of these issues in later decades was the election of Ronald Reagan in 1980, who opposed the New Deal.\n\n\n=== Memorials ===\n\nRoosevelt's home in Hyde Park is now a National Historic Site and home to his Presidential library. Washington, D.C., hosts two memorials to the former president. The largest, the 7+1\u20442-acre (3-hectare) Roosevelt Memorial, is located next to the Jefferson Memorial on the Tidal Basin. A more modest memorial, a block of marble in front of the National Archives building suggested by Roosevelt himself, was erected in 1965. Roosevelt's leadership in the March of Dimes is one reason he is commemorated on the American dime. Roosevelt has also appeared on several U.S. Postage stamps. On April 29, 1945, seventeen days after Roosevelt's death, the carrier USS Franklin D. Roosevelt was launched and served from 1945 to 1977. London's Westminster Abbey also has a stone tablet memorial to President Roosevelt that was unveiled by Attlee and Churchill in 1948. Welfare Island was renamed after Roosevelt in September 1973.\n\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== See also ==\nCultural depictions of Franklin D. Roosevelt\nAugust Adolph Gennerich \u2013 his bodyguard\nList of Allied World War II conferences\nList of federal political sex scandals in the United States\nSunshine Special (automobile) \u2013 Roosevelt's limousine\nAir Mail scandal\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== Works cited ==\n\n\n== External links ==\nFranklin D. Roosevelt Presidential Library and Museum\nFranklin Delano Roosevelt Memorial, Washington, DC\nFull text and audio of a number of Roosevelt's speeches \u2013 Miller Center of Public Affairs\nFranklin Delano Roosevelt collected news and commentary at The New York Times\nFranklin D. Roosevelt at IMDb\nFranklin Delano Roosevelt: A Resource Guide from the Library of Congress\nAppearances on C-SPAN\n\"Life Portrait of Franklin D. Roosevelt\", from C-SPAN's American Presidents: Life Portraits, October 11, 1999\nThe Presidents: FDR \u2013 an American Experience documentary\nFranklin Delano Roosevelt: Selections from His Writings\nWorks by Franklin Delano Roosevelt at Project Gutenberg\nWorks by Franklin D. Roosevelt at LibriVox (public domain audiobooks) \nWorks by or about Franklin D. Roosevelt at Internet Archive"}, {"id": 63, "title": "Advanced Encryption Standard", "content": "The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: [\u02c8r\u025binda\u02d0l]), is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.AES is a variant of the Rijndael block cipher developed by two Belgian cryptographers, Joan Daemen and Vincent Rijmen, who submitted a proposal to NIST during the AES selection process. Rijndael is a family of ciphers with different key and block sizes. For AES, NIST selected three members of the Rijndael family, each with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits.\nAES has been adopted by the U.S. government. It supersedes the Data Encryption Standard (DES), which was published in 1977. The algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data.\nIn the United States, AES was announced by the NIST as U.S. FIPS PUB 197 (FIPS 197) on November 26, 2001. This announcement followed a five-year standardization process in which fifteen competing designs were presented and evaluated, before the Rijndael cipher was selected as the most suitable.AES is included in the ISO/IEC 18033-3 standard.  AES became effective as a U.S. federal government standard on May 26, 2002, after approval by U.S. Secretary of Commerce Donald Evans. AES is available in many different encryption packages, and is the first (and only) publicly accessible cipher approved by the U.S. National Security Agency (NSA) for top secret information when used in an NSA approved cryptographic module.\n\n\n== Definitive standards ==\nThe Advanced Encryption Standard (AES) is defined in each of:\n\nFIPS PUB 197: Advanced Encryption Standard (AES)\nISO/IEC 18033-3: Block ciphers\n\n\n== Description of the ciphers ==\nAES is based on a design principle known as a substitution\u2013permutation network, and is efficient in both software and hardware. Unlike its predecessor DES, AES does not use a Feistel network. AES is a variant of Rijndael, with a fixed block size of 128 bits, and a key size of 128, 192, or 256 bits. By contrast, Rijndael per se is specified with block and key sizes that may be any multiple of 32 bits, with a minimum of 128 and a maximum of 256 bits. Most AES calculations are done in a particular finite field.\nAES operates on a 4 \u00d7 4 column-major order array of 16 bytes b0,\u2009b1,\u2009...,\u2009b15 termed the state:\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    b\n                    \n                      0\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      4\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      8\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      12\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      5\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      9\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      13\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      6\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      10\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      14\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      3\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      7\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      11\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      15\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}b_{0}&b_{4}&b_{8}&b_{12}\\\\b_{1}&b_{5}&b_{9}&b_{13}\\\\b_{2}&b_{6}&b_{10}&b_{14}\\\\b_{3}&b_{7}&b_{11}&b_{15}\\end{bmatrix}}}\n  The key size used for an AES cipher specifies the number of transformation rounds that convert the input, called the plaintext, into the final output, called the ciphertext. The number of rounds are as follows:\n\n10 rounds for 128-bit keys.\n12 rounds for 192-bit keys.\n14 rounds for 256-bit keys.Each round consists of several processing steps, including one that depends on the encryption key itself. A set of reverse rounds are applied to transform ciphertext back into the original plaintext using the same encryption key.\n\n\n=== High-level description of the algorithm ===\n KeyExpansion \u2013 round keys are derived from the cipher key using the AES key schedule. AES requires a separate 128-bit round key block for each round plus one more.\nInitial round key addition:\n AddRoundKey \u2013 each byte of the state is combined with a byte of the round key using bitwise xor.\n9, 11 or 13 rounds:\n SubBytes \u2013 a non-linear substitution step where each byte is replaced with another according to a lookup table.\n ShiftRows \u2013 a transposition step where the last three rows of the state are shifted cyclically a certain number of steps.\n MixColumns \u2013 a linear mixing operation which operates on the columns of the state, combining the four bytes in each column.\n AddRoundKey\nFinal round (making 10, 12 or 14 rounds in total):\n SubBytes\n ShiftRows\n AddRoundKey\n\n\n=== The  SubBytes step ===\n\nIn the  SubBytes step, each byte \n  \n    \n      \n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{i,j}}\n   in the state array is replaced with a  SubByte \n  \n    \n      \n        S\n        (\n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle S(a_{i,j})}\n   using an 8-bit substitution box. Note that before round 0, the state array is simply the plaintext/input. This operation provides the non-linearity in the cipher. The S-box used is derived from the multiplicative inverse over GF(28), known to have good non-linearity properties. To avoid attacks based on simple algebraic properties, the S-box is constructed by combining the inverse function with an invertible affine transformation. The S-box is also chosen to avoid any fixed points (and so is a derangement), i.e., \n  \n    \n      \n        S\n        (\n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n        )\n        \u2260\n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle S(a_{i,j})\\neq a_{i,j}}\n  , and also any opposite fixed points, i.e., \n  \n    \n      \n        S\n        (\n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n        )\n        \u2295\n        \n          a\n          \n            i\n            ,\n            j\n          \n        \n        \u2260\n        \n          \n            FF\n          \n          \n            16\n          \n        \n      \n    \n    {\\displaystyle S(a_{i,j})\\oplus a_{i,j}\\neq {\\text{FF}}_{16}}\n  .\nWhile performing the decryption, the  InvSubBytes step (the inverse of  SubBytes) is used, which requires first taking the inverse of the affine transformation and then finding the multiplicative inverse.\n\n\n=== The  ShiftRows step ===\nThe  ShiftRows step operates on the rows of the state; it cyclically shifts the bytes in each row by a certain offset. For AES, the first row is left unchanged. Each byte of the second row is shifted one to the left. Similarly, the third and fourth rows are shifted by offsets of two and three respectively. In this way, each column of the output state of the  ShiftRows step is composed of bytes from each column of the input state. The importance of this step is to avoid the columns being encrypted independently, in which case AES would degenerate into four independent block ciphers.\n\n\n=== The  MixColumns step ===\n\nIn the  MixColumns step, the four bytes of each column of the state are combined using an invertible linear transformation. The  MixColumns function takes four bytes as input and outputs four bytes, where each input byte affects all four output bytes. Together with  ShiftRows,  MixColumns provides diffusion in the cipher.\nDuring this operation, each column is transformed using a fixed matrix (matrix left-multiplied by column gives new value of column in the state):\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    b\n                    \n                      0\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      1\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      2\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      3\n                      ,\n                      j\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  2\n                \n                \n                  3\n                \n                \n                  1\n                \n                \n                  1\n                \n              \n              \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  1\n                \n              \n              \n                \n                  1\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n              \n              \n                \n                  3\n                \n                \n                  1\n                \n                \n                  1\n                \n                \n                  2\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      0\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      2\n                      ,\n                      j\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      3\n                      ,\n                      j\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n        0\n        \u2264\n        j\n        \u2264\n        3\n      \n    \n    {\\displaystyle {\\begin{bmatrix}b_{0,j}\\\\b_{1,j}\\\\b_{2,j}\\\\b_{3,j}\\end{bmatrix}}={\\begin{bmatrix}2&3&1&1\\\\1&2&3&1\\\\1&1&2&3\\\\3&1&1&2\\end{bmatrix}}{\\begin{bmatrix}a_{0,j}\\\\a_{1,j}\\\\a_{2,j}\\\\a_{3,j}\\end{bmatrix}}\\qquad 0\\leq j\\leq 3}\n  Matrix multiplication is composed of multiplication and addition of the entries. Entries are bytes treated as coefficients of polynomial of order \n  \n    \n      \n        \n          x\n          \n            7\n          \n        \n      \n    \n    {\\displaystyle x^{7}}\n  . Addition is simply XOR. Multiplication is modulo irreducible polynomial \n  \n    \n      \n        \n          x\n          \n            8\n          \n        \n        +\n        \n          x\n          \n            4\n          \n        \n        +\n        \n          x\n          \n            3\n          \n        \n        +\n        x\n        +\n        1\n      \n    \n    {\\displaystyle x^{8}+x^{4}+x^{3}+x+1}\n  . If processed bit by bit, then, after shifting, a conditional XOR with 1B16 should be performed if the shifted value is larger than FF16 (overflow must be corrected by subtraction of generating polynomial). These are special cases of the usual multiplication in \n  \n    \n      \n        GF\n        \u2061\n        (\n        \n          2\n          \n            8\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {GF} (2^{8})}\n  .\nIn more general sense, each column is treated as a polynomial over \n  \n    \n      \n        GF\n        \u2061\n        (\n        \n          2\n          \n            8\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {GF} (2^{8})}\n   and is then multiplied modulo \n  \n    \n      \n        \n          \n            01\n          \n          \n            16\n          \n        \n        \u22c5\n        \n          z\n          \n            4\n          \n        \n        +\n        \n          \n            01\n          \n          \n            16\n          \n        \n      \n    \n    {\\displaystyle {01}_{16}\\cdot z^{4}+{01}_{16}}\n   with a fixed polynomial \n  \n    \n      \n        c\n        (\n        z\n        )\n        =\n        \n          \n            03\n          \n          \n            16\n          \n        \n        \u22c5\n        \n          z\n          \n            3\n          \n        \n        +\n        \n          \n            01\n          \n          \n            16\n          \n        \n        \u22c5\n        \n          z\n          \n            2\n          \n        \n        +\n        \n          \n            01\n          \n          \n            16\n          \n        \n        \u22c5\n        z\n        +\n        \n          \n            02\n          \n          \n            16\n          \n        \n      \n    \n    {\\displaystyle c(z)={03}_{16}\\cdot z^{3}+{01}_{16}\\cdot z^{2}+{01}_{16}\\cdot z+{02}_{16}}\n  . The coefficients are displayed in their hexadecimal equivalent of the binary representation of bit polynomials from \n  \n    \n      \n        GF\n        \u2061\n        (\n        2\n        )\n        [\n        x\n        ]\n      \n    \n    {\\displaystyle \\operatorname {GF} (2)[x]}\n  . The  MixColumns step can also be viewed as a multiplication by the shown particular MDS matrix in the finite field \n  \n    \n      \n        GF\n        \u2061\n        (\n        \n          2\n          \n            8\n          \n        \n        )\n      \n    \n    {\\displaystyle \\operatorname {GF} (2^{8})}\n  . This process is described further in the article Rijndael MixColumns.\n\n\n=== The  AddRoundKey ===\nIn the  AddRoundKey step, the subkey is combined with the state. For each round, a subkey is derived from the main key using Rijndael's key schedule; each subkey is the same size as the state. The subkey is added by combining of the state with the corresponding byte of the subkey using bitwise XOR.\n\n\n=== Optimization of the cipher ===\nOn systems with 32-bit or larger words, it is possible to speed up execution of this cipher by combining the  SubBytes and  ShiftRows steps with the  MixColumns step by transforming them into a sequence of table lookups. This requires four 256-entry 32-bit tables (together occupying 4096 bytes).  A round can then be performed with 16 table lookup operations and 12 32-bit exclusive-or operations, followed by four 32-bit exclusive-or operations in the  AddRoundKey step.  Alternatively, the table lookup operation can be performed with a single 256-entry 32-bit table (occupying 1024 bytes) followed by circular rotation operations.\nUsing a byte-oriented approach, it is possible to combine the  SubBytes,  ShiftRows, and  MixColumns steps into a single round operation.\n\n\n== Security ==\nThe National Security Agency (NSA) reviewed all the AES finalists, including Rijndael, and stated that all of them were secure enough for U.S. Government non-classified data. In June 2003, the U.S. Government announced that AES could be used to protect classified information:\n\nThe design and strength of all key lengths of the AES algorithm (i.e., 128, 192 and 256) are sufficient to protect classified information up to the SECRET level. TOP SECRET information will require use of either the 192 or 256 key lengths. The implementation of AES in products intended to protect national security systems and/or information must be reviewed and certified by NSA prior to their acquisition and use.\nAES has 10 rounds for 128-bit keys, 12 rounds for 192-bit keys, and 14 rounds for 256-bit keys.\nBy 2006, the best known attacks were on 7 rounds for 128-bit keys, 8 rounds for 192-bit keys, and 9 rounds for 256-bit keys.\n\n\n=== Known attacks ===\nFor cryptographers, a cryptographic \"break\" is anything faster than a brute-force attack \u2013 i.e., performing one trial decryption for each possible key in sequence. A break can thus include results that are infeasible with current technology. Despite being impractical, theoretical breaks can sometimes provide insight into vulnerability patterns. The largest successful publicly known brute-force attack against a widely implemented block-cipher encryption algorithm was against a 64-bit RC5 key by distributed.net in 2006.The key space increases by a factor of 2 for each additional bit of key length, and if every possible value of the key is equiprobable, this translates into a doubling of the average brute-force key search time. This implies that the effort of a brute-force search increases exponentially with key length. Key length in itself does not imply security against attacks, since there are ciphers with very long keys that have been found to be vulnerable.\nAES has a fairly simple algebraic framework. In 2002, a theoretical attack, named the \"XSL attack\", was announced by Nicolas Courtois and Josef Pieprzyk, purporting to show a weakness in the AES algorithm, partially due to the low complexity of its nonlinear components. Since then, other papers have shown that the attack, as originally presented, is unworkable; see XSL attack on block ciphers.\nDuring the AES selection process, developers of competing algorithms wrote of Rijndael's algorithm \"we are concerned about [its] use ... in security-critical applications.\" In October 2000, however, at the end of the AES selection process, Bruce Schneier, a developer of the competing algorithm Twofish, wrote that while he thought successful academic attacks on Rijndael would be developed someday, he \"did not believe that anyone will ever discover an attack that will allow someone to read Rijndael traffic.\"Until May 2009, the only successful published attacks against the full AES were side-channel attacks on some specific implementations. In 2009, a new related-key attack was discovered that exploits the simplicity of AES's key schedule and has a complexity of 2119. In December 2009 it was improved to 299.5. This is a follow-up to an attack discovered earlier in 2009 by Alex Biryukov, Dmitry Khovratovich, and Ivica Nikoli\u0107, with a complexity of 296 for one out of every 235 keys. However, related-key attacks are not of concern in any properly designed cryptographic protocol, as a properly designed protocol (i.e., implementational software) will take care not to allow related keys, essentially by constraining an attacker's means of selecting keys for relatedness.\nAnother attack was blogged by Bruce Schneier\non July 30, 2009, and released as a preprint\non August 3, 2009. This new attack, by Alex Biryukov, Orr Dunkelman, Nathan Keller, Dmitry Khovratovich, and Adi Shamir, is against AES-256 that uses only two related keys and 239 time to recover the complete 256-bit key of a 9-round version, or 245 time for a 10-round version with a stronger type of related subkey attack, or 270 time for an 11-round version. 256-bit AES uses 14 rounds, so these attacks are not effective against full AES.\nThe practicality of these attacks with stronger related keys has been criticized, for instance, by the paper on chosen-key-relations-in-the-middle attacks on AES-128 authored by Vincent Rijmen in 2010.In November 2009, the first known-key distinguishing attack against a reduced 8-round version of AES-128 was released as a preprint.\nThis known-key distinguishing attack is an improvement of the rebound, or the start-from-the-middle attack, against AES-like permutations, which view two consecutive rounds of permutation as the application of a so-called Super-S-box. It works on the 8-round version of AES-128, with a time complexity of 248, and a memory complexity of 232.  128-bit AES uses 10 rounds, so this attack is not effective against full AES-128.\nThe first key-recovery attacks on full AES were by Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger, and were published in 2011. The attack is a biclique attack and is faster than brute force by a factor of about four. It requires 2126.2 operations to recover an AES-128 key. For AES-192 and AES-256, 2190.2 and 2254.6 operations are needed, respectively. This result has been further improved to 2126.0 for AES-128, 2189.9 for AES-192 and 2254.3 for AES-256, which are the current best results in key recovery attack against AES.\nThis is a very small gain, as a 126-bit key (instead of 128-bits) would still take billions of years to brute force on current and foreseeable hardware. Also, the authors calculate the best attack using their technique on AES with a 128-bit key requires storing 288 bits of data. That works out to about 38 trillion terabytes of data, which is more than all the data stored on all the computers on the planet in 2016. As such, there are no practical implications on AES security. The space complexity has later been improved to 256 bits, which is 9007 terabytes (while still keeping a time complexity of 2126.2).\nAccording to the Snowden documents, the NSA is doing research on whether a cryptographic attack based on tau statistic may help to break AES.At present, there is no known practical attack that would allow someone without knowledge of the key to read data encrypted by AES when correctly implemented.\n\n\n=== Side-channel attacks ===\nSide-channel attacks do not attack the cipher as a black box, and thus are not related to cipher security as defined in the classical context, but are important in practice. They attack implementations of the cipher on hardware or software systems that inadvertently leak data. There are several such known attacks on various implementations of AES.\nIn April 2005, D. J. Bernstein announced a cache-timing attack that he used to break a custom server that used OpenSSL's AES encryption. The attack required over 200 million chosen plaintexts. The custom server was designed to give out as much timing information as possible (the server reports back the number of machine cycles taken by the encryption operation). However, as Bernstein pointed out, \"reducing the precision of the server's timestamps, or eliminating them from the server's responses, does not stop the attack: the client simply uses round-trip timings based on its local clock, and compensates for the increased noise by averaging over a larger number of samples.\"In October 2005, Dag Arne Osvik, Adi Shamir and Eran Tromer presented a paper demonstrating several cache-timing attacks against the implementations in AES found in OpenSSL and Linux's dm-crypt partition encryption function. One attack was able to obtain an entire AES key after only 800 operations triggering encryptions, in a total of 65 milliseconds. This attack requires the attacker to be able to run programs on the same system or platform that is performing AES.\nIn December 2009 an attack on some hardware implementations was published that used differential fault analysis and allows recovery of a key with a complexity of 232.In November 2010 Endre Bangerter, David Gullasch and Stephan Krenn published a paper which described a practical approach to a \"near real time\" recovery of secret keys from AES-128 without the need for either cipher text or plaintext. The approach also works on AES-128 implementations that use compression tables, such as OpenSSL. Like some earlier attacks, this one requires the ability to run unprivileged code on the system performing the AES encryption, which may be achieved by malware infection far more easily than commandeering the root account.In March 2016, Ashokkumar C., Ravi Prakash Giri and Bernard Menezes presented a side-channel attack on AES implementations that can recover the complete 128-bit AES key in just 6\u20137 blocks of plaintext/ciphertext, which is a substantial improvement over previous works that require between 100 and a million encryptions. The proposed attack requires standard user privilege and key-retrieval algorithms run under a minute.\nMany modern CPUs have built-in hardware instructions for AES, which protect against timing-related side-channel attacks.\n\n\n=== Quantum attacks ===\nAES-256 is considered to be quantum resistant, as it has similar quantum resistance to AES-128's resistance against traditional, non-quantum, attacks. AES-192 and AES-128 are not considered quantum resistant due to their smaller key sizes. AES-192 has a strength of 96-bits against quantum attacks and AES-128 has 64-bits of strength against quantum attacks, making them both insecure.\n\n\n== NIST/CSEC validation ==\nThe Cryptographic Module Validation Program (CMVP) is operated jointly by the United States Government's National Institute of Standards and Technology (NIST) Computer Security Division and the Communications Security Establishment (CSE) of the Government of Canada. The use of cryptographic modules validated to NIST FIPS 140-2 is required by the United States Government for encryption of all data that has a classification of Sensitive but Unclassified (SBU) or above. From NSTISSP #11, National Policy Governing the Acquisition of Information Assurance: \u201cEncryption products for protecting classified information will be certified by NSA, and encryption products intended for protecting sensitive information will be certified in accordance with NIST FIPS 140-2.\u201dThe Government of Canada also recommends the use of FIPS 140 validated cryptographic modules in unclassified applications of its departments.\nAlthough NIST publication 197 (\u201cFIPS 197\u201d) is the unique document that covers the AES algorithm, vendors typically approach the CMVP under FIPS 140 and ask to have several algorithms (such as Triple DES or SHA1) validated at the same time. Therefore, it is rare to find cryptographic modules that are uniquely FIPS 197 validated and NIST itself does not generally take the time to list FIPS 197 validated modules separately on its public web site. Instead, FIPS 197 validation is typically just listed as an \"FIPS approved: AES\" notation (with a specific FIPS 197 certificate number) in the current list of FIPS 140 validated cryptographic modules.\nThe Cryptographic Algorithm Validation Program (CAVP) allows for independent validation of the correct implementation of the AES algorithm. Successful validation results in being listed on the NIST validations page. This testing is a pre-requisite for the FIPS 140-2 module validation. However, successful CAVP validation in no way implies that the cryptographic module implementing the algorithm is secure. A cryptographic module lacking FIPS 140-2 validation or specific approval by the NSA is not deemed secure by the US Government and cannot be used to protect government data.FIPS 140-2 validation is challenging to achieve both technically and fiscally. There is a standardized battery of tests as well as an element of source code review that must be passed over a period of a few weeks. The cost to perform these tests through an approved laboratory can be significant (e.g., well over $30,000 US) and does not include the time it takes to write, test, document and prepare a module for validation. After validation, modules must be re-submitted and re-evaluated if they are changed in any way. This can vary from simple paperwork updates if the security functionality did not change to a more substantial set of re-testing if the security functionality was impacted by the change.\n\n\n== Test vectors ==\nTest vectors are a set of known ciphers for a given input and key. NIST distributes the reference of AES test vectors as AES Known Answer Test (KAT) Vectors.\n\n\n== Performance ==\nHigh speed and low RAM requirements were some of the criteria of the AES selection process. As the chosen algorithm, AES performed well on a wide variety of hardware, from 8-bit smart cards to high-performance computers.\nOn a Pentium Pro, AES encryption requires 18 clock cycles per byte, equivalent to a throughput of about 11 MiB/s for a 200 MHz processor.\nOn Intel Core and AMD Ryzen CPUs supporting AES-NI instruction set extensions, throughput can be multiple GiB/s (even over 15 GiB/s on an i7-12700k).\n\n\n== Implementations ==\n\n\n== See also ==\nAES modes of operation\nDisk encryption\nEncryption\nWhirlpool \u2013 hash function created by Vincent Rijmen and Paulo S. L. M. Barreto\nList of free and open-source software packages\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\"256bit key \u2013 128bit block \u2013 AES\". Cryptography \u2013 256 bit Ciphers: Reference source code and submissions to international cryptographic designs contests. EmbeddedSW.\n\"Advanced Encryption Standard (AES)\" (PDF). Federal Information Processing Standards. 26 November 2001. doi:10.6028/NIST.FIPS.197. 197.\nAES algorithm archive information \u2013 (old, unmaintained)\n\"Part 3: Block ciphers\" (PDF). Information technology \u2013 Security techniques \u2013 Encryption algorithms (2nd ed.). ISO. 2010-12-15. ISO/IEC 18033-3:2010(E). Archived (PDF) from the original on 2022-10-09.\nAnimation of Rijndael \u2013 AES deeply explained and animated using Flash (by Enrique Zabala / University ORT / Montevideo / Uruguay). This animation (in English, Spanish, and German) is also part of CrypTool 1 (menu Indiv. Procedures \u2192 Visualization of Algorithms \u2192 AES).\nHTML5 Animation of Rijndael \u2013 Same Animation as above made in HTML5."}, {"id": 64, "title": "Cars (film)", "content": "Cars is a 2006 American animated sports comedy film produced by Pixar Animation Studios for Walt Disney Pictures. The film was directed by John Lasseter, co-directed by Joe Ranft, produced by Darla K. Anderson, and written by Dan Fogelman, Lasseter, Ranft, Kiel Murray, Phil Lorin, and Jorgen Klubien, and was the final film independently produced by Pixar after its purchase by Disney in January 2006. The film features an ensemble voice cast of Owen Wilson, Paul Newman (in his final voice acting theatrical film role), Bonnie Hunt, Larry the Cable Guy, Tony Shalhoub, Cheech Marin, Michael Wallis, George Carlin, Paul Dooley, Jenifer Lewis, Guido Quaroni, Michael Keaton, Katherine Helmond, John Ratzenberger and Richard Petty, while race car drivers Dale Earnhardt Jr. (as \"Junior\"), Mario Andretti, Michael Schumacher and car enthusiast Jay Leno (as \"Jay Limo\") voice themselves.\nCars is set in a world populated entirely by anthropomorphic vehicles. The film follows a selfish and arrogant young racecar named Lightning McQueen who, on the way to the biggest race of his life, becomes stranded in a forgotten town called Radiator Springs, where he learns to be humbler and more respectful towards others.\nDevelopment for Cars started in 1998, after finishing the production of A Bug's Life, with a new script titled The Yellow Car, which was about an electric car living in a gas-guzzling world with Klubien writing. It was announced that the producers agreed that it could be the next Pixar film after A Bug's Life, scheduled for a 1999 release, particularly around June 4; the idea was later scrapped in favor of Toy Story 2. Shortly after, production was resumed with major script changes. The film was inspired by Lasseter's experiences on a cross-country road trip. Randy Newman composed the film's score, while artists such as Sheryl Crow, Rascal Flatts, John Mayer and Brad Paisley contributed to the film's soundtrack.\nCars premiered on May 26, 2006, at Lowe's Motor Speedway in Concord, North Carolina and was theatrically released in the United States on June 9, to generally positive reviews and commercial success, grossing $462 million worldwide against a budget of $120 million, becoming the sixth-highest-grossing film of 2006. The film received two nominations at the 79th Academy Awards, including Best Animated Feature, but lost to Happy Feet (but won both the Annie Award for Best Animated Feature and the Golden Globe Award for Best Animated Feature Film). The film was released on DVD on November 7, 2006, on VHS in limited quantities on February 19, 2007, and on Blu-ray on November 6, 2007. The film was accompanied by the short One Man Band for its theatrical and home media releases. The film was dedicated to Joe Ranft, who died in a car crash during the film's production.\nThe success of Cars launched a multimedia franchise and a series of two sequels produced by Pixar and two spin-offs produced by Disneytoon Studios, starting with Cars 2 (2011).\n\n\n== Plot ==\nIn a world populated by anthropomorphic vehicles, the Dinoco 400 race marks the climax of the Piston Cup season. The event intensifies a rivalry between the retiring seven-time champion, Strip \"The King\" Weathers, the cunning Chick Hicks, and the talented but arrogant rookie, Lightning McQueen. Desperate to win and gain entry into the prestigious Dinoco team, Lightning struggles with teamwork due to his self-centered attitude. During the high-stakes race, Lightning avoids a major collision instigated by Chick but loses the lead by refusing to take a pit stop, causing his rear tires to blow out before he can win. The race ends in a three-way tie, setting the stage for a decisive race at the Los Angeles International Speedway in one week.\nAfter the race, Lightning rushes through the night on the interstate to reach California with his transporter, Mack. A mishap leaves Lightning stranded alone in the rundown desert town of Radiator Springs, where he inadvertently damages the main road. As a result, Lightning receives an unexpected community service assignment: repaving the road under the supervision of the town's judge, a Hudson Hornet named Doc Hudson, who is prejudiced against Lightning for being a race car.\nLightning repaves the road shoddily in a rush to leave, and Doc challenges him to a race for his freedom on the condition that he starts over and repaves the road correctly if he loses. The overconfident Lightning, having never raced on a dirt track before, spins out on an unbanked turn and crashes. Doc handles the track with no problems and wins the race. Over time, Lightning begins to warm up to the town and befriends its residents, especially Mater, a rusty tow truck, and Sally, a Porsche 911 who dreams of reviving Radiator Springs. As he bonds with the locals, Lightning helps rejuvenate Radiator Springs and develops a newfound appreciation for its charm. He discovers the town was once a bustling attraction for drivers on Route 66 before the construction of Interstate 40 caused them to lose all their business traffic. Lightning also discovers that the bitter Doc, reticent about his past, used to race as the legendary Fabulous Hudson Hornet until a disastrous crash ended his career. Lightning is dumbfounded that Doc considers his previous Piston Cups as being worthless.\nLightning finishes repaving the road and decides to stay in Radiator Springs with his new friends, having lost interest in the race. Still, Doc alerts the media of Lightning's location, leading them and Mack to descend on the town and force Lightning to leave. Doc immediately regrets his actions after seeing the residents turn against him for making Lightning leave. At the race, Lightning initially struggles but is buoyed by the sudden arrival of his friends from Radiator Springs, who come to his aid in the pit. With Doc now acting as his crew chief, Lightning stages a remarkable comeback and takes the lead. On the final lap, as Lightning closes in towards the Finish Line, Chick employs a PIT maneuver that intentionally wrecks The King, rendering him unable to continue. Fearing that The King's career may end as Doc's did, Lightning halts just before the finish line and pushes The King across, allowing Chick to win the Piston Cup while ensuring The King's safe finish.\nThe crowd and media furiously condemn Chick's Piston Cup victory while praising Lightning's integrity and sportsmanship. Although offered a Dinoco sponsorship, Lightning politely declines and chooses to be with his current sponsor, Rust-eze Bumper Ointment, out of newfound respect and loyalty for them. Returning to Radiator Springs, he reunites with Sally, Mater, and the other cars and declares his intention to establish his racing headquarters there and revitalize the town, while also training under Doc's mentorship.\n\n\n== Voice cast ==\n\nOwen Wilson as Lightning McQueen, a red, custom built 2006 racecar who is described by John Lasseter in the Los Angeles Times as \"a hybrid between a stock car and a more curvaceous Le Mans endurance race car\"\nPaul Newman as Doc Hudson, a navy-blue 1951 Hudson Hornet who is later revealed to be the Fabulous Hudson Hornet\nBonnie Hunt as Sally Carrera, a sky-blue 2002 996-series Porsche 911 Carrera\nLarry the Cable Guy as Mater, a rusty blue tow truck inspired by a 1951 International Harvester L-170 \"boom\" truck and a mid-1950s Chevrolet Task Force\nTony Shalhoub as Luigi, a yellow 1959 Fiat 500\nCheech Marin as Ramone, a custom 1959 Chevrolet Impala Lowrider who has different colors in each sequence of the film\nMichael Wallis as Sheriff, a 1949 Mercury Eight Coupe (police package)\nGeorge Carlin as Fillmore, an aquamarine 1960 Volkswagen Type 2\nPaul Dooley as Sarge, a 1941 Willys jeep in the style of the US Military's usage\nJenifer Lewis as Flo, an aquamarine 1957 General Motors Motorama show car\nGuido Quaroni as Guido, a custom blue forklift, who resembles a BMW Isetta at the front and only speaks Italian\nRichard Petty as Strip \"The King\" Weathers, a blue 1970 Plymouth Superbird stock car\nMichael Keaton as Chick Hicks, a green race car described by Pixar as a generic 1980s stock car who is Lightning McQueen's rival\nKatherine Helmond as Lizzie, a black 1923 Ford Model T\nJohn Ratzenberger as Mack, a custom red 1985 Mack Super-Liner\nJoe Ranft as Red, a 1960s-style, red and silver fire engine (the design is most closely resembled to be a mid-1960s truck), and Jerry Recycled Batteries, a grumpy red cab over Peterbilt who Lightning mistakes for Mack while he is lost. These were Ranft's last two voice roles before his death in August 2005.\nJeremy Piven (US) / Jeremy Clarkson (UK) as Harv, Lightning McQueen's agent who is never seen on-screen\nBob Costas as Bob Cutlass, a grey 1999 Oldsmobile Aurora and announcer for the Piston Cup races\nDarrell Waltrip as Darrell Cartrip, a grey, red, yellow, and blue 1977 Chevrolet Monte Carlo and Piston Cup racing announcer\nHumpy Wheeler as Tex Dinoco, a gold 1975 Cadillac Coupe de Ville and owner of Dinoco\nLynda Petty as Lynda Weathers, a Chrysler Town and Country station wagon and Strip Weathers' wife\nDale Earnhardt Jr. as \"Junior\"#8, a generic stock car\nMichael Schumacher as Michael Schumacher Ferrari, a red Ferrari F430\nTom and Ray Magliozzi as Rusty and Dusty Rust-eze, a 1963 Dodge Dart and a 1967 Dodge A100 who are the owners of Rust-eze\nRichard Kind and Edie McClurg as Van and Minny, a forest green 2003 Ford Windstar and a violet 1996 Chrysler Town and Country\nLindsey Collins and Elissa Knight as Mia and Tia, the red identical twin 1992 Mazda MX-5 (NA) sisters\nMario Andretti as Mario Andretti#11, a 1967 Ford Fairlane\nSarah Clark as Kori Turbowitz, a turquoise generic sports car and race announcer\nJay Leno as Jay Limo, a blue Lincoln Town Car who appears in a cameo\nJonas Rivera as Boost, a violet Nissan Silvia who is the leader of the Tuner Gang\nE.J. Holowicki as DJ, a blue Scion XB and member of the Tuner Gang\nAdrian Ochoa as Wingo, a green and purple Mitsubishi Eclipse and member of the Tuner Gang.\nLou Romano as Snot Rod, an orange Plymouth Barracuda and member of the Tuner Gang who sneezes often\nJess Harnell (uncredited) as Sven the Governator, a light yellow Humvee caricature whose voice resembles that of Arnold Schwarzenegger\nMike \"No Name\" Nelson as Not Chuck, a red forklift of Lightning McQueen's former racing teamTom Hanks, Tim Allen, Billy Crystal, John Goodman, Dave Foley and John Ratzenberger reprise their vocal roles from previous Pixar films during an end-credits sequence featuring automobile spoofs of Toy Story, Monsters, Inc., and A Bug's Life. Cars was the final Pixar film worked on by Joe Ranft who died in a crash a year before the film's release, aged 45. The film was the second to be dedicated to his memory, after Corpse Bride. The memorial showed the roles he had done in the other films directed by John Lasseter during the credits. This is also the last (non-documentary) movie for Paul Newman before his retirement in 2007 and his death in 2008. It turned out to be the highest-grossing film of his career.\n\n\n== Production ==\n\n\n=== Development ===\nThe development of Cars began in 1998, when Pixar finished production of A Bug's Life. At that time, Jorgen Klubien began writing a new script called The Yellow Car, which was about an electric car living in a gas-guzzling world inspired by The Ugly Duckling, an idea triggered by the poor reception his fellow countrymen gave the Mini-El car. Some of the original drawings and characters were developed in 1998 and the producers agreed that Cars could be the next Pixar film after A Bug's Life and be released in early 1999, particularly around June 4. However, the idea was scrapped in favor of Toy Story 2. Later, production resumed with major script changes, like giving Mater, Doc and a few other characters bigger parts.John Lasseter said that inspiration for the film's story came after he took a cross-country road trip with his wife and five sons in 2000. When he returned to the studio after vacation, he contacted Michael Wallis, a Route 66 historian. Wallis then led eleven Pixar animators in rented white Cadillacs on two different road trips across the route to research the film. In 2001, the film's working title was Route 66 (after U.S. Route 66), but the title was changed to Cars in order to avoid confusion with the 1960s television series of the same name. In addition, Lightning McQueen's racing number was originally going to be 57 (a reference to 1957, Lasseter's birth year), but was changed to 95 (a reference to 1995, the year Pixar's first feature film Toy Story was released).In 2006, Lasseter spoke about the inspiration for the film, saying: \"I have always loved cars. In one vein, I have Disney blood, and in the other, there's motor oil. The notion of combining these two great passions in my life\u2014cars and animation\u2014was irresistible. When Joe (Ranft) and I first started talking about this film in 1998, we knew we wanted to do something with cars as characters. Around that same time, we watched a documentary called 'Divided Highways,' which dealt with the interstate highway and how it affected the small towns along the way. We were so moved by it and began thinking about what it must have been like in these small towns that got bypassed. That's when we started really researching Route 66, but we still hadn't quite figured out what the story for the film was going to be. I used to travel that highway with my family as a child when we visited our family in St. Louis.\"Years later in 2013, Klubien said the film was both his best and most bitter experience because he was fired before it premiered and because he feels Lasseter wrote him out of the story of how the film got made.\n\n\n=== Animation ===\nFor the cars themselves, Lasseter also visited the design studios of the Big Three Detroit automakers, particularly J Mays of Ford Motor Company. Lasseter learned how real cars were designed.In 2006, Lasseter spoke about how they worked hard to make the animation believable, saying: \"It took many months of trial and error, and practicing test animation, to figure out how each car moves and how their world works. Our supervising animators, Doug Sweetland and Scott Clark, and the directing animators, Bobby Podesta and James Ford Murphy, did an amazing job working with the animation team to determine the unique movements for each character based on its age and the type of car it was. Some cars are like sports cars and they're much tighter in their suspension. Others are older '50s cars that are a lot looser and have more bounce to them. We wanted to get that authenticity in there but also to make sure each car had a unique personality. We also wanted each animator to be able to put some of themself in the character and give it their own spin. Every day in dailies, it was so much fun because we would see things that we had never seen in our lives. The world of cars came alive in a believable and unexpected way.\"Unlike most anthropomorphic cars, the eyes of the cars in this film were placed on the windshield (which resembles the Tonka Talking Trucks, the characters from Tex Avery's One Cab's Family short and Disney's own Susie the Little Blue Coupe), rather than within the headlights. According to production designer Bob Pauley, \"From the very beginning of this project, John Lasseter had it in his mind to have the eyes be in the windshield. For one thing, it separates our characters from the more common approach where you have little cartoon eyes in the headlights. For another, he thought that having the eyes down near the mouth at the front end of the car feels more like a snake. With the eyes set in the windshield, the point of view is more human-like, and made it feel like the whole car could be involved in the animation of the character.\" This decision was facetiously criticized by automotive blog Jalopnik.In 2006, the supervising animator of the film, Scott Clark, spoke about the challenges of animating car characters, saying: \"Getting a full range of performance and emotion from these characters and making them still seem like cars was a tough assignment, but that's what animation does best. You use your imagination, and you make the movements and gestures fit with the design. Our car characters may not have arms and legs, but we can lean the tires in or out to suggest hands opening up or closing in. We can use steering to point a certain direction. We also designed a special eyelid and an eyebrow for the windshield that lets us communicate an expressiveness that cars don't have.\" Doug Sweetland, who also served as supervising animator, also spoke about the challenges, saying: \"It took a different kind of animator to really be able to interpret the Cars models, than it did to interpret something like The Incredibles models. With The Incredibles, the animator could get reference for the characters by shooting himself and watching the footage. But with Cars, it departs completely from any reference. Yes they're cars, but no car can do what our characters do. It's pure fantasy. It took a lot of trial and error to get them to look right.\"Lasseter also explained that the film started with pencil and paper designs, saying: \"Truth to materials. Starting with pencil-and-paper designs from production designer Bob Pauley, and continuing through the modeling, articulation, and shading of the characters, and finally into animation, the production team worked hard to have the car characters remain true to their origins.\" Character department manager Jay Ward also explained how they wanted the cars to look as realistic as possible, saying: \"John didn't want the cars to seem clay-like or mushy. He insisted on truth to materials. This was a huge thing for him. He told us that steel needs to feel like steel. Glass should feel like glass. These cars need to feel heavy. They weigh three or four thousand pounds. When they move around, they need to have that feel. They shouldn't appear light or overly bouncy to the point where the audience might see them as rubber toys.\" According to directing animator James Ford Murphy, \"Originally, the car models were built so they could basically do anything. John kept reminding us that these characters are made of metal and they weigh several thousand pounds. They can't stretch. He showed us examples of very loose animation to illustrate what not to do.\"Character shading supervisor on the film Thomas Jordan explained that chrome and car paint were the main challenges on the film, saying: \"Chrome and car paint were our two main challenges on this film. We started out by learning as much as we could. At the local body shop, we watched them paint a car, and we saw the way they mixed the paint and applied the various coats. We tried to dissect what goes into the real paint and recreated it in the computer. We figured out that we needed a base paint, which is where the color comes from, and the clearcoat, which provides the reflection. We were then able to add in things like metallic flake to give it a glittery sparkle, a pearlescent quality that might change color depending on the angle, and even a layer of pin-striping for characters like Ramone.\" Supervising technical director on the film Eben Ostby explained that the biggest challenge for the technical team was creating the metallic and painted surfaces of the car characters, and the reflections that those surfaces generate, saying: \"Given that the stars of our film are made of metal, John had a real desire to see realistic reflections, and more beautiful lighting than we've seen in any of our previous films. In the past, we've mostly used environment maps and other matte-based technology to cheat reflections, but for Cars we added a ray-tracing capability to our existing Renderman program to raise the bar for Pixar.\"Rendering lead Jessica McMackin spoke about the use of ray tracing on the film, saying: \"In addition to creating accurate reflections, we used ray tracing to achieve other effects. We were able to use this approach to create accurate shadows, like when there are multiple light sources and you want to get a feathering of shadows at the edges. Or occlusion, which is the absence of ambient light between two surfaces, like a crease in a shirt. A fourth use is irradiance. An example of this would be if you had a piece of red paper and held it up to a white wall, the light would be colored by the paper and cast a red glow on the wall.\" Character supervisor Tim Milliron explained that the film uses a ground\u2013locking system that kept the cars firmly planted on the road, saying: \"The ground-locking system is one of the things I'm most proud of on this film. In the past, characters have never known about their environment in any way. A simulation pass was required if you wanted to make something like that happen. On Cars, this system is built into the models themselves, and as you move the car around, the vehicle sticks to the ground. It was one of those things that we do at Pixar where we knew going in that it had to be done, but we had no idea how to do it.\"Technical director Lisa Forsell explained that to enhance the richness and beauty of the desert landscapes surrounding Radiator Springs, the filmmakers created a department responsible for matte paintings and sky flats, saying: \"Digital matte paintings are a way to get a lot of visual complexity without necessarily having to build complex geometry, and write complex shaders. We spent a lot time working on the clouds and their different formations. They tend to be on several layers and they move relative to each other. The clouds do in fact have some character and personality. The notion was that just as people see themselves in the clouds, cars see various car-shaped clouds. It's subtle, but there are definitely some that are shaped like a sedan. And if you look closely, you'll see some that look like tire treads. The fact that so much attention is put on the skies speaks to the visual level of the film. Is there a story point? Not really. There is no pixel on the screen that does not have an extraordinary level of scrutiny and care applied to it. There is nothing that is just throw-away.\"Computers used in the development of the film were four times faster than those used in The Incredibles and 1,000 times faster than those used in Toy Story. To build the cars, the animators used computer platforms similar to those used in the design of real-world automobiles.\n\n\n== Soundtrack ==\n\nThe Cars soundtrack was released by Walt Disney Records on June 6, 2006. Nine tracks on the soundtrack are by popular artists, while the remaining eleven are score cues by Randy Newman. It has two versions of the classic Bobby Troup jazz standard \"Route 66\" (popularized by Nat King Cole), one by Chuck Berry and a new version recorded specifically for the film's credits performed by John Mayer. Brad Paisley contributed two of the nine tracks to the album, one being \"Find Yourself\" used for the end credits.\n\n\n== Release ==\nCars was originally going to be released on November 4, 2005, but on December 7, 2004, its release date was moved to June 9, 2006. Analysts looked at the release date change as a sign from Pixar that they were preparing for the pending end of the Disney distribution contract by either preparing non-Disney materials to present to other studios or they were buying time to see what happened with Michael Eisner's situation at Disney. When Pixar's chief executive Steve Jobs made the release date announcement, he stated that the reasoning was due to wanting to put all Pixar films on a summer release schedule with DVD sales occurring during the holiday shopping season.\n\n\n=== Home media ===\nCars was released on DVD, in wide- and full-screen editions, on November 7, 2006, in the United States and Canada. This DVD was also released on October 25, 2006, in Australia and New Zealand and on November 27, 2006, in the United Kingdom. The release includes the DVD-exclusive short film Mater and the Ghostlight and the film's theatrical short One Man Band as well as a 16-minute-long documentary about the film entitled Inspiration for Cars, which features director John Lasseter. This THX certified release also features an Easter egg in the main menu, which is a 45-second clip showing a Cars version of Boundin'. A VHS was released on February 19, 2007, to members of Disney's home video club.According to the Walt Disney Company, five million copies of the DVD were sold the first two days it was available. The first week, it sold 6,250,856 units and 15,370,791 in total ($246,198,859). Unlike previous Pixar DVD releases, there is no two-disc special edition, and no plans to release one in the future. According to Sara Maher, DVD Production Manager at Pixar, John Lasseter and Pixar were preoccupied with productions like Ratatouille.In the US and Canada, there were bonus discs available with the purchase of the film at Wal-Mart and at Target. The former featured a Geared-Up Bonus DVD Disc that focused on the music of the film, including the music video to \"Life Is A Highway\", The Making of \"Life Is A Highway\", Cars: The Making of the Music, and Under The Hood, a special that originally aired on the ABC Family cable channel. The latter's bonus was a Rev'd Up DVD Disc that featured material mostly already released as part of the official Cars podcast and focused on the inspiration and production of the movie.Cars was also released on Blu-ray Disc on November 6, 2007, one year after the DVD release. It was the first Pixar film to be released on Blu-ray (alongside Ratatouille and Pixar Short Films Collection, Volume 1), and was re-released as a Blu-ray Disc and DVD combo pack and DVD only edition in April 2011. The film was released for the first time in 3D on October 29, 2013, as part of Cars: Ultimate Collector's Edition, which included the releases on Blu-ray, Blu-ray 3D, and DVD. In 2019, Cars was released on 4K Ultra HD Blu-ray.\n\n\n== Reception ==\n\n\n=== Box office ===\nIn its opening weekend, Cars earned $60 million in 3,985 theaters in the United States, ranking number one at the box office. For three years, it would hold the record for having the highest opening weekend for any car-oriented film until it was surpassed by Fast & Furious in 2009. In the United States, the film held onto the number one spot for two weeks before being surpassed by Click and then by Superman Returns the following weekend. The film then earned $33.7 million during its second weekend while competing against The Fast and the Furious: Tokyo Drift and Nacho Libre. Later on, Cars would team up with another Disney film, Pirates of the Caribbean: Dead Man's Chest, which was released a month later. Around this time, it had approached the $200 million mark, becoming the third film of the year to do so, following X-Men: The Last Stand and The Da Vinci Code. It went on to gross $462 million worldwide and $244 million in the United States. Cars was the second-highest-grossing animated film of 2006, behind Ice Age: The Meltdown.\n\n\n=== Critical response ===\nOn Rotten Tomatoes, the film has an approval rating of 75% based on 204 reviews and an average rating of 6.9/10. The website's critics consensus reads, \"Cars offers visual treats that more than compensate for its somewhat thinly written story, adding up to a satisfying diversion for younger viewers.\" On Metacritic, the film has a score of 73 out of 100 based on 39 critics reviews, indicating \"generally favorable reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A\" on an A+ to F scale.William Arnold of the Seattle Post-Intelligencer praised it as \"one of Pixar's most imaginative and thoroughly appealing movies ever,\" and Lisa Schwarzbaum of Entertainment Weekly called it \"a work of American art as classic as it is modern.\" Roger Ebert of the Chicago Sun-Times gave the film three out of four stars, saying that it \"tells a bright and cheery story, and then has a little something profound lurking around the edges. In this case, it's a sense of loss.\" Peter Travers of Rolling Stone gave the film three and a half stars out of four, saying, \"Fueled with plenty of humor, action, heartfelt drama, and amazing new technical feats, Cars is a high octane delight for moviegoers of all ages.\" Richard Corliss of Time gave the film a positive review, saying, \"Existing both in turbo-charged today and the gentler '50s, straddling the realms of Pixar styling and old Disney heart, this new-model Cars is an instant classic.\" Brian Lowry of Variety gave the film a negative review, saying, \"Despite representing another impressive technical achievement, it's the least visually interesting of the computer-animation boutique's movies, and -- in an ironic twist for a story about auto racing -- drifts slowly through its semi-arid midsection.\" Robert Wilonsky of The Village Voice gave the film a positive review, saying, \"What ultimately redeems Cars from turning out a total lemon is its soul. Lasseter loves these animated inanimate objects as though they were kin, and it shows in every beautifully rendered frame.\" Ella Taylor of L.A. Weekly gave the film a positive review, saying, \"Cars cheerfully hitches cutting-edge animation to a folksy narrative plugging friendship, community and a Luddite mistrust of high tech.\"Gene Seymour of Newsday gave the film three out of four stars, saying, \"And as pop flies go, Cars is pretty to watch, even as it loops, drifts and, at times, looks as if it's just hanging in midair.\" Colin Covert of the Star Tribune gave the film a positive review, saying, \"It takes everything that's made Pixar shorthand for animation excellence -- strong characters, tight pacing, spot-on voice casting, a warm sense of humor and visuals that are pure, pixilated bliss -- and carries them to the next stage.\" Kenneth Turan of the Los Angeles Times gave the film four out of five stars, saying, \"What's surprising about this supremely engaging film is the source of its curb appeal: It has heart.\" Stephen Hunter of The Washington Post gave the film a positive review, saying, \"It's the latest concoction from the geniuses at Pixar, probably the most inventive of the Computer Generated Imagery shop -- and the film's great fun, if well under the level of the first Toy Story.\" Jessica Reaves of the Chicago Tribune gave the film two and a half stars out of four, saying, \"While it's a technically perfect movie, its tone is too manic, its characters too jaded and, in the end, its story too empty to stand up to expectations.\" James Berardinelli of ReelViews gave the film three out of four stars, saying, \"While Cars may cross the finish line ahead of any of 2006's other animated films, it's several laps behind its Pixar siblings.\" Lisa Kennedy of The Denver Post gave the film three out of four stars, saying, \"Cars idles at times. And it's not until its final laps that the movie gains the emotional traction we've come to expect from the Toy Story and Nemo crews.\" Amy Biancolli of the Houston Chronicle gave the film three out of four stars, saying, \"It thunders ahead with breezy abandon, scoring big grins on its way.\"Claudia Puig of USA Today gave the film a positive review, saying, \"The animation is stunningly rendered. But the story is always the critical element in Pixar movies, and Cars' story is heartfelt with a clear and unabashed moral.\" David Edelstein of New York Magazine gave the film a positive review, saying, \"Like the Toy Story films, Cars is a state-of-the-computer-art plea on behalf of outmoded, wholesome fifties technology, with a dash of Zen by way of George Lucas.\" Moira MacDonald of The Seattle Times gave the film three and a half stars out of four, saying, \"Though the central idea of nostalgia for a quieter, small-town life may well be lost on this movie's young audience -- Cars finds a pleasant and often sparkling groove.\" Mick LaSalle of the San Francisco Chronicle gave the film two out of five stars, saying, \"Cars might get us into car world as a gimmick, but it doesn't get us into car world as a state of mind. Thus, the animation, rather than seeming like an expression of the movie's deeper truth, becomes an impediment to it.\" Derek Adams of Time Out gave the film a positive review, saying, \"There are many other brilliant scenes, some just as funny but there are just as many occasions where you feel the film's struggling to fire on all cylinders. Still, it's a Pixar film, right? And they're always worth a gander no matter what anyone says.\"\n\n\n=== Accolades ===\n\nCars had a highly successful run during the 2006 awards season. Many film critic associations such as the Broadcast Film Critics Association and the National Board of Review named it the best Animated Feature Film of 2006. Cars also received the title of Best Reviewed Animated Feature of 2006 from Rotten Tomatoes. Randy Newman and James Taylor received a Grammy Award for the song \"Our Town\", which later went on to be nominated for the Academy Award for Best Original Song (an award it lost to \"I Need to Wake Up\" from An Inconvenient Truth). The film also earned an Oscar nomination for Best Animated Feature alongside Monster House, but both films lost to Happy Feet. Cars was also selected as the Favorite Family Movie at the 33rd People's Choice Awards. The most prestigious award that Cars received was the inaugural Golden Globe Award for Best Animated Feature Film. Cars also won the highest award for animation in 2006, the Best Animated Feature Annie Award.  In 2008, the American Film Institute nominated this film for its Top 10 Animation Films list.\n\n\n== Video game ==\n\nA video game of the same name was released on June 6, 2006, for Game Boy Advance, Microsoft Windows, Nintendo DS, GameCube, PlayStation 2, PlayStation Portable, and Xbox. It was also released on October 23, 2006, for Xbox 360 and November 16, 2006, for Wii. Much like the film, the video game got mainly positive reviews. GameSpot gave 7.0 out of 10 for Xbox 360, Wii, and PlayStation 2 versions, 7.6 out of 10 for the GameCube and Xbox versions, and 7.4 out of 10 for the PSP version. Metacritic gave 65 out of 100 for the Wii version, 54 out of 100 for the DS version, 73 out of 100 for the PC version, 71 out of 100 for the PlayStation 2 version, and 70 out of 100 for the PSP version.\n\n\n== Similar films ==\nMarco Aur\u00e9lio Can\u00f4nico of Folha de S.Paulo described The Little Cars series (Os Carrinhos in Portuguese), a Brazilian computer graphics film series by V\u00eddeo Brinquedo, as a derivative of Cars. Can\u00f4nico discussed whether lawsuits from Pixar would appear. The Brazilian Ministry of Culture posted Marcus Aurelius Can\u00f4nico's article on its website.It has also been noted that the plot of Cars mirrors that of Doc Hollywood, a 1991 romantic comedy which stars Michael J. Fox as a hotshot young doctor who eventually acquires an appreciation for small town values and falls in love with a local law student as result of being sentenced to work at the town hospital after causing a traffic collision in a small town. Some have gone so far as to say that the makers of Cars plagiarized the script of Doc Hollywood.\n\n\n== Literature ==\n2006: CARS: The Junior Novelization, RH/Disney, ISBN 978-0736422918\n\n\n== Expanded franchise ==\n\n\n=== Sequels ===\n\nThe first sequel, titled Cars 2, was released on June 24, 2011. It was directed again by John Lasseter, who was inspired for the film while traveling around the world promoting the first film. In the sequel, Lightning McQueen and Mater head to Japan and Europe to compete in the World Grand Prix, but Mater becomes sidetracked with international espionage.The second sequel, titled Cars 3, was released on June 16, 2017. Directed by Brian Fee, the film focuses on Lightning McQueen, now a veteran racer, who after being overshadowed by a new generation of racecars, gets help from Cruz Ramirez, a young performance coupe, to instruct him for the increasingly high-tech world and defeat his new rival Jackson Storm.\n\n\n=== Spin-offs ===\n\nAn animated feature film spin-off called Planes, produced by DisneyToon Studios, was released on August 9, 2013. A sequel to Planes, titled Planes: Fire & Rescue, was released on July 18, 2014.\n\n\n=== Television series ===\nCars has also spawned a television series of short films titled Cars Toons, which ran from October 27, 2008, to June 5, 2012 (as Mater's Tall Tales) and March 22, 2013, to May 20, 2014 (as Tales from Radiator Springs). A Disney+ streaming series, titled Cars on the Road, premiered on September 8, 2022.\n\n\n== See also ==\nMandeville-Anthony v. Walt Disney Co., a federal court case in which Mandeville claimed Disney infringed on his copyrighted ideas by creating Cars\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website from Disney\nOfficial website from Pixar\nCars at AllMovie\nCars at The Big Cartoon DataBase\nCars at IMDb\nCars at the TCM Movie Database\nCars at Disney A to Z"}, {"id": 65, "title": "Public-key cryptography", "content": "Public-key cryptography, or asymmetric cryptography, is the field of cryptographic systems that use pairs of related keys. Each key pair consists of a public key and a corresponding private key. Key pairs are generated with cryptographic algorithms based on mathematical problems termed one-way functions. Security of public-key cryptography depends on keeping the private key secret; the public key can be openly distributed without compromising security.In a public-key encryption system, anyone with a public key can encrypt a message, yielding a ciphertext, but only those who know the corresponding private key can decrypt the ciphertext to obtain the original message.For example, a journalist can publish the public key of an encryption key pair on a web site so that sources can send secret messages to the news organization in ciphertext.\nOnly the journalist who knows the corresponding private key can decrypt the ciphertexts to obtain the sources' messages\u2014an eavesdropper reading email on its way to the journalist cannot decrypt the ciphertexts.\nHowever, public-key encryption does not conceal metadata like what computer a source used to send a message, when they sent it, or how long it is.\nPublic-key encryption on its own also does not tell the recipient anything about who sent a message\u2014it just conceals the content of a message in a ciphertext that can only be decrypted with the private key.\nIn a digital signature system, a sender can use a private key together with a message to create a signature.\nAnyone with the corresponding public key can verify whether the signature matches the message, but a forger who does not know the private key cannot find any message/signature pair that will pass verification with the public key.For example, a software publisher can create a signature key pair and include the public key in software installed on computers.\nLater, the publisher can distribute an update to the software signed using the private key, and any computer receiving an update can confirm it is genuine by verifying the signature using the public key.\nAs long as the software publisher keeps the private key secret, even if a forger can distribute malicious updates to computers, they cannot convince the computers that any malicious updates are genuine.\nPublic key algorithms are fundamental security primitives in modern cryptosystems, including applications and protocols which offer assurance of the confidentiality, authenticity and non-repudiability of electronic communications and data storage. They underpin numerous Internet standards, such as Transport Layer Security (TLS), SSH, S/MIME and PGP. Some public key algorithms provide key distribution and secrecy (e.g., Diffie\u2013Hellman key exchange), some provide digital signatures (e.g., Digital Signature Algorithm), and some provide both (e.g., RSA). Compared to symmetric encryption, asymmetric encryption is rather slower than good symmetric encryption, too slow for many purposes. Today's cryptosystems (such as TLS, Secure Shell) use both symmetric encryption and asymmetric encryption, often by using asymmetric encryption to securely exchange a secret key which is then used for symmetric encryption.\n\n\n== Description ==\nBefore the mid-1970s, all cipher systems used symmetric key algorithms, in which the same cryptographic key is used with the underlying algorithm by both the sender and the recipient, who must both keep it secret. Of necessity, the key in every such system had to be exchanged between the communicating parties in some secure way prior to any use of the system \u2013 for instance, via a secure channel. This requirement is never trivial and very rapidly becomes unmanageable as the number of participants increases, or when secure channels are not available, or when, (as is sensible cryptographic practice), keys are frequently changed. In particular, if messages are meant to be secure from other users, a separate key is required for each possible pair of users.\nBy contrast, in a public key system, the public keys can be disseminated widely and openly, and only the corresponding private keys need be kept secret by its owner.\nTwo of the best-known uses of public key cryptography are:\n\nPublic key encryption, in which a message is encrypted with the intended recipient's public key. For properly chosen and used algorithms, messages cannot in practice be decrypted by anyone who does not possess the matching private key, who is thus presumed to be the owner of that key and so the person associated with the public key. This can be used to ensure confidentiality of a message.\nDigital signatures, in which a message is signed with the sender's private key and can be verified by anyone who has access to the sender's public key. This verification proves that the sender had access to the private key, and therefore is very likely to be the person associated with the public key. It also proves that the signature was prepared for that exact message, since verification will fail for any other message one could devise without using the private key.One important issue is confidence/proof that a particular public key is authentic, i.e. that it is correct and belongs to the person or entity claimed, and has not been tampered with or replaced by some (perhaps malicious) third party. There are several possible approaches, including:\nA public key infrastructure (PKI), in which one or more third parties \u2013 known as certificate authorities \u2013 certify ownership of key pairs. TLS relies upon this. This implies that the PKI system (software, hardware, and management) is trust-able by all involved.\nA \"web of trust\" which decentralizes authentication by using individual endorsements of links between a user and the public key belonging to that user. PGP uses this approach, in addition to lookup in the domain name system (DNS). The DKIM system for digitally signing emails also uses this approach.\n\n\n== Applications ==\nThe most obvious application of a public key encryption system is for encrypting communication to provide confidentiality \u2013 a message that a sender encrypts using the recipient's public key which can be decrypted only by the recipient's paired private key.\nAnother application in public key cryptography is the digital signature. Digital signature schemes can be used for sender authentication.\nNon-repudiation systems use digital signatures to ensure that one party cannot successfully dispute its authorship of a document or communication.\nFurther applications built on this foundation include: digital cash, password-authenticated key agreement, time-stamping services and non-repudiation protocols.\n\n\n=== Hybrid cryptosystems ===\nBecause asymmetric key algorithms are nearly always much more computationally intensive than symmetric ones, it is common to use a public/private asymmetric key-exchange algorithm to encrypt and exchange a symmetric key, which is then used by  symmetric-key cryptography to transmit data using the now-shared symmetric key for a symmetric key encryption algorithm. PGP, SSH, and the SSL/TLS family of schemes use this procedure;  they are thus called hybrid cryptosystems. The initial asymmetric cryptography-based key exchange to share a server-generated symmetric key from the server to client has the advantage of not requiring that a symmetric key be pre-shared manually, such as on printed paper or discs transported by a courier, while providing the higher data throughput of symmetric key cryptography over asymmetric key cryptography for the remainder of the shared connection.\n\n\n== Weaknesses ==\nAs with all security-related systems, it is important to identify potential weaknesses.  Aside from poor choice of an asymmetric key algorithm (there are few which are widely regarded as satisfactory) or too short a key length, the chief security risk is that the private key of a pair becomes known.  All security of messages, authentication, etc., will then be lost.\nAdditionally, with the advent of quantum computing, many asymmetric key algorithms are considered vulnerable to attacks, and new quantum-resistant schemes are being developed to overcome the problem.\n\n\n=== Algorithms ===\nAll public key schemes are in theory susceptible to a \"brute-force key search attack\". However, such an attack is impractical if the amount of computation needed to succeed \u2013 termed the \"work factor\" by Claude Shannon \u2013 is out of reach of all potential attackers. In many cases, the work factor can be increased by simply choosing a longer key. But other algorithms may inherently have much lower work factors, making resistance to a brute-force attack (e.g., from longer keys) irrelevant. Some special and specific algorithms have been developed to aid in attacking some public key encryption algorithms; both RSA and ElGamal encryption have known attacks that are much faster than the brute-force approach. None of these are sufficiently improved to be actually practical, however.\nMajor weaknesses have been found for several formerly promising asymmetric key algorithms. The \"knapsack packing\" algorithm was found to be insecure after the development of a new attack. As with all cryptographic functions, public-key implementations may be vulnerable to side-channel attacks that exploit information leakage to simplify the search for a secret key. These are often independent of the algorithm being used. Research is underway to both discover, and to protect against, new attacks.\n\n\n=== Alteration of public keys ===\nAnother potential security vulnerability in using asymmetric keys is the possibility of a \"man-in-the-middle\" attack, in which the communication of public keys is intercepted by a third party (the \"man in the middle\") and then modified to provide different public keys instead. Encrypted messages and responses must, in all instances, be intercepted, decrypted, and re-encrypted by the attacker using the correct public keys for the different communication segments so as to avoid suspicion.\nA communication is said to be insecure where data is transmitted in a manner that allows for interception (also called \"sniffing\"). These terms refer to reading the sender's private data in its entirety. A communication is particularly unsafe when interceptions can not be prevented or monitored by the sender.A man-in-the-middle attack can be difficult to implement due to the complexities of modern security protocols. However, the task becomes simpler when a sender is using insecure media such as public networks, the Internet, or wireless communication. In these cases an attacker can compromise the communications infrastructure rather than the data itself. A hypothetical malicious staff member at an Internet Service Provider (ISP) might find a man-in-the-middle attack relatively straightforward. Capturing the public key would only require searching for the key as it gets sent through the ISP's communications hardware;  in properly implemented asymmetric key schemes, this is not a significant risk.\nIn some advanced man-in-the-middle attacks, one side of the communication will see the original data while the other will receive a malicious variant. Asymmetric man-in-the-middle attacks can prevent users from realizing their connection is compromised. This remains so even when one user's data is known to be compromised because the data appears fine to the other user. This can lead to confusing disagreements between users such as \"it must be on your end!\" when neither user is at fault. Hence, man-in-the-middle attacks are only fully preventable when the communications infrastructure is physically controlled by one or both parties; such as via a wired route inside the sender's own building. In summation, public keys are easier to alter when the communications hardware used by a sender is controlled by an attacker.\n\n\n=== Public key infrastructure ===\nOne approach to prevent such attacks involves the use of a public key infrastructure (PKI); a set of roles, policies, and procedures needed to create, manage, distribute, use, store and revoke digital certificates and manage public-key encryption. However, this has potential weaknesses.\nFor example, the certificate authority issuing the certificate must be trusted by all participating parties to have properly checked the identity of the key-holder, to have ensured the correctness of the public key when it issues a certificate, to be secure from computer piracy, and to have made arrangements with all participants to check all their certificates before protected communications can begin. Web browsers, for instance, are supplied with a long list of \"self-signed identity certificates\" from PKI providers \u2013 these are used to check the bona fides of the certificate authority and then, in a second step, the certificates of potential communicators. An attacker who could subvert one of those certificate authorities into issuing a certificate for a bogus public key could then mount a \"man-in-the-middle\" attack as easily as if the certificate scheme were not used at all. In an alternative scenario rarely discussed, an attacker who penetrates an authority's servers and obtains its store of certificates and keys (public and private) would be able to spoof, masquerade, decrypt, and forge transactions without limit.\nDespite its theoretical and potential problems, this approach is widely used. Examples include TLS and its predecessor SSL, which are commonly used to provide security for web browser transactions (for example, to securely send credit card details to an online store).\nAside from the resistance to attack of a particular key pair, the security of the certification hierarchy must be considered when deploying public key systems. Some certificate authority \u2013 usually a purpose-built program running on a server computer \u2013 vouches for the identities assigned to specific private keys by producing a digital certificate. Public key digital certificates are typically valid for several years at a time, so the associated private keys must be held securely over that time. When a private key used for certificate creation higher in the PKI server hierarchy is compromised, or accidentally disclosed, then a \"man-in-the-middle attack\" is possible, making any subordinate certificate wholly insecure.\n\n\n== Examples ==\nExamples of well-regarded asymmetric key techniques for varied purposes include:\n\nDiffie\u2013Hellman key exchange protocol\nDSS (Digital Signature Standard), which incorporates the Digital Signature Algorithm\nElGamal\nElliptic-curve cryptography\nElliptic Curve Digital Signature Algorithm (ECDSA)\nElliptic-curve Diffie\u2013Hellman (ECDH)\nEd25519 and Ed448 (EdDSA)\nX25519 and X448 (ECDH/EdDH)\nVarious password-authenticated key agreement techniques\nPaillier cryptosystem\nRSA encryption algorithm (PKCS#1)\nCramer\u2013Shoup cryptosystem\nYAK authenticated key agreement protocolExamples of asymmetric key algorithms not yet widely adopted include:\n\nNTRUEncrypt cryptosystem\nKyber\nMcEliece cryptosystemExamples of notable \u2013 yet insecure \u2013 asymmetric key algorithms include:\n\nMerkle\u2013Hellman knapsack cryptosystemExamples of protocols using asymmetric key algorithms include:\n\nS/MIME\nGPG, an implementation of OpenPGP, and an Internet Standard\nEMV, EMV Certificate Authority\nIPsec\nPGP\nZRTP, a secure VoIP protocol\nTransport Layer Security standardized by IETF and its predecessor Secure Socket Layer\nSILC\nSSH\nBitcoin\nOff-the-Record Messaging\n\n\n== History ==\nDuring the early history of cryptography, two parties would rely upon a key that they would exchange by means of a secure, but non-cryptographic, method such as a face-to-face meeting, or a trusted courier. This key, which both parties must then keep absolutely secret, could then be used to exchange encrypted messages. A number of significant practical difficulties arise with this approach to distributing keys.\n\n\n=== Anticipation ===\nIn his 1874 book The Principles of Science, William Stanley Jevons wrote:\nCan the reader say what two numbers multiplied together will produce the number 8616460799? I think it unlikely that anyone but myself will ever know.\nHere he described the relationship of one-way functions to cryptography, and went on to discuss specifically the factorization problem used to create a trapdoor function. In July 1996, mathematician Solomon W. Golomb said: \"Jevons anticipated a key feature of the RSA Algorithm for public key cryptography, although he certainly did not invent the concept of public key cryptography.\"\n\n\n=== Classified discovery ===\nIn 1970, James H. Ellis, a British cryptographer at the UK Government Communications Headquarters (GCHQ), conceived of the possibility of \"non-secret encryption\", (now called public key cryptography), but could see no way to implement it. In 1973, his colleague Clifford Cocks implemented what has become known as the RSA encryption algorithm, giving a practical method of \"non-secret encryption\", and in 1974 another GCHQ mathematician and cryptographer, Malcolm J. Williamson, developed what is now known as Diffie\u2013Hellman key exchange. \nThe scheme was also passed to the US's National Security Agency. Both organisations had a military focus and only limited computing power was available in any case;  the potential of public key cryptography remained unrealised by either organization:\n\nI judged it most important for military use ... if you can share your key rapidly and electronically, you have a major advantage over your opponent. Only at the end of the evolution from Berners-Lee designing an open internet architecture for CERN, its adaptation and adoption for the Arpanet ... did public key cryptography realise its full potential.\n\u2014Ralph Benjamin\n\nThese discoveries were not publicly acknowledged for 27 years, until the research was declassified by the British government in 1997.\n\n\n=== Public discovery ===\nIn 1976, an asymmetric key cryptosystem was published by Whitfield Diffie and Martin Hellman who, influenced by Ralph Merkle's work on public key distribution, disclosed a method of public key agreement. This method of key exchange, which uses exponentiation in a finite field, came to be known as Diffie\u2013Hellman key exchange. This was the first published practical method for establishing a shared secret-key over an authenticated (but not confidential) communications channel without using a prior shared secret. Merkle's \"public key-agreement technique\" became known as Merkle's Puzzles, and was invented in 1974 and only published in 1978. This makes asymmetric encryption a rather new field in cryptography although cryptography itself dates back more than 2,000 years.In 1977, a generalization of Cocks's scheme was independently invented by Ron Rivest, Adi Shamir and Leonard Adleman, all then at MIT. The latter authors published their work in 1978 in Martin Gardner's Scientific American column, and the algorithm came to be known as RSA, from their initials. RSA uses exponentiation modulo a product of two very large primes, to encrypt and decrypt, performing both public key encryption and public key digital signatures. Its security is connected to the extreme difficulty of factoring large integers, a problem for which there is no known efficient general technique (though prime factorization may be obtained through brute-force attacks; this grows much more difficult the larger the prime factors are). A description of the algorithm was published in the Mathematical Games column in the August 1977 issue of Scientific American.Since the 1970s, a large number and variety of encryption, digital signature, key agreement, and other techniques have been developed, including the Rabin cryptosystem, ElGamal encryption, DSA and ECC.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nOral history interview with Martin Hellman, Charles Babbage Institute, University of Minnesota. Leading cryptography scholar Martin Hellman discusses the circumstances and fundamental insights of his invention of public key cryptography with collaborators Whitfield Diffie and Ralph Merkle at Stanford University in the mid-1970s.\nAn account of how GCHQ kept their invention of PKE secret until 1997"}, {"id": 66, "title": "Quantum cryptography", "content": "Quantum cryptography is the science of exploiting quantum mechanical properties to perform cryptographic tasks. The best known example of quantum cryptography is quantum key distribution, which offers an information-theoretically secure solution to the key exchange problem. The advantage of quantum cryptography lies in the fact that it allows the completion of various cryptographic tasks that are proven or conjectured to be impossible using only classical (i.e. non-quantum) communication. For example, it is impossible to copy data encoded in a quantum state. If one attempts to read the encoded data, the quantum state will be changed due to wave function collapse (no-cloning theorem). This could be used to detect eavesdropping in quantum key distribution (QKD).\n\n\n== History ==\nIn the early 1970s, Stephen Wiesner, then at Columbia University in New York, introduced the concept of quantum conjugate coding. His seminal paper titled \"Conjugate Coding\" was rejected by the IEEE Information Theory Society but was eventually published in 1983 in SIGACT News. In this paper he showed how to store or transmit two messages by encoding them in two \"conjugate observables\", such as linear and circular polarization of photons, so that either, but not both, properties may be received and decoded. It was not until Charles H. Bennett, of the IBM's Thomas J. Watson Research Center, and Gilles Brassard met in 1979 at the 20th IEEE Symposium on the Foundations of Computer Science, held in Puerto Rico, that they discovered how to incorporate Wiesner's findings. \"The main breakthrough came when we realized that photons were never meant to store information, but rather to transmit it.\" In 1984, building upon this work, Bennett and Brassard proposed a method for secure communication, which is now called BB84. Independently, in 1991 Artur Ekert proposed to use Bell's inequalities to achieve secure key distribution. Ekert's protocol for the key distribution, as it was subsequently shown by Dominic Mayers and Andrew Yao, offers device-independent quantum key distribution.\nCompanies that manufacture quantum cryptography systems include MagiQ Technologies, Inc. (Boston), ID Quantique (Geneva), QuintessenceLabs (Canberra, Australia), Toshiba (Tokyo), QNu Labs (India) and SeQureNet (Paris).\n\n\n== Advantages ==\nCryptography is the strongest link in the chain of data security. However, interested parties cannot assume that cryptographic keys will remain secure indefinitely. Quantum cryptography has the potential to encrypt data for longer periods than classical cryptography. Using classical cryptography, scientists cannot guarantee encryption beyond approximately 30 years, but some stakeholders could use longer periods of protection. Take, for example, the healthcare industry. As of 2017, 85.9% of office-based physicians are using electronic medical record systems to store and transmit patient data. Under the Health Insurance Portability and Accountability Act, medical records must be kept secret. Quantum key distribution can protect electronic records for periods of up to 100 years. Also, quantum cryptography has useful applications for governments and militaries as, historically, governments have kept military data secret for periods of over 60 years. There also has been proof that quantum key distribution can travel through a noisy channel over a long distance and be secure. It can be reduced from a noisy quantum scheme to a classical noiseless scheme. This can be solved with classical probability theory. This process of having consistent protection over a noisy channel can be possible through the implementation of quantum repeaters. Quantum repeaters have the ability to resolve quantum communication errors in an efficient way. Quantum repeaters, which are quantum computers, can be stationed as segments over the noisy channel to ensure the security of communication. Quantum repeaters do this by purifying the segments of the channel before connecting them creating a secure line of communication. Sub-par quantum repeaters can provide an efficient amount of security through the noisy channel over a long distance.\n\n\n== Applications ==\nQuantum cryptography is a general subject that covers a broad range of cryptographic practices and protocols. Some of the most notable applications and protocols are discussed below.\n\n\n=== Quantum key distribution ===\n\nThe best-known and developed application of quantum cryptography is QKD, which is the process of using quantum communication to establish a shared key between two parties (Alice and Bob, for example) without a third party (Eve) learning anything about that key, even if Eve can eavesdrop on all communication between Alice and Bob. If Eve tries to learn information about the key being established, discrepancies will arise causing Alice and Bob to notice. Once the key is established, it is then typically used for encrypted communication using classical techniques. For instance, the exchanged key could be used for symmetric cryptography (e.g. one-time pad).\nThe security of quantum key distribution can be proven mathematically without imposing any restrictions on the abilities of an eavesdropper, something not possible with classical key distribution. This is usually described as \"unconditional security\", although there are some minimal assumptions required, including that the laws of quantum mechanics apply and that Alice and Bob are able to authenticate each other, i.e. Eve should not be able to impersonate Alice or Bob as otherwise a man-in-the-middle attack would be possible.\nWhile QKD is secure, its practical application faces some challenges. There are in fact limitations for the key generation rate at increasing transmission distances. Recent studies have allowed important advancements in this regard. In 2018, the protocol of twin-field QKD was proposed as a mechanism to overcome the limits of lossy communication. The rate of the twin field protocol was shown to overcome the secret key-agreement capacity of the lossy communication channel, known as repeater-less PLOB bound, at 340 km of optical fiber; its ideal rate surpasses this bound already at 200 km and follows the rate-loss scaling of the higher repeater-assisted secret key-agreement capacity (see figure 1 of and figure 11 of for more details). The protocol suggests that optimal key rates are achievable on \"550 kilometers of standard optical fibre\", which is already commonly used in communications today. The theoretical result was confirmed in the first experimental demonstration of QKD beyond the PLOB bound which has been characterized as the first effective quantum repeater. Notable developments in terms of achieving high rates at long distances are the sending-not-sending (SNS) version of the TF-QKD protocol. and the no-phase-postselected twin-field scheme.\n\n\n=== Mistrustful quantum cryptography ===\nIn mistrustful cryptography the participating parties do not trust each other. For example, Alice and Bob collaborate to perform some computation where both parties enter some private inputs. But Alice does not trust Bob and Bob does not trust Alice. Thus, a secure implementation of a cryptographic task requires that after completing the computation, Alice can be guaranteed that Bob has not cheated and Bob can be guaranteed that Alice has not cheated either. Examples of tasks in mistrustful cryptography are commitment schemes and secure computations, the latter including the further examples of coin flipping and oblivious transfer. Key distribution does not belong to the area of mistrustful cryptography. Mistrustful quantum cryptography studies the area of mistrustful cryptography using quantum systems.\nIn contrast to quantum key distribution where unconditional security can be achieved based only on the laws of quantum physics, in the case of various tasks in mistrustful cryptography there are no-go theorems showing that it is impossible to achieve unconditionally secure protocols based only on the laws of quantum physics. However, some of these tasks can be implemented with unconditional security if the protocols not only exploit quantum mechanics but also special relativity. For example, unconditionally secure quantum bit commitment was shown impossible by Mayers and by Lo and Chau. Unconditionally secure ideal quantum coin flipping was shown impossible by Lo and Chau. Moreover, Lo showed that there cannot be unconditionally secure quantum protocols for one-out-of-two oblivious transfer and other secure two-party computations. However, unconditionally secure relativistic protocols for coin flipping and bit-commitment have been shown by Kent.\n\n\n==== Quantum coin flipping ====\n\nUnlike quantum key distribution, quantum coin flipping is a protocol that is used between two participants who do not trust each other. The participants communicate via a quantum channel and exchange information through the transmission of qubits. But because Alice and Bob do not trust each other, each expects the other to cheat. Therefore, more effort must be spent on ensuring that neither Alice nor Bob can gain a significant advantage over the other to produce a desired outcome. An ability to influence a particular outcome is referred to as a bias, and there is a significant focus on developing protocols to reduce the bias of a dishonest player, otherwise known as cheating. Quantum communication protocols, including quantum coin flipping, have been shown to provide significant security advantages over classical communication, though they may be considered difficult to realize in the practical world.A coin flip protocol generally occurs like this:\nAlice chooses a basis (either rectilinear or diagonal) and generates a string of photons to send to Bob in that basis.\nBob randomly chooses to measure each photon in a rectilinear or diagonal basis, noting which basis he used and the measured value.\nBob publicly guesses which basis Alice used to send her qubits.\nAlice announces the basis she used and sends her original string to Bob.\nBob confirms by comparing Alice's string to his table. It should be perfectly correlated with the values Bob measured using Alice's basis and completely uncorrelated with the opposite.Cheating occurs when one player attempts to influence, or increase the probability of a particular outcome. The protocol discourages some forms of cheating; for example, Alice could cheat at step 4 by claiming that Bob incorrectly guessed her initial basis when he guessed correctly, but Alice would then need to generate a new string of qubits that perfectly correlates with what Bob measured in the opposite table. Her chance of generating a matching string of qubits will decrease exponentially with the number of qubits sent, and if Bob notes a mismatch, he will know she was lying. Alice could also generate a string of photons using a mixture of states, but Bob would easily see that her string will correlate partially (but not fully) with both sides of the table, and know she cheated in the process. There is also an inherent flaw that comes with current quantum devices. Errors and lost qubits will affect Bob's measurements, resulting in holes in Bob's measurement table. Significant losses in measurement will affect Bob's ability to verify Alice's qubit sequence in step 5.\nOne theoretically surefire way for Alice to cheat is to utilize the Einstein-Podolsky-Rosen (EPR) paradox. Two photons in an EPR pair are anticorrelated; that is, they will always be found to have opposite polarizations, provided that they are measured in the same basis. Alice could generate a string of EPR pairs, sending one photon per pair to Bob and storing the other herself. When Bob states his guess, she could measure her EPR pair photons in the opposite basis and obtain a perfect correlation to Bob's opposite table. Bob would never know she cheated. However, this requires capabilities that quantum technology currently does not possess, making it impossible to do in practice. To successfully execute this, Alice would need to be able to store all the photons for a significant amount of time as well as measure them with near perfect efficiency. This is because any photon lost in storage or in measurement would result in a hole in her string that she would have to fill by guessing. The more guesses she has to make, the more she risks detection by Bob for cheating.\n\n\n==== Quantum commitment ====\nIn addition to quantum coin-flipping, quantum commitment protocols are implemented when distrustful parties are involved.  A commitment scheme allows a party Alice to fix a certain value (to \"commit\") in such a way that Alice cannot change that value while at the same time ensuring that the recipient Bob cannot learn anything about that value until Alice reveals it. Such commitment schemes are commonly used in cryptographic protocols (e.g. Quantum coin flipping, Zero-knowledge proof, secure two-party computation, and Oblivious transfer).\nIn the quantum setting, they would be particularly useful: Cr\u00e9peau and Kilian showed that from a commitment and a quantum channel, one can construct an unconditionally secure protocol for performing so-called oblivious transfer. Oblivious transfer, on the other hand, had been shown by Kilian to allow implementation of almost any distributed computation in a secure way (so-called secure multi-party computation). (Note: The results by Cr\u00e9peau and Kilian together do not directly imply that given a commitment and a quantum channel one can perform secure multi-party computation. This is because the results do not guarantee \"composability\", that is, when plugging them together, one might lose security.)\nUnfortunately, early quantum commitment protocols were shown to be flawed. In fact, Mayers showed that (unconditionally secure) quantum commitment is impossible: a computationally unlimited attacker can break any quantum commitment protocol.Yet, the result by Mayers does not preclude the possibility of constructing quantum commitment protocols (and thus secure multi-party computation protocols) under assumptions that are much weaker than the assumptions needed for commitment protocols that do not use quantum communication. The bounded quantum storage model described below is an example for a setting in which quantum communication can be used to construct commitment protocols. A breakthrough in November 2013 offers \"unconditional\" security of information by harnessing quantum theory and relativity, which has been successfully demonstrated on a global scale for the first time. More recently, Wang et al., proposed another commitment scheme in which the \"unconditional hiding\" is perfect.Physical unclonable functions can be also exploited for the construction of cryptographic commitments.\n\n\n=== Bounded- and noisy-quantum-storage model ===\nOne possibility to construct unconditionally secure quantum commitment and quantum oblivious transfer (OT) protocols is to use the bounded quantum storage model (BQSM). In this model, it is assumed that the amount of quantum data that an adversary can store is limited by some known constant Q. However, no limit is imposed on the amount of classical (i.e., non-quantum) data the adversary may store.\nIn the BQSM, one can construct commitment and oblivious transfer protocols. The underlying idea is the following: The protocol parties exchange more than Q quantum bits (qubits). Since even a dishonest party cannot store all that information (the quantum memory of the adversary is limited to Q qubits), a large part of the data will have to be either measured or discarded. Forcing dishonest parties to measure a large part of the data allows the protocol to circumvent the impossibility result, commitment and oblivious transfer protocols can now be implemented.The protocols in the BQSM presented by Damg\u00e5rd, Fehr, Salvail, and Schaffner do not assume that honest protocol participants store any quantum information; the technical requirements are similar to those in quantum key distribution protocols. These protocols can thus, at least in principle, be realized with today's technology. The communication complexity is only a constant factor larger than the bound Q on the adversary's quantum memory.\nThe advantage of the BQSM is that the assumption that the adversary's quantum memory is limited is quite realistic. With today's technology, storing even a single qubit reliably over a sufficiently long time is difficult. (What \"sufficiently long\" means depends on the protocol details. By introducing an artificial pause in the protocol, the amount of time over which the adversary needs to store quantum data can be made arbitrarily large.)\nAn extension of the BQSM is the noisy-storage model introduced by Wehner, Schaffner and Terhal. Instead of considering an upper bound on the physical size of the adversary's quantum memory, an adversary is allowed to use imperfect quantum storage devices of arbitrary size. The level of imperfection is modelled by noisy quantum channels. For high enough noise levels, the same primitives as in the BQSM can be achieved and the BQSM forms a special case of the noisy-storage model.\nIn the classical setting, similar results can be achieved when assuming a bound on the amount of classical (non-quantum) data that the adversary can store. It was proven, however, that in this model also the honest parties have to use a large amount of memory (namely the square-root of the adversary's memory bound). This makes these protocols impractical for realistic memory bounds. (Note that with today's technology such as hard disks, an adversary can cheaply store large amounts of classical data.)\n\n\n=== Position-based quantum cryptography ===\nThe goal of position-based quantum cryptography is to use the geographical location of a player as its (only) credential. For example, one wants to send a message to a player at a specified position with the guarantee that it can only be read if the receiving party is located at that particular position. In the basic task of position-verification, a player, Alice, wants to convince the (honest) verifiers that she is located at a particular point. It has been shown by Chandran et al. that position-verification using classical protocols is impossible against colluding adversaries (who control all positions except the prover's claimed position). Under various restrictions on the adversaries, schemes are possible.\nUnder the name of 'quantum tagging', the first position-based quantum schemes have been investigated in 2002 by Kent. A US-patent was granted in 2006. The notion of using quantum effects for location verification first appeared in the scientific literature in 2010. After several other quantum protocols for position verification have been suggested in 2010, Buhrman et al. claimed a general impossibility result: using an enormous amount of quantum entanglement (they use a doubly exponential number of EPR pairs, in the number of qubits the honest player operates on), colluding adversaries are always able to make it look to the verifiers as if they were at the claimed position. However, this result does not exclude the possibility of practical schemes in the bounded- or noisy-quantum-storage model (see above). Later Beigi and K\u00f6nig improved the amount of EPR pairs needed in the general attack against position-verification protocols to exponential. They also showed that a particular protocol remains secure against adversaries who controls only a linear amount of EPR pairs. It is argued in that due to time-energy coupling the possibility of formal unconditional location verification via quantum effects remains an open problem. It is worth mentioning that the study of position-based quantum cryptography has also connections with the protocol of port-based quantum teleportation, which is a more advanced version of quantum teleportation, where many EPR pairs are simultaneously used as ports.\n\n\n=== Device-independent quantum cryptography ===\n\nA quantum cryptographic protocol is device-independent if its security does not rely on trusting that the quantum devices used are truthful. Thus the security analysis of such a protocol needs to consider scenarios of imperfect or even malicious devices [3]. Mayers and Yao proposed the idea of designing quantum protocols using \"self-testing\" quantum apparatus, the internal operations of which can be uniquely determined by their input-output statistics. Subsequently, Roger Colbeck in his Thesis proposed the use of Bell tests for checking the honesty of the devices. Since then, several problems have been shown to admit unconditional secure and device-independent protocols, even when the actual devices performing the Bell test are substantially \"noisy\", i.e., far from being ideal. These problems include\nquantum key distribution, randomness expansion, and randomness amplification.In 2018, theoretical studies performed by Arnon- Friedman et al. suggest that exploiting a property of entropy that is later referred to as \"Entropy Accumulation Theorem (EAT)\" , an extension of Asymptotic equipartition property, can guarantee the security of a device independent protocol.\n\n\n== Post-quantum cryptography ==\n\nQuantum computers may become a technological reality; it is therefore important to study cryptographic schemes used against adversaries with access to a quantum computer. The study of such schemes is often referred to as post-quantum cryptography. The need for post-quantum cryptography arises from the fact that many popular encryption and signature schemes (schemes based on ECC and RSA) can be broken using Shor's algorithm for factoring and computing discrete logarithms on a quantum computer. Examples for schemes that are, as of today's knowledge, secure against quantum adversaries are McEliece and lattice-based schemes, as well as most symmetric-key algorithms. Surveys of post-quantum cryptography are available.There is also research into how existing cryptographic techniques have to be modified to be able to cope with quantum adversaries. For example, when trying to develop zero-knowledge proof systems that are secure against quantum adversaries, new techniques need to be used: In a classical setting, the analysis of a zero-knowledge proof system usually involves \"rewinding\", a technique that makes it necessary to copy the internal state of the adversary. In a quantum setting, copying a state is not always possible (no-cloning theorem); a variant of the rewinding technique has to be used.Post quantum algorithms are also called \"quantum resistant\", because \u2013 unlike quantum key distribution \u2013 it is not known or provable that there will not be potential future quantum attacks against them. Even though they may possibly be vulnerable to quantum attacks in the future, the NSA is announcing plans to transition to quantum resistant algorithms. The National Institute of Standards and Technology (NIST) believes that it is time to think of quantum-safe primitives.\n\n\n== Quantum cryptography beyond key distribution ==\nSo far, quantum cryptography has been mainly identified with the development of quantum key distribution protocols. Unfortunately, symmetric cryptosystems with keys that have been distributed by means of quantum key distribution become inefficient for large networks (many users), because of the necessity for the establishment and the manipulation of many pairwise secret keys (the so-called \"key-management problem\"). Moreover, this distribution alone does not address many other cryptographic tasks and functions, which are of vital importance in everyday life. Kak's three-stage protocol has been proposed as a method for secure communication that is entirely quantum unlike quantum key distribution, in which the cryptographic transformation uses classical algorithmsBesides quantum commitment and oblivious transfer (discussed above), research on quantum cryptography beyond key distribution revolves around quantum message authentication, quantum digital signatures, quantum one-way functions and public-key encryption, quantum fingerprinting and entity authentication (for example, see Quantum readout of PUFs), etc.\n\n\n== Y-00 Protocol ==\nH. P. Yuen presented Y-00 as a stream cipher using quantum noise around 2000 and applied it for the U.S. Defense Advanced Research Projects Agency (DARPA) High-Speed and High-Capacity Quantum Cryptography Project as an alternative to quantum key distribution.\nThe review paper summarizes it well.Unlike quantum key distribution protocols, the main purpose of Y-00 is to transmit a message without eavesdrop-monitoring, not to distribute a key. Therefore, privacy amplification may be used only for key distributions. Currently, research is being conducted mainly in Japan and China: e.g.The principle of operation is as follows. First, legitimate users share a key and change it to a pseudo-random keystream using the same pseudo-random number generator. Then, the legitimate parties can perform conventional optical communications based on the shared key by transforming it appropriately. For attackers who do not share the key, the wire-tap channel model of Aaron D. Wyner is implemented. The legitimate users' advantage based on the shared key is called \"advantage creation\". The goal is to achieve longer covert communication than the information-theoretic security limit (one-time pad) set by Shannon. The source of the noise in the above wire-tap channel is the uncertainty principle of the electromagnetic field itself, which is a theoretical consequence of the theory of laser described by Roy J. Glauber and E. C. George Sudarshan (coherent state). Therefore, existing optical communication technologies are sufficient for implementation that some reviews describes: e.g.\nFurthermore, since it uses ordinary communication laser light, it is compatible with existing communication infrastructure and can be used for high-speed \nand long-distance communication and routing.Although the main purpose of the protocol is to transmit the message, key distribution is possible by simply replacing the message with a key.  Since it is a symmetric key cipher, it must share the initial key previously; however, a method of the initial key agreement was also proposed.On the other hand, it is currently unclear what implementation realizes information-theoretic security, and security of this protocol has long been a matter of debate.\n\n\n== Implementation in practice ==\nIn theory, quantum cryptography seems to be a successful turning point in the information security sector. However, no cryptographic method can ever be absolutely secure.  In practice, quantum cryptography is only conditionally secure, dependent on a key set of assumptions.\n\n\n=== Single-photon source assumption ===\nThe theoretical basis for quantum key distribution assumes the use of single-photon sources. However, such sources are difficult to construct, and most real-world quantum cryptography systems use faint laser sources as a medium for information transfer. These multi-photon sources open the possibility for eavesdropper attacks, particularly a photon splitting attack. An eavesdropper, Eve, can split the multi-photon source and retain one copy for herself. The other photons are then transmitted to Bob without any measurement or trace that Eve captured a copy of the data. Scientists believe they can retain security with a multi-photon source by using decoy states that test for the presence of an eavesdropper. However, in 2016, scientists developed a near perfect single photon source and estimate that one could be developed in the near future.\n\n\n=== Identical detector efficiency assumption ===\nIn practice, multiple single-photon detectors are used in quantum key distribution devices, one for Alice and one for Bob. These photodetectors are tuned to detect an incoming photon during a short window of only a few nanoseconds. Due to manufacturing differences between the two detectors, their respective detection windows will be shifted by some finite amount. An eavesdropper, Eve, can take advantage of this detector inefficiency by measuring Alice's qubit and sending a \"fake state\" to Bob. Eve first captures the photon sent by Alice and then generates another photon to send to Bob. Eve manipulates the phase and timing of the \"faked\" photon in a way that prevents Bob from detecting the presence of an eavesdropper. The only way to eliminate this vulnerability is to eliminate differences in photodetector efficiency, which is difficult to do given finite manufacturing tolerances that cause optical path length differences, wire length differences, and other defects.\n\n\n=== Deprecation of quantum key distributions from governmental institutions ===\nBecause of the practical problems with quantum key distribution, some governmental organizations recommend the use of Post-Quantum Cryptography (quantum resistant cryptography) instead. For example, National Security Agency of USA, European Union Agency for Cybersecurity of EU (ENISA), UK's National Cyber Security Centre, French Secretariat for Defense and Security (ANSSI), and German Federal Office for Information Security (BSI)  recommend Post-Quantum Cryptography.\nFor example, the U.S. National Security Agency addresses five issues:\nQuantum key distribution is only a partial solution. QKD generates keying material for an encryption algorithm that provides confidentiality. Such keying material could also be used in symmetric key cryptographic algorithms to provide integrity and authentication if one has the cryptographic assurance that the original QKD transmission comes from the desired entity (i.e. entity source authentication). QKD does not provide a means to authenticate the QKD transmission source. Therefore, source authentication requires the use of asymmetric cryptography or pre-placed keys to provide that authentication. Moreover, the confidentiality services QKD offers can be provided by quantum-resistant cryptography, which is typically less expensive with a better understood risk profile.\nQuantum key distribution requires special purpose equipment. QKD is based on physical properties, and its security derives from unique physical layer communications. This requires users to lease dedicated fiber connections or physically manage free-space transmitters. It cannot be implemented in software or as a service on a network, and cannot be easily integrated into existing network equipment. Since QKD is hardware-based it also lacks flexibility for upgrades or security patches.\nQuantum key distribution increases infrastructure costs and insider-threat risks. QKD networks frequently necessitate the use of trusted relays, entailing additional cost for secure facilities and additional security risk from insider threats. This eliminates many use cases from consideration.\nSecuring and validating quantum key distribution is a significant challenge. The actual security provided by a QKD system is not the theoretical unconditional security from the laws of physics (as modeled and often suggested), but rather the more limited security that can be achieved by hardware and engineering designs. The tolerance for error in cryptographic security, however, is many orders of magnitude smaller than what is available in most physical engineering scenarios, making it very difficult to validate. The specific hardware used to perform QKD can introduce vulnerabilities, resulting in several well-publicized attacks on commercial QKD systems.\nQuantum key distribution increases the risk of denial of service. The sensitivity to an eavesdropper as the theoretical basis for QKD security claims also shows that denial of service is a significant risk for QKD.In response to problem 1 above, attempts to deliver authentication keys using post-quantum cryptography (or quantum-resistant cryptography) have been proposed worldwide. On the other hand, quantum-resistant cryptography is cryptography belonging to the class of computational security. In 2015, a research result was already published that \"sufficient care must be taken in implementation to achieve information-theoretic security for the system as a whole when authentication keys that are not information-theoretic secure are used\" (if the authentication key is not information-theoretically secure, an attacker can break it to bring all classical and quantum communications under control and relay them to launch a man-in-the-middle attack).\nEricsson, a private company, also cites and points out the above problems and then presents a report that it may not be able to support the zero trust security model, which is a recent trend in network security technology.\n\n\n== References =="}, {"id": 67, "title": "Credit history", "content": "A credit history is a record of a borrower's responsible repayment of debts.\nA credit report is a record of the borrower's credit history from a number of sources, including banks, credit card companies, collection agencies, and governments.  A borrower's credit score is the result of a mathematical algorithm applied to a credit report and other sources of information to predict future delinquency.In many countries, when a customer submits an application for credit from a bank, credit card company, or a store, their information is forwarded to a credit bureau. The credit bureau matches the name, address and other identifying information on the credit applicant with information retained by the bureau in its files. The gathered records are then used by lenders to determine an individual's credit worthiness; that is, determining an individual's ability and track record of repaying a debt. The willingness to repay a debt is indicated by how timely past payments have been made to other lenders.  Lenders like to see consumer debt obligations paid regularly and on time, and therefore focus particularly on missed payments and may not, for example, consider an overpayment as an offset for a missed payment.\n\n\n== Credit history usage ==\nThere has been much discussion over the accuracy of the data in consumer reports. In general, industry participants maintain that the data in credit reports is very accurate. The credit bureaus point to their own study of 52 million credit reports to highlight that the data in reports is very accurate. The Consumer Data Industry Association testified before the United States Congress that less than two percent of those reports that resulted in a consumer dispute had data deleted because it was in error. Nonetheless, there is widespread concern that information in credit reports is prone to error. Thus Congress has enacted a series of laws aimed to resolve both the errors and the perception of errors.\nIf a US consumer disputes some information in a credit report, the credit bureau has 30 days to verify the data. Over 70 percent of these consumer disputes are resolved within 14 days and then the consumer is notified of the resolution. The Federal Trade Commission states that one large credit bureau notes 95 percent of those who dispute an item seem satisfied with the outcome.The other factor in determining whether a lender will provide a consumer credit or a loan is dependent on income. The higher the income, all other things being equal, the more credit the consumer can access. However, lenders make credit granting decisions based on both ability to repay a debt (income) and willingness (the credit report) as indicated by a history of regular, unmissed payments.\nThese factors help lenders determine whether to extend credit, and on what terms. With the adoption of risk-based pricing on almost all lending in the financial services industry, this report has become even more important since it is usually the sole element used to choose the annual percentage rate (APR), grace period and other contractual obligations of the credit card or loan.\n\n\n== Calculating a credit score ==\nCredit scores vary from one scoring model to another, but in general the FICO scoring system is the standard in U.S., Canada and other global areas. The factors are similar and may include:\n\nPayment history (35% contribution on the FICO scale): A record of negative information can lower a consumer's credit rating or score. In general risk scoring systems look for any of the following negative events; charge offs, collections, late payments, repossessions, foreclosures, settlements, bankruptcies, liens, and judgements. Within this category, FICO considers the severity of the negative item, the age of the negative items and the prevalence of negative items. Newer unpaid or delinquent debt is considered worse than older unpaid or delinquent debts. More severe is worse than less severe.  And, many is worse than few.\nDebt (30% contribution on the FICO score): This category considers the amount and type of debt carried by a consumer as reflected on their credit reports.  The amount of debt you have divided by your total credit limit is called the credit utilization ratio.  There are three types of debt considered in this calculation.\nRevolving debt: This is credit card debt, retail card debt and some petroleum cards. And while home equity lines of credit have revolving terms the bulk of debt considered is true unsecured revolving debt incurred on plastic. The most important measurement from this category is called \"Revolving Utilization\", which is the relationship between the consumer's aggregate credit card balances and the available credit card limits, also called \"open to buy\".  This is expressed as a percentage and is calculated by dividing the aggregate credit card balances by the aggregate credit limits and multiplying the result by 100, thus yielding the utilization percentage. The higher that percentage, the lower the cardholder's score will likely be. This is why closing credit cards is generally not a good idea for someone trying to improve their credit scores. Closing one or more credit card accounts will reduce their total available credit limits and likely increase the utilization percentage unless the cardholder reduces their balances at the same pace.\nInstallment debt: This is debt where there is a fixed payment for a fixed period of time. An auto loan is a good example as the cardholder is generally making the same payment for 36, 48, or 60 months. While installment debt is considered in risk scoring systems, it is a distant second in its importance behind the revolving credit card debt. Installment debt is generally secured by an asset like a car, home, or boat. As such, consumers will use extraordinary efforts to make their payments so their asset is not repossessed by the lender for non-payment.\nOpen debt: This is the least common type of debt. This is debt that must be paid in full each month. An example is any one of the variety of charge cards that are \"pay in full\" products. The American Express Green card is a common example. Open debt is treated like revolving credit card debt in older versions of the FICO scoring system but is excluded from the revolving utilization calculation in newer versions.\nTime in file (Credit File Age) (15% contribution on the FICO scale): The older the cardholder's credit report, the more stable it is, in general. As such, their score should benefit from an old credit report.  This \"age\" is determined two ways; the age of the cardholder's credit file and the average age of the accounts on their credit file. The age of their credit file is determined by the oldest account's \"date opened\", which sets the age of the credit file. The average age is set by averaging the age of every account on the credit report, whether open or closed.\nAccount Diversity (10% contribution on the FICO scale): A cardholder's credit score will benefit by having a diverse set of account types on their credit file.  Having experience across multiple account types (installment, revolving, auto, mortgage, cards, etc.) is generally a good thing for their scores because they are proving the ability to manage different account types.\nThe Search for a New Credit (Credit inquiries) (10% contribution on the FICO scale): An inquiry is noted every time a company requests some information from a consumer's credit file. There are several kinds of inquiries that may or may not affect one's credit score. Inquiries that have no effect on the creditworthiness of a consumer (also known as \"soft inquiries\"), which remain on a consumer's credit reports for 6 months and are never visible to lenders or credit scoring models, are:\nPrescreening inquiries where a credit bureau may sell a person's contact information to an institution that issues credit cards, loans and insurance based on certain criteria that the lender has established.\nA creditor also checks its customers' credit files periodically. This is referred to as Account Management, Account Maintenance or Account Review.\nA credit counseling agency, with the client's permission, can obtain a client's credit report with no adverse action.\nA consumer can check his or her own credit report without impacting creditworthiness. This is referred to as a \"consumer disclosure\" inquiry.\nEmployment screening inquiries\nInsurance related inquiries\nUtility related inquiries\nInquiries that can have an effect on the creditworthiness of a consumer, and are visible to lenders and credit scoring models, (also known as \"hard inquiries\") are made by lenders when consumers are seeking credit or a loan, in connection with permissible purpose. Lenders, when granted a permissible purpose, as defined by the Fair Credit Reporting Act, can \"pull\" a consumer file for the purposes of extending credit to a consumer. Hard inquiries can, but do not always, affect the borrower's credit score. Keeping credit inquiries to a minimum can help a person's credit rating. A lender may perceive many inquiries over a short period of time on a person's report as a signal that the person is in financial difficulty, and may consider that person a poor credit risk.\n\n\n== Acquiring and understanding credit reports and scores ==\nConsumers can typically check their credit history by requesting credit reports from credit agencies and demanding correction of information if necessary.\nIn the United States, the Fair Credit Reporting Act governs businesses that compile credit reports. These businesses range from the big three credit reporting agencies, Experian, Equifax, TransUnion, to specialty credit reporting agencies that cater to specific clients including payday lenders, utility companies, casinos, landlords, medical service providers, and employers. One Fair Credit Reporting Act requirement is that the consumer credit reporting agencies it governs provide a free copy of the credit reports for any consumer who requests it, once per year.\nThe government of Canada offers a free publication called Understanding Your Credit Report and Credit Score. This publication provides sample credit report and credit score documents with explanations of the notations and codes that are used. It also contains general information on how to build or improve credit history, and how to check for signs that identity theft has occurred. The publication is available online through http://www.fcac.gc.ca, the site of the Financial Consumer Agency of Canada. Paper copies can also be ordered at no charge for residents of Canada.\nIn some countries, in addition to privately owned credit bureaus, credit records are also maintained by the central bank. Particularly, in Spain, the Central Credit Register is kept by the Bank of Spain. In this country, individuals can obtain their credit reports free of charge by requesting them online or by mail.\n\n\n== Credit history of immigrants ==\nCredit history usually stays within one country. Even within the same credit card network or within the same multinational credit bureau, information is not shared between different countries. For example, Equifax Canada does not share credit information with Equifax in the United States. If a person has been living in Canada for many years and then moves to USA, when they apply for credit in the U.S., they may not be approved because of a lack of U.S. credit history, even if they had an excellent credit rating in their home country.\nAn immigrant may end up establishing a credit history from scratch in the new country. Therefore, it is usually difficult for immigrants to obtain credit cards and mortgages until after they have worked in the new country with a stable income for several years.\nSome lenders do take into account credit history from other countries, but this practice is not common. Among credit card companies, American Express can transfer credit cards from one country to another and in this way help start a credit history.\n\n\n== Adverse credit ==\nAdverse credit history, also called sub-prime credit history, non-status credit history, impaired credit history, poor credit history, and bad credit history, is a negative credit rating.\nA negative credit rating is often considered undesirable to lenders and other extenders of credit for the purposes of loaning money or capital.In the U.S., a consumer's credit history is compiled into a credit report by credit bureaus or consumer reporting agencies. The data reported to these agencies are primarily provided to them by creditors and includes detailed records of the relationship a person has with the creditor. Detailed account information, including payment history, credit limits, high and low balances, and any aggressive actions taken to recover overdue debts, are all reported regularly (usually monthly). This information is reviewed by a lender to determine whether to approve a loan and on what terms.\nAs credit became more popular, it became more difficult for lenders to evaluate and approve credit card and loan applications in a timely and efficient manner. To address this issue, credit scoring was adopted. A benefit of scoring was that it made credit available to more consumers and at less cost.Credit scoring is the process of using a proprietary mathematical algorithm to create a numerical value that describes an applicant's overall creditworthiness. Scores, frequently based on numbers (ranging from 300\u2013850 for consumers in the United States), statistically analyze a credit history, in comparison to other debtors, and gauge the magnitude of financial risk. Since lending money to a person or company is a risk, credit scoring offers a standardized way for lenders to assess that risk rapidly and \"without prejudice\". All credit bureaus also offer credit scoring as a supplemental service.\nCredit scores assess the likelihood that a borrower will repay a loan or other credit obligation based on factors like their borrowing and repayment history, the types of credit they have taken out and the overall length of their credit history. The higher the score, the better the credit history and the higher the probability that the loan will be repaid on time. When creditors report an excessive number of late payments, or trouble with collecting payments, the score suffers. Similarly, when adverse judgments and collection agency activity are reported, the score decreases even more. Repeated delinquencies or public record entries can lower the score and trigger what is called a negative credit rating or adverse credit history.\nA consumer's credit score is a number calculated from factors such as the amount of credit outstanding versus how much they owe, their past ability to pay all their bills on time, how long they have had credit, types of credit used and number of inquiries. The three major consumer reporting agencies, Equifax, Experian and TransUnion all sell credit scores to lenders. Fair Isaac is one of the major developers of credit scores used by these consumer reporting agencies. The complete way in which a consumer's FICO score is calculated is complex. One of the factors in a consumer's FICO score is credit checks on their credit history. When a lender requests a credit score, it can cause a small drop in the credit score. That is because, as stated above, a number of inquiries over a relatively short period of time can indicate the consumer is in a financially difficult situation.\n\n\n== Consequences ==\nThe information in a credit report is sold by credit agencies to organizations that are considering whether to offer credit to individuals or companies. It is also available to other entities with a \"permissible purpose\", as defined by the Fair Credit Reporting Act. The consequence of a negative credit rating is typically a reduction in the likelihood that a lender will approve an application for credit under favorable terms, if at all. Interest rates on loans are significantly affected by credit history; the higher the credit rating, the lower the interest, while the lower the credit rating, the higher the interest. The increased interest is used to offset the higher rate of default within the low credit rating group of individuals.\nIn the United States, insurance, housing, and employment can be denied based on a negative credit rating. A new study shows that employer credit checks on job seekers are preventing them from entering the working circle. Estimated figures indicate that one in four unemployed Americans have been required to go through a credit check when applying for a job. The size of this phenomenon has become a major concern of the US administration. Federal regulations require employers to receive permission from job candidates before running credit checks, but it will be impossible to enforce employer disclosure as to the reason for job denial.Note that it is not the credit reporting agencies that decide whether a credit history is \"adverse\". It is the individual lender or creditor which makes that decision; each lender has its own policy on what scores fall within their guidelines. The specific scores that fall within a lender's guidelines are most often not disclosed to the applicant due to competitive reasons. In the United States, a creditor is required to give the reasons for denying credit to an applicant immediately and must also provide the name and address of the credit reporting agency who provided data that was used to make the decision.\n\n\n== Abuse ==\nAstute consumers and criminal minded people have been able to identify and exploit vulnerabilities in the credit scoring systems to obtain credit. For example, previous ownership of a credit card may significantly increase an individual's ability to obtain further credit, while privacy issues may prevent a fraud from being exposed. Certain telecommunication companies and their relationship with credit reporting bureaus have enabled fabricated credit files to be created by the exploit of privacy blocks, which deny any third party entity to actual information held by the government. While the credit reporting system is designed to protect both lenders and borrowers, there are loopholes which can allow opportunistic individuals to abuse the system. A few of the motivations and techniques for credit abuse include churning, rapidfire credit applications, repetitive credit checks, selective credit freezes, applications for small business rather than personal credit, piggybacking and hacking, as it happened with Equifax in April and September 2017.Additionally, fraud can be committed on consumers by credit reporting agencies themselves. In 2013, Equifax and TransUnion were fined $23.3 million by the Consumer Financial Protection Bureau (U.S.) for deceiving customers about the cost of their services. Services advertised as $1 were actually billed at $200 per year.\n\n\n== See also ==\nAlternative data\nBusiness credit monitoring\nComparison of free credit monitoring services\nCredit bureau\nCredit card\nCredit score in the United States\nCredit zombie\nCriticism of credit scoring systems in the United States\nIdentity theft\nFair Credit Reporting Act\nFair and Accurate Credit Transactions Act\nFair Debt Collection Practices Act\nOffice of Fair Trading\nSeasoned tradeline\n\n\n== References ==\n\n\n== Further reading ==\nLauer, Josh (2017). Creditworthy: A History of Consumer Surveillance and Financial Identity in America. New York: Columbia University Press. ISBN 9780231168083. OCLC 980857936. Creditworthy: A History of Consumer Surveillance and Financial Identity in America at Google Books."}, {"id": 68, "title": "Battery electric vehicle", "content": "A battery electric vehicle (BEV), pure electric vehicle, only-electric vehicle, fully electric vehicle or all-electric vehicle is a type of electric vehicle (EV) that exclusively uses chemical energy stored in rechargeable battery packs, with no secondary source of propulsion (a hydrogen fuel cell, internal combustion engine, etc.). BEVs use electric motors and motor controllers instead of internal combustion engines (ICEs) for propulsion. They derive all power from battery packs and thus have no internal combustion engine, fuel cell, or fuel tank. BEVs include \u2013 but are not limited to  \u2013 motorcycles, bicycles, scooters, skateboards, railcars, watercraft, forklifts, buses, trucks, and cars.\nIn 2016, there were 210 million electric bikes worldwide used daily. Cumulative global sales of highway-capable light-duty pure electric car vehicles passed the one million unit milestone in September 2016. As of October 2020, the world's top selling all-electric car in history is the Tesla Model 3, with an estimated 645,000 sales, followed by the Nissan Leaf with over 500,000 sales as of September 2020.\n\n\n== History ==\n\nDuring the 1880s, Gustave Trouv\u00e9, Thomas Parker and Andreas Flocken built experimental electric cars, but the first practical battery electric vehicles appeared during the 1890s. Battery vehicle milk floats expanded in 1931, and by 1967, gave Britain the largest electric vehicle fleet in the world.\n\n\n== Terminology ==\n\nHybrid electric vehicles use both electric motors and internal combustion engines, and are not considered pure or all-electric vehicles.Hybrid electric vehicles whose batteries can be charged externally are called plug-in hybrid electric vehicles (PHEV) and run as BEVs during their charge-depleting mode. PHEVs with a series powertrain are also called range-extended electric vehicles (REEVs), such as the Chevrolet Volt and Fisker Karma.\nPlug-in electric vehicles (PEVs) are a subcategory of electric vehicles that includes battery electric vehicles (BEVs) and plug-in hybrid vehicles (PHEVs).\nThe electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles (aka all-combustion vehicles) belong to one of the two categories.In China, plug-in electric vehicles, together with hybrid electric vehicles are called new energy vehicles (NEVs). However, in the United States, neighborhood electric vehicles (NEVs) are battery electric vehicles that are legally limited to roads with posted speed limits no higher than 45 miles per hour (72 km/h), are usually built to have a top speed of 30 miles per hour (48 km/h), and have a maximum loaded weight of 3,000 pounds (1,400 kg).\n\n\n== Vehicles by type ==\nThe concept of battery electric vehicles is to use charged batteries on board vehicles for propulsion. Battery electric cars are becoming more and more attractive with the higher oil prices and the advancement of new battery technology (lithium-ion) that have higher power and energy density (i.e., greater possible acceleration and more range with fewer batteries). Compared to older battery types such as lead-acid batteries. Lithium-ion batteries for example now have an energy density of 0.9\u20132.63 MJ/L whereas lead-acid batteries had an energy density of 0.36 MJ/L (so 2.5 to 7.3x higher). There is still a long way to go if comparing it to petroleum-based fuels and biofuels, however (gasoline having an energy density of 34.2 MJ/L -38x to 12.92x higher- and ethanol having an energy of 24 MJ/L -26x to 9.12x higher-). This is partially offset by higher conversion efficiency of electric motors \u2013 BEVs travel roughly 3x further than similar-size internal combustion vehicles per MJ of stored energy.\nBEVs include automobiles, light trucks, and neighborhood electric vehicles.\n\n\n=== Rail ===\nBattery electric railcars:Battery electric trains in the form of BEMUs (battery electric multiple units) are operated commercially in Japan. They are charged via pantographs, either when driving on electrified railway lines or during stops at specially equipped train stations. They use battery power for propulsion when driving on railway lines that are not electrified, and have successfully replaced diesel multiple units on some such lines.\nOther countries have also tested or ordered such vehicles.\n\nLocomotives:\nElectric rail trolley: \n\n\n=== Electric bus ===\n\nChattanooga, Tennessee, operates nine zero-fare electric buses, which have been in operation since 1992 and have carried 11.3 million passengers and covered a distance of 3,100,000 kilometres (1,900,000 mi). They were made locally by Advanced Vehicle Systems. Two of these buses were used for the 1996 Summer Olympics in Atlanta.Beginning in the summer of 2000, Hong Kong Airport began operating a 16-passenger Mitsubishi Rosa electric shuttle bus, and in the fall of 2000, New York City began testing a 66-passenger battery-powered school bus, an all-electric version of the Blue Bird TC/2000. A similar bus was operated in Napa Valley, California, for 14 months ending in April 2004.The 2008 Beijing Olympics used a fleet of 50 electric buses, which have a range of 130 km (81 mi) with the air conditioning on. They use lithium-ion batteries, and consume about 1 kW\u22c5h/mi (0.62 kW\u22c5h/km; 2.2 MJ/km). The buses were designed by the Beijing Institute of Technology and built by the Jinghua Coach. The batteries are replaced with fully charged ones at the recharging station to allow 24-hour operation of the buses.In France, the electric bus phenomenon is in development, but some buses are already operating in numerous cities. PVI, a medium-sized company located in the Paris region, is one of the leaders of the market with its brand Gepebus (offering Oreos 2X and Oreos 4X).In the United States, the first battery-electric, fast-charge bus has been in operation in Pomona, California, since September 2010 at Foothill Transit. The Proterra EcoRide BE35 uses lithium-titanate batteries and is able to fast-charge in less than 10 minutes.In 2012, heavy-duty trucks and buses contributed 7% of global warming emissions in California.In 2014, the first production model all-electric school bus was delivered to the Kings Canyon Unified School District in California's San Joaquin Valley. The bus was one of four the district ordered. This battery-electric school bus, which has four sodium nickel batteries, is the first modern electric school bus approved for student transportation by any state.In 2016, including the light heavy-duty vehicles, there were roughly 1.5 million heavy-duty vehicles in California.\nThe same technology is used to power the Mountain View Community Shuttles. This technology was supported by the California Energy Commission, and the shuttle program is being supported by Google.\n\n\n==== Thunder Sky ====\nThunder Sky (based in Hong Kong) builds lithium-ion batteries used in submarines and has three models of electric buses, the 10/21 passenger EV-6700 with a range of 280 km (170 mi) under 20 mins quick-charge, the EV-2009 city buses, and the 43 passenger EV-2008 highway bus, which has a range of 300 km (190 mi) under quick-charge (20 mins to 80 percent), and 350 km (220 mi) under full charge (25 mins). The buses will also be built in the United States and Finland.\n\n\n==== Free Tindo ====\nTindo is an all-electric bus from Adelaide, Australia. The Tindo (aboriginal word for sun) is made by Designline International in New Zealand and gets its electricity from a solar PV system on Adelaide's central bus station. Rides are zero-fare as part of Adelaide's public transport system.\n\n\n==== First Fast-Charge, Battery-Electric Transit Bus ====\nProterra's EcoRide BE35 transit bus, called the Ecoliner by Foothill Transit in West Covina, California, is a heavy-duty, fast charge, battery-electric bus. Proterra's ProDrive drive-system uses a UQM motor and regenerative braking that captures 90 percent of the available energy and returns it to the TerraVolt energy storage system, which in turn increases the total distance the bus can drive by 31\u201335 percent. It can travel 30\u201340 miles (48\u201364 km) on a single charge, is up to 600 percent more fuel-efficient than a typical diesel or CNG bus, and produces 44 percent less carbon than CNG. Proterra buses have had several problems, most notably in Philadelphia where the entire fleet was removed from service.\n\n\n=== Electric trucks ===\n\nFor most of the 20th century, the majority of the world's battery electric road vehicles were British milk floats. The 21st century saw the massive development of BYD electric trucks.\n\n\n=== Electric vans ===\n\nIn March 2012, Smith Electric Vehicles announced the release of the Newton Step-Van, an all-electric, zero-emission vehicle built on the versatile Newton platform that features a walk-in body produced by Indiana-based Utilimaster.BYD supplies DHL with electric distribution fleet of commercial BYD T3.\n\n\n=== Electric cars ===\n\nA battery-powered electric car is an automobile which is propelled by electric motors.\nAlthough electric cars often give good acceleration and have generally acceptable top speed, the lower specific energy of production batteries available in 2015 compared with carbon-based fuels means that electric cars need batteries that are a fairly large fraction of the vehicle mass but still often give a relatively low range between charges. Recharging can also take significant lengths of time. For journeys within a single battery charge, rather than long journeys, electric cars are practical forms of transportation and can be recharged overnight.\nElectric cars can significantly reduce city pollution by having zero emissions. Vehicle greenhouse gas savings depend on how the electricity is generated.Electric cars are having a major impact in the auto industry given advantages in city pollution, less dependence on oil and combustion, and scarcity and expected rise in gasoline prices. World governments are pledging billions to fund development of electric vehicles and their components.Formula E is a fully electric international single-seater championship. The series was conceived in 2012, and the inaugural championship started in Beijing on 13 September 2014. The series is sanctioned by the FIA. Alejandro Agag is the current CEO of Formula E.\nThe Formula E championship is currently contested by ten teams with two drivers each (after the withdrawal of Team Trulli, there are temporarily only nine teams competing). Racing generally takes place on temporary city-center street circuits which are approximately 2 to 3.4 kilometres (1.2 to 2.1 mi) long. Currently, only the Mexico City ePrix takes place on a road course, a modified version of the Aut\u00f3dromo Hermanos Rodr\u00edguez.\n\n\n=== Special-purpose vehicles ===\n\nSpecial-purpose vehicles come in a wide range of types, ranging from relatively common ones such as golf carts, things like electric golf trolleys, milk floats, all-terrain vehicles, neighborhood electric vehicles, and a wide range of other devices. Certain manufacturers specialize in electric-powered \"in plant\" work machines.\n\n\n=== Electric motorcycles, scooters and rickshaws ===\n\nThree-wheeled vehicles include electric rickshaws, a powered variant of the cycle rickshaw. The large-scale adoption of electric two-wheelers can reduce traffic noise and road congestion but may necessitate adaptations of the existing urban infrastructure and safety regulations.Ather Energy from India has launched their BLDC motor powered Ather 450 electric scooter with Lithium Ion batteries in 2018. Also from India, AVERA  \u2013 a new and renewable energy company is going to launch two models of electric scooters at the end of 2018, with Lithium Iron Phosphate Battery technology.\n\n\n=== Electric bicycles ===\n\nIndia is the world\u2019s biggest market for bicycles at 22 million units per year. By 2024, electric two-wheelers will be a $2 billion market with over 3 million units being sold in India.The Indian government is launching schemes and incentives to promote the adoption of electric vehicles in the country, and is aiming to be a manufacturing hub for electric vehicles within the next five years.China has experienced an explosive growth of sales of non-assisted e-bikes including the scooter type, with annual sales jumping from 56,000 units in 1998 to over 21 million in 2008, and reaching an estimated 120 million e-bikes on the road in early 2010. China is the world's leading manufacturer of e-bikes, with 22.2 million units produced in 2009.\n\n\n=== Personal transporters ===\nAn increasing variety of personal transporters are being manufactured, including the one-wheeled self-balancing unicycles, self-balancing scooters, electric kick scooters, and electric skateboards.\n\n\n=== Electric boats ===\n\nSeveral battery electric ships operate throughout the world, some for business. Electric ferries are being operated and constructed.\n\n\n== Technology ==\n\n\n=== Motor controllers ===\n\nThe motor controller receives a signal from potentiometers linked to the accelerator pedal, and it uses this signal to determine how much electric power is needed. This DC power is supplied by the battery pack, and the controller regulates the power to the motor, supplying either variable pulse width DC or variable frequency variable amplitude AC, depending on the motor type. The controller also handles regenerative braking, whereby electrical power is gathered as the vehicle slows down and this power recharges the battery. In addition to power and motor management, the controller performs various safety checks such as anomaly detection, functional safety tests and failure diagnostics.\n\n\n=== Battery pack ===\n\nMost electric vehicles today use an electric battery, consisting of electrochemical cells with external connections in order to provide power to the vehicle.Battery technology for EVs has developed from early lead-acid batteries used in the late 19th century to the 2010s, to lithium-ion batteries which are found in most EVs today. The overall battery is referred to as a battery pack, which is a group of multiple battery modules and cells. For example, the Tesla Model S battery pack has up to 7,104 cells, split into 16 modules with 6 groups of 74 cells in each. Each cell has a nominal voltage of 3\u20134 volts, depending on its chemical composition.\n\n\n=== Motors ===\n\nElectric cars have traditionally used series wound DC motors, a form of brushed DC electric motor. Separately excited and permanent magnet are just two of the types of DC motors available. More recent electric vehicles have made use of a variety of AC motor types, as these are simpler to build and have no brushes that can wear out. These are usually induction motors or brushless AC electric motors which use permanent magnets. There are several variations of the permanent magnet motor which offer simpler drive schemes and/or lower cost including the brushless DC electric motor.\nOnce electric power is supplied to the motor (from the controller), the magnetic field interaction inside the motor will turn the drive shaft and ultimately the vehicle's wheels.\n\n\n== Economy ==\nEV battery storage is a key element for the global energy transition which is dependent on more electricity storage right now. As energy availability is the most important factor for the vitality of an economy the mobile storage infrastructure of EV batteries can be seen as one of the most meaningful infrastructure projects facilitating the energy transition to a fully sustainable economy based on renewables. A meta-study graphically showing the importance of electricity storage depicts the technology in context.\n\n\n== Environmental impact ==\n\n\n=== Power generation ===\nElectric vehicles produce no greenhouse gas (GHG) emissions in operation, but the electricity used to power them may do so in its generation. The two factors driving the emissions of battery electric vehicles are the carbon intensity of the electricity used to recharge the Electric Vehicle (commonly expressed in grams of CO2 per kWh) and the consumption of the specific vehicle (in kilometers/kWh).\nThe carbon intensity of electricity varies depending on the source of electricity where it is consumed. A country with a high share of renewable energy in its electricity mix will have a low C.I. In the European Union, in 2013, the carbon intensity had a strong geographic variability but in most of the member states, electric vehicles were \"greener\" than conventional ones. On average, electric cars saved 50\u201360% of CO2 emissions compared to diesel and gasoline fuelled engines.\nMoreover, the de-carbonisation process is constantly reducing the GHG emissions due to the use of electric vehicles. In the European Union, on average, between 2009 and 2013 there was a reduction in the electricity carbon intensity of 17%. In a life-cycle assessment perspective, considering the GHG necessary to build the battery and its end-of-life, the GHG savings are 10\u201313% lower.The open source VencoPy model framework can be used to study the interactions between vehicles, owners, and the electricity system at large.\n\n\n=== Vehicle construction ===\nGHGs are also emitted when the electric vehicle is being manufactured. The lithium-ion batteries used in the vehicle take more materials and energy to produce because of the extraction process of the lithium and cobalt essential to the battery. This means the bigger the electric vehicle, the more carbon dioxide emitted. The same size-to-emission relationship applies to manufacturing of all products.\nThe mines that are used to produce the lithium and cobalt used in the battery are also creating problems for the environment, as fish are dying up to 240 km (150 mi) downstream from mining operations due to chemical leaks and the chemicals also leak into the water sources the people that live near the mines use, creating health problems for the animals and people that live nearby.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nWitkin, Jim. Building Better Batteries for Electric Cars, The New York Times, 31 March 2011, p. F4. Published online 30 March 2011. Discusses rechargeable batteries and the new-technology lithium ion battery.\n\n\n== External links ==\nPatentsU.S. Patent 523,354, Emil E. Keller, Electrically Propelled Perambulator, 1894\nU.S. Patent 594,805, Hiram Percy Maxim, Motor vehicle, 1897\nU.S. Patent 772,571, Hiram Percy Maxim et al., Electric motor vehicle, 1904OrganizationsBattery Vehicle Society (UK)\nZap-Map \u2013 the UK national directory of recharging points.\nThe European Association for Battery, Hybrid and Fuel Cell Electric Vehicles (AVERE).\nCzech EV Club \u2013 (CZ) Eng. section in photogallery.\nAlternative Technology Association Electric Vehicle Interest Group.\nAustralian Electric Vehicle Association.\nElectric Car Society.NewsReasons to buy an electric car in 2013\nAeroVironment Awarded U.S. Patent For Electric Vehicle Energy Data Management And Control (Green Car Congress)\nSolar charging station for Ford Focus Electric VehicleStudies\"Application of Life-Cycle Assessment to Nanoscale Technology: Lithium-ion Batteries for Electric Vehicles\" (PDF). U.S. Environmental Protection Agency. 13 April 2013. Archived from the original (PDF) on 2 December 2013. Retrieved 3 April 2018.\n\"Hybrid and Electric Vehicles \u2013 The Electric Drive Gains Traction\" (PDF). International Energy Agency (IEA). May 2013. Archived from the original (PDF) on 26 February 2021. Retrieved 3 April 2018.\nLee, Henry; Lovellette, Grant (July 2011). \"Will Electric Cars Transform the U.S. Vehicle Market?\" (PDF). Belfer Center for Science and International Affairs, Harvard University. Archived from the original (PDF) on 1 August 2011. Retrieved 3 April 2018."}, {"id": 69, "title": "Solar cell", "content": "A solar cell or photovoltaic cell (PV cell) is an electronic device that converts the energy of light directly into electricity by means of the photovoltaic effect. It is a form of photoelectric cell, a device whose electrical characteristics (such as current, voltage, or resistance) vary when exposed to light. Individual solar cell devices are often the electrical building blocks of photovoltaic modules, known colloquially as \"solar panels\". The common single-junction silicon solar cell can produce a maximum open-circuit voltage of approximately 0.5 to 0.6 volts.Photovoltaic cells may operate under sunlight or artificial light. In addition to producing energy, they can be used as a photodetector (for example infrared detectors), detecting light or other electromagnetic radiation near the visible range, or measuring light intensity.\nThe operation of a PV cell requires three basic attributes:\n\nThe absorption of light, generating excitons (bound electron-hole pairs), unbound electron-hole pairs (via excitons), or plasmons.\nThe separation of charge carriers of opposite types.\nThe separate extraction of those carriers to an external circuit.In contrast, a solar thermal collector supplies heat by absorbing sunlight, for the purpose of either direct heating or indirect electrical power generation from heat. A \"photoelectrolytic cell\" (photoelectrochemical cell), on the other hand, refers either to a type of photovoltaic cell (like that developed by Edmond Becquerel and modern dye-sensitized solar cells), or to a device that splits water directly into hydrogen and oxygen using only solar illumination.\nPhotovoltaic cells and solar collectors are the two means of producing solar power.\n\n\n== Applications ==\nAssemblies of solar cells are used to make solar modules that generate electrical power from sunlight, as distinguished from a \"solar thermal module\" or \"solar hot water panel\". A solar array generates solar power using solar energy.\n\n\n=== Vehicular applications ===\nApplication of solar cells as an alternative energy source for vehicular applications is a growing industry. Electric vehicles that operate off of solar energy and/or sunlight are commonly referred to as solar cars. These vehicles use solar panels to convert absorbed light into electrical energy that is then stored in batteries. There are multiple input factors that affect the output power of solar cells such as temperature, material properties, weather conditions, solar irradiance and more.The first instance of photovoltaic cells within vehicular applications was around midway through the second half of the 1900's. In an effort to increase publicity and awareness in solar powered transportation Hans Tholstrup decided to set up the first edition of the World Solar Challenge in 1987. It was a 3000 km race across the Australian outback where competitors from industry research groups and top universities around the globe were invited to compete. General Motors ended up winning the event by a significant margin with their Sunraycer vehicle that achieved speeds of over 40 mph. Contrary to popular belief however solar powered cars are one of the oldest alternative energy vehicles.Current solar vehicles harness energy from the Sun via Solar panels which are a collected group of solar cells working in tandem towards a common goal. These solid-state devices use quantum mechanical transitions in order to convert a given amount of solar power into electrical power. The electricity produced as a result is then stored in the vehicle's battery in order to run the motor of the vehicle. Batteries in solar-powered vehicles differ from those in standard ICE cars because they are fashioned in a way to impart more power towards the electrical components of the vehicle for a longer duration.\n\n\n=== Cells, modules, panels and systems ===\n\nMultiple solar cells in an integrated group, all oriented in one plane, constitute a solar photovoltaic panel or module. Photovoltaic modules often have a sheet of glass on the sun-facing side, allowing light to pass while protecting the semiconductor wafers. Solar cells are usually connected in series creating additive voltage. Connecting cells in parallel yields a higher current.\nHowever, problems in paralleled cells such as shadow effects can shut down the weaker (less illuminated) parallel string (a number of series connected cells) causing substantial power loss and possible damage because of the reverse bias applied to the shadowed cells by their illuminated partners.Although modules can be interconnected to create an array with the desired peak DC voltage and loading current capacity, which can be done with or without using independent MPPTs (maximum power point trackers) or, specific to each module, with or without module level power electronic (MLPE) units such as microinverters or DC-DC optimizers. Shunt diodes can reduce shadowing power loss in arrays with series/parallel connected cells.\n\nBy 2020, the United States cost per watt for a utility scale system had declined to $0.94.\n\n\n== History ==\n\nThe photovoltaic effect was experimentally demonstrated first by French physicist Edmond Becquerel. In 1839, at age 19, he built the world's first photovoltaic cell in his father's laboratory. Willoughby Smith first described the \"Effect of Light on Selenium during the passage of an Electric Current\" in a 20 February 1873 issue of Nature. In 1883 Charles Fritts built the first solid state photovoltaic cell by coating the semiconductor selenium with a thin layer of gold to form the junctions; the device was only around 1% efficient. Other milestones include:\n\n1888 \u2013 Russian physicist Aleksandr Stoletov built the first cell based on the outer photoelectric effect discovered by Heinrich Hertz in 1887.\n1904 \u2013 Julius Elster, together with Hans Friedrich Geitel, devised the first practical photoelectric cell.\n1905 \u2013 Albert Einstein proposed a new quantum theory of light and explained the photoelectric effect in a landmark paper, for which he received the Nobel Prize in Physics in 1921.\n1941 \u2013 Vadim Lashkaryov discovered p-n-junctions in Cu2O and Ag2S protocells.\n1946 \u2013 Russell Ohl patented the modern junction semiconductor solar cell, while working on the series of advances that would lead to the transistor.\n1948 - Introduction to the World of Semiconductors states Kurt Lehovec may have been the first to explain the photo-voltaic effect in the peer reviewed journal Physical Review.\n1954 \u2013 The first practical photovoltaic cell was publicly demonstrated at Bell Laboratories. The inventors were Calvin Souther Fuller, Daryl Chapin and Gerald Pearson.\n1958 \u2013 Solar cells gained prominence with their incorporation onto the Vanguard I satellite.\n\n\n=== Space applications ===\n\nSolar cells were first used in a prominent application when they were proposed and flown on the Vanguard satellite in 1958, as an alternative power source to the primary battery power source. By adding cells to the outside of the body, the mission time could be extended with no major changes to the spacecraft or its power systems. In 1959 the United States launched Explorer 6, featuring large wing-shaped solar arrays, which became a common feature in satellites. These arrays consisted of 9600 Hoffman solar cells.\nBy the 1960s, solar cells were (and still are) the main power source for most Earth orbiting satellites and a number of probes into the solar system, since they offered the best power-to-weight ratio. However, this success was possible because in the space application, power system costs could be high, because space users had few other power options, and were willing to pay for the best possible cells. The space power market drove the development of higher efficiencies in solar cells up until the National Science Foundation \"Research Applied to National Needs\" program began to push development of solar cells for terrestrial applications.\nIn the early 1990s the technology used for space solar cells diverged from the silicon technology used for terrestrial panels, with the spacecraft application shifting to gallium arsenide-based III-V semiconductor materials, which then evolved into the modern III-V multijunction photovoltaic cell used on spacecraft.\nIn recent years, research has moved towards designing and manufacturing lightweight, flexible, and highly efficient solar cells. Terrestrial solar cell technology generally uses photovoltaic cells that are laminated with a layer of glass for strength and protection. Space applications for solar cells require that the cells and arrays are both highly efficient and extremely lightweight. Some newer technology implemented on satellites are multi-junction photovoltaic cells, which are composed of different PN junctions with varying bandgaps in order to utilize a wider spectrum of the sun's energy. Additionally, large satellites require the use of large solar arrays to produce electricity. These solar arrays need to be broken down to fit in the geometric constraints of the launch vehicle the satellite travels on before being injected into orbit. Historically, solar cells on satellites consisted of several small terrestrial panels folded together. These small panels would be unfolded into a large panel after the satellite is deployed in its orbit. Newer satellites aim to use flexible rollable solar arrays that are very lightweight and can be packed into a very small volume. The smaller size and weight of these flexible arrays drastically decreases the overall cost of launching a satellite due to the direct relationship between payload weight and launch cost of a launch vehicle.In 2020, the US Naval Research Laboratory conducted its first test of solar power generation in a satellite, the Photovoltaic Radio-frequency Antenna Module (PRAM) experiment aboard the Boeing X-37.\n\n\n=== Improved manufacturing methods ===\nImprovements were gradual over the 1960s. This was also the reason that costs remained high, because space users were willing to pay for the best possible cells, leaving no reason to invest in lower-cost, less-efficient solutions. The price was determined largely by the semiconductor industry; their move to integrated circuits in the 1960s led to the availability of larger boules at lower relative prices. As their price fell, the price of the resulting cells did as well. These effects lowered 1971 cell costs to some $100 per watt.In late 1969 Elliot Berman joined Exxon's task force which was looking for projects 30 years in the future and in April 1973 he founded Solar Power Corporation (SPC), a wholly owned subsidiary of Exxon at that time. The group had concluded that electrical power would be much more expensive by 2000, and felt that this increase in price would make alternative energy sources more attractive. He conducted a market study and concluded that a price per watt of about $20/watt would create significant demand. The team eliminated the steps of polishing the wafers and coating them with an anti-reflective layer, relying on the rough-sawn wafer surface. The team also replaced the expensive materials and hand wiring used in space applications with a printed circuit board on the back, acrylic plastic on the front, and silicone glue between the two, \"potting\" the cells. Solar cells could be made using cast-off material from the electronics market. By 1973 they announced a product, and SPC convinced Tideland Signal to use its panels to power navigational buoys, initially for the U.S. Coast Guard.\n\n\n=== Research and industrial production ===\nResearch into solar power for terrestrial applications became prominent with the U.S. National Science Foundation's Advanced Solar Energy Research and Development Division within the \"Research Applied to National Needs\" program, which ran from 1969 to 1977, and funded research on developing solar power for ground electrical power systems. A 1973 conference, the \"Cherry Hill Conference\", set forth the technology goals required to achieve this goal and outlined an ambitious project for achieving them, kicking off an applied research program that would be ongoing for several decades. The program was eventually taken over by the Energy Research and Development Administration (ERDA), which was later merged into the U.S. Department of Energy.\nFollowing the 1973 oil crisis, oil companies used their higher profits to start (or buy) solar firms, and were for decades the largest producers. Exxon, ARCO, Shell, Amoco (later purchased by BP) and Mobil all had major solar divisions during the 1970s and 1980s. Technology companies also participated, including General Electric, Motorola, IBM, Tyco and RCA.\n\n\n== Declining costs and exponential growth ==\n\nAdjusting for inflation, it cost $96 per watt for a solar module in the mid-1970s. Process improvements and a very large boost in production have brought that figure down more than 99%, to 30\u00a2 per watt in 2018 \n\nand as low as 20\u00a2 per watt in 2020.\nSwanson's law is an observation similar to Moore's Law that states that solar cell prices fall 20% for every doubling of industry capacity. It was featured in an article in the British weekly newspaper The Economist in late 2012. Balance of system costs were then higher than those of the panels. Large commercial arrays could be built, as of 2018, at below $1.00 a watt, fully commissioned.As the semiconductor industry moved to ever-larger boules, older equipment became inexpensive. Cell sizes grew as equipment became available on the surplus market; ARCO Solar's original panels used cells 2 to 4 inches (50 to 100 mm) in diameter. Panels in the 1990s and early 2000s generally used 125 mm wafers; since 2008, almost all new panels use 156 mm cells. The widespread introduction of flat screen televisions in the late 1990s and early 2000s led to the wide availability of large, high-quality glass sheets to cover the panels.\nDuring the 1990s, polysilicon (\"poly\") cells became increasingly popular. These cells offer less efficiency than their monosilicon (\"mono\") counterparts, but they are grown in large vats that reduce cost. By the mid-2000s, poly was dominant in the low-cost panel market, but more recently the mono returned to widespread use.\nManufacturers of wafer-based cells responded to high silicon prices in 2004\u20132008 with rapid reductions in silicon consumption. In 2008, according to Jef Poortmans, director of IMEC's organic and solar department, current cells use 8\u20139 grams (0.28\u20130.32 oz) of silicon per watt of power generation, with wafer thicknesses in the neighborhood of 200 microns. Crystalline silicon panels dominate worldwide markets and are mostly manufactured in China and Taiwan. By late 2011, a drop in European demand dropped prices for crystalline solar modules to about $1.09 per watt down sharply from 2010. Prices continued to fall in 2012, reaching $0.62/watt by 4Q2012.Solar PV is growing fastest in Asia, with China and Japan currently accounting for half of worldwide deployment. Global installed PV capacity reached at least 301 gigawatts in 2016, and grew to supply 1.3% of global power by 2016.It was anticipated that electricity from PV will be competitive with wholesale electricity costs all across Europe and the energy payback time of crystalline silicon modules can be reduced to below 0.5 years by 2020.Falling costs are considered one of the biggest factors in the rapid growth of renewable energy, with the cost of solar photovoltaic electricity falling by ~85% between 2010 (when solar and wind made up 1.7% of global electricity generation) and 2021 (where they made up 8.7%). In 2019 solar cells accounted for ~3 % of the world's electricity generation.\n\n\n=== Subsidies and grid parity ===\nSolar-specific feed-in tariffs vary by country and within countries. Such tariffs encourage the development of solar power projects. Widespread grid parity, the point at which photovoltaic electricity is equal to or cheaper than grid power without subsidies, likely requires advances on all three fronts. Proponents of solar hope to achieve grid parity first in areas with abundant sun and high electricity costs such as in California and Japan. In 2007 BP claimed grid parity for Hawaii and other islands that otherwise use diesel fuel to produce electricity. George W. Bush set 2015 as the date for grid parity in the US. The Photovoltaic Association reported in 2012 that Australia had reached grid parity (ignoring feed in tariffs).The price of solar panels fell steadily for 40 years, interrupted in 2004 when high subsidies in Germany drastically increased demand there and greatly increased the price of purified silicon (which is used in computer chips as well as solar panels). The recession of 2008 and the onset of Chinese manufacturing caused prices to resume their decline. In the four years after January 2008 prices for solar modules in Germany dropped from \u20ac3 to \u20ac1 per peak watt. During that same time production capacity surged with an annual growth of more than 50%. China increased market share from 8% in 2008 to over 55% in the last quarter of 2010. In December 2012 the price of Chinese solar panels had dropped to $0.60/Wp (crystalline modules). (The abbreviation Wp stands for watt peak capacity, or the maximum capacity under optimal conditions.)\nAs of the end of 2016, it was reported that spot prices for assembled solar panels (not cells) had fallen to a record-low of US$0.36/Wp. The second largest supplier, Canadian Solar Inc., had reported costs of US$0.37/Wp in the third quarter of 2016, having dropped $0.02 from the previous quarter, and hence was probably still at least breaking even. Many producers expected costs would drop to the vicinity of $0.30 by the end of 2017. It was also reported that new solar installations were cheaper than coal-based thermal power plants in some regions of the world, and this was expected to be the case in most of the world within a decade.\n\n\n== Theory ==\n\nA solar cell is made of semiconducting materials, such as silicon, that have been fabricated into a p\u2013n junction. Such junctions are made by doping one side of the device p-type and the other n-type, for example in the case of silicon by introducing small concentrations of boron or phosphorus respectively.\nIn operation, photons in sunlight hit the solar cell and are absorbed by the semiconductor.  When the photons are absorbed, electrons are excited from the valence band to the conduction band (or from occupied to unoccupied molecular orbitals in the case of an organic solar cell), producing electron-hole pairs. \nIf the electron-hole pairs are created near the junction between p-type and n-type materials the local electric field sweeps them apart to opposite electrodes, producing an excess of electrons on one side and an excess of holes on the other. When the solar cell is unconnected (or the external electrical load is very high) the electrons and holes will ultimately restore equilibrium by diffusing back across the junction against the field and recombine with each other giving off heat, but if the load is small enough then it is easier for equilibrium to be restored by the excess electrons going around the external circuit, doing useful work along the way.\nAn array of solar cells converts solar energy into a usable amount of direct current (DC) electricity. An inverter can convert the power to alternating current (AC).\nThe most commonly known solar cell is configured as a large-area p\u2013n junction made from silicon. Other possible solar cell types are organic solar cells, dye sensitized solar cells, perovskite solar cells, quantum dot solar cells etc. The illuminated side of a solar cell generally has a transparent conducting film for allowing light to enter into the active material and to collect the generated charge carriers. Typically, films with high transmittance and high electrical conductance such as indium tin oxide, conducting polymers or conducting nanowire networks are used for the purpose.\n\n\n== Efficiency ==\n\nSolar cell efficiency may be broken down into reflectance efficiency, thermodynamic efficiency, charge carrier separation efficiency and conductive efficiency. The overall efficiency is the product of these individual metrics.\nThe power conversion efficiency of a solar cell is a parameter which is defined by the fraction of incident power converted into electricity.A solar cell has a voltage dependent efficiency curve, temperature coefficients, and allowable shadow angles.\nDue to the difficulty in measuring these parameters directly, other parameters are substituted: thermodynamic efficiency, quantum efficiency, integrated quantum efficiency, VOC ratio, and fill factor. Reflectance losses are a portion of quantum efficiency under \"external quantum efficiency\". Recombination losses make up another portion of quantum efficiency, VOC ratio, and fill factor. Resistive losses are predominantly categorized under fill factor, but also make up minor portions of quantum efficiency, VOC ratio.\nThe fill factor is the ratio of the actual maximum obtainable power to the product of the open-circuit voltage and short-circuit current. This is a key parameter in evaluating performance. In 2009, typical commercial solar cells had a fill factor > 0.70. Grade B cells were usually between 0.4 and 0.7. Cells with a high fill factor have a low equivalent series resistance and a high equivalent shunt resistance, so less of the current produced by the cell is dissipated in internal losses.\nSingle p\u2013n junction crystalline silicon devices are now approaching the theoretical limiting power efficiency of 33.16%, noted as the Shockley\u2013Queisser limit in 1961. In the extreme, with an infinite number of layers, the corresponding limit is 86% using concentrated sunlight.\nIn 2014, three companies broke the record of 25.6% for a silicon solar cell. Panasonic's was the most efficient. The company moved the front contacts to the rear of the panel, eliminating shaded areas. In addition they applied thin silicon films to the (high quality silicon) wafer's front and back to eliminate defects at or near the wafer surface.In 2015, a 4-junction GaInP/GaAs//GaInAsP/GaInAs solar cell achieved a new laboratory record efficiency of 46.1% (concentration ratio of sunlight = 312) in a French-German collaboration between the Fraunhofer Institute for Solar Energy Systems (Fraunhofer ISE), CEA-LETI and SOITEC.In September 2015, Fraunhofer ISE announced the achievement of an efficiency above 20% for epitaxial wafer cells. The work on optimizing the atmospheric-pressure chemical vapor deposition (APCVD) in-line production chain was done in collaboration with NexWafe GmbH, a company spun off from Fraunhofer ISE to commercialize production.For triple-junction thin-film solar cells, the world record is 13.6%, set in June 2015.In 2016, researchers at Fraunhofer ISE announced a GaInP/GaAs/Si triple-junction solar cell with two terminals reaching 30.2% efficiency without concentration.In 2017, a team of researchers at National Renewable Energy Laboratory (NREL), EPFL and CSEM (Switzerland) reported record one-sun efficiencies of 32.8% for dual-junction GaInP/GaAs solar cell devices. In addition, the dual-junction device was mechanically stacked with a Si solar cell, to achieve a record one-sun efficiency of 35.9% for triple-junction solar cells.\n\n\n== Materials ==\nSolar cells are typically named after the semiconducting material they are made of. These materials must have certain characteristics in order to absorb sunlight. Some cells are designed to handle sunlight that reaches the Earth's surface, while others are optimized for use in space. Solar cells can be made of a single layer of light-absorbing material (single-junction) or use multiple physical configurations (multi-junctions) to take advantage of various absorption and charge separation mechanisms.\nSolar cells can be classified into first, second and third generation cells. The first generation cells\u2014also called conventional, traditional or wafer-based cells\u2014are made of crystalline silicon, the commercially predominant PV technology, that includes materials such as polysilicon and monocrystalline silicon. Second generation cells are thin film solar cells, that include amorphous silicon, CdTe and CIGS cells and are commercially significant in utility-scale photovoltaic power stations, building integrated photovoltaics or in small stand-alone power system. The third generation of solar cells includes a number of thin-film technologies often described as emerging photovoltaics\u2014most of them have not yet been commercially applied and are still in the research or development phase. Many use organic materials, often organometallic compounds as well as inorganic substances. Despite the fact that their efficiencies had been low and the stability of the absorber material was often too short for commercial applications, there is research into these technologies as they promise to achieve the goal of producing low-cost, high-efficiency solar cells. As of 2016, the most popular and efficient solar cells were those made from thin wafers of silicon which are also the oldest solar cell technology.\n\n\n=== Crystalline silicon ===\n\nBy far, the most prevalent bulk material for solar cells is crystalline silicon (c-Si), also known as \"solar grade silicon\".  Bulk silicon is separated into multiple categories according to crystallinity and crystal size in the resulting ingot, ribbon or wafer. These cells are entirely based around the concept of a p\u2013n junction. Solar cells made of c-Si are made from wafers between 160 and 240 micrometers thick.\n\n\n==== Monocrystalline silicon ====\n\nMonocrystalline silicon (mono-Si) solar cells feature a single-crystal composition that enables electrons to move more freely than in a multi-crystal configuration. Consequently, monocrystalline solar panels deliver a higher efficiency than their multicrystalline counterparts. The corners of the cells look clipped, like an octagon, because the wafer material is cut from cylindrical ingots, that are typically grown by the Czochralski process. Solar panels using mono-Si cells display a distinctive pattern of small white diamonds.\n\n\n==== Epitaxial silicon development ====\nEpitaxial wafers of crystalline silicon can be grown on a monocrystalline silicon \"seed\" wafer by chemical vapor deposition (CVD), and then detached as self-supporting wafers of some standard thickness (e.g., 250 \u00b5m) that can be manipulated by hand, and directly substituted for wafer cells cut from monocrystalline silicon ingots. Solar cells made with this \"kerfless\" technique can have efficiencies approaching those of wafer-cut cells, but at appreciably lower cost if the CVD can be done at atmospheric pressure in a high-throughput inline process. The surface of epitaxial wafers may be textured to enhance light absorption.In June 2015, it was reported that heterojunction solar cells grown epitaxially on n-type monocrystalline silicon wafers had reached an efficiency of 22.5% over a total cell area of 243.4 cm\n  \n    \n      \n        \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle ^{2}}\n  .\n\n\n==== Polycrystalline silicon ====\n\nPolycrystalline silicon, or multicrystalline silicon (multi-Si) cells are made from cast square ingots\u2014large blocks of molten silicon carefully cooled and solidified. They consist of small crystals giving the material its typical metal flake effect. Polysilicon cells are the most common type used in photovoltaics and are less expensive, but also less efficient, than those made from monocrystalline silicon.\n\n\n==== Ribbon silicon ====\nRibbon silicon is a type of polycrystalline silicon\u2014it is formed by drawing flat thin films from molten silicon and results in a polycrystalline structure. These cells are cheaper to make than multi-Si, due to a great reduction in silicon waste, as this approach does not require sawing from ingots. However, they are also less efficient.\n\n\n==== Mono-like-multi silicon (MLM) ====\nThis form was developed in the 2000s and introduced commercially around 2009. Also called cast-mono, this design uses polycrystalline casting chambers with small \"seeds\" of mono material. The result is a bulk mono-like material that is polycrystalline around the outsides. When sliced for processing, the inner sections are high-efficiency mono-like cells (but square instead of \"clipped\"), while the outer edges are sold as conventional poly. This production method results in mono-like cells at poly-like prices.\n\n\n=== Thin film ===\n\nThin-film technologies reduce the amount of active material in a cell. Most designs sandwich active material between two panes of glass. Since silicon solar panels only use one pane of glass, thin film panels are approximately twice as heavy as crystalline silicon panels, although they have a smaller ecological impact (determined from life cycle analysis). \n\n\n==== Cadmium telluride ====\n\nCadmium telluride is the only thin film material so far to rival crystalline silicon in cost/watt. However cadmium is highly toxic and tellurium (anion: \"telluride\") supplies are limited. The cadmium present in the cells would be toxic if released. However, release is impossible during normal operation of the cells and is unlikely during fires in residential roofs. A square meter of CdTe contains approximately the same amount of Cd as a single C cell nickel-cadmium battery, in a more stable and less soluble form.\n\n\n==== Copper indium gallium selenide ====\n\nCopper indium gallium selenide (CIGS) is a direct band gap material. It has the highest efficiency (~20%) among all commercially significant thin film materials (see CIGS solar cell). Traditional methods of fabrication involve vacuum processes including co-evaporation and sputtering. Recent developments at IBM and Nanosolar attempt to lower the cost by using non-vacuum solution processes.\n\n\n==== Silicon thin film ====\nSilicon thin-film cells are mainly deposited by chemical vapor deposition (typically plasma-enhanced, PE-CVD) from silane gas and hydrogen gas. Depending on the deposition parameters, this can yield amorphous silicon (a-Si or a-Si:H), protocrystalline silicon or nanocrystalline silicon (nc-Si or nc-Si:H), also called microcrystalline silicon.Amorphous silicon is the most well-developed thin film technology to-date. An amorphous silicon (a-Si) solar cell is made of non-crystalline or microcrystalline silicon. Amorphous silicon has a higher bandgap (1.7 eV) than crystalline silicon (c-Si) (1.1 eV), which means it absorbs the visible part of the solar spectrum more strongly than the higher power density infrared portion of the spectrum. The production of a-Si thin film solar cells uses glass as a substrate and deposits a very thin layer of silicon by plasma-enhanced chemical vapor deposition (PECVD).\nProtocrystalline silicon with a low volume fraction of nanocrystalline silicon is optimal for high open-circuit voltage. Nc-Si has about the same bandgap as c-Si and nc-Si and a-Si can advantageously be combined in thin layers, creating a layered cell called a tandem cell. The top cell in a-Si absorbs the visible light and leaves the infrared part of the spectrum for the bottom cell in nc-Si.\n\n\n==== Gallium arsenide thin film ====\nThe semiconductor material gallium arsenide (GaAs) is also used for single-crystalline thin film solar cells. Although GaAs cells are very expensive, they hold the world's record in efficiency for a single-junction solar cell at 28.8%. Typically fabricated on crystalline silicon wafer with a 41% fill factor, by moving to porous silicon fill factor can be increased to 56% with potentially reduced cost.  Using less active GaAs material by fabricating nanowires is another potential pathway to cost reduction. GaAs is more commonly used in multijunction photovoltaic cells for concentrated photovoltaics (CPV, HCPV) and for solar panels on spacecraft, as the industry favours efficiency over cost for space-based solar power. Based on the previous literature and some theoretical analysis, there are several reasons why GaAs has such high power conversion efficiency. First, GaAs bandgap is 1.43ev which is almost ideal for solar cells. Second, because Gallium is a by-product of the smelting of other metals, GaAs cells are relatively insensitive to heat and it can keep high efficiency when temperature is quite high. Third, GaAs has the wide range of design options. Using GaAs as active layer in solar cell, engineers can have multiple choices of other layers which can better generate electrons and holes in GaAs.\n\n\n=== Multijunction cells ===\n\nMulti-junction cells consist of multiple thin films, each essentially a solar cell grown on top of another, typically using metalorganic vapour phase epitaxy. Each layer has a different band gap energy to allow it to absorb electromagnetic radiation over a different portion of the spectrum. Multi-junction cells were originally developed for special applications such as satellites and space exploration, but are now used increasingly in terrestrial concentrator photovoltaics (CPV), an emerging technology that uses lenses and curved mirrors to concentrate sunlight onto small, highly efficient multi-junction solar cells. By concentrating sunlight up to a thousand times, High concentration photovoltaics (HCPV) has the potential to outcompete conventional solar PV in the future.:\u200a21,\u200a26\u200aTandem solar cells based on monolithic, series connected, gallium indium phosphide (GaInP), gallium arsenide (GaAs), and germanium (Ge) p\u2013n junctions, are increasing sales, despite cost pressures. Between December 2006 and December 2007, the cost of 4N gallium metal rose from about $350 per kg to $680 per kg. Additionally, germanium metal prices have risen substantially to $1000\u20131200 per kg this year. Those materials include gallium (4N, 6N and 7N Ga), arsenic (4N, 6N and 7N) and germanium, pyrolitic boron nitride (pBN) crucibles for growing crystals, and boron oxide, these products are critical to the entire substrate manufacturing industry.A triple-junction cell, for example, may consist of the semiconductors: GaAs, Ge, and GaInP2. Triple-junction GaAs solar cells were used as the power source of the Dutch four-time World Solar Challenge winners Nuna in 2003, 2005 and 2007 and by the Dutch solar cars Solutra (2005), Twente One (2007) and 21Revolution (2009). GaAs based multi-junction devices are the most efficient solar cells to date. On 15 October 2012, triple junction metamorphic cells reached a record high of 44%.\n\n\n==== GaInP/Si dual-junction solar cells ====\nIn 2016, a new approach was described for producing hybrid photovoltaic wafers combining the high efficiency of III-V multi-junction solar cells with the economies and wealth of experience associated with silicon. The technical complications involved in growing the III-V material on silicon at the required high temperatures, a subject of study for some 30 years, are avoided by epitaxial growth of silicon on GaAs at low temperature by plasma-enhanced chemical vapor deposition (PECVD).Si single-junction solar cells have been widely studied for decades and are reaching their practical efficiency of ~26% under 1-sun conditions. Increasing this efficiency may require adding more cells with bandgap energy larger than 1.1 eV to the Si cell, allowing to convert short-wavelength photons for generation of additional voltage. A dual-junction solar cell with a band gap of 1.6\u20131.8 eV as a top cell can reduce thermalization loss, produce a high external radiative efficiency and achieve theoretical efficiencies over 45%. A tandem cell can be fabricated by growing the GaInP and Si cells. Growing them separately can overcome the 4% lattice constant mismatch between Si and the most common III\u2013V layers that prevent direct integration into one cell. The two cells therefore are separated by a transparent glass slide so the lattice mismatch does not cause strain to the system. This creates a cell with four electrical contacts and two junctions that demonstrated an efficiency of 18.1%. With a fill factor (FF) of 76.2%, the Si bottom cell reaches an efficiency of 11.7% (\u00b1 0.4) in the tandem device, resulting in a cumulative tandem cell efficiency of 29.8%. This efficiency exceeds the theoretical limit of 29.4% and the record experimental efficiency value of a Si 1-sun solar cell, and is also higher than the record-efficiency 1-sun GaAs device. However, using a GaAs substrate is expensive and not practical. Hence researchers try to make a cell with two electrical contact points and one junction, which does not need a GaAs substrate. This means there will be direct integration of GaInP and Si.\n\n\n== Research in solar cells ==\n\n\n=== Perovskite solar cells ===\n\nPerovskite solar cells are solar cells that include a perovskite-structured material as the active layer. Most commonly, this is a solution-processed hybrid organic-inorganic tin or lead halide based material. Efficiencies have increased from below 5% at their first usage in 2009 to 25.5% in 2020, making them a very rapidly advancing technology and a hot topic in the solar cell field. Researchers at University of Rochester reported in 2023 that significant further improvements in cell efficiency can be achieved by utilizing Purcell effect.Perovskite solar cells are also forecast to be extremely cheap to scale up, making them a very attractive option for commercialisation. So far most types of perovskite solar cells have not reached sufficient operational stability to be commercialised, although many research groups are investigating ways to solve this. Energy and environmental sustainability of perovskite solar cells and tandem perovskite are shown to be dependent on the structures. Photonic front contacts for light management can improve the perovskite cells' performance, via enhanced broadband absorption, while allowing better operational stability due to protection against the harmful high-energy (above Visible) radiation. The inclusion of the toxic element lead in the most efficient perovskite solar cells is a potential problem for commercialisation.\n\n\n=== Bifacial solar cells ===\n\nWith a transparent rear side, bifacial solar cells can absorb light from both the front and rear sides. Hence, they can produce more electricity than conventional monofacial solar cells. The first patent of bifacial solar cells was filed by Japanese researcher Hiroshi Mori, in 1966. Later, it is said that Russia was the first to deploy bifacial solar cells in their space program in the 1970s. In 1976, the Institute for Solar Energy of the Technical University of Madrid, began a research program for the development of bifacial solar cells led by Prof. Antonio Luque. Based on 1977 US and Spanish patents by Luque, a practical bifacial cell was proposed with a front face as anode and a rear face as cathode; in previously reported proposals and attempts both faces were anodic and interconnection between cells was complicated and expensive. In 1980, Andr\u00e9s Cuevas, a PhD student in Luque's team, demonstrated experimentally a 50% increase in output power of bifacial solar cells, relative to identically oriented and tilted monofacial ones, when a white background was provided. In 1981 the company Isofoton was founded in M\u00e1laga to produce the developed bifacial cells, thus becoming the first industrialization of this PV cell technology. With an initial production capacity of 300 kW/yr of bifacial solar cells, early landmarks of Isofoton's production were the 20kWp power plant in San Agust\u00edn de Guadalix, built in 1986 for Iberdrola, and an off grid installation by 1988 also of 20kWp in the village of Noto Gouye Diama (Senegal) funded by the Spanish international aid and cooperation programs.\nDue to the reduced manufacturing cost, companies have again started to produce commercial bifacial modules since 2010. By 2017, there were at least eight certified PV manufacturers providing bifacial modules in North America. The International Technology Roadmap for Photovoltaics (ITRPV) predicted that the global market share of bifacial technology will expand from less than 5% in 2016 to 30% in 2027.Due to the significant interest in the bifacial technology, a recent study has investigated the performance and optimization of bifacial solar modules worldwide. The results indicate that, across the globe, ground-mounted bifacial modules can only offer ~10% gain in annual electricity yields compared to the monofacial counterparts for a ground albedo coefficient of 25% (typical for concrete and vegetation groundcovers). However, the gain can be increased to ~30% by elevating the module 1 m above the ground and enhancing the ground albedo coefficient to 50%. Sun et al. also derived a set of empirical equations that can optimize bifacial solar modules analytically. In addition, there is evidence that bifacial panels work better than traditional panels in snowy environments as bifacials on dual-axis trackers made 14% more electricity in a year than their monofacial counterparts and 40% during the peak winter months.An online simulation tool is available to model the performance of bifacial modules in any arbitrary location across the entire world. It can also optimize bifacial modules as a function of tilt angle, azimuth angle, and elevation above the ground.\n\n\n=== Intermediate band ===\n\nIntermediate band photovoltaics in solar cell research provides methods for exceeding the Shockley\u2013Queisser limit on the efficiency of a cell. It introduces an intermediate band (IB) energy level in between the valence and conduction bands. Theoretically, introducing an IB allows two photons with energy less than the bandgap to excite an electron from the valence band to the conduction band. This increases the induced photocurrent and thereby efficiency.Luque and Marti first derived a theoretical limit for an IB device with one midgap energy level using detailed balance. They assumed no carriers were collected at the IB and that the device was under full concentration. They found the maximum efficiency to be 63.2%, for a bandgap of 1.95eV with the IB 0.71eV from either the valence or conduction band.\nUnder one sun illumination the limiting efficiency is 47%. Several means are under study to realize IB semiconductors with such optimum 3-bandgap configuration, namely via materials engineering (controlled inclusion of deep level impurities or highly-mismatched alloys) and nano-structuring (quantum-dots in host hetero-crystals).\n\n\n=== Liquid inks ===\nIn 2014, researchers at California NanoSystems Institute discovered using kesterite and perovskite improved electric power conversion efficiency for solar cells.\n\n\n=== Upconversion and downconversion ===\nPhoton upconversion is the process of using two low-energy (e.g., infrared) photons to produce one higher energy photon; downconversion is the process of using one high energy photon (e.g., ultraviolet) to produce two lower energy photons. Either of these techniques could be used to produce higher efficiency solar cells by allowing solar photons to be more efficiently used. The difficulty, however, is that the conversion efficiency of existing phosphors exhibiting up- or down-conversion is low, and is typically narrow band.\nOne upconversion technique is to incorporate lanthanide-doped materials (Er3+, Yb3+, Ho3+ or a combination), taking advantage of their luminescence to convert infrared radiation to visible light. Upconversion process occurs when two infrared photons are absorbed by rare-earth ions to generate a (high-energy) absorbable photon. As example, the energy transfer upconversion process (ETU), consists in successive transfer processes between excited ions in the near infrared. The upconverter material could be placed below the solar cell to absorb the infrared light that passes through the silicon. Useful ions are most commonly found in the trivalent state. Er+ ions have been the most used. Er3+ ions absorb solar radiation around 1.54 \u00b5m. Two Er3+ ions that have absorbed this radiation can interact with each other through an upconversion process. The excited ion emits light above the Si bandgap that is absorbed by the solar cell and creates an additional electron\u2013hole pair that can generate current. However, the increased efficiency was small. In addition, fluoroindate glasses have low phonon energy and have been proposed as suitable matrix doped with Ho3+ ions.\n\n\n=== Light-absorbing dyes ===\n\nDye-sensitized solar cells (DSSCs) are made of low-cost materials and do not need elaborate manufacturing equipment, so they can be made in a DIY fashion. In bulk it should be significantly less expensive than older solid-state cell designs. DSSC's can be engineered into flexible sheets and although its conversion efficiency is less than the best thin film cells, its price/performance ratio may be high enough to allow them to compete with fossil fuel electrical generation.\nTypically a ruthenium metalorganic dye (Ru-centered) is used as a monolayer of light-absorbing material, which is adsorbed onto a thin film of titanium dioxide. The dye-sensitized solar cell depends on this mesoporous layer of nanoparticulate titanium dioxide (TiO2) to greatly amplify the surface area (200\u2013300 m2/g TiO2, as compared to approximately 10 m2/g of flat single crystal) which allows for a greater number of dyes per solar cell area (which in term in increases the current). The photogenerated electrons from the light absorbing dye are passed on to the n-type TiO2 and the holes are absorbed by an electrolyte on the other side of the dye. The circuit is completed by a redox couple in the electrolyte, which can be liquid or solid. This type of cell allows more flexible use of materials and is typically manufactured by screen printing or ultrasonic nozzles, with the potential for lower processing costs than those used for bulk solar cells. However, the dyes in these cells also suffer from degradation under heat and UV light and the cell casing is difficult to seal due to the solvents used in assembly. Due to this reason, researchers have developed solid-state dye-sensitized solar cells that use a solid electrolyte to avoid leakage. The first commercial shipment of DSSC solar modules occurred in July 2009 from G24i Innovations.\n\n\n=== Quantum dots ===\n\nQuantum dot solar cells (QDSCs) are based on the Gratzel cell, or dye-sensitized solar cell architecture, but employ low band gap semiconductor nanoparticles, fabricated with crystallite sizes small enough to form quantum dots (such as CdS, CdSe, Sb2S3, PbS, etc.), instead of organic or organometallic dyes as light absorbers. Due to the toxicity associated with Cd and Pb based compounds there are also a series of \"green\" QD sensitizing materials in development (such as CuInS2, CuInSe2 and CuInSeS). QD's size quantization allows for the band gap to be tuned by simply changing particle size. They also have high extinction coefficients and have shown the possibility of multiple exciton generation.In a QDSC, a mesoporous layer of titanium dioxide nanoparticles forms the backbone of the cell, much like in a DSSC. This TiO2 layer can then be made photoactive by coating with semiconductor quantum dots using chemical bath deposition, electrophoretic deposition or successive ionic layer adsorption and reaction. The electrical circuit is then completed through the use of a liquid or solid redox couple. The efficiency of QDSCs has increased to over 5% shown for both liquid-junction and solid state cells, with a reported peak efficiency of 11.91%. In an effort to decrease production costs, the Prashant Kamat research group demonstrated a solar paint made with TiO2 and CdSe that can be applied using a one-step method to any conductive surface with efficiencies over 1%. However, the absorption of quantum dots (QDs) in QDSCs is weak at room temperature. The plasmonic nanoparticles can be utilized to address the weak absorption of QDs (e.g., nanostars). Adding an external infrared pumping source to excite intraband and interband transition of QDs is another solution.\n\n\n=== Organic/polymer solar cells ===\n\nOrganic solar cells and polymer solar cells are built from thin films (typically 100 nm) of organic semiconductors including polymers, such as polyphenylene vinylene and small-molecule compounds like copper phthalocyanine (a blue or green organic pigment) and carbon fullerenes and fullerene derivatives such as PCBM.\nThey can be processed from liquid solution, offering the possibility of a simple roll-to-roll printing process, potentially leading to inexpensive, large-scale production. In addition, these cells could be beneficial for some applications where mechanical flexibility and disposability are important. Current cell efficiencies are, however, very low, and practical devices are essentially non-existent.\nEnergy conversion efficiencies achieved to date using conductive polymers are very low compared to inorganic materials. However, Konarka Power Plastic reached efficiency of 8.3% and organic tandem cells in 2012 reached 11.1%.The active region of an organic device consists of two materials, one electron donor and one electron acceptor. When a photon is converted into an electron hole pair, typically in the donor material, the charges tend to remain bound in the form of an exciton, separating when the exciton diffuses to the donor-acceptor interface, unlike most other solar cell types. The short exciton diffusion lengths of most polymer systems tend to limit the efficiency of such devices. Nanostructured interfaces, sometimes in the form of bulk heterojunctions, can improve performance.In 2011, MIT and Michigan State researchers developed solar cells with a power efficiency close to 2% with a transparency to the human eye greater than 65%, achieved by selectively absorbing the ultraviolet and near-infrared parts of the spectrum with small-molecule compounds. Researchers at UCLA more recently developed an analogous polymer solar cell, following the same approach, that is 70% transparent and has a 4% power conversion efficiency. These lightweight, flexible cells can be produced in bulk at a low cost and could be used to create power generating windows.\nIn 2013, researchers announced polymer cells with some 3% efficiency. They used block copolymers, self-assembling organic materials that arrange themselves into distinct layers. The research focused on P3HT-b-PFTBT that separates into bands some 16 nanometers wide.\n\n\n=== Adaptive cells ===\nAdaptive cells change their absorption/reflection characteristics depending on environmental conditions. An adaptive material responds to the intensity and angle of incident light. At the part of the cell where the light is most intense, the cell surface changes from reflective to adaptive, allowing the light to penetrate the cell. The other parts of the cell remain reflective increasing the retention of the absorbed light within the cell.In 2014, a system was developed that combined an adaptive surface with a glass substrate that redirect the absorbed to a light absorber on the edges of the sheet. The system also includes an array of fixed lenses/mirrors to concentrate light onto the adaptive surface. As the day continues, the concentrated light moves along the surface of the cell. That surface switches from reflective to adaptive when the light is most concentrated and back to reflective after the light moves along.\n\n\n=== Surface texturing ===\nFor the past years, researchers have been trying to reduce the price of solar cells while maximizing efficiency. Thin-film solar cell is a cost-effective second generation solar cell with much reduced thickness at the expense of light absorption efficiency. Efforts to maximize light absorption efficiency with reduced thickness have been made. Surface texturing is one of techniques used to reduce optical losses to maximize light absorbed. Currently, surface texturing techniques on silicon photovoltaics are drawing much attention. Surface texturing could be done in multiple ways. Etching single crystalline silicon substrate can produce randomly distributed square based pyramids on the surface using anisotropic etchants. Recent studies show that c-Si wafers could be etched down to form nano-scale inverted pyramids. Multicrystalline silicon solar cells, due to poorer crystallographic quality, are less effective than single crystal solar cells, but mc-Si solar cells are still being used widely due to less manufacturing difficulties. It is reported that multicrystalline solar cells can be surface-textured to yield solar energy conversion efficiency comparable to that of monocrystalline silicon cells, through isotropic etching or photolithography techniques. Incident light rays onto a textured surface do not reflect back out to the air as opposed to rays onto a flat surface. Rather some light rays are bounced back onto the other surface again due to the geometry of the surface. This process significantly improves light to electricity conversion efficiency, due to increased light absorption. This texture effect as well as the interaction with other interfaces in the PV module is a challenging optical simulation task. A particularly efficient method for modeling and optimization is the OPTOS formalism. In 2012, researchers at MIT reported that c-Si films textured with nanoscale inverted pyramids could achieve light absorption comparable to 30 times thicker planar c-Si. In combination with anti-reflective coating, surface texturing technique can effectively trap light rays within a thin film silicon solar cell. Consequently, required thickness for solar cells decreases with the increased absorption of light rays.\n\n\n=== Encapsulation ===\nSolar cells are commonly encapsulated in a transparent polymeric resin to protect the delicate solar cell regions for coming into contact with moisture, dirt, ice, and other conditions expected either during operation or when used outdoors. The encapsulants are commonly made from polyvinyl acetate or glass. Most encapsulants are uniform in structure and composition, which increases light collection owing to light trapping from total internal reflection of light within the resin. Research has been conducted into structuring the encapsulant to provide further collection of light. Such encapsulants have included roughened glass surfaces, diffractive elements, prism arrays, air prisms, v-grooves, diffuse elements, as well as multi-directional waveguide arrays. Prism arrays show an overall 5% increase in the total solar energy conversion. Arrays of vertically aligned broadband waveguides provide a 10% increase at normal incidence, as well as wide-angle collection enhancement of up to 4%, with optimized structures yielding up to a 20% increase in short circuit current. Active coatings that convert infrared light into visible light have shown a 30% increase. Nanoparticle coatings inducing plasmonic light scattering increase wide-angle conversion efficiency up to 3%. Optical structures have also been created in encapsulation materials to effectively \"cloak\" the metallic front contacts.\n\n\n=== Autonomous maintenance ===\nNovel self-cleaning mechanisms for solar panels are being developed. For instance, in 2019 via wet-chemically etched nanowires and a hydrophobic coating on the surface water droplets could remove 98% of dust particles, which may be especially relevant for applications in the desert.\n\n\n== Manufacture ==\nSolar cells share some of the same processing and manufacturing techniques as other semiconductor devices. However, the strict requirements for cleanliness and quality control of semiconductor fabrication are more relaxed for solar cells, lowering costs.\nPolycrystalline silicon wafers are made by wire-sawing block-cast silicon ingots into 180 to 350 micrometer wafers. The wafers are usually lightly p-type-doped. A surface diffusion of n-type dopants is performed on the front side of the wafer. This forms a p\u2013n junction a few hundred nanometers below the surface.\nAnti-reflection coatings are then typically applied to increase the amount of light coupled into the solar cell. Silicon nitride has gradually replaced titanium dioxide as the preferred material, because of its excellent surface passivation qualities. It prevents carrier recombination at the cell surface. A layer several hundred nanometers thick is applied using plasma-enhanced chemical vapor deposition. Some solar cells have textured front surfaces that, like anti-reflection coatings, increase the amount of light reaching the wafer. Such surfaces were first applied to single-crystal silicon, followed by multicrystalline silicon somewhat later.\nA full area metal contact is made on the back surface, and a grid-like metal contact made up of fine \"fingers\" and larger \"bus bars\" are screen-printed onto the front surface using a silver paste. This is an evolution of the so-called \"wet\" process for applying electrodes, first described in a US patent filed in 1981 by Bayer AG. The rear contact is formed by screen-printing a metal paste, typically aluminium. Usually this contact covers the entire rear, though some designs employ a grid pattern. The paste is then fired at several hundred degrees Celsius to form metal electrodes in ohmic contact with the silicon. Some companies use an additional electroplating step to increase efficiency. After the metal contacts are made, the solar cells are interconnected by flat wires or metal ribbons, and assembled into modules or \"solar panels\". Solar panels have a sheet of tempered glass on the front, and a polymer encapsulation on the back.\nDifferent types of manufacturing and recycling partly determine how effective it is in decreasing emissions and having a positive environmental effect. Such differences and effectiveness could be quantified for production of the most optimal types of products for different purposes in different regions across time.\n\n\n=== Manufacturers and certification ===\n\nNational Renewable Energy Laboratory tests and validates solar technologies. Three reliable groups certify solar equipment: UL and IEEE (both U.S. standards) and IEC.\nSolar cells are manufactured in volume in Japan, Germany, China, Taiwan, Malaysia and the United States, whereas Europe, China, the U.S., and Japan have dominated (94% or more as of 2013) in installed systems. Other nations are acquiring significant solar cell production capacity.\nGlobal PV cell/module production increased by 10% in 2012 despite a 9% decline in solar energy investments according to the annual \"PV Status Report\" released by the European Commission's Joint Research Centre. Between 2009 and 2013 cell production has quadrupled.\n\n\n==== China ====\n\nSince 2013 China has been the world's leading installer of solar photovoltaics (PV). As of September 2018, sixty percent of the world's solar photovoltaic modules were made in China. As of May 2018, the largest photovoltaic plant in the world is located in the Tengger desert in China. In 2018, China added more photovoltaic installed capacity (in GW) than the next 9 countries combined. As of 2022, China's share in the manufacturing of solar panels exceeded 80% across all manufacturing stages.\n\n\n==== Malaysia ====\n\nIn 2014, Malaysia was the world's third largest manufacturer of photovoltaics equipment, behind China and the European Union.\n\n\n==== United States ====\n\nSolar energy production in the U.S. has doubled from 2013 to 2019. This was driven first by the falling price of quality silicon, and later simply by the globally plunging cost of photovoltaic modules. In 2018, the U.S. added 10.8GW of installed solar photovoltaic energy, an increase of 21%.Latin America : Latin America has emerged as a promising region for solar energy development in recent years, with over 10 GW of installations in 2020. The solar market in Latin America has been driven by abundant solar resources, falling costs, competitive auctions and growing electricity demand. Some of the leading countries for solar energy in Latin America are Brazil, Mexico, Chile and Argentina. However, the solar market in Latin America also faces some challenges, such as political instability, financing gaps and power transmission bottlenecks.\nMiddle East and Africa : The Middle East and Africa has also experienced significant growth in solar energy deployment in recent years, with over 8 GW installations in 2020. The solar market in the Middle East and Africa has been driven by the low-cost generation of solar energy, the diversification of energy sources, the fight against climate change and rural electrification are motivated. Some of the notable countries for solar energy in the Middle East and Africa are Saudi Arabia, United Arab Emirates, Egypt, Morocco and South Africa. However, the solar market in the Middle East and Africa also faces several obstacles, including social unrest, regulatory uncertainty and technical barriers.\n\n\n=== Materials sourcing ===\nLike many other energy generation technologies, the manufacture of solar cells, especially its rapid expansion, has many environmental and supply-chain implications. Global mining may adapt and potentially expand for sourcing the needed minerals which vary per type of solar cell. Recycling solar panels could be a source for materials that would otherwise need to be mined.\n\n\n== Disposal ==\nSolar cells degrade over time and lose their efficiency. Solar cells in extreme climates, such as desert or polar, are more prone to degradation due to exposure to harsh UV light and snow loads respectively. Usually, solar panels are given a lifespan of 25\u201330 years before they get decommissioned.The International Renewable Energy Agency estimated that the amount of solar panel electronic waste generated in 2016 was 43,500\u2013250,000 metric tons. This number is estimated to increase substantially by 2030, reaching an estimated waste volume of 60\u201378 million metric tons in 2050.\n\n\n=== Recycling ===\n\nThe most widely used solar cells in the market are crystalline solar cells. A product is truly recyclable if it can harvested again. In the 2016 Paris Agreement, 195 countries agreed to reduce their carbon emissions by shifting their focus away from fossil fuels and towards renewable energy sources. Owing to this, Solar will be a major contributor to electricity generation all over the world. So, there will be a plethora of solar panels to be recycled after the end of their life cycle. In fact, many researchers around the globe have voiced their concern about finding ways to use silicon cells after recycling.Additionally, these cells have hazardous elements/compounds, including lead (Pb), cadmium (Cd) or cadmium sulfide (CdS), selenium (Se), and barium (Ba) as dopants aside from the valuables silicon (Si), aluminum (Al), silver (Ag), and copper (Cu). The harmful elements/compounds if not disposed of with the proper technique can have severe harmful effects on human life and wildlife alike.RECYCLING\nThere are various ways c-Si can be recycled. Mainly thermal and chemical separation methods are used. This happens in two stages\nPV solar cell separation: in thermal delamination, the ethylene vinyl acetate (EVA) is removed and materials such as glass, Tedlar\u00ae, aluminium frame, steel, copper and plastics are separated;\ncleansing the surface of PV solar cells: unwanted layers (antireflection layer, metal coating and p\u2013n semiconductor) are removed from the silicon solar cells separated from the PV modules; as a result, the silicon substrate, suitable for re-use, can be recovered.CONVERSION\nA research study was conducted by scientists to see how efficiently the solar panels were made from nanosilicon and nanosilicon/graphite hybrids. The experiment techniques consist of\n1.    Recovery of PV Cells from End-of-Life PV Module \u2013 This is a patented technique where the solar panels are deconstructed and each material is cleaned separately.\n2.    Purification of Broken PV Cells \u2013 40 g of broken PV cells were placed in a glass bottle of 500ml which contained 20% KOH (potassium oxide). Heat treatment of this aqueous solution was done at 80\u2009\u00b0C for 0.5\u2009h. All Al metal and other impurities were dissolved in a 20% KOH solution, and the solid PV silicon was deposited as sediment. The solid PV was dried in a vacuum and 32 g of impurity-free PV recycled silicon was obtained.\n3.    Conversion of Purified PV Recycled Silicon into Nanosilicon and Nanosilicon/Graphite Hybrid Production - A large-scale planetary ball mill (PULVERISETTE P5 5/4 classic line) was used. Impurity-free PV recycled cells/silicon were loaded inside a stainless-steel milling container together with five hardened steel balls (diameter of 25.4\u2009mm). The sample was milled at a rotation speed of 160\u2009rpm for 15\u2009h at room temperature under an argon atmosphere of 300\u2009kPa. During high-energy ball milling, particle size was reduced to nanometer level (<100\u2009nm). The same process was used to produce a PV nano-Si/graphite hybrid except for commercial graphite powder (Product-282863, Sigma-Aldrich, powder <20\u2009\u03bcm, synthetic) which was added with eight hardened steel balls. The mixture was milled at a rotation speed of 160\u2009rpm for 20\u2009h at room temperature under an argon atmosphere of 300\u2009kPa. A hybrid of PV nano-Si/graphite with a weight ratio of 5\u2009wt% PV nano-Si and 95\u2009wt% graphite was obtained.\nThe obtained PV nano-Si/graphite electrode showed excellent cyclic stability with high-capacity retention even after long-term 600 cycles. These results proved that silicon can be easily converted into nano-Si/graphite hybrids and harvested into PV modules and can work with the same efficiency as a c-Si module.\nCHALLENGES\nThere are a lot of different PV modules in the market which have different compositions. So, it is difficult to have a common PV cell breakdown process. Also, recyclers have to do quality control which is not possible if different PV modules have to be recycled. There are also various applications of pure Si outside of the Solar industry and the recyclers might be tempted to sell there if they get a higher value for the product.Other questions that need to be answered are\nTo whom do the recyclers sell the recovered modules, components, and/or materials?\nWhat are the costs for different recycling scenarios?\nLocation of recycling facilities?\nWould mobile recycling facilities make more sense over centralized ones?\nWhat infrastructure should be established for waste module collection?\nOn the policy side, the main questions are the following:\nWho should pay for waste module recycling?The First Solar panel recycling plant opened in Rousset, France in 2018. It was set to recycle 1300 tonnes of solar panel waste a year, and can increase its capacity to 4000 tonnes. If recycling is driven only by market-based prices, rather than also environmental regulations, the economic incentives for recycling remain uncertain and as of 2021 the environmental impact of different types of developed recycling techniques still need to be quantified.\n\n\n== See also ==\n Renewable energy portal\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nPV Lighthouse Calculators and Resources for photovoltaic scientists and engineers\nPhotovoltaics CDROM online Archived 15 April 2014 at the Wayback Machine\nSolar cell manufacturing techniques\nRenewable Energy: Solar at Curlie\nSolar Energy Laboratory at University of Southampton\nNASA's Photovoltaic Info\nGreen, M. A.; Emery, K.; Hishikawa, Y.; Warta, W. (2010). \"Solar cell efficiency tables (version 36)\". Progress in Photovoltaics: Research and Applications. 18 (5): 346. doi:10.1002/pip.1021.\n\"Electric Energy From Sun Produced by Light Cell\" Popular Mechanics, July 1931 article on various 1930s research on solar cells\nWong, Lydia H.; Zakutayev, Andriy; Major, Jonathan D.; Hao, Xiaojing; Walsh, Aron; Todorov, Teodor K.; Saucedo, Edgardo (2019). \"Emerging inorganic solar cell efficiency tables (Version 1)\". Journal of Physics: Energy. 1 (3): 032001. Bibcode:2019JPEn....1c2001W. doi:10.1088/2515-7655/ab2338. S2CID 250871748."}, {"id": 70, "title": "Shor\u2019s algorithm", "content": "Shor's algorithm is a quantum algorithm for finding the prime factors of an integer. It was developed in 1994 by the American mathematician Peter Shor. It is one of the few known quantum algorithms with compelling potential applications and strong evidence of superpolynomial speedup compared to best known classical (that is, non-quantum) algorithms. On the other hand, factoring numbers of practical significance requires far more qubits than available in the near future. Another concern is that noise in quantum circuits may undermine results, requiring additional qubits for quantum error correction.\nShor proposed multiple similar algorithms solving the factoring problem, the discrete logarithm problem, and the period finding problem. \"Shor's algorithm\" usually refers to his algorithm solving factoring, but may also refer to each of the three. The discrete logarithm algorithm and the factoring algorithm are instances of the period finding algorithm, and all three are instances of the hidden subgroup problem.\nOn a quantum computer, to factor an integer \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , Shor's algorithm runs in polynomial time, meaning the time taken is polynomial in \n  \n    \n      \n        log\n        \u2061\n        N\n      \n    \n    {\\displaystyle \\log N}\n  , the size of the integer given as input. Specifically, it takes quantum gates of order \n  \n    \n      \n        O\n        \n        \n          (\n          \n            (\n            log\n            \u2061\n            N\n            \n              )\n              \n                2\n              \n            \n            (\n            log\n            \u2061\n            log\n            \u2061\n            N\n            )\n            (\n            log\n            \u2061\n            log\n            \u2061\n            log\n            \u2061\n            N\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle O\\!\\left((\\log N)^{2}(\\log \\log N)(\\log \\log \\log N)\\right)}\n   using fast multiplication, or even \n  \n    \n      \n        O\n        \n        \n          (\n          \n            (\n            log\n            \u2061\n            N\n            \n              )\n              \n                2\n              \n            \n            (\n            log\n            \u2061\n            log\n            \u2061\n            N\n            )\n          \n          )\n        \n      \n    \n    {\\displaystyle O\\!\\left((\\log N)^{2}(\\log \\log N)\\right)}\n   utilizing the asymptotically fastest multiplication algorithm currently known due to Harvey and Van Der Hoven, thus demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is consequently in the complexity class BQP. This is significantly faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time: \n  \n    \n      \n        O\n        \n        \n          (\n          \n            e\n            \n              1.9\n              (\n              log\n              \u2061\n              N\n              \n                )\n                \n                  1\n                  \n                    /\n                  \n                  3\n                \n              \n              (\n              log\n              \u2061\n              log\n              \u2061\n              N\n              \n                )\n                \n                  2\n                  \n                    /\n                  \n                  3\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle O\\!\\left(e^{1.9(\\log N)^{1/3}(\\log \\log N)^{2/3}}\\right)}\n  .\n\n\n== Feasibility and impact ==\nIf a quantum computer with a sufficient number of qubits could operate without succumbing to quantum noise and other quantum-decoherence phenomena, then Shor's algorithm could be used to break public-key cryptography schemes, such as\n\nThe RSA scheme\nThe Finite Field Diffie-Hellman key exchange\nThe Elliptic Curve Diffie-Hellman key exchangeRSA is based on the assumption that factoring large integers is computationally intractable. As far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor integers in polynomial time. However, Shor's algorithm shows that factoring integers is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers, and for the study of new quantum-computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.\n\n\n=== Physical implementation ===\nGiven the high error rates of contemporary quantum computers and too few qubits to use quantum error correction, laboratory demonstrations obtain correct results only in a fraction of attempts.\nIn 2001, Shor's algorithm was demonstrated by a group at IBM, who factored \n  \n    \n      \n        15\n      \n    \n    {\\displaystyle 15}\n   into \n  \n    \n      \n        3\n        \u00d7\n        5\n      \n    \n    {\\displaystyle 3\\times 5}\n  , using an NMR implementation of a quantum computer with seven qubits. After IBM's implementation, two independent groups implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits. In 2012, the factorization of \n  \n    \n      \n        15\n      \n    \n    {\\displaystyle 15}\n   was performed with solid-state qubits. Later, in 2012, the factorization of \n  \n    \n      \n        21\n      \n    \n    {\\displaystyle 21}\n   was achieved. In 2019, an attempt was made to factor the number \n  \n    \n      \n        35\n      \n    \n    {\\displaystyle 35}\n   using Shor's algorithm on an IBM Q System One, but the algorithm failed because of accumulating errors.  Though larger numbers have been factored by quantum computers using other algorithms, these algorithms are similar to classical brute-force checking of factors, so unlike Shor's algorithm, they are not expected to ever perform better than classical factoring algorithms.Theoretical analyses of Shor's algorithm assume a quantum computer free of noise and errors. However, near-term practical implementations will have to deal with such undesired phenomena (when more qubits are available, Quantum error correction can help). In 2023, Jin-Yi Cai studied the impact of noise and concluded that there is a special class of numbers (products of two primes from A073024, which are dense in the semiprimes), Shor's algorithm cannot factor such numbers in the presence of noise. Hence error-correction will be needed to be able to factor all numbers with Shor's algorithm.\n\n\n== Algorithm ==\nThe problem that we are trying to solve is: given an odd composite number \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , find its integer factors.\nTo achieve this, Shor's algorithm consists of two parts:\n\nA classical reduction of the factoring problem to the problem of order-finding. This reduction is similar to that used for other factoring algorithms, such as the quadratic sieve.\nA quantum algorithm to solve the order-finding problem.\n\n\n=== Classical reduction ===\nA complete factoring algorithm is possible if we're able to efficiently factor arbitrary \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   into just two integers \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   greater than 1, since if either \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   or \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n   are not prime then the factoring algorithm can in turn be run on those until only primes remain.\nA basic observation is that, using Euclid's algorithm, we can always compute the GCD between two integers efficiently. In particular, this means we can check efficiently whether \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is even, in which case 2 is trivially a factor. Let us thus assume that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is odd for the remainder of this discussion. Afterwards, we can use efficient classical algorithms to check if \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is a prime power. For prime powers, efficient classical factorization algorithms exist, hence the rest of the quantum algorithm may assume that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is not a prime power.\nIf those easy cases do not produce a nontrivial factor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , the algorithm proceeds to handle the remaining case. We pick a random integer \n  \n    \n      \n        2\n        \u2264\n        a\n        <\n        N\n      \n    \n    {\\displaystyle 2\\leq a<N}\n  . A possible nontrivial divisor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   can be found by computing \n  \n    \n      \n        gcd\n        (\n        a\n        ,\n        N\n        )\n      \n    \n    {\\displaystyle \\gcd(a,N)}\n  , which can be done classically and efficiently using the Euclidean algorithm. If this produces a nontrivial factor (meaning \n  \n    \n      \n        gcd\n        (\n        a\n        ,\n        N\n        )\n        \u2260\n        1\n      \n    \n    {\\displaystyle \\gcd(a,N)\\neq 1}\n  ), the algorithm is finished, and the other nontrivial factor is \n  \n    \n      \n        \n          \n            N\n            \n              gcd\n              (\n              a\n              ,\n              N\n              )\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {N}{\\gcd(a,N)}}}\n  . If a nontrivial factor was not identified, then that means that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   and the choice of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   are coprime. Here, the algorithm runs the quantum subroutine, which will return the order \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , meaning\n\n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        \u2261\n        1\n        \n          mod\n          \n            N\n          \n        \n        .\n      \n    \n    {\\displaystyle a^{r}\\equiv 1{\\bmod {N}}.}\n  The quantum subroutine requires that \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   are coprime, which is true since at this point in the algorithm, \n  \n    \n      \n        gcd\n        (\n        a\n        ,\n        N\n        )\n      \n    \n    {\\displaystyle \\gcd(a,N)}\n   did not produce a nontrivial factor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  . It can be seen from the equivalence that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   divides \n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle a^{r}-1}\n  , written \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r}-1}\n  . This can be factored using difference of squares: Since we have factored the expression in this way, the algorithm doesn't work for odd \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   (because \n  \n    \n      \n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{r/2}}\n   must be an integer), meaning the algorithm would have to restart with a new \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  . Hereafter we can therefore assume \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   is even. It cannot be the case that \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        \u2212\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r/2}-1}\n  , since this would imply \n  \n    \n      \n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        \u2261\n        1\n        \n          mod\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle a^{r/2}\\equiv 1{\\bmod {N}}}\n  , which would contradictorily imply that \n  \n    \n      \n        \n          \n            r\n            2\n          \n        \n      \n    \n    {\\textstyle {\\frac {r}{2}}}\n   would be the order of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , which was already \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  . At this point, it may or may not be the case that \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r/2}+1}\n  . If it is not true that \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r/2}+1}\n  , then that means we are able to find a nontrivial factor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  . We computeIf \n  \n    \n      \n        d\n        =\n        1\n      \n    \n    {\\displaystyle d=1}\n  , then that means \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r/2}+1}\n   was true, and a nontrivial factor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   cannot be achieved from \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , and the algorithm must restart with a new \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  . Otherwise, we have found a nontrivial factor of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , with the other being \n  \n    \n      \n        \n          \n            N\n            d\n          \n        \n      \n    \n    {\\textstyle {\\frac {N}{d}}}\n  , and the algorithm is finished. For this step, it is also equivalent to compute \n  \n    \n      \n        gcd\n        (\n        N\n        ,\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        +\n        1\n        )\n      \n    \n    {\\displaystyle \\gcd(N,a^{r/2}+1)}\n  ; it will produce a nontrivial factor if \n  \n    \n      \n        gcd\n        (\n        N\n        ,\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        \u2212\n        1\n        )\n      \n    \n    {\\displaystyle \\gcd(N,a^{r/2}-1)}\n   is nontrivial, and will not if it's trivial (where \n  \n    \n      \n        N\n        \u2223\n        \n          a\n          \n            r\n            \n              /\n            \n            2\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle N\\mid a^{r/2}+1}\n  ).\n\nThe algorithm restated shortly follows: let \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   be odd, and not a prime power. We want to output two nontrivial factors of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  .It has been shown that this will be likely to succeed after a few runs. In practice, a single call to the quantum order-finding subroutine is enough to completely factor \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   with very high probability of success if one uses a more advanced reduction.\n\n\n=== Quantum order-finding subroutine ===\nThe goal of the quantum subroutine of Shor's algorithm is, given coprime integers \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   and \n  \n    \n      \n        1\n        <\n        a\n        <\n        N\n      \n    \n    {\\displaystyle 1<a<N}\n  , to find the order \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   modulo \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , which is the smallest positive integer such that \n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        \u2261\n        1\n        \n          \n          (\n          mod\n          \n          N\n          )\n        \n      \n    \n    {\\displaystyle a^{r}\\equiv 1{\\pmod {N}}}\n  . To achieve this, Shor's algorithm uses a quantum circuit involving two registers. The second register uses \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   qubits, where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the smallest integer such that \n  \n    \n      \n        N\n        \u2264\n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle N\\leq 2^{n}}\n  . The size of the first register determines how accurate of an approximation the circuit produces. It can be shown that using \n  \n    \n      \n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2n+1}\n   qubits gives sufficient accuracy to find \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  . The exact quantum circuit depends on the parameters \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  , which define the problem. In what follows, bra\u2013ket notation will be used to denote quantum states, and \n  \n    \n      \n        \u2297\n      \n    \n    {\\displaystyle \\otimes }\n   will denote the tensor product rather than logical OR or logical XOR.\nThe algorithm consists of two main steps:\n\nUse quantum phase estimation with unitary \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   representing the operation of multiplying by \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   (modulo \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  ), and input state \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            2\n            n\n            +\n            1\n          \n        \n        \u2297\n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes 2n+1}\\otimes |1\\rangle }\n   (where the second register is \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |1\\rangle }\n   made from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   qubits). The eigenvalues of this \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   encode information about the period, and \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |1\\rangle }\n   can be seen to be writable as a sum of its eigenvectors. Thanks to these properties, the quantum phase estimation stage gives as output a random integer of the form \n  \n    \n      \n        \n          \n            j\n            r\n          \n        \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\frac {j}{r}}2^{2n+1}}\n   for random \n  \n    \n      \n        j\n        =\n        0\n        ,\n        1\n        ,\n        .\n        .\n        .\n        ,\n        r\n        \u2212\n        1\n      \n    \n    {\\displaystyle j=0,1,...,r-1}\n  .\nUse the continued fractions algorithm to extract the period \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   from the measurement outcomes obtained in the previous stage. This is a procedure to post-process (with a classical computer) the measurement data obtained from measuring the output quantum states, and retrieve the period.The connection with quantum phase estimation was not discussed in the original formulation of Shor's algorithm, but was later proposed by Kitaev.\n\n\n==== Quantum phase estimation ====\nIn general the quantum phase estimation algorithm, for any unitary \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   and eigenstate \n  \n    \n      \n        \n          |\n        \n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle |\\psi \\rangle }\n   such that \n  \n    \n      \n        U\n        \n          |\n        \n        \u03c8\n        \u27e9\n        =\n        \n          e\n          \n            2\n            \u03c0\n            i\n            \u03b8\n          \n        \n        \n          |\n        \n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle U|\\psi \\rangle =e^{2\\pi i\\theta }|\\psi \\rangle }\n  , sends inputs states \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n        \n          |\n        \n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle |\\psi \\rangle }\n   into output states close to \n  \n    \n      \n        \n          |\n        \n        \u03d5\n        \u27e9\n        \n          |\n        \n        \u03c8\n        \u27e9\n      \n    \n    {\\displaystyle |\\phi \\rangle |\\psi \\rangle }\n  , where \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   is an integer close to \n  \n    \n      \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n        \u03b8\n      \n    \n    {\\displaystyle 2^{2n+1}\\theta }\n  . In other words, it sends each eigenstate \n  \n    \n      \n        \n          |\n        \n        \n          \u03c8\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |\\psi _{j}\\rangle }\n   of \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   into a state close to the associated eigenvalue. For the purposes of quantum order-finding, we employ this strategy using the unitary defined by the actionThe action of \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   on states \n  \n    \n      \n        \n          |\n        \n        k\n        \u27e9\n      \n    \n    {\\displaystyle |k\\rangle }\n   with \n  \n    \n      \n        N\n        \u2264\n        k\n        <\n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle N\\leq k<2^{n}}\n   is not crucial to the functioning of the algorithm, but needs to be included to ensure the overall transformation is a well-defined quantum gate. Implementing the circuit for quantum phase estimation with \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   requires being able to efficiently implement the gates \n  \n    \n      \n        \n          U\n          \n            \n              2\n              \n                j\n              \n            \n          \n        \n      \n    \n    {\\displaystyle U^{2^{j}}}\n  . This can be accomplished via modular exponentiation, which is the slowest part of the algorithm.\nThe gate thus defined satisfies \n  \n    \n      \n        \n          U\n          \n            r\n          \n        \n        =\n        I\n      \n    \n    {\\displaystyle U^{r}=I}\n  , which immediately implies that its eigenvalues are the \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  -th roots of unity \n  \n    \n      \n        \n          \u03c9\n          \n            r\n          \n          \n            k\n          \n        \n        =\n        \n          e\n          \n            2\n            \u03c0\n            i\n            k\n            \n              /\n            \n            r\n          \n        \n      \n    \n    {\\displaystyle \\omega _{r}^{k}=e^{2\\pi ik/r}}\n  . Furthermore, each eigenvalue \n  \n    \n      \n        \n          \u03c9\n          \n            r\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\omega _{r}^{k}}\n   has an eigenvector of the form \n  \n    \n      \n        \n          |\n        \n        \n          \u03c8\n          \n            j\n          \n        \n        \u27e9\n        =\n        \n          r\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        \n          \u2211\n          \n            k\n            =\n            0\n          \n          \n            r\n            \u2212\n            1\n          \n        \n        \n          \u03c9\n          \n            r\n          \n          \n            \u2212\n            k\n            j\n          \n        \n        \n          |\n        \n        \n          a\n          \n            k\n          \n        \n        \u27e9\n      \n    \n    {\\textstyle |\\psi _{j}\\rangle =r^{-1/2}\\sum _{k=0}^{r-1}\\omega _{r}^{-kj}|a^{k}\\rangle }\n  , and these eigenvectors are such that\n\nwhere the last identity follows from the geometric series formula, which implies \n  \n    \n      \n        \n          \u2211\n          \n            j\n            =\n            0\n          \n          \n            r\n            \u2212\n            1\n          \n        \n        \n          \u03c9\n          \n            r\n          \n          \n            j\n            k\n          \n        \n        =\n        0\n      \n    \n    {\\textstyle \\sum _{j=0}^{r-1}\\omega _{r}^{jk}=0}\n  .\nUsing quantum phase estimation on an input state \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            2\n            n\n            +\n            1\n          \n        \n        \n          |\n        \n        \n          \u03c8\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes 2n+1}|\\psi _{j}\\rangle }\n   would result in an output \n  \n    \n      \n        \n          |\n        \n        \n          \u03d5\n          \n            j\n          \n        \n        \u27e9\n        \n          |\n        \n        \n          \u03c8\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |\\phi _{j}\\rangle |\\psi _{j}\\rangle }\n   with each \n  \n    \n      \n        \n          \u03d5\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\phi _{j}}\n   representing a superposition of integers that approximate \n  \n    \n      \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle 2^{2n+1}j/r}\n  , with the most accurate measurement having a chance of \n  \n    \n      \n        >=\n        \n          \n            4\n            \n              \u03c0\n              \n                2\n              \n            \n          \n        \n        \u2248\n        0.4053\n      \n    \n    {\\textstyle >={\\frac {4}{\\pi ^{2}}}\\approx 0.4053}\n   of being measured (which can be approximated arbitrarily close [from below] to 1 using extra qubits). Thus using as input \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            2\n            n\n            +\n            1\n          \n        \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes 2n+1}|1\\rangle }\n   instead, the output is a superposition of such states with \n  \n    \n      \n        j\n        =\n        0\n        ,\n        .\n        .\n        .\n        ,\n        r\n        \u2212\n        1\n      \n    \n    {\\displaystyle j=0,...,r-1}\n  . In other words, using this input amounts to running quantum phase estimation on a superposition of eigenvectors of \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  . More explicitly, the quantum phase estimation circuit implements the transformationMeasuring the first register, we now have a balanced probability \n  \n    \n      \n        1\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle 1/r}\n   to find each \n  \n    \n      \n        \n          |\n        \n        \n          \u03d5\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |\\phi _{j}\\rangle }\n  , each one giving an integer approximation to \n  \n    \n      \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle 2^{2n+1}j/r}\n  , which can be divided by \n  \n    \n      \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle 2^{2n+1}}\n   to get a decimal approximation for \n  \n    \n      \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle j/r}\n  .\n\n\n==== Continued fraction algorithm to retrieve the period ====\nThen, we apply the continued fractions algorithm to find integers \n  \n    \n      \n        b\n      \n    \n    {\\textstyle b}\n   and \n  \n    \n      \n        c\n      \n    \n    {\\textstyle c}\n  , where \n  \n    \n      \n        \n          \n            b\n            c\n          \n        \n      \n    \n    {\\textstyle {\\frac {b}{c}}}\n   gives the best fraction approximation for the approximation measured from the circuit, for \n  \n    \n      \n        b\n        ,\n        c\n        <\n        N\n      \n    \n    {\\textstyle b,c<N}\n   and coprime \n  \n    \n      \n        b\n      \n    \n    {\\textstyle b}\n   and \n  \n    \n      \n        c\n      \n    \n    {\\textstyle c}\n  . The number of qubits in the first register, \n  \n    \n      \n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2n+1}\n  , which determines the accuracy of the approximation, guarantees thatgiven the best approximation from the superposition of \n  \n    \n      \n        \n          |\n        \n        \n          \u03d5\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\textstyle |\\phi _{j}\\rangle }\n   was measured (which can be made arbitrarily likely by using extra bits and truncating the output). However, while \n  \n    \n      \n        b\n      \n    \n    {\\textstyle b}\n   and \n  \n    \n      \n        c\n      \n    \n    {\\textstyle c}\n   are coprime, it may be the case that \n  \n    \n      \n        j\n      \n    \n    {\\textstyle j}\n   and \n  \n    \n      \n        r\n      \n    \n    {\\textstyle r}\n   are not coprime. Because of that, \n  \n    \n      \n        b\n      \n    \n    {\\textstyle b}\n   and \n  \n    \n      \n        c\n      \n    \n    {\\textstyle c}\n   may have lost some factors that were in \n  \n    \n      \n        j\n      \n    \n    {\\textstyle j}\n   and \n  \n    \n      \n        r\n      \n    \n    {\\textstyle r}\n  . This can be remedied by rerunning the quantum subroutine an arbitrary number of times, to produce a list of fraction approximationswhere \n  \n    \n      \n        s\n      \n    \n    {\\textstyle s}\n   is the number of times the algorithm was run. Each \n  \n    \n      \n        \n          c\n          \n            k\n          \n        \n      \n    \n    {\\textstyle c_{k}}\n   will have different factors taken out of it because the circuit will (likely) have measured multiple different possible values of \n  \n    \n      \n        j\n      \n    \n    {\\textstyle j}\n  . To recover the actual \n  \n    \n      \n        r\n      \n    \n    {\\textstyle r}\n   value, we can take the least common multiple of each \n  \n    \n      \n        \n          c\n          \n            k\n          \n        \n      \n    \n    {\\textstyle c_{k}}\n  :The least common multiple will be the order \n  \n    \n      \n        r\n      \n    \n    {\\textstyle r}\n   of the original integer \n  \n    \n      \n        a\n      \n    \n    {\\textstyle a}\n   with high probability.\n\n\n==== Choosing the size of the first register ====\nPhase estimation requires choosing the size of the first register to determine the accuracy of the algorithm, and for the quantum subroutine of Shor's algorithm, \n  \n    \n      \n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2n+1}\n   qubits is sufficient to guarantee that the optimal bitstring measured from phase estimation (meaning the \n  \n    \n      \n        \n          |\n        \n        k\n        \u27e9\n      \n    \n    {\\displaystyle |k\\rangle }\n   where \n  \n    \n      \n        k\n        \n          /\n        \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n      \n    \n    {\\textstyle k/2^{2n+1}}\n   is the most accurate approximation of the phase from phase estimation) will allow the actual value of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   to be recovered.\nEach \n  \n    \n      \n        \n          |\n        \n        \n          \u03d5\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |\\phi _{j}\\rangle }\n   before measurement in Shor's algorithm represents a superposition of integers approximating \n  \n    \n      \n        \n          2\n          \n            2\n            n\n            +\n            1\n          \n        \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle 2^{2n+1}j/r}\n  . Let \n  \n    \n      \n        \n          |\n        \n        k\n        \u27e9\n      \n    \n    {\\displaystyle |k\\rangle }\n   represent the most optimal integer in \n  \n    \n      \n        \n          |\n        \n        \n          \u03d5\n          \n            j\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle |\\phi _{j}\\rangle }\n  . The following theorem guarantees that the continued fractions algorithm will recover \n  \n    \n      \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle j/r}\n   from \n  \n    \n      \n        k\n        \n          /\n        \n        \n          2\n          \n            2\n            \n              n\n            \n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle k/2^{2{n}+1}}\n  :\n\n As \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is the optimal bitstring from phase estimation, \n  \n    \n      \n        k\n        \n          /\n        \n        \n          2\n          \n            2\n            \n              n\n            \n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle k/2^{2{n}+1}}\n   is accurate to \n  \n    \n      \n        j\n        \n          /\n        \n        r\n      \n    \n    {\\displaystyle j/r}\n   by \n  \n    \n      \n        2\n        n\n        +\n        1\n      \n    \n    {\\displaystyle 2n+1}\n   bits. Thus,which implys that the continued fractions algorithm will recover \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   and \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   (or with their greatest common divisor taken out).\n\n\n=== The bottleneck ===\nThe runtime bottleneck of Shor's algorithm is quantum modular exponentiation, which is by far slower than the quantum Fourier transform and classical pre-/post-processing. There are several approaches to constructing and optimizing circuits for modular exponentiation. The simplest and (currently) most practical approach is to mimic conventional arithmetic circuits with reversible gates, starting with ripple-carry adders. Knowing the base and the modulus of exponentiation facilitates further optimizations. Reversible circuits typically use on the order of \n  \n    \n      \n        \n          n\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle n^{3}}\n   gates for \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   qubits. Alternative techniques asymptotically improve gate counts by using quantum Fourier transforms, but are not competitive with fewer than 600 qubits owing to high constants.\n\n\n== Period finding and discrete logarithms ==\nShor's algorithms for the discrete log and the order finding problems are instances of an algorithm solving the period finding problem.. All three are instances of the hidden subgroup problem.\n\n\n=== Shor's algorithm for discrete logarithms ===\nGiven a group \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   with order \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   and generator \n  \n    \n      \n        g\n        \u2208\n        G\n      \n    \n    {\\displaystyle g\\in G}\n  , suppose we know that \n  \n    \n      \n        x\n        =\n        \n          g\n          \n            r\n          \n        \n        \u2208\n        G\n      \n    \n    {\\displaystyle x=g^{r}\\in G}\n  , for some \n  \n    \n      \n        r\n        \u2208\n        \n          \n            Z\n          \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle r\\in \\mathbb {Z} _{p}}\n  , and we wish to compute \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , which is the discrete logarithm: \n  \n    \n      \n        r\n        =\n        \n          \n            log\n            \n              g\n            \n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle r={\\log _{g}}(x)}\n  . Consider the abelian group \n  \n    \n      \n        \n          \n            Z\n          \n          \n            p\n          \n        \n        \u00d7\n        \n          \n            Z\n          \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {Z} _{p}\\times \\mathbb {Z} _{p}}\n  , where each factor corresponds to modular addition of values. Now, consider the function\n\n  \n    \n      \n        f\n        :\n        \n          \n            Z\n          \n          \n            p\n          \n        \n        \u00d7\n        \n          \n            Z\n          \n          \n            p\n          \n        \n        \u2192\n        G\n        \n        ;\n        \n        f\n        (\n        a\n        ,\n        b\n        )\n        =\n        \n          g\n          \n            a\n          \n        \n        \n          x\n          \n            \u2212\n            b\n          \n        \n        .\n      \n    \n    {\\displaystyle f\\colon \\mathbb {Z} _{p}\\times \\mathbb {Z} _{p}\\to G\\;;\\;f(a,b)=g^{a}x^{-b}.}\n  This gives us an abelian hidden subgroup problem, where \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   corresponds to a group homomorphism. The kernel corresponds to the multiples of \n  \n    \n      \n        (\n        r\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (r,1)}\n  . So, if we can find the kernel, we can find \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  . A quantum algorithm for solving this problem exists. This algorithm is, like the factor-finding algorithm, due to Peter Shor and both are implemented by creating a superposition through using Hadamard gates, followed by implementing \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   as a quantum transform, followed finally by a quantum Fourier transform. Due to this, the quantum algorithm for computing the discrete logarithm is also occasionally referred to as \"Shor's Algorithm.\"\nThe order-finding problem can also be viewed as a hidden subgroup problem. To see this, consider the group of integers under addition, and for a given \n  \n    \n      \n        a\n        \u2208\n        \n          Z\n        \n      \n    \n    {\\displaystyle a\\in \\mathbb {Z} }\n   such that: \n  \n    \n      \n        \n          a\n          \n            r\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle a^{r}=1}\n  , the function\n\n  \n    \n      \n        f\n        :\n        \n          Z\n        \n        \u2192\n        \n          Z\n        \n        \n        ;\n        \n        f\n        (\n        x\n        )\n        =\n        \n          a\n          \n            x\n          \n        \n        ,\n        \n        f\n        (\n        x\n        +\n        r\n        )\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle f\\colon \\mathbb {Z} \\to \\mathbb {Z} \\;;\\;f(x)=a^{x},\\;f(x+r)=f(x).}\n  For any finite abelian group \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  , a quantum algorithm exists for solving the hidden subgroup for \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   in polynomial time.\n\n\n== See also ==\nGEECM, a factorization algorithm said to be \"often much faster than Shor's\"\nGrover's algorithm\n\n\n== References ==\n\n\n== Further reading ==\nNielsen, Michael A. & Chuang, Isaac L. (2010), Quantum Computation and Quantum Information, 10th Anniversary Edition, Cambridge University Press, ISBN 9781107002173.\nPhillip Kaye, Raymond Laflamme, Michele Mosca, An introduction to quantum computing, Oxford University Press, 2007, ISBN 0-19-857049-X\n\"Explanation for the man in the street\" by Scott Aaronson, \"approved\" by Peter Shor. (Shor wrote \"Great article, Scott! That\u2019s the best job of explaining quantum computing to the man on the street that I\u2019ve seen.\"). An alternate metaphor for the QFT was presented in one of the comments. Scott Aaronson suggests the following 12 references as further reading (out of \"the 10105000 quantum algorithm tutorials that are already on the web.\"):\nShor, Peter W. (1997), \"Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer\", SIAM J. Comput., 26 (5): 1484\u20131509, arXiv:quant-ph/9508027v2, Bibcode:1999SIAMR..41..303S, doi:10.1137/S0036144598347011. Revised version of the original paper by Peter Shor (\"28 pages, LaTeX. This is an expanded version of a paper that appeared in the Proceedings of the 35th Annual Symposium on Foundations of Computer Science, Santa Fe, NM, Nov. 20--22, 1994. Minor revisions made January, 1996\").\nQuantum Computing and Shor's Algorithm, Matthew Hayward's Quantum Algorithms Page, 2005-02-17, imsa.edu, LaTeX2HTML version of the original LaTeX document, also available as PDF or postscript document.\nQuantum Computation and Shor's Factoring Algorithm, Ronald de Wolf, CWI and University of Amsterdam, January 12, 1999, 9 page postscript document.\nShor's Factoring Algorithm, Notes from Lecture 9 of Berkeley CS 294\u20132, dated 4 Oct 2004, 7 page postscript document.\nChapter 6 Quantum Computation Archived 2020-04-30 at the Wayback Machine, 91 page postscript document, Caltech, Preskill, PH229.\nQuantum computation: a tutorial by Samuel L. Braunstein.\nThe Quantum States of Shor's Algorithm, by Neal Young, Last modified: Tue May 21 11:47:38 1996.\nIII. Breaking RSA Encryption with a Quantum Computer: Shor's Factoring Algorithm, Lecture notes on Quantum computation, Cornell University, Physics 481\u2013681, CS 483; Spring, 2006 by N. David Mermin. Last revised 2006-03-28, 30 page PDF document.\nLavor, C.; Manssur, L. R. U.; Portugal, R. (2003). \"Shor's Algorithm for Factoring Large Integers\". arXiv:quant-ph/0303175.\nLomonaco, Jr (2000). \"Shor's Quantum Factoring Algorithm\". arXiv:quant-ph/0010034.  This paper is a written version of a one-hour lecture given on Peter Shor's quantum factoring algorithm. 22 pages.\nChapter 20 Quantum Computation, from Computational Complexity: A Modern Approach, Draft of a book: Dated January 2007, Sanjeev Arora and Boaz Barak, Princeton University. Published as Chapter 10 Quantum Computation of Sanjeev Arora, Boaz Barak, \"Computational Complexity: A Modern Approach\", Cambridge University Press, 2009, ISBN 978-0-521-42426-4\nA Step Toward Quantum Computing: Entangling 10 Billion Particles Archived 2011-01-20 at the Wayback Machine, from \"Discover Magazine\", Dated January 19, 2011.\nJosef Gruska - Quantum Computing Challenges also in Mathematics unlimited: 2001 and beyond, Editors Bj\u00f6rn Engquist, Wilfried Schmid, Springer, 2001, ISBN 978-3-540-66913-5\n\n\n== External links ==\nVersion 1.0.0 of libquantum: contains a C language implementation of Shor's algorithm with their simulated quantum computer library, but the width variable in shor.c should be set to 1 to improve the runtime complexity.\nPBS Infinite Series created two videos explaining the math behind Shor's algorithm, \"How to Break Cryptography\" and \"Hacking at Quantum Speed with Shor's Algorithm\"."}, {"id": 71, "title": "Financial management", "content": "Financial management is the business function concerned with profitability, expenses, cash and credit. These are often grouped together under the rubric of maximizing the value of the firm for stockholders. The discipline is then tasked with the \"efficient acquisition and deployment\" of both short- and long-term financial resources, to ensure the objectives of the enterprise are achieved.Financial managers (FM) are specialized professionals directly reporting to senior management, often the financial director (FD); the function is seen as 'staff', and not 'line'.\n\n\n== Role ==\n\nFinancial management is generally concerned with short term working capital management, focusing on current assets and current liabilities,  and managing fluctuations in foreign currency and product cycles, often through hedging.\nThe function also entails the efficient and effective day-to-day management of funds, and thus overlaps treasury management.\nIt is also involved with long term  strategic financial management, focused on i.a. capital structure management, including capital raising, capital budgeting (capital allocation between business units or products), and dividend policy;\nthese latter, in large corporates, being more the domain of \"corporate finance.\"\nSpecific tasks:\n\nProfit maximization happens when marginal cost is equal to marginal revenue. This is the main objective of financial management.\nMaintaining proper cash flow is a short run objective of financial management. It is necessary for operations to pay the day-to-day expenses e.g. raw material, electricity bills, wages, rent etc. A good cash flow ensures the survival of company; see cashflow forecast.\nMinimization on capital cost in financial management can help operations gain more profit.\nEstimating the requirement of funds: Businesses make forecast on funds needed in both short run and long run, hence, they can improve the efficiency of funding. The estimation is based on the budget e.g. sales budget, production budget; see budget analyst.\nDetermining the capital structure: Capital structure is how a firm finances its overall operations and growth by using different sources of funds. Once the requirement of funds has estimated, the financial manager should decide the mix of debt and equity and also types of debt.\n\n\n== Relationship with other areas of finance ==\nTwo areas of finance directly overlap financial management:\n(i) Managerial finance is the (academic) branch of finance concerned with the managerial application of financial techniques;\n(ii) Corporate finance is mainly concerned with the longer term capital budgeting, and typically is more relevant to large corporations.\nInvestment management, also related, is the professional asset management of various securities (shares, bonds and other securities/assets).\nIn the context of financial management, the function sits with treasury; usually the management of the various short-term financial legal instruments (contractual duties, obligations, or rights) appropriate to the company's cash- and liquidity management requirements. See Treasury management \u00a7 Functions.\nThe term \"financial management\" refers to a company's financial strategy, while personal finance or financial life management refers to an individual's management strategy. A financial planner, or personal financial planner, is a professional who prepares financial plans here.\n\n\n== Financial management systems ==\nFinancial management systems are the software and technology used by organizations to connect, store, and report on assets, income, and expenses.\nHere, the discipline relies on a range of products, from spreadsheets (invariably as a starting point, and frequently in total) through commercial EPM and BI tools, often BusinessObjects (SAP), OBI EE (Oracle), Cognos (IBM), and Power BI (Microsoft).\nSee Financial modeling \u00a7 Accounting for discussion.\n\n\n== See also ==\nFinancial management for IT services, financial management of IT assets and resources\nFinancial Management Service, a bureau of the U.S. Treasury which provides financial services for the government.\nFinancial mismanagement\nFinancial risk management \u00a7 Corporate finance\nFP&A\nManagerial finance\n\n\n== References ==\n\n\n== Further reading ==\nLawrence Gitman and Chad J. Zutter  (2019). Principles of Managerial Finance, 14th edition, Addison-Wesley Publishing, ISBN 978-0133507690.\nClive Marsh (2009). Mastering Financial Management, Financial Times Prentice Hall ISBN 978-0-273-72454-4\nJames Van Horne and John Wachowicz (2009). Fundamentals of Financial Management, 13th ed., Pearson Education Limited. ISBN 9705614229"}, {"id": 72, "title": "Limited overs cricket", "content": "Limited overs cricket, also known as one-day cricket or white ball cricket, is a version of the sport of cricket in which a match is generally completed in one day. There are a number of formats, including List A cricket (8-hour games), Twenty20 cricket (3-hour games), and 100-ball cricket (2.5 hours). The name reflects the rule that in the match each team bowls a set maximum number of overs (sets of 6 legal balls), usually between 20 and 50, although shorter and longer forms of limited overs cricket have been played.\nThe concept contrasts with Test and first-class matches, which can take up to five days to complete. One-day cricket is popular with spectators as it can encourage aggressive, risky, entertaining batting, often results in cliffhanger endings, and ensures that a spectator can watch an entire match without committing to five days of continuous attendance.\n\n\n== Structure ==\nEach team bats only once, and each innings is limited to a set number of overs, usually fifty in a One Day International and between forty and sixty in a List A. List A is a classification of the limited-overs (one-day) form of cricket, technically as the domestic level.\nDespite its name, important one-day matches, international and domestic, often have two days set aside, the second day being a \"reserve\" day to allow more chance of the game being completed if a result is not possible on the first day (for instance if play is prevented or interrupted by rain).\n\n\n=== Tiebreaker ===\n\nIn some tied limited-overs games, a Super Over is played, wherein each team bats for a one-over innings with two wickets in hand. A tied Super Over may be followed by another Super Over.\n\n\n=== Player restrictions ===\n\n\n==== Bowling restrictions ====\nIn almost all competitive one-day games, a restriction is placed on the number of overs that may be bowled by any one bowler. This is to prevent a side playing two top-class bowlers with extremely good stamina who can bowl throughout their opponents' innings. The usual limitation is set so that a side must include at least five players who bowl i.e. each bowler can only bowl 20% of the overs. For example, the usual limit for twenty-over cricket is four overs per bowler, for forty-over cricket eight per bowler and for fifty-over cricket ten per bowler. There are exceptions: Pro Cricket in the United States restricted bowlers to five overs each, thus leaving a side requiring only four bowlers.\n\n\n==== Fielding restrictions ====\n\n\n=== White balls ===\n\nLimited over cricket is usually played with white balls rather than the traditional red balls.  This was introduced because the team batting second is likely to need to play under floodlights and a white ball is easier to see under these conditions.  The white balls are supposed to be otherwise identical to traditional balls, but according to BBC Sport, some cricketers claim that the harder surface causes white balls to swing more.\n\n\n== History ==\nThe idea for a one-day, limited 50-over cricket tournament, was first played in the inaugural match of the All India Pooja Cricket Tournament in 1951 at Tripunithura in Kochi, Kerala. It is thought to be the brain child of KV Kelappan Thampuran, a former cricketer and the first Secretary of the Kerala Cricket Association. The first limited-overs tournament between first-class English teams was the Midlands Knock-Out Cup, which took place in May 1962. Played with 65-over innings, the Cup was organised by Mike Turner, secretary of the Leicestershire County Cricket Club. The competition was small, with three other county teams participating in addition to Leicestershire. However, it drew commercial television coverage and positive commentary by journalists, who noted the potential to attract sponsors and spectators amid declining attendance levels.The following year, the first full-scale one-day competition between first-class teams was played, the knock-out Gillette Cup, won by Sussex. The number of overs was reduced to 60 for the 1964 season. League one-day cricket also began in England, when the John Player Sunday League was started in 1969 with 40-over matches. Both these competitions have continued every season since inauguration, though the sponsorship has changed. There is now one 50-over competition, which is called the Royal London One-Day Cup.\nThe first Limited Overs International (LOI) or One-Day International (ODI) match was played between Australia and England in Melbourne on 5 January 1971, and the quadrennial cricket World Cup began in 1975. Many of the \"packaging\" innovations, such as coloured clothing, were as a result of World Series Cricket, a \"rebel\" series set up outside the cricketing establishment by Australian entrepreneur Kerry Packer. For more details, see History of cricket.\nTwenty20, a curtailed form of one-day cricket with 20 overs (120 legal balls) per side, was first played in England in 2003. It has proven very popular, and several Twenty20 matches have been played between national teams. It makes several changes to the usual laws of cricket, including the use of a Super Over (one or more additional overs played by each team) to decide the result of tied matches.\n100-ball cricket (2.5-hour games), another form of one-day cricket with 100 deliveries per side, launched in England in 2021. It is designed to further shorten game time and to attract a new audience. It makes further changes to the usual laws of cricket, such as the involvement of overs that last 5 balls each.\nThere are now also T10 leagues with a format of 10 overs per side (resulting in 90-minute games). The Emirates Cricket Board also launched Ninety\u201390 Bash, an upcoming annual franchise-based 90-ball cricket league in the United Arab Emirates.\n\n\n== One Day Internationals ==\n\nOne Day International matches are usually played in brightly coloured clothing often in a \"day-night\" format where the first innings of the day occurs in the afternoon and the second occurs under stadium lights.\n\n\n=== One Day International tournaments ===\nIn the early days of ODI cricket, the number of overs was generally 60 overs per side, and matches were also played with 40, 45 or 55 overs per side, but now it has been uniformly fixed at 50 overs.\nEvery four years, the Cricket World Cup involves all the Test-playing nations and other national sides who qualify through the ICC World Cup Qualifier. It usually consists of round-robin stages, followed by semi-finals and a final.  The International Cricket Council (ICC) determines the venue far in advance.\nThe ICC Champions Trophy involves all the Test-playing nations, and is held between World Cups. It usually consists of a round-robin group stage, semifinals, and a final.\nEach Test-playing country often hosts triangular tournaments, between the host nation and two touring sides. There is usually a round-robin group , and then the leading two teams play each other in a final, or sometimes a best-of-three final. When there is only one touring side, there is still often a best-of-five or best-of-seven series of limited overs matches.\nThe ICC World Cricket League is an ODI competition for national teams with Associate or Affiliate status.\n\n\n== List A status ==\n\nList A cricket is a classification of the limited-overs (one-day) form of the sport of cricket. Much as domestic first-class cricket is the level below international Test match cricket, so List A cricket is the domestic level of one-day cricket below One Day Internationals. Twenty20 matches do not qualify for the present.\nMost cricketing nations have some form of domestic List A competition. The number of overs in List A cricket ranges from forty to sixty overs per side.\nThe Association of Cricket Statisticians and Historians created this category for the purpose of providing an equivalent to first-class cricket, to allow the generation of career records and statistics for comparable one-day matches. Only the more important one-day competitions in each country, plus matches against a touring Test team, are included. The categorisation of cricket matches as \"List A\" was not officially endorsed by the International Cricket Council until 2006, when the ICC announced that it and its member associations would be determining this classification in a manner similar to that done for first class matches.Matches that qualify as List A:\n\nOne Day Internationals (ODIs)\nOther international matches\nPremier one-day tournaments in each country\nOfficial matches of a touring Test team against main first-class teamsMatches that do not qualify as List A:\n\nWorld Cup warm-up matches\nOther Tourist matches (for example, against first-class teams that are not part of the main domestic first-class competition, such as universities)\nFestival and friendly matches\n\n\n== Domestic competitions ==\nDomestic one-day competitions exist in almost every country where cricket is played. The table below lists the limited overs tournaments that take place in each full member nation.\n\n\n== One-day records ==\n\nThe world record for the highest innings total in any List A limited overs match is 496 for 4 by Surrey against Gloucestershire in their Friends Provident Trophy 50-overs match at the Oval, London on 29 April 2007. That surpassed the 443 for nine by Sri Lanka against the Netherlands in their One Day International 50-overs match at Amstelveen on 4 July 2006, which was the record ODI score at the time. On 19 June 2018, England set a new international record, totalling 481 for 6 against Australia at Trent Bridge. The lowest ever total is 23 by Yorkshire against Middlesex at Headingley in 1974 in a 40-overs match. The record low score in ODIs was set by Zimbabwe, who managed just 35 against Sri Lanka in Harare on 25 April 2004.\nThe most runs scored by both sides in any List A limited overs match is 872: Australia, batting first, scored 434 for four in 50 overs, and yet were beaten by South Africa who scored 438 for nine with a ball to spare during their One Day International at Johannesburg in 2006.\nThe highest individual innings is 268 by Ali Brown for Surrey against Glamorgan in a 50-overs match at The Oval in 2002. The best bowling figures are eight for 15 by Rahul Sanghvi for Delhi against Himachal Pradesh in a 50-overs match at Una in 1997. The highest international individual innings is by Rohit Sharma who scored 264. The highest score in any formal limited overs match is believed to be United's 630 for five against Bay Area in a 45 overs match at Richmond, California in August 2006.The most runs in an over was scored by Herschelle Gibbs of the South African cricket team when, in the 2007 Cricket World Cup in the West Indies, he hit 6 sixes in one over bowled by Daan van Bunge of the Netherlands.This record is shared by Yuvraj Singh of India who achieved this feat in the 2007 ICC World Twenty20 in South Africa, he hit 6 sixes in an over bowled by Stuart Broad of England.\nSachin Tendulkar holds the record of being the first male cricketer to score a double century in ODIs (200 not out). He achieved this feat against South Africa on 24 February 2010, at Gwalior, India. Virender Sehwag is the second male cricketer to score a double century, when he scored 219 before being caught out against West Indies on 8 December 2011, at Indore, India. Rohit Sharma became the third male cricketer to score a double century, when he scored 209 against Australia on 2 November 2013.\n\n\n== See also ==\nT10 League\nDuckworth\u2013Lewis\u2013Stern method\n\n\n== References ==\n\n\n== Sources =="}, {"id": 73, "title": "Solar Cell Efficiency", "content": "Solar-cell efficiency refers to the portion of energy in the form of sunlight that can be converted via photovoltaics into electricity by the solar cell.\nThe efficiency of the solar cells used in a photovoltaic system, in combination with latitude and climate, determines the annual energy output of the system. For example, a solar panel with 20% efficiency and an area of 1 m2 will produce 200 kWh/yr at Standard Test Conditions if exposed to the Standard Test Condition solar irradiance value of 1000 W/m2 for 2.74 hours a day. Usually solar panels are exposed to sunlight for longer than this in a given day, but the solar irradiance is less than 1000 W/m2 for most of the day. A solar panel can produce more when the sun is high in the sky and will produce less in cloudy conditions or when the sun is low in the sky, usually the sun is lower in the sky in the winter. \nTwo location dependant factors that affect solar PV yield are the dispersion and intensity of solar radiation. These two variables can vary greatly between each country. The global regions that have high radiation levels throughout the year are the middle east, Northern Chile, Australia, China, and Southwestern USA. In a high-yield solar area like central Colorado, which receives annual insolation of 2000 kWh/m2/year, a panel can be expected to produce 400 kWh of energy per year. However, in Michigan, which receives only 1400 kWh/m2/year, annual energy yield will drop to 280 kWh for the same panel. At more northerly European latitudes, yields are significantly lower: 175 kWh annual energy yield in southern England under the same conditions.\nSeveral factors affect a cell's conversion efficiency, including its reflectance, thermodynamic efficiency, charge carrier separation efficiency, charge carrier collection efficiency and conduction efficiency values. Because these parameters can be difficult to measure directly, other parameters are measured instead, including quantum efficiency, open-circuit voltage (VOC) ratio, and \u00a7 Fill factor.  Reflectance losses are accounted for by the quantum efficiency value, as they affect \"external quantum efficiency\". Recombination losses are accounted for by the quantum efficiency, VOC ratio, and fill factor values. Resistive losses are predominantly accounted for by the fill factor value, but also contribute to the quantum efficiency and VOC ratio values. \nAs of 2022, the world record for solar cell efficiency is 47.1%, set in 2019 by multi-junction concentrator solar cells developed at National Renewable Energy Laboratory (NREL), Golden, Colorado, USA. This record was set in lab conditions, under extremely concentrated light. The record in real-world conditions is also held by NREL, who developed triple junction cells with a tested efficiency of 39.5%.\n\n\n== Factors affecting energy conversion efficiency ==\nThe factors affecting energy conversion efficiency were expounded in a landmark paper by William Shockley and Hans Queisser in 1961. See Shockley\u2013Queisser limit for more detail.\n\n\n=== Thermodynamic-efficiency limit and infinite-stack limit ===\n\nIf one has a source of heat at temperature Ts and cooler heat sink at temperature Tc, the maximum theoretically possible value for the ratio of work (or electric power) obtained to heat supplied is 1-Tc/Ts, given by a Carnot heat engine. If we take 6000 K for the temperature of the sun and 300 K for ambient conditions on earth, this comes to 95%. In 1981, Alexis de Vos and Herman Pauwels showed that this is achievable with a stack of an infinite number of cells with band gaps ranging from infinity (the first cells encountered by the incoming photons) to zero, with a voltage in each cell very close to the open-circuit voltage, equal to 95% of the band gap of that cell, and with 6000 K blackbody radiation coming from all directions. However, the 95% efficiency thereby achieved means that the electric power is 95% of the net amount of light absorbed \u2013 the stack emits radiation as it has non-zero temperature, and this radiation has to be subtracted from the incoming radiation when calculating the amount of heat being transferred and the efficiency. They also considered the more relevant problem of maximizing the power output for a stack being illuminated from all directions by 6000 K blackbody radiation. In this case, the voltages must be lowered to less than 95% of the band gap (the percentage is not constant over all the cells). The maximum theoretical efficiency calculated is 86.8% for a stack of an infinite number of cells, using the incoming concentrated sunlight radiation. When the incoming radiation comes only from an area of the sky the size of the sun, the efficiency limit drops to 68.7%.\n\n\n=== Ultimate efficiency ===\nNormal photovoltaic systems however have only one p\u2013n junction and are therefore subject to a lower efficiency limit, called the \"ultimate efficiency\" by Shockley and Queisser. Photons with an energy below the band gap of the absorber material cannot generate an electron-hole pair, so their energy is not converted to useful output, and only generates heat if absorbed. For photons with an energy above the band gap energy, only a fraction of the energy above the band gap can be converted to useful output. When a photon of greater energy is absorbed, the excess energy above the band gap is converted to kinetic energy of the carrier combination. The excess kinetic energy is converted to heat through phonon interactions as the kinetic energy of the carriers slows to equilibrium velocity. Traditional single-junction cells with an optimal band gap for the solar spectrum have a maximum theoretical efficiency of 33.16%, the Shockley\u2013Queisser limit .Solar cells with multiple band gap absorber materials improve efficiency by dividing the solar spectrum into smaller bins where the thermodynamic efficiency limit is higher for each bin.\n\n\n=== Quantum efficiency ===\n\nAs described above, when a photon is absorbed by a solar cell it can produce an electron-hole pair. One of the carriers may reach the p\u2013n junction and contribute to the current produced by the solar cell; such a carrier is said to be collected. Or, the carriers recombine with no net contribution to cell current.\nQuantum efficiency refers to the percentage of photons that are converted to electric current (i.e., collected carriers) when the cell is operated under short circuit conditions. There are two types of quantum that are usually referred to when talking about solar cells. The external quantum efficiency, that relates to the external measurable properties of the solar cell. The \"external\" quantum efficiency of a silicon solar cell includes the effect of optical losses such as transmission and reflection. In particular, some measures can be taken to reduce these losses. The reflection losses, which can account for up to 10% of the total incident energy, can be dramatically decreased using a technique called texturization, a light trapping method that modifies the average light path.The second type is the internal quantum efficiency, this measurement of the internal quantum efficiency gives a deeper insight of the internal material parameters like the absorption coefficient or internal luminescence quantum efficiency. The internal quantum efficiency is mainly used when it comes to the understanding of the potential of a certain material rather than a device.Quantum efficiency is most usefully expressed as a spectral measurement (that is, as a function of photon wavelength or energy). Since some wavelengths are absorbed more effectively than others, spectral measurements of quantum efficiency can yield valuable information about the quality of the semiconductor bulk and surfaces. However, the quantum efficiency alone is not the same as overall energy conversion efficiency, as it does not convey information about the fraction of power that is converted by the solar cell.\n\n\n=== Maximum power point ===\nA solar cell may operate over a wide range of voltages (V) and currents (I). By increasing the resistive load on an irradiated cell continuously from zero (a short circuit) to a very high value (an open circuit) one can determine the maximum power point, the point that maximizes V\u00d7I; that is, the load for which the cell can deliver maximum electrical power at that level of irradiation. (The output power is zero in both the short circuit and open circuit extremes).\nThe maximum power point of a solar cell is affected by its temperature. Knowing the technical data of certain solar cell, its power output at a certain temperature can be obtained by \n  \n    \n      \n        P\n        (\n        T\n        )\n        =\n        \n          P\n          \n            S\n            T\n            C\n          \n        \n        +\n        \n          \n            \n              d\n              P\n            \n            \n              d\n              T\n            \n          \n        \n        (\n        \n          T\n          \n            c\n            e\n            l\n            l\n          \n        \n        \u2212\n        \n          T\n          \n            S\n            T\n            C\n          \n        \n        )\n      \n    \n    {\\displaystyle P(T)=P_{STC}+{\\frac {dP}{dT}}(T_{cell}-T_{STC})}\n  , where \n  \n    \n      \n        \n          P\n          \n            S\n            T\n            C\n          \n        \n      \n    \n    {\\displaystyle P_{STC}}\n   is the power generated at the standard testing condition; \n  \n    \n      \n        \n          T\n          \n            c\n            e\n            l\n            l\n          \n        \n      \n    \n    {\\displaystyle T_{cell}}\n   is the actual temperature of the solar cell.\nA high quality, monocrystalline silicon solar cell, at 25 \u00b0C cell temperature, may produce 0.60 V open-circuit (VOC). The cell temperature in full sunlight, even with 25 \u00b0C air temperature, will probably be close to 45 \u00b0C, reducing the open-circuit voltage to 0.55 V per cell. The voltage drops modestly, with this type of cell, until the short-circuit current is approached (ISC). Maximum power (with 45 \u00b0C cell temperature) is typically produced with 75% to 80% of the open-circuit voltage (0.43 V in this case) and 90% of the short-circuit current. This output can be up to 70% of the VOC x ISC product. The short-circuit current (ISC) from a cell is nearly proportional to the illumination, while the open-circuit voltage (VOC) may drop only 10% with an 80% drop in illumination. Lower-quality cells have a more rapid drop in voltage with increasing current and could produce only 1/2 VOC at 1/2 ISC. The usable power output could thus drop from 70% of the VOC x ISC product to 50% or even as little as 25%. Vendors who rate their solar cell \"power\" only as VOC x ISC, without giving load curves, can be seriously distorting their actual performance.\nThe maximum power point of a photovoltaic varies with incident illumination. For example, accumulation of dust on photovoltaic panels reduces the maximum power point. Recently, new research to remove dust from solar panels has been developed by utilizing electrostatic cleaning systems. In such systems, an applied electrostatic field at the surface of the solar panels causes the dust particles to move in a \"flip-flop\" manner. Then, due to gravity and the fact that the solar panels are slightly slanted, the dust particles get pulled downward by gravity. These systems only require a small power consumption and enhance the performance of the solar cells, especially when installed in the desert, where dust accumulation contributes to decreasing the solar panel's performance. Also, for systems large enough to justify the extra expense, a maximum power point tracker tracks the instantaneous power by continually measuring the voltage and current (and hence, power transfer), and uses this information to dynamically adjust the load so the maximum power is always transferred, regardless of the variation in lighting.\n\n\n=== Fill factor ===\nAnother defining term in the overall behaviour of a solar cell is the fill factor (FF). This factor is a measure of quality of a solar cell. This is the available power at the maximum power point (Pm) divided by the open circuit voltage (VOC) and the short circuit current (ISC):\n\n  \n    \n      \n        F\n        F\n        =\n        \n          \n            \n              P\n              \n                m\n              \n            \n            \n              \n                V\n                \n                  O\n                  C\n                \n              \n              \u00d7\n              \n                I\n                \n                  S\n                  C\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \u03b7\n              \u00d7\n              \n                A\n                \n                  c\n                \n              \n              \u00d7\n              G\n            \n            \n              \n                V\n                \n                  O\n                  C\n                \n              \n              \u00d7\n              \n                I\n                \n                  S\n                  C\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle FF={\\frac {P_{m}}{V_{OC}\\times I_{SC}}}={\\frac {\\eta \\times A_{c}\\times G}{V_{OC}\\times I_{SC}}}.}\n  The fill factor can be represented graphically by the IV sweep, where it is the ratio of the different rectangular areas.The fill factor is directly affected by the values of the cell's series, shunt resistances and diodes losses. Increasing the shunt resistance (Rsh) and decreasing the series resistance (Rs) lead to a higher fill factor, thus resulting in greater efficiency, and bringing the cell's output power closer to its theoretical maximum.Typical fill factors range from 50% to 82%. The fill factor for a normal silicon PV cell is 80%.\n\n\n== Comparison ==\n\nEnergy conversion efficiency is measured by dividing the electrical output by the incident light power. Factors influencing output include spectral distribution, spatial distribution of power, temperature, and resistive load. IEC standard 61215 is used to compare the performance of cells and is designed around standard (terrestrial, temperate) temperature and conditions (STC): irradiance of 1 kW/m2, a spectral distribution close to solar radiation through AM (airmass) of 1.5 and a cell temperature 25 \u00b0C.  The resistive load is varied until the peak or maximum power point (MPP) is achieved.  The power at this point is recorded as Watt-peak (Wp).  The same standard is used for measuring the power and efficiency of PV modules.\nAir mass affects output. In space, where there is no atmosphere, the spectrum of the sun is relatively unfiltered. However, on earth, air filters the incoming light, changing the solar spectrum. The filtering effect ranges from Air Mass 0 (AM0) in space, to approximately Air Mass 1.5 on Earth. Multiplying the spectral differences by the quantum efficiency of the solar cell in question yields the efficiency. Terrestrial efficiencies typically are greater than space efficiencies. For example, a silicon solar cell in space might have an efficiency of 14% at AM0, but 16% on earth at AM 1.5. Note, however, that the number of incident photons in space is considerably larger, so the solar cell might produce considerably more power in space, despite the lower efficiency as indicated by reduced percentage of the total incident energy captured.\nSolar cell efficiencies vary from 6% for amorphous silicon-based solar cells to 44.0% with multiple-junction production cells and 44.4% with multiple dies assembled into a hybrid package. Solar cell energy conversion efficiencies for commercially available multicrystalline Si solar cells are around 14\u201319%. The highest efficiency cells have not always been the most economical \u2013 for example a 30% efficient multijunction cell based on exotic materials such as gallium arsenide or indium selenide produced at low volume might well cost one hundred times as much as an 8% efficient amorphous silicon cell in mass production, while delivering only about four times the output.\nHowever, there is a way to \"boost\" solar power. By increasing the light intensity, typically photogenerated carriers are increased, increasing efficiency by up to 15%. These so-called \"concentrator systems\" have only begun to become cost-competitive as a result of the development of high efficiency GaAs cells. The increase in intensity is typically accomplished by using concentrating optics. A typical concentrator system may use a light intensity 6\u2013400 times the sun, and increase the efficiency of a one sun GaAs cell from 31% at AM 1.5 to 35%.\nA common method used to express economic costs is to calculate a price per delivered kilowatt-hour (kWh). The solar cell efficiency in combination with the available irradiation has a major influence on the costs, but generally speaking the overall system efficiency is important. Commercially available solar cells (as of 2006) reached system efficiencies between 5 and 19%.\nUndoped crystalline silicon devices are approaching the theoretical limiting efficiency of 29.43%. In 2017, efficiency of 26.63% was achieved in an amorphous silicon/crystalline silicon heterojunction cell that place both positive and negative contacts on the back of the cell.\n\n\n=== Energy payback ===\n\nThe energy payback time is defined as the recovery time required for generating the energy spent for manufacturing a modern photovoltaic module. In 2008, it was estimated to be from 1 to 4 years depending on the module type and location. With a typical lifetime of 20 to 30 years, this means that modern solar cells would be net energy producers, i.e., they would generate more energy over their lifetime than the energy expended in producing them. Generally, thin-film technologies\u2014despite having comparatively low conversion efficiencies\u2014achieve significantly shorter energy payback times than conventional systems (often < 1 year).A study published in 2013 which the existing literature found that energy payback time was between 0.75 and 3.5 years with thin film cells being at the lower end and multi-si-cells having a payback time of 1.5\u20132.6 years. A 2015 review assessed the energy payback time and EROI of solar photovoltaics. In this meta study, which uses an insolation of 1,700 kWh/m2/year and a system lifetime of 30 years, mean harmonized EROIs between 8.7 and 34.2 were found. Mean harmonized energy payback time varied from 1.0 to 4.1 years. Crystalline silicon devices achieve on average an energy payback period of 2 years.Like any other technology, solar cell manufacture is dependent on the existence of a complex global industrial manufacturing system. This includes the fabrication systems typically accounted for in estimates of manufacturing energy; the contingent mining, refining and global transportation systems; and other energy intensive support systems including finance, information, and security systems. The difficulty in measuring such energy overhead confers some uncertainty on any estimate of payback times.\n\n\n== Technical methods of improving efficiency ==\n\n\n=== Choosing optimum transparent conductor ===\nThe illuminated side of some types of solar cells, thin films, have a transparent conducting film to allow light to enter into the active material and to collect the generated charge carriers. Typically, films with high transmittance and high electrical conductance such as indium tin oxide, conducting polymers or conducting nanowire networks are used for the purpose. There is a trade-off between high transmittance and electrical conductance, thus optimum density of conducting nanowires or conducting network structure should be chosen for high efficiency.\n\n\n=== Promoting light scattering ===\nThe inclusion of light-scattering effects in solar cells is a photonic strategy to increase the absorption for the lower-energy sunlight photons (chiefly in near-infrared range) for which the photovoltaic material presents reduced absorption coefficient. Such light-trapping scheme is accomplished by the deviation of the light rays from the incident direction, thereby increasing their path length in the cells' absorber. Conventional approaches used to implement light diffusion are based on textured rear/front surfaces, but many alternative optical designs have been demonstrated with promising results based in diffraction gratings, arrays of metal or dielectric nano/micro particles, wave-optical micro-structuring, among others. When applied in the devices' front these structures can act as geometric anti-reflective coatings, simultaneously reducing the reflection of out-going light.\nFor instance, lining the light-receiving surface of the cell with nano-sized metallic studs can substantially increase the cell efficiency. Light reflects off these studs at an oblique angle to the cell, increasing the length of the light path through the cell. This increases the number of photons absorbed by the cell and the amount of current generated. The main materials used for the nano-studs are silver, gold, and aluminium. Gold and silver are not very efficient, as they absorb much of the light in the visible spectrum, which contains most of the energy present in sunlight, reducing the amount of light reaching the cell. Aluminium absorbs only ultraviolet radiation, and reflects both visible and infra-red light, so energy loss is minimized. Aluminium can increase cell efficiency up to 22% (in lab conditions).\n\n\n=== Anti-reflective coatings and textures ===\nAnti-reflective coatings are engineered to reduce the sunlight reflected from the solar cells, therefore enhancing the light transmitted into the photovoltaic absorber. This can be accomplished by causing the destructive interference of the reflected light waves, such as with coatings based on the front (multi-)layer composition, and/or by geometric refractive-index matching caused by the surface topography, with many architectures inspired by nature. For example, the nipple-array, a hexagonal array of subwavelength conical nanostructures, can be seen at the surface of the moth's eyes. It was reported that utilizing this sort of surface architecture minimizes the reflection losses by 25%, converting the additional captured photon to a 12% increase in a solar cell's energy.The use of front micro-structures, such as those achieved with texturizing or other photonic features, can also be used as a method to achieve anti-reflectiveness, in which the surface of a solar cell is altered so that the impinging light experiences a gradually increasing effective refractive-index when travelling from air towards the photovoltaic material. These surfaces can be created by etching or using lithography. Concomitantly, they promote light scattering effects which further enhance the absorption, particularly of the longer wavelength sunlight photons. Adding a flat back surface in addition to texturizing the front surface further helps to trap the light within the cell, thus providing a longer optical path.\n\n\n=== Radiative cooling ===\n\nAn increase in solar cell temperature of approximately 1 \u00b0C causes an efficiency decrease of about 0.45%. To prevent this, a transparent silica crystal layer can be applied to solar panels. The silica layer acts as a thermal black body which emits heat as infrared radiation into space, cooling the cell up to 13 \u00b0C. Radiative cooling can thus extend the life of solar cells. Full-system integration of solar energy and radiative cooling is referred to as a combined SE\u2013RC system, which have demonstrated higher energy gain per unit area when compared to non-integrated systems.\n\n\n=== Rear surface passivation ===\n\nSurface passivation is critical to solar cell efficiency. Many improvements have been made to the front side of mass-produced solar cells, but the aluminium back-surface is impeding efficiency improvements. The efficiency of many solar cells has benefitted by creating so-called passivated emitter and rear cells (PERCs). The chemical deposition of a rear-surface dielectric passivation layer stack that is also made of a thin silica or aluminium oxide film topped with a silicon nitride film helps to improve efficiency in silicon solar cells. This helped increase cell efficiency for commercial Cz-Si wafer material from just over 17% to over 21% by the mid-2010s, and the cell efficiency for quasi-mono-Si to a record 19.9%.\nConcepts of the rear surface passivation for silicon solar cells has also been implemented for CIGS solar cells. The rear surface passivation shows the potential to improve the efficiency. Al2O3 and SiO2 have been used as the passivation materials. Nano-sized point contacts on Al2O3 layer and line contacts on SiO2 layer provide the electrical connection of CIGS absorber to the rear electrode Molybdenum. The point contacts on the Al2O3 layer are created by e-beam lithography and the line contacts on the SiO2 layer are created using photolithography. Also, the implementation of the passivation layers does not change the morphology of the CIGS layers.\n\n\n=== Thin film materials ===\n\nAlthough not constituting a direct strategy to improve efficiency, thin film materials show a lot of promise for solar cells in terms of low costs and adaptability to existing structures and frameworks in technology. Since the materials are so thin, they lack the optical absorption of bulk material solar cells. Attempts to correct this have been demonstrated, such as light-trapping schemes promoting light scattering. Also important is thin film surface recombination. Since this is the dominant recombination process of nanoscale thin-film solar cells, it is crucial to their efficiency. Adding a passivating thin layer of silicon dioxide could reduce recombination.\n\n\n=== Tandem cells ===\n\nTandem solar cells combine two materials to increase efficiency. In 2022 a device was announced that combined multiple perovskite with multiple layers of silicon. Perovskites harvest blue light, while silicon picks up red and infrared wavelengths. The cell achieved  32.5% efficiency.\n\n\n== See also ==\nEnvironmental impact of the energy industry\nEnergy efficiency\n\n\n== References ==\n\n\n== External links ==\n\nSolar electric at Curlie\n\"How Can We Increase the Efficiency of Solar Panels?\".\n\"Factors That Affect Solar Panel Efficiency\". 18 July 2021."}, {"id": 74, "title": "Power", "content": "In social science and politics, power is the social production of an effect that determines the capacities, actions, beliefs, or conduct of actors. Power does not exclusively refer to the threat or use of force (coercion) by one actor against another, but may also be exerted through diffuse means (such as institutions). Power may also take structural forms, as it orders actors in relation to one another (such as distinguishing between a master and an enslaved person, a householder and their relatives, an employer and their employees, a parent and a child, a political representative and their voters, etc.), and discursive forms, as categories and language may lend legitimacy to some behaviors and groups over others.The term authority is often used for power that is perceived as legitimate or socially approved by the social structure. Power can be seen as evil or unjust; however, power can also be seen as good and as something inherited or given for exercising humanistic objectives that will help, move, and empower others as well.\nScholars have distinguished the differences between soft power and hard power.\n\n\n== Theories ==\n\n\n=== Five bases of power ===\n\nIn a now-classic study (1959), social psychologists John R. P. French and Bertram Raven developed a schema of sources of power by which to analyse how power plays work (or fail to work) in a specific relationship.\nAccording to French and Raven, power must be distinguished from influence in the following way: power is that state of affairs that holds in a given relationship, A-B, such that a given influence attempt by A over B makes A's desired change in B more likely. Conceived this way, power is fundamentally relative; it depends on the specific understandings A and B each apply to their relationship and requires B's recognition of a quality in A that would motivate B to change in the way A intends. A must draw on the 'base' or combination of bases of power appropriate to the relationship to effect the desired outcome. Drawing on the wrong power base can have unintended effects, including a reduction in A's own power.\nFrench and Raven argue that there are five significant categories of such qualities, while not excluding other minor categories. Further bases have since been adduced, in particular by Gareth Morgan in his 1986 book, Images of Organization.\n\n\n==== Legitimate power ====\n\nAlso called \"positional power\", legitimate power is the power of an individual because of the relative position and duties of the holder of the position within an organization. Legitimate power is formal authority delegated to the holder of the position. It is usually accompanied by various attributes of power, such as a uniform, a title, or an imposing physical office.\nIn simple terms, power can be expressed as being upward or downward. With downward power, a company's superiors influence subordinates to attain organizational goals. When a company exhibits upward power, subordinates influence the decisions of their leader or leaders.\n\n\n==== Referent power ====\n\nReferent power is the power or ability of individuals to attract others and build loyalty. It is based on the charisma and interpersonal skills of the powerholder. A person may be admired because of a specific personal trait, and this admiration creates the opportunity for interpersonal influence. Here, the person under power desires to identify with these personal qualities and gains satisfaction from being an accepted follower. Nationalism and patriotism count towards an intangible sort of referent power. For example, soldiers fight in wars to defend the honor of the country. This is the second-least obvious power but the most effective. Advertisers have long used the referent power of sports figures for product endorsements, for example. The charismatic appeal of the sports star supposedly leads to an acceptance of the endorsement, although the individual may have little real credibility outside the sports arena. Abuse is possible when someone who is likable yet lacks integrity and honesty rises to power, placing them in a situation to gain personal advantage at the cost of the group's position. Referent power is unstable alone and is not enough for a leader who wants longevity and respect. When combined with other sources of power, however, it can help a person achieve great success.\n\n\n==== Expert power ====\n\nExpert power is an individual's power deriving from the skills or expertise of the person and the organization's needs for those skills and expertise. Unlike the others, this type of power is usually highly specific and limited to the particular area in which the expert is trained and qualified. When they have knowledge and skills that enable them to understand a situation, suggest solutions, use solid judgment, and generally outperform others, then people tend to listen to them. When individuals demonstrate expertise, people tend to trust them and respect what they say. As subject-matter experts, their ideas will have more value, and others will look to them for leadership in that area.\n\n\n==== Reward power ====\n\nReward power depends on the ability of the power wielder to confer valued material rewards; it refers to the degree to which the individual can give others a reward of some kind, such as benefits, time off, desired gifts, promotions, or increases in pay or responsibility. This power is obvious, but it is also ineffective if abused. People who abuse reward power can become pushy or be reprimanded for being too forthcoming or 'moving things too quickly'. If others expect to be rewarded for doing what someone wants, there is a high probability that they will do it. The problem with this basis of power is that the rewarder may not have as much control over rewards as may be required. Supervisors rarely have complete control over salary increases, and managers often cannot control all actions in isolation; even a company CEO needs permission from the board of directors for some actions. When an individual uses up available rewards or the rewards do not have enough perceived value for others, their power weakens. One of the frustrations of using rewards is that they often need to be bigger each time if they are to have the same motivational impact. Even then, if rewards are given frequently, people can become so satiated by the reward it loses its effectiveness.\nIn terms of cancel culture, the mass ostracization used to reconcile unchecked injustice and abuse of power is an \"upward power.\" Policies for policing the internet against these processes as a pathway for creating due process for handling conflicts, abuses, and harm that is done through established processes are known as \"downward power.\"\n\n\n==== Coercive power ====\n\nCoercive power is the application of negative influences. It includes the ability to defer or withhold other rewards. The desire for valued rewards or the fear of having them withheld can ensure the obedience of those under power. Coercive power tends to be the most obvious but least effective form of power, as it builds resentment and resistance from the people who experience it. Threats and punishment are common tools of coercion. Implying or threatening that someone will be fired, demoted, denied privileges, or given undesirable assignments \u2013 these are characteristics of using coercive power. Extensive use of coercive power is rarely appropriate in an organizational setting, and relying on these forms of power alone will result in a very cold, impoverished style of leadership. This is a type of power commonly seen in the fashion industry by coupling with legitimate power; it is referred to in the industry-specific literature as \"glamorization of structural domination and exploitation\".\n\n\n=== Principles in interpersonal relationships ===\nAccording to Laura K. Guerrero and Peter A. Andersen in Close Encounters: Communication in Relationships:\nPower as a perception: Power is a perception in the sense that some people can have objective power but still have trouble influencing others. People who use power cues and act powerfully and proactively tend to be perceived as powerful by others. Some people become influential even though they don't overtly use powerful behavior.\nPower as a relational concept: Power exists in relationships. The issue here is often how much relative power a person has in comparison to one's partner. Partners in close and satisfying relationships often influence each other at different times in various arenas.\nPower as resource-based: Power usually represents a struggle over resources. The more scarce and valued resources are, the more intense and protracted the power struggles. The scarcity hypothesis indicates that people have the most power when the resources they possess are hard to come by or are in high demand. However, scarce resources lead to power only if they are valued within a relationship.\nThe principle of least interest and dependence power: The person with less to lose has greater power in the relationship. Dependence power indicates that those who are dependent on their relationship or partner are less powerful, especially if they know their partner is uncommitted and might leave them. According to interdependence theory, the quality of alternatives refers to the types of relationships and opportunities people could have if they were not in their current relationship. The principle of least interest suggests that if a difference exists in the intensity of positive feelings between partners, the partner who feels the most positive is at a power disadvantage. There's an inverse relationship between interest in a relationship and the degree of relational power.\nPower as enabling or disabling: Power can be enabling or disabling. Research has shown that people are more likely to have an enduring influence on others when they engage in dominant behavior that reflects social skill rather than intimidation. Personal power is protective against pressure and excessive influence by others and/or situational stress. People who communicate through self-confidence and expressive, composed behavior tend to be successful in achieving their goals and maintaining good relationships. Power can be disabling when it leads to destructive patterns of communication. This can lead to the chilling effect, where the less powerful person often hesitates to communicate dissatisfaction, and the demand withdrawal pattern, which is when one person makes demands and the other becomes defensive and withdraws (Mawasha, 2006). Both effects have negative consequences for relational satisfaction.\nPower as a prerogative: The prerogative principle states that the partner with more power can make and break the rules. Powerful people can violate norms, break relational rules, and manage interactions without as much penalty as powerless people. These actions may reinforce the powerful person's dependence on power. In addition, the more powerful person has the prerogative to manage both verbal and nonverbal interactions. They can initiate conversations, change topics, interrupt others, initiate touch, and end discussions more easily than less powerful people. (See expressions of dominance.)\n\n\n=== Rational choice framework ===\nGame theory, with its foundations in the Walrasian theory of rational choice, is increasingly used in various disciplines to help analyze power relationships. One rational-choice definition of power is given by Keith Dowding in his book Power.\nIn rational choice theory, human individuals or groups can be modelled as 'actors' who choose from a 'choice set' of possible actions in order to try to achieve desired outcomes. An actor's 'incentive structure' comprises (its beliefs about) the costs associated with different actions in the choice set and the likelihoods that different actions will lead to desired outcomes.\nIn this setting, we can differentiate between:\n\noutcome power \u2013 the ability of an actor to bring about or help bring about outcomes;\nsocial power \u2013 the ability of an actor to change the incentive structures of other actors in order to bring about outcomes.This framework can be used to model a wide range of social interactions where actors have the ability to exert power over others. For example, a 'powerful' actor can take options away from another's choice set; can change the relative costs of actions; can change the likelihood that a given action will lead to a given outcome; or might simply change the other's beliefs about its incentive structure.\nAs with other models of power, this framework is neutral as to the use of 'coercion'. For example, a threat of violence can change the likely costs and benefits of different actions; so can a financial penalty in a 'voluntarily agreed' contract, or indeed a friendly offer.\n\n\n=== Cultural hegemony ===\nIn the Marxist tradition, the Italian writer Antonio Gramsci elaborated on the role of ideology in creating a cultural hegemony, which becomes a means of bolstering the power of capitalism and of the nation-state. Drawing on Niccol\u00f2 Machiavelli in The Prince and trying to understand why there had been no Communist revolution in Western Europe while it was claimed there had been one in Russia, Gramsci conceptualised this hegemony as a centaur, consisting of two halves. The back end, the beast, represented the more classic material image of power: power through coercion, through brute force, be it physical or economic. But the capitalist hegemony, he argued, depended even more strongly on the front end, the human face, which projected power through 'consent'. In Russia, this power was lacking, allowing for a revolution. However, in Western Europe, specifically in Italy, capitalism had succeeded in exercising consensual power, convincing the working classes that their interests were the same as those of capitalists. In this way, a revolution had been avoided.\nWhile Gramsci stresses the significance of ideology in power structures, Marxist-feminist writers such as Michele Barrett stress the role of ideologies in extolling the virtues of family life. The classic argument to illustrate this point of view is the use of women as a 'reserve army of labour'. In wartime, it is accepted that women perform masculine tasks, while after the war, the roles are easily reversed. Therefore, according to Barrett, the destruction of capitalist economic relations is necessary but not sufficient for the liberation of women.\n\n\n=== Tarnow ===\nEugen Tarnow considers what power hijackers have over air plane passengers and draws similarities with power in the military. He shows that power over an individual can be amplified by the presence of a group. If the group conforms to the leader's commands, the leader's power over an individual is greatly enhanced, while if the group does not conform, the leader's power over an individual is nil.\n\n\n=== Foucault ===\n\nFor Michel Foucault, the real power will always rely on the ignorance of its agents. No single human, group, or actor runs the dispositif (machine or apparatus), but power is dispersed through the apparatus as efficiently and silently as possible, ensuring its agents do whatever is necessary. It is because of this action that power is unlikely to be detected and remains elusive to 'rational' investigation. Foucault quotes a text reputedly written by political economist Jean Baptiste Antoine Auget de Montyon, entitled Recherches et consid\u00e9rations sur la population de la France (1778), but turns out to be written by his secretary Jean-Baptise Moheau (1745\u20131794), and by emphasizing biologist Jean-Baptiste Lamarck, who constantly refers to milieus as a plural adjective and sees into the milieu as an expression as nothing more than water, air, and light confirming the genus within the milieu, in this case the human species, relates to a function of the population and its social and political interaction in which both form an artificial and natural milieu. This milieu (both artificial and natural) appears as a target of intervention for power, according to Foucault, which is radically different from the previous notions on sovereignty, territory, and disciplinary space interwoven into social and political relations that function as a species (biological species). Foucault originated and developed the concept of \"docile bodies\" in his book Discipline and Punish. He writes, \"A body is docile that may be subjected, used, transformed and improved.\n\n\n=== Clegg ===\nStewart Clegg proposes another three-dimensional model with his \"circuits of power\" theory. This model likens the production and organization of power to an electric circuit board consisting of three distinct interacting circuits: episodic, dispositional, and facilitative. These circuits operate at three levels: two are macro and one is micro. The episodic circuit is at the micro level and is constituted of irregular exercise of power as agents address feelings, communication, conflict, and resistance in day-to-day interrelations. The outcomes of the episodic circuit are both positive and negative. The dispositional circuit is constituted of macro level rules of practice and socially constructed meanings that inform member relations and legitimate authority. The facilitative circuit is constituted of macro level technology, environmental contingencies, job design, and networks, which empower or disempower and thus punish or reward agency in the episodic circuit. All three independent circuits interact at \"obligatory passage points\", which are channels for empowerment or disempowerment.\n\n\n=== Galbraith ===\nJohn Kenneth Galbraith (1908\u20132006) in The Anatomy of Power (1983)\nsummarizes the types of power as  \"condign\" (based on  force), \"compensatory\" (through the use of various resources) or \"conditioned\" (the result of persuasion), and the sources of power as \"personality\" (individuals), \"property\" (power-wielders' material resources), and/or \"organizational\" (from sitting higher in an organisational power structure).\n\n\n=== Gene Sharp ===\nGene Sharp, an American professor of political science, believes that power ultimately depends on its bases. Thus, a political regime maintains power because people accept and obey its dictates, laws, and policies. Sharp cites the insight of \u00c9tienne de La Bo\u00e9tie.\nSharp's key theme is that power is not monolithic; that is, it does not derive from some intrinsic quality of those who are in power. For Sharp, political power, the power of any state \u2013 regardless of its particular structural organization \u2013 ultimately derives from the subjects of the state. His fundamental belief is that any power structure relies upon the subjects' obedience to the orders of the ruler(s). If subjects do not obey, leaders have no power.His work is thought to have been influential in the overthrow of Slobodan Milo\u0161evi\u0107, in the 2011 Arab Spring, and other nonviolent revolutions.\n\n\n=== Bj\u00f6rn Kraus ===\nBj\u00f6rn Kraus deals with the epistemological perspective on power regarding the question of the possibilities of interpersonal influence by developing a special form of constructivism (named relational constructivism). Instead of focusing on the valuation and distribution of power, he asks first and foremost what the term can describe at all. Coming from Max Weber's definition of power, he realizes that the term power has to be split into \"instructive power\" and \"destructive power\".:\u200a105\u200a:\u200a126\u200a More precisely, instructive power means the chance to determine the actions and thoughts of another person, whereas destructive power means the chance to diminish the opportunities of another person. How significant this distinction really is, becomes evident by looking at the possibilities of rejecting power attempts: Rejecting instructive power is possible; rejecting destructive power is not. By using this distinction, proportions of power can be analyzed in a more sophisticated way, helping to sufficiently reflect on matters of responsibility.:\u200a139 f.\u200a This perspective permits people to get over an \"either-or-position\" (either there is power or there isn't), which is common, especially in epistemological discourses about power theories, and to introduce the possibility of an \"as well as-position\".:\u200a120\u200a\n\n\n=== Unmarked categories ===\nThe idea of unmarked categories originated in feminism. As opposed to looking at social difference by focusing on what or whom is perceived to be different, theorists who use the idea of unmarked categories insist that one must also look at how whatever is \"normal\" comes to be perceived as unremarkable and what effects this has on social relations. Attending the unmarked category is thought to be a way to analyze linguistic and cultural practices to provide insight into how social differences, including power, are produced and articulated in everyday occurrences.According to the idea of unmarked categories, when the cultural practices of people who occupy positions of relative power or can more easily exercise power seem obvious, they tend not to be explicitly articulated and therefore are perceived as default or baseline practices against which others are evaluated as different, deviant, or aberrant. The unmarked category becomes the standard against which to measure everything else. For example, it is posited that if a protagonist's race is not indicated, most Western readers will assume the protagonist is white; if a sexual identity is not indicated, it will be assumed the protagonist is heterosexual; if the gender of a body is not indicated, it is assumed to be male; if no disability is indicated, it will be assumed the protagonist is able-bodied. These assumptions do not, however, mean the unmarked category is superior, preferable, or more \"natural,\" nor that the practices associated with the unmarked category require less social effort to enact.Although the unmarked category is typically not explicitly noticed and often goes overlooked, it is still necessarily visible. As visible but unnoticed and unremarkable, membership in the unmarked category can be an index of power. For example, whiteness forms an unmarked category not commonly noticeable to the powerful, as they often fall within this category. Social groups can hold this view of power in terms of a variety of social distinctions, such as race, class, gender, ability, and sexuality.\n\n\n=== Counterpower ===\n\nThe term 'counter-power' (sometimes written 'counterpower') is used in a range of situations to describe the countervailing force that can be utilised by the oppressed to counterbalance or erode the power of elites. A general definition has been provided by the anthropologist David Graeber as 'a collection of social institutions set in opposition to the state and capital: from self-governing communities to radical labor unions to popular militias'. Graeber also notes that counter-power can also be referred to as 'anti-power' and 'when institutions [of counter-power] maintain themselves in the face of the state, this is usually referred to as a 'dual power' situation'. Tim Gee, in his 2011 book Counterpower: Making Change Happen, put forward the theory that those disempowered by governments' and elite groups' power can use counterpower to counter this. In Gee's model, counterpower is split into three categories: idea counterpower, economic counterpower, and physical counterpower.Although the term has come to prominence through its use by participants in the global justice/anti-globalization movement of the 1990s onwards, the word has been used for at least 60 years; for instance, Martin Buber's 1949 book 'Paths in Utopia' includes the line 'Power abdicates only under the stress of counter-power'.:\u200a13\u200a\n\n\n=== Other theories ===\nThomas Hobbes (1588\u20131679) defined power as a man's \"present means, to obtain some future apparent good\" (Leviathan, Ch. 10).\nThe thought of Friedrich Nietzsche underlies much 20th-century analysis of power. Nietzsche disseminated ideas on the \"will to power,\" which he saw as the domination of other humans as much as the exercise of control over one's environment.\nSome schools of psychology, notably those associated with Alfred Adler, place power dynamics at the core of their theory (where orthodox Freudians might place sexuality).\nA generalization of power is given as \"what counts as a means of determining a subject's position in a given competition\".\n\n\n== Psychological research ==\nRecent experimental psychology suggests that the more power one has, the less one takes on the perspective of others, implying that the powerful have less empathy. Adam Galinsky, along with several coauthors, found that when those who are reminded of their powerlessness are instructed to draw Es on their forehead, they are 3 times more likely to draw them such that they are legible to others than those who are reminded of their power. Powerful people are also more likely to take action. In one example, powerful people turned off an irritatingly close fan twice as much as less powerful people. Researchers have documented the bystander effect: they found that powerful people are three times as likely to first offer help to a \"stranger in distress\".A study involving over 50 college students suggested that those primed to feel powerful through stating 'power words' were less susceptible to external pressure, more willing to give honest feedback, and more creative.\n\n\n=== Empathy gap ===\n\n\"Power is defined as a possibility to influence others.\":\u200a1137\u200aThe use of power has evolved from centuries. Gaining prestige, honor and reputation is one of the central motives for gaining power in human nature. Power also relates with empathy gaps because it limits the interpersonal relationship and compares the power differences. Having power or not having power can cause a number of psychological consequences. It leads to strategic versus social responsibilities. Research experiments were done as early as 1968 to explore power conflict.\n\n\n==== Past research ====\nEarlier, research proposed that increased power relates to increased rewards and leads one to approach things more frequently. In contrast, decreased power relates to more constraint, threat and punishment which leads to inhibitions. It was concluded that being powerful leads one to successful outcomes, to develop negotiation strategies and to make more self-serving offers.Later, research proposed that differences in power lead to strategic considerations. Being strategic can also mean to defend when one is opposed or to hurt the decision-maker. It was concluded that facing one with more power leads to strategic consideration whereas facing one with less power leads to a social responsibility.\n\n\n==== Bargaining games ====\nBargaining games were explored in 2003 and 2004. These studies compared behavior done in different power given situations.In an ultimatum game, the person in given power offers an ultimatum and the recipient would have to accept that offer or else both the proposer and the recipient will receive no reward.In a dictator game, the person in given power offers a proposal and the recipient would have to accept that offer. The recipient has no choice of rejecting the offer.\n\n\n===== Conclusion =====\nThe dictator game gives no power to the recipient whereas the ultimatum game gives some power to the recipient. The behavior observed was that the person offering the proposal would act less strategically than would the one offering in the ultimatum game. Self-serving also occurred and a lot of pro-social behavior was observed.When the counterpart recipient is completely powerless, lack of strategy, social responsibility and moral consideration is often observed from the behavior of the proposal given (the one with the power).\n\n\n=== Abusive power and control ===\n\nOne can regard power as evil or unjust; however, power can also be seen as good and as something inherited or given for exercising humanistic objectives that will help, move, and empower others as well. In general, power derives from the factors of interdependence between two entities and the environment. The use of power need not involve force or the threat of force (coercion). An example of using power without oppression is the concept \"soft power\" (as compared to hard power). Much of the recent sociological debate about power revolves around the issue of its means to enable \u2013 in other words, power as a means to make social actions possible as much as it may constrain or prevent them.Abusive power and control (or controlling behaviour or coercive control) involve the ways in which abusers gain and maintain power and control over victims for abusive purposes such as psychological, physical, sexual, or financial abuse. Such abuse can have various causes \u2013 such as personal gain, personal gratification, psychological projection, devaluation, envy or because some abusers enjoy exercising power and control.\nControlling abusers may use multiple tactics to exert power and control over their victims. The tactics themselves are psychologically and sometimes physically abusive. Control may be helped through economic abuse, thus limiting the victim's actions as they may then lack the necessary resources to resist the abuse. Abusers aim to control and intimidate victims or to influence them to feel that they do not have an equal voice in the relationship.Manipulators and abusers may control their victims with a range of tactics, including:\npositive reinforcement (such as praise, superficial charm, flattery, ingratiation, love bombing, smiling, gifts, attention)\nnegative reinforcement\nintermittent or partial reinforcement\npsychological punishment (such as nagging, silent treatment, swearing, threats, intimidation, emotional blackmail, guilt trips, inattention)\ntraumatic tactics (such as verbal abuse or explosive anger)The vulnerabilities of the victim are exploited, with those who are particularly vulnerable being most often selected as targets. Traumatic bonding can occur between the abuser and victim as the result of ongoing cycles of abuse in which the intermittent reinforcement of reward and punishment fosters powerful emotional bonds that are resistant to change, as well as a climate of fear. An attempt may be made to normalise, legitimise, rationalise, deny, or minimise the abusive behaviour, or to blame the victim for it.Isolation, gaslighting, mind games, lying, disinformation, propaganda, destabilisation, brainwashing and divide and rule are other strategies that are often used. The victim may be plied with alcohol or drugs or deprived of sleep to help disorientate them.Certain personality-types feel particularly compelled to control other people.\n\n\n== Tactics ==\nIn everyday situations people use a variety of power tactics to push or prompt other people into particular actions. Many examples exist of common power tactics employed every day. Some of these tactics include bullying, collaboration, complaining, criticizing, demanding, disengaging, evading, humor, inspiring, manipulating, negotiating, socializing, and supplicating. One can classify such power tactics along three different dimensions:\nSoft and hard: Soft tactics take advantage of the relationship between the influencer and the target. They are more indirect and interpersonal (e.g., collaboration, socializing). Conversely, hard tactics are harsh, forceful, direct, and rely on concrete outcomes. However, they are not more powerful than soft tactics. In many circumstances, fear of social exclusion can be a much stronger motivator than some kind of physical punishment.\nRational and nonrational: Rational tactics of influence make use of reasoning, logic, and sound judgment, whereas nonrational tactics may rely on emotionality or misinformation. Examples of each include bargaining and persuasion, and evasion and put-downs, respectively.\nUnilateral and bilateral: Bilateral tactics, such as collaboration and negotiation, involve reciprocity on the part of both the person influencing and their target. Unilateral tactics, on the other hand, develop without any participation on the part of the target. These tactics include disengagement and the deployment of fait accomplis.People tend to vary in their use of power tactics, with different types of people opting for different tactics. For instance, interpersonally oriented people tend to use soft and rational tactics. Moreover, extroverts use a greater variety of power tactics than do introverts. People will also choose different tactics based on the group situation, and based on whom they wish to influence. People also tend to shift from soft to hard tactics when they face resistance.\n\n\n=== Balance of power ===\nBecause power operates both relationally and reciprocally, sociologists speak of the \"balance of power\" between parties to a relationship:\nall parties to all relationships have some power: the sociological examination of power concerns itself with discovering and describing the relative strengths: equal or unequal, stable or subject to periodic change. Sociologists usually analyse relationships in which the parties have relatively equal or nearly equal power in terms of constraint rather than of power. In this context, \"power\" has a connotation of unilateralism. If this were not so, then all relationships could be described in terms of \"power\", and its meaning would be lost. Given that power is not innate and can be granted to others, to acquire power one must possess or control a form of power currency.\n\n\n=== Political power in authoritarian regimes ===\nIn authoritarian regimes, political power is concentrated in the hands of a single leader or a small group of leaders who exercise almost complete control over the government and its institutions. Because some authoritarian leaders are not elected by a majority, their main threat is that posed by the masses. They often maintain their power through political control tactics like:\n\nRepression: The state targets actors who challenge their beliefs. Can be done directly or indirectly.Autocrats repress actors they perceive as having irreconcilable interests, and cooperate with those they think have reconcilable ones.\nBecause of preference falsification- distinguishing between an individual's private preference and public preference- sometimes repression in itself is not enough.\nIndoctrination: The state controls public education and uses propaganda to diffuse its views and values into society.A one standard deviation increase in pro-regime propaganda reduces the odds of protest the following day by 15%.\nCoercive distribution: The state distributes welfare and resources to keep people dependent while offering benefits to people they know they can manipulate.\nInfiltration: The state assigns people to go into grassroot level to sway the public in favor of the authoritarian regime.Although several regimes follow these general forms of control, different authoritarian sub-regime types rely on different political control tactics.\n\n\n== Effects ==\nPower changes those in the position of power and those who are targets of that power.\n\n\n=== Approach/inhibition theory ===\nDeveloped by D. Keltner and colleagues, approach/inhibition theory assumes that having power and using power alters psychological states of individuals. The theory is based on the notion that most organisms react to environmental events in two common ways. The reaction of approach is associated with action, self-promotion, seeking rewards, increased energy and movement. Inhibition, on the contrary, is associated with self-protection, avoiding threats or danger, vigilance, loss of motivation and an overall reduction in activity.\nOverall, approach/inhibition theory holds that power promotes approach tendencies, while a reduction in power promotes inhibition tendencies.\n\n\n=== Positive ===\nPower prompts people to take action\nMakes individuals more responsive to changes within a group and its environment\nPowerful people are more proactive, more likely to speak up, make the first move, and lead negotiation\nPowerful people are more focused on the goals appropriate in a given situation and tend to plan more task-related activities in a work setting\nPowerful people tend to experience more positive emotions, such as happiness and satisfaction, and they smile more than low-power individuals\nPower is associated with optimism about the future because more powerful individuals focus their attention on more positive aspects of the environment\nPeople with more power tend to carry out executive cognitive functions more rapidly and successfully, including internal control mechanisms that coordinate attention, decision-making, planning, and goal-selection\n\n\n=== Negative ===\nPowerful people are prone to take risky, inappropriate, or unethical decisions and often overstep their boundaries\nThey tend to generate negative emotional reactions in their subordinates, particularly when there is a conflict in the group\nWhen individuals gain power, their self-evaluation become more positive, while their evaluations of others become more negative\nPower tends to weaken one's social attentiveness, which leads to difficulty understanding other people's point of view\nPowerful people also spend less time collecting and processing information about their subordinates and often perceive them in a stereotypical fashion\nPeople with power tend to use more coercive tactics, increase social distance between themselves and subordinates, believe that non-powerful individuals are untrustworthy, and devalue work and ability of less powerful individuals\n\n\n== Reactions ==\n\n\n=== Tactics ===\nA number of studies demonstrate that harsh power tactics (e.g. punishment (both personal and impersonal), rule-based sanctions, and non-personal rewards) are less effective than soft tactics (expert power, referent power, and personal rewards). It is probably because harsh tactics generate hostility, depression, fear, and anger, while soft tactics are often reciprocated with cooperation. Coercive and reward power can also lead group members to lose interest in their work, while instilling a feeling of autonomy in one's subordinates can sustain their interest in work and maintain high productivity even in the absence of monitoring.Coercive influence creates conflict that can disrupt entire group functioning. When disobedient group members are severely reprimanded, the rest of the group may become more disruptive and uninterested in their work, leading to negative and inappropriate activities spreading from one troubled member to the rest of the group. This effect is called Disruptive contagion or ripple effect and it is strongly manifested when reprimanded member has a high status within a group, and authority's requests are vague and ambiguous.\n\n\n=== Resistance to coercive influence ===\nCoercive influence can be tolerated when the group is successful, the leader is trusted, and the use of coercive tactics is justified by group norms. Furthermore, coercive methods are more effective when applied frequently and consistently to punish prohibited actions.However, in some cases, group members chose to resist the authority's influence. When low-power group members have a feeling of shared identity, they are more likely to form a Revolutionary Coalition, a subgroup formed within a larger group that seeks to disrupt and oppose the group's authority structure. Group members are more likely to form a revolutionary coalition and resist an authority when authority lacks referent power, uses coercive methods, and asks group members to carry out unpleasant assignments. It is because these conditions create reactance, individuals strive to reassert their sense of freedom by affirming their agency for their own choices and consequences.\n\n\n=== Kelman's compliance-identification-internalization theory of conversion ===\nHerbert Kelman identified three basic, step-like reactions that people display in response to coercive influence: compliance, identification, and internalization. This theory explains how groups convert hesitant recruits into zealous followers over time.\nAt the stage of compliance, group members comply with authority's demands, but personally do not agree with them. If authority does not monitor the members, they will probably not obey.\nIdentification occurs when the target of the influence admires and therefore imitates the authority, mimics authority's actions, values, characteristics, and takes on behaviours of the person with power. If prolonged and continuous, identification can lead to the final stage \u2013 internalization.\nWhen internalization occurs, individual adopts the induced behaviour because it is congruent with his/her value system. At this stage, group members no longer carry out authority orders but perform actions that are congruent with their personal beliefs and opinions. Extreme obedience often requires internalization.\n\n\n== Power literacy ==\nPower literacy refers to how one perceives power, how it is formed and accumulates, and the structures that support it and who is in control of it. Education can be helpful for heightening power literacy. In a 2014 TED talk Eric Liu notes that \"we don't like to talk about power\" as \"we find it scary\" and \"somehow evil\" with it having a \"negative moral valence\" and states that the pervasiveness of power illiteracy causes a concentration of knowledge, understanding and clout. Joe L. Kincheloe describes a \"cyber-literacy of power\" that is concerned with the forces that shape knowledge production and the construction and transmission of meaning, being more about engaging knowledge than \"mastering\" information, and a \"cyber-power literacy\" that is focused on transformative knowledge production and new modes of accountability.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nDolata, Ulrich; Schrape, Jan-Felix (2018). Collectivity and Power on the Internet. A Sociological Perspective. London Cham: Springer. doi:10.1007/978-3-319-78414-4. ISBN 978-3319784137.\nBitar, Amer (2020). Bedouin Visual Leadership in the Middle East: The Power of Aesthetics and Practical Implications. Springer Nature. ISBN 978-3030573973.\nVatiero M. (2009), Understanding Power. A 'Law and Economics' Approach Archived 30 July 2020 at the Wayback Machine, VDM Verlag. ISBN 978-3639202656\nMichael Eldred, Social Ontology: Recasting Political Philosophy Through a Phenomenology of Whoness Ontos, Frankfurt 2008 ISBN 978-3938793787\nMirko Vagnoni, Charles V and the Furyat the Prado Museum: The Power of the King's Body as Image, Eik\u00f3n / Imago: Vol. 6 No. 2 (2017). 49\u201366. Charles V and the Fury at the Prado Museum: The Power of the King\u2019s Body as Image\nSimmel, Georg Superiority and Subordination as Subject-Matter of Sociology\nSimmel, Georg Superiority and Subordination as Subject-Matter of Sociology II\nKanter, R. M. (1979). Power failures in management circuits. Harvard Business Review.\nForbes: World's Most Powerful Women Define Power on YouTube"}, {"id": 75, "title": "List of Presidents of the United States Presidential firsts", "content": "In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to December 6, 2023.\n\n\n== Age of presidents ==\nThe median age at inauguration of incoming U.S. presidents is 55 years. The specific years and days median is 55 years and 104.5 days, which falls midway between how old Warren G. Harding was in 1921 and Lyndon B. Johnson was in 1963.\nArticle Two of the United States Constitution provides that U.S. presidents must be at least 35 years old at the time of taking office. The youngest person to become U.S. president was Theodore Roosevelt, who, at age 42, succeeded to the office after the assassination of William McKinley. The youngest at the time of his election to the office was John F. Kennedy, at age 43. The oldest person elected president was Joe Biden, the nation's current president, at age 77. Biden celebrated a birthday between Election Day and Inauguration Day making him 78 when sworn into office.Assassinated at age 46, John F. Kennedy was the youngest president at the end of his tenure, and his lifespan was the shortest of any president. At age 50, Theodore Roosevelt was the youngest person to become a former president. The oldest president at the end of his tenure was Ronald Reagan at 77; this distinction will eventually fall upon Joe Biden, who is currently 81.\nJames K. Polk had the shortest retirement of any president, dying 3 months after leaving office at age 53 (the youngest president to die of natural causes). Jimmy Carter's retirement, now 42 years, is the longest in American presidential history. Additionally, at age 99, Carter is both the oldest of the six living U.S. presidents, and the nation's longest-lived president. Barack Obama, at age 62 is the youngest living president.\n\n\n== Presidential age-related data ==\n\n\n=== Notes ===\n\n\n== Graphical representation ==\nThis is a graphical lifespan timeline of the presidents of the United States. They are listed in order of office, with Grover Cleveland listed in the order of his first presidency.\n\nThe following chart shows presidents by their age (living presidents in green), with the years of their presidency in blue. The vertical blue line at 35 years indicates the minimum age to be president.\n\n\n== References ==\n\n\n== Sources ==\nFrank Freidel and Hugh S. Sidey, \"The Presidents of the United States\". The White House.\nRobert S. Summers, \"POTUS: Presidents of the United States\". Internet Public Library."}, {"id": 76, "title": "List of Presidents of the United States by education", "content": "Most presidents of the United States received a college education, even most of the earliest. Of the first seven presidents, five were college graduates. College degrees have set the presidents apart from the general population, and presidents have held degrees even though it was quite rare and unnecessary for practicing most occupations, including law. Of the 45 individuals to have been the president, 25 of them graduated from a private undergraduate college, nine graduated from a public undergraduate college, and 12 held no degree.  Every president since 1953 has had a bachelor's degree, reflecting the increasing importance of higher education in the United States.\n\n\n== List by university attended ==\n\n\n=== Did not graduate from college ===\nGeorge Washington (Although the death of Washington's father ended his formal schooling, he received a surveyor's certificate from the College of William & Mary. Washington believed strongly in formal education, and his will left money and/or stocks to support three educational institutions, including George Washington University and Washington and Lee University)\nJames Monroe (attended the College of William and Mary, but dropped out to fight in the Revolutionary War)\nAndrew Jackson\nMartin Van Buren\nWilliam Henry Harrison (attended Hampden Sydney College for three years but did not graduate and then attended University of Pennsylvania School of Medicine but never received a degree)\nZachary Taylor\nMillard Fillmore (founded the University at Buffalo)\nAbraham Lincoln (had only about a year of formal schooling of any kind)\nAndrew Johnson  (no formal schooling of any kind)\nGrover Cleveland\nWilliam McKinley (attended Allegheny College, but did not graduate; also attended Albany Law School, but also did not graduate)\nHarry S. Truman (went to business college and law school, but did not graduate)\n\n\n=== Undergraduate ===\nA.^ Kennedy enrolled, but did not attend\n\n\n==== Additional undergraduate information ====\nSome presidents attended more than one institution. George Washington never attended college, though The College of William & Mary did issue him a surveyor's certificate. Two presidents have attended a foreign college at the undergraduate level: John Quincy Adams at Leiden University and Bill Clinton at the University of Oxford (John F. Kennedy intended to study at the London School of Economics, but failed to attend as he fell ill before classes began.)\nThree presidents have attended the United States Service academies: Ulysses S. Grant and Dwight D. Eisenhower graduated from the United States Military Academy at West Point, while Jimmy Carter graduated from the United States Naval Academy at Annapolis, Maryland. No presidents have graduated from the United States Coast Guard Academy or the much newer U.S. Air Force Academy.  Eisenhower also graduated from the Army Command and General Staff College, Army Industrial College and Army War College. These were not degree granting institutions when Eisenhower attended, but were part of his professional education as a career soldier.\n\n\n=== Graduate school ===\nA total of 20 presidents attended some form of graduate school (including professional schools). Among them, eleven presidents received a graduate degree during their lifetimes; two more received graduate degrees posthumously.\n\n\n==== Business school ====\n\n\n==== Graduate school ====\n\n\n==== Medical school ====\n\n\n==== Law school ====\nSeveral presidents who were lawyers did not attend law school, but became lawyers after independent study under the tutelage of established attorneys. Some had attended college before beginning their legal studies, and several studied law without first having attended college.  Presidents who were lawyers but did not attend law school include: John Adams; Thomas Jefferson; James Madison; James Monroe; John Quincy Adams; Andrew Jackson; Martin Van Buren; John Tyler; James K. Polk; Millard Fillmore; James Buchanan; Abraham Lincoln; James A. Garfield; Grover Cleveland; Benjamin Harrison; and Calvin Coolidge.\nPresidents who were admitted to the bar after a combination of law school and independent study include; Franklin Pierce; Chester A. Arthur; William McKinley; and Woodrow Wilson.\n\n\n== List by graduate degree earned ==\n\n\n=== Ph.D. (research doctorate) ===\n\n\n=== M.B.A. (Master of Business Administration) ===\n\n\n=== M.A. (Master of Arts) ===\nNote: John Adams and John Quincy Adams, along with George W. Bush are the only presidents to date to attain master's degrees.\n\n\n=== J.D. or LL.B. (law degree) ===\nNote: Hayes, Taft, Nixon and Ford were awarded LL.B. degrees.  When most U.S. law schools began to award the J.D. as the professional law degree in the 1960s, previous graduates had the choice of converting their LL.B. degrees to a J.D.  Duke University Law School made the change in 1968, and Yale Law School in 1971.\n\n\n== List by president ==\n\n\n== Other academic associations ==\n\n\n=== Faculty member ===\n\n\n=== School rector or president ===\n\n\n=== School trustee or governor ===\n\n\n== See also ==\nList of prime ministers of Australia by education\nList of prime ministers of Canada by academic degrees\nList of presidents of the Philippines by education\nList of prime ministers of the United Kingdom by education\n\n\n== References =="}, {"id": 77, "title": "Algorithm", "content": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n\n== History ==\n\n\n=== Ancient algorithms ===\nSince antiquity, step-by-step procedures for solving mathematical problems have been attested. This includes Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later; e.g. Shulba Sutras, Kerala School, and Br\u0101hmasphu\u1e6dasiddh\u0101nta), The Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC, e.g. sieve of Eratosthenes and Euclidean algorithm), and Arabic mathematics (9th century, e.g. cryptographic algorithms for code-breaking based on frequency analysis).\n\n\n=== Al-Khw\u0101rizm\u012b and the term algorithm ===\nAround 825, Mu\u1e25ammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b wrote kit\u0101b al-\u1e25is\u0101b al-hind\u012b (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-\u1e25is\u0101b al-hind\u012b (\"Addition and subtraction in Indian arithmetic\"). Both of these texts are lost in the original Arabic at this time. (However, his other book on algebra remains.)In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu\u2013Arabic numeral system and arithmetic appeared: Liber Alghoarismi de practica arismetrice (attributed to John of Seville) and Liber Algorismi de numero Indorum (attributed to Adelard of Bath). Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi (\"Thus spoke Al-Khwarizmi\").In 1240, Alexander of Villedieu writes a Latin text titled Carmen de Algorismo. It begins with:\n\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\nwhich translates to:\n\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.\n\n\n=== English evolution of the word ===\nAround 1230, the English word algorism is attested and then by Chaucer in 1391. English adopted the French term.In the 15th century, under the influence of the Greek word \u1f00\u03c1\u03b9\u03b8\u03bc\u03cc\u03c2 (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.\nIn 1656, in the English dictionary Glossographia, it says:\n\nAlgorism ([Latin] algorismus) the Art or use of Cyphers, or of numbering by Cyphers; skill in accounting.\nAugrime ([Latin] algorithmus) skil in accounting or numbring.\n\nIn 1658, in the first edition of The New World of English Words, it says:\n\nAlgorithme, (a word compounded of Arabick and Spanish,) the art of reckoning by Cyphers.\n\nIn 1706, in the sixth edition of The New World of English Words, it says:\n\nAlgorithm, the Art of computing or reckoning by numbers, which contains the five principle Rules of Arithmetick, viz. Numeration, Addition, Subtraction, Multiplication and Division; to which may be added Extraction of Roots: It is also call'd Logistica Numeralis.\nAlgorism, the practical Operation in the several Parts of Specious Arithmetick or Algebra; sometimes it is taken for the Practice of Common Arithmetick by the ten Numeral Figures.\n\nIn 1751, in the Young Algebraist's Companion, Daniel Fenning contrasts the terms algorism and algorithm as follows:\n\nAlgorithm signifies the first Principles, and Algorism the practical Part, or knowing how to put the Algorithm in Practice.\n\nSince at least 1811, the term algorithm is attested to mean a \"step-by-step procedure\" in English.In 1842, in the Dictionary of Science, Literature and Art, it says:\n\nALGORITHM, signifies the art of computing in reference to some particular subject, or in some particular way; as the algorithm of numbers; the algorithm of the differential calculus.\n\n\n=== Machine usage ===\nIn 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936\u201337 and 1939.\n\n\n== Informal definition ==\n\nOne informal definition is \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\nor cook-book recipe.In general, a program is an algorithm only if it stops eventually\u2014even though infinite loops may sometimes prove desirable.\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\n\nNo human being can write fast enough, or long enough, or small enough\u2020 ( \u2020\"smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability\u2014a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\n\n== Formalization ==\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform\u2014in a specific order\u2014to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):\n\n Minsky: \"But we will also maintain, with Turing ... that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute\".\n Gurevich: \"\u2026 Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine \u2026 according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case\u2014due to a major theorem of computability theory known as the halting problem.\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data is regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\nFor some of these computational processes, the algorithm must be rigorously defined and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"\u2014an idea that is described more formally by flow of control.\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception\u2014one that attempts to describe a task in discrete, \"mechanical\" terms. Associated with this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\n\n\n== Expressing algorithms ==\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but they are also often used as a way to define or document algorithms.\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state-transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\nRepresentations of algorithms can be classified into three accepted levels of Turing machine description, as follows:\n1 High-level description\n\"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\n2 Implementation description\n\"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\n3 Formal description\nMost detailed, \"lowest level\", gives the Turing machine's \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Examples.\n\n\n== Design ==\n\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.\nTypical steps in the development of algorithms:\n\nProblem definition\nDevelopment of a model\nSpecification of the algorithm\nDesigning an algorithm\nChecking the correctness of the algorithm\nAnalysis of algorithm\nImplementation of algorithm\nProgram testing\nDocumentation preparation\n\n\n== Computer algorithms ==\n\"Elegant\" (compact) programs, \"good\" (fast) programs: The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nKnuth: \" ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity, and elegance, etc.\"Chaitin: \" ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"\u2014such a proof would solve the Halting problem (ibid).\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)\u2014an elegant program may take more steps to complete a computation than one that is less elegant. An example that uses Euclid's algorithm appears below.\nComputers (and computors), models of computation: A computer (or human \"computer\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of the location replaced by 0: L \u2190 0), SUCCESSOR (e.g. L \u2190 L+1), and DECREMENT (e.g. L \u2190 L \u2212 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT. However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is convenient; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z \u2190 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they do not, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\nStructured programming, canonical structures: Per the Church\u2013Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types\u2014conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three B\u00f6hm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The B\u00f6hm\u2013Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.\n\n\n== Examples ==\n\n\n=== Algorithm example ===\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\nHigh-level description:\n\nIf there are no numbers in the set, then there is no highest number.\nAssume the first number in the set is the largest number in the set.\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\n\n=== Euclid's algorithm ===\n\nIn mathematics, the Euclidean algorithm or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c.\u2009300 BC). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\n\nEuclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l \u2212 q\u00d7s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\n\n==== Computer language for Euclid's algorithm ====\nOnly a few instruction types are required to execute Euclid's algorithm\u2014some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\n\n==== An inelegant program for Euclid's algorithm ====\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2\u20134:\nINPUT:\n\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\nINPUT L, S\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\nR \u2190 L\n\nE0: [Ensure r \u2265 s.]\n\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\nIF R > S THEN\nthe contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\nGOTO step 7\nELSE\nswap the contents of R and S.\n4 L \u2190 R (this first step is redundant, but is useful for later discussion).\n5 R \u2190 S\n6 S \u2190 L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n\n7 IF S > R THEN\ndone measuring so\nGOTO 10\nELSE\nmeasure again,\n8 R \u2190 R \u2212 S\n9 [Remainder-loop]:\nGOTO 7.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\n10 IF R = 0 THEN\ndone so\nGOTO step 15\nELSE\nCONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n\n11 L \u2190 R\n12 R \u2190 S\n13 S \u2190 L\n14 [Repeat the measuring process]:\nGOTO 7\n\nOUTPUT:\n\n15 [Done. S contains the greatest common divisor]:\nPRINT S\n\nDONE:\n\n16 HALT, END, STOP.\n\n\n==== An elegant program for Euclid's algorithm ====\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by \u2190.\n\nHow \"Elegant\" works: instead of an outer \"Euclid loop\", \"Elegant\" calculates the remainder of a division using modulo and shifts the variables A and B in each iteration. The following algorithm can be used with programming languages from the C-family:\n\nThe standard function abs changes negative integer to positive integer\nWhen input A or B has the value 0, the algorithm stops and the result is 0\nIf input A is greater than input B, the algorithm will automatically swap variables A and B during the first iteration via modulo\nThe iteration (a do while loop) starts and only stops when the variable B is set to 0:\n% calculates the modulo of division A and B, which reduces the number (e.g.: 23 = 3 \u00d7 6 + remainder 5)\nA is equated with B\nB is equated with the modulo-result\nThe iteration continues as long as B is greater than 0\nWhen the iteration stops, variable A always contains the greatest common divisor\n\n\n=== Testing the Euclid algorithms ===\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\n\n=== Measuring and improving the Euclid algorithms ===\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"\u2014that is, it computes the function intended by its author\u2014then the question becomes, can it be improved?\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\n\n== Algorithmic analysis ==\n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  , using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of \n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\displaystyle O(1)}\n  , if the space required to store the input numbers is not counted, or \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   if it is counted.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n  ) outperforms a sequential search (cost \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   ) when used for table lookups on sorted lists or arrays.\n\n\n=== Formal versus empirical ===\n\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\n\n=== Execution efficiency ===\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\n\n== Classification ==\nThere are various ways to classify algorithms, each with its own merits.\n\n\n=== By implementation ===\nOne way to classify algorithms is by implementation means.\n\nRecursion\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms are algorithms that take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms are algorithms that use multiple machines connected with a computer network. Parallel and distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. For example, a CPU would be an example of a parallel algorithm. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\nQuantum algorithm\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\n\n\n=== By design paradigm ===\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\nBrute-force or exhaustive search\nBrute force is a method of problem-solving that involves systematically trying every possible option until the optimal solution is found. This approach can be very time consuming, as it requires going through every possible combination of variables. However, it is often used when other methods are not available or too complex. Brute force can be used to solve a variety of problems, including finding the shortest path between two points and cracking passwords.\nDivide and conquer\nA divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\n\n=== Optimization problems ===\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures\u2014meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems\u2014and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd\u2013Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\nThe greedy method\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\n\n=== By field of study ===\n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\n\n=== By complexity ===\n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\n\n=== Continuous algorithms ===\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations\u2014such algorithms are studied in numerical analysis; or\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\n\n== Algorithm = Logic + Control ==\nIn logic programming, algorithms are viewed as having both \"a logic component, which specifies the knowledge to be  used in solving problems, and a control component, which determines the problem-solving strategies by means of which that knowledge is used.\"The Euclidean algorithm illustrates this view of an algorithm. Here is a logic programming representation, using :- to represent \"if\", and the relation gcd(A, B, C) to represent the function gcd(A, B) = C: \n\nIn the logic programming language Ciao the gcd relation can be represented directly in functional notation:\n\nThe Ciao implementation translates the functional notation into a relational representation in   Prolog, extracting the embedded subtractions, A-B and B-A, as separate conditions:\n\nThe resulting program has a purely logical (and \"declarative\") reading, as a recursive (or inductive) definition, which is independent of how the logic is used to solve problems:\n\nDifferent problem-solving strategies turn the logic into different algorithms. In theory, given a pair of integers A and B, forward (or \"bottom-up\") reasoning could be used to generate all instances of the gcd relation, terminating when the desired gcd of A and B is generated. Of course, forward reasoning is entirely useless in this case. But in other cases, such as the definition of the Fibonacci sequence and Datalog, forward reasoning can be an efficient problem solving strategy. (See for example the logic program for computing fibonacci numbers in Algorithm = Logic + Control).\nIn contrast with the inefficiency of forward reasoning in this example, backward (or \"top-down\") reasoning using SLD resolution turns the logic into the Euclidean algorithm:\n\nOne of the advantages of the logic programming representation of the algorithm is that its purely logical reading makes it easier to verify that the algorithm is correct relative to the standard non-recursive definition of gcd. Here is the standard definition written in Prolog:\n\nThis definition, which is the specification of the Euclidean algorithm, is also executable in Prolog: Backward reasoning treats the specification as the brute-force algorithm that iterates through all of the integers C between 1 and A, checking whether C divides both A and B, and then for each such C iterates again through all of the integers D between 1 and A, until it finds a C such that C is greater than or equal to all of the D that also divide both A and B. Although this algorithm is hopelessly inefficient, it shows that formal specifications can often be written in logic programming form, and they can be executed by Prolog, to check that they correctly represent informal requirements.\n\n\n== Legal issues ==\n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent.\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\n\n== History: Development of the notion of \"algorithm\" ==\n\n\n=== Ancient Near East ===\nThe earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c.\u20092500 BC described the earliest division algorithm. During the Hammurabi dynasty c.\u20091800 \u2013 c.\u20091600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c.\u20091550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,:\u200aCh 9.2\u200a and the Euclidean algorithm, which was first described in Euclid's Elements (c.\u2009300 BC).:\u200aCh 9.1\u200a\n\n\n=== Discrete and distinguishable symbols ===\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16\u201341). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post\u2013Turing machine computations.\n\n\n=== Manipulation of symbols as \"place holders\" for numbers: algebra ===\nMuhammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khw\u0101rizm\u012b, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (c.\u20091680):\n\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\n\n\n=== Cryptographic algorithms ===\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\n\n\n=== Mechanical contrivances with discrete states ===\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"\u2014the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer\u2014Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator\u2014and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\nLogical machines 1870 \u2013 Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc.] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony \u2013 the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (c.\u20091870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (c.\u20091910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\n\n\n=== Mathematics during the 19th century up to the mid-20th century ===\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888\u20131889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \"'formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910\u20131913).\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902\u201303), and the Richard Paradox. The resultant considerations led to Kurt G\u00f6del's paper (1931)\u2014he specifically cites the paradox of the liar\u2014that completely reduces rules of recursion to numbers.\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's \u03bb-calculus a finely honed definition of \"general recursion\" from the work of G\u00f6del acting on suggestions of Jacques Herbrand (cf. G\u00f6del's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"\u2014in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\n\n=== Emil Post (1936) and Alan Turing (1936\u201337, 1939) ===\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\n\n\"a two-way infinite sequence of spaces or boxes ... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time. ... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ... a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post\u2013Turing machineAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.\nTuring\u2014his model of computation is now called a Turing machine\u2014begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares that the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"Turing's reduction yields the following:\n\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to G\u00f6del, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability \u2020 with effective calculability...\n\"\u2020 We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\n\n\n=== J. B. Rosser (1939) and S. C. Kleene (1943) ===\nJ. Barkley Rosser defined an \"effective [mathematical] method\" in the following manner (italicization added):\n\n\"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225\u2013226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of \u03bb-definability, in particular, Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and G\u00f6del and their use of recursion, in particular, G\u00f6del's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936\u201337) in their mechanism-models of computation.\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church\u2013Turing thesis. But he did this in the following context (boldface in original):\n\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\n\n\n=== History after 1950 ===\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church\u2013Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Bibliography ==\n\nZaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76\u201399. https://doi.org/10.2307/3027363\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nAlgorithms at Curlie\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures \u2013 National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository \u2013 State University of New York at Stony Brook\nCollected Algorithms of the ACM \u2013 Associations for Computing Machinery\nThe Stanford GraphBase Archived December 6, 2015, at the Wayback Machine \u2013 Stanford University"}, {"id": 78, "title": "Energy storage", "content": "Energy storage is the capture of energy produced at one time for use at a later time to reduce imbalances between energy demand and energy production. A device that stores energy is generally called an accumulator or battery. Energy comes in multiple forms including radiation, chemical, gravitational potential, electrical potential, electricity, elevated temperature, latent heat and kinetic. Energy storage involves converting energy from forms that are difficult to store to more conveniently or economically storable forms.\nSome technologies provide short-term energy storage, while others can endure for much longer. Bulk energy storage is currently dominated by hydroelectric dams, both conventional as well as pumped. Grid energy storage is a collection of methods used for energy storage on a large scale within an electrical power grid.\nCommon examples of energy storage are the rechargeable battery, which stores chemical energy readily convertible to electricity to operate a mobile phone; the hydroelectric dam, which stores energy in a reservoir as gravitational potential energy; and ice storage tanks, which store ice frozen by cheaper energy at night to meet peak daytime demand for cooling. Green hydrogen, from the electrolysis of water, is a more economical means of long-term renewable energy storage in terms of capital expenditures than pumped-storage hydroelectricity or batteries. Fossil fuels such as coal and gasoline store ancient energy derived from sunlight by organisms that later died, became buried and over time were then converted into these fuels. Food (which is made by the same process as fossil fuels) is a form of energy stored in chemical form.\n\n\n== History ==\nIn the 20th century grid, electrical power was largely generated by burning fossil fuel. When less power was required, less fuel was burned. Hydropower, a mechanical energy storage method, is the most widely adopted mechanical energy storage, and has been in use for centuries. Large hydropower dams have been energy storage sites for more than one hundred years. Concerns with air pollution, energy imports, and global warming have spawned the growth of renewable energy such as solar and wind power. Wind power is uncontrolled and may be generating at a time when no additional power is needed. Solar power varies with cloud cover and at best is only available during daylight hours, while demand often peaks after sunset (see duck curve). Interest in storing power from these intermittent sources grows as the renewable energy industry begins to generate a larger fraction of overall energy consumption.Off grid electrical use was a niche market in the 20th century, but in the 21st century, it has expanded. Portable devices are in use all over the world. Solar panels are now common in the rural settings worldwide. Access to electricity is now a question of economics and financial viability, and not solely on technical aspects. Electric vehicles are gradually replacing combustion-engine vehicles. However, powering long-distance transportation without burning fuel remains in development.\n\n\n== Methods ==\n\n\n=== Outline ===\nThe following list includes a variety of types of energy storage:\n\n\n=== Mechanical ===\nEnergy can be stored in water pumped to a higher elevation using pumped storage methods or by moving solid matter to higher locations (gravity batteries). \nOther commercial mechanical methods include compressing air and flywheels that convert electric energy into internal energy or kinetic energy and then back again when electrical demand peaks.\n\n\n==== Hydroelectricity ====\n\nHydroelectric dams with reservoirs can be operated to provide electricity at times of peak demand. \nWater is stored in the reservoir during periods of low demand and released when demand is high. \nThe net effect is similar to pumped storage, but without the pumping loss.\nWhile a hydroelectric dam does not directly store energy from other generating units, it behaves equivalently by lowering output in periods of excess electricity from other sources. \nIn this mode, dams are one of the most efficient forms of energy storage, because only the timing of its generation changes. \nHydroelectric turbines have a start-up time on the order of a few minutes.\n\n\n==== Pumped hydro ====\n\nWorldwide, pumped-storage hydroelectricity (PSH) is the largest-capacity form of active grid energy storage available, and, as of March 2012, the Electric Power Research Institute (EPRI) reports that PSH accounts for more than 99% of bulk storage capacity worldwide, representing around 127,000 MW. PSH energy efficiency varies in practice between 70% and 80%, with claims of up to 87%.At times of low electrical demand, excess generation capacity is used to pump water from a lower source into a higher reservoir. When demand grows, water is released back into a lower reservoir (or waterway or body of water) through a turbine, generating electricity. Reversible turbine-generator assemblies act as both a pump and turbine (usually a Francis turbine design). Nearly all facilities use the height difference between two water bodies. Pure pumped-storage plants shift the water between reservoirs, while the \"pump-back\" approach is a combination of pumped storage and conventional hydroelectric plants that use natural stream-flow.\n\n\n==== Compressed air ====\n\nCompressed air energy storage (CAES) uses surplus energy to compress air for subsequent electricity generation. Small-scale systems have long been used in such applications as propulsion of mine locomotives. The compressed air is stored in an  underground reservoir, such as a salt dome.\nCompressed-air energy storage (CAES) plants can bridge the gap between production volatility and load. CAES storage addresses the energy needs of consumers by effectively providing readily available energy to meet demand. Renewable energy sources like wind and solar energy vary. So at times when they provide little power, they need to be supplemented with other forms of energy to meet energy demand. Compressed-air energy storage plants can take in the surplus energy output of renewable energy sources during times of energy over-production. This stored energy can be used at a later time when demand for electricity increases or energy resource availability decreases.Compression of air creates heat; the air is warmer after compression. Expansion requires heat. If no extra heat is added, the air will be much colder after expansion. If the heat generated during compression can be stored and used during expansion, efficiency improves considerably. A CAES system can deal with the heat in three ways. Air storage can be adiabatic, diabatic, or isothermal. Another approach uses compressed air to power vehicles.\n\n\n==== Flywheel ====\n\nFlywheel energy storage (FES) works by accelerating a rotor (a flywheel) to a very high speed, holding energy as rotational energy. When energy is added the rotational speed of the flywheel increases, and when energy is extracted, the speed declines, due to conservation of energy.\nMost FES systems use electricity to accelerate and decelerate the flywheel, but devices that directly use mechanical energy are under consideration.FES systems have rotors made of high strength carbon-fiber composites, suspended by magnetic bearings and spinning at speeds from 20,000 to over 50,000 revolutions per minute (rpm) in a vacuum enclosure. Such flywheels can reach maximum speed (\"charge\") in a matter of minutes. The flywheel system is connected to a combination electric motor/generator.\nFES systems have relatively long lifetimes (lasting decades with little or no maintenance; full-cycle lifetimes quoted for flywheels range from in excess of 105, up to 107, cycles of use), high specific energy (100\u2013130 W\u00b7h/kg, or 360\u2013500 kJ/kg) and power density.\n\n\n==== Solid mass gravitational ====\n\nChanging the altitude of solid masses can store or release energy via an elevating system driven by an electric motor/generator. Studies suggest energy can begin to be released with as little as 1 second warning, making  the method a useful supplemental feed into an electricity grid to balance load surges.Efficiencies can be as high as 85% recovery of stored energy.This can be achieved by siting the masses inside old vertical  mine shafts or in specially constructed towers where the heavy weights are  winched up to store energy and allowed a controlled descent to release it. At 2020 a prototype vertical store is being built in Edinburgh, Scotland Potential energy storage or gravity energy storage was under active development in 2013 in association with the California Independent System Operator. It examined the movement of earth-filled hopper rail cars driven by electric locomotives from lower to higher elevations.Other proposed methods include:-\n\nusing rails, cranes, or elevators to move weights up and down;\nusing high-altitude solar-powered balloon platforms supporting winches to raise and lower solid masses slung underneath them,\nusing winches supported by an ocean barge to take advantage of a 4 km (13,000 ft) elevation difference between the sea surface and the seabed,\n\n\n=== Thermal ===\n\nThermal energy storage (TES) is the temporary storage or removal of heat.\n\n\n==== Sensible heat thermal ====\nSensible heat storage take advantage of sensible heat in a material to store energy.Seasonal thermal energy storage (STES) allows heat or cold to be used months after it was collected from waste energy or natural sources. The material can be stored in contained aquifers, clusters of boreholes in geological substrates such as sand or crystalline bedrock, in lined pits filled with gravel and water, or water-filled mines. Seasonal thermal energy storage (STES) projects often have paybacks in four to six years. An example is Drake Landing Solar Community in Canada, for which 97% of the year-round heat is provided by solar-thermal collectors on garage roofs, enabled by a borehole thermal energy store (BTES). In Braedstrup, Denmark, the community's solar district heating system also uses STES, at a temperature of 65 \u00b0C (149 \u00b0F). A heat pump, which runs only while surplus wind power is available. It is used to raise the temperature to 80 \u00b0C (176 \u00b0F) for distribution. When wind energy is not available, a gas-fired boiler is used. Twenty percent of Braedstrup's heat is solar.\n\n\n==== Latent heat thermal (LHTES) ====\nLatent heat thermal energy storage systems work by transferring heat to or from a material to change its phase. A phase-change is the melting, solidifying, vaporizing or liquifying. Such a material is called a phase change material (PCM). Materials used in LHTESs often have a high latent heat so that at their specific temperature, the phase change absorbs a large amount of energy, much more than sensible heat.A steam accumulator is a type of LHTES where the phase change is between liquid and gas and uses the latent heat of vaporization of water. Ice storage air conditioning systems use off-peak electricity to store cold by freezing water into ice. The stored cold in ice releases during melting process and can be used for cooling at peak hours.\n\n\n==== Cryogenic thermal energy storage ====\n\nAir can be liquefied by cooling using electricity and stored as a cryogen with existing technologies. The liquid air can then be expanded through a turbine and the energy recovered as electricity. The system was demonstrated at a pilot plant in the  UK in 2012.\nIn 2019, Highview announced plans to build a 50 MW in the North of England and northern Vermont, with the proposed facility able to store five to eight hours of energy, for a 250-400 MWh storage capacity.\n\n\n==== Carnot battery ====\n\nElectrical energy can be stored thermally by resistive heating or heat pumps, and the stored heat can be converted back to electricity via Rankine cycle or Brayton cycle. This technology has been studied to retrofit coal-fired power plants into fossil-fuel free generation systems. Coal-fired boilers are replaced by high-temperature heat storage charged by excess electricity from renewable energy sources. In 2020, German Aerospace Center started to construct the world's first large-scale Carnot battery system, which has 1,000 MWh storage capacity.\n\n\n=== Electrochemical ===\n\n\n==== Rechargeable battery ====\n\nA rechargeable battery comprises one or more electrochemical cells. It is known as a 'secondary cell' because its electrochemical reactions are electrically reversible. Rechargeable batteries come in many shapes and sizes, ranging from button cells to megawatt grid systems.\nRechargeable batteries have lower total cost of use and environmental impact than non-rechargeable (disposable) batteries. Some rechargeable battery types are available in the same form factors as disposables. Rechargeable batteries have higher initial cost but can be recharged very cheaply and used many times.\nCommon rechargeable battery chemistries include:\n\nLead\u2013acid battery: Lead acid batteries hold the largest market share of electric storage products. A single cell produces about 2V when charged. In the charged state the metallic lead negative electrode and the lead sulfate positive electrode are immersed in a dilute sulfuric acid (H2SO4) electrolyte. In the discharge process electrons are pushed out of the cell as lead sulfate is formed at the negative electrode while the electrolyte is reduced to water.\nLead-acid battery technology has been developed extensively. Upkeep requires minimal labor and its cost is low. The battery's available energy capacity is subject to a quick discharge resulting in a low life span and low energy density.Nickel\u2013cadmium battery (NiCd): Uses nickel oxide hydroxide and metallic cadmium as electrodes. Cadmium is a toxic element, and was banned for most uses by the European Union in 2004. Nickel\u2013cadmium batteries have been almost completely replaced by nickel\u2013metal hydride (NiMH) batteries.\nNickel\u2013metal hydride battery (NiMH): First commercial types were available in 1989. These are now a common consumer and industrial type. The battery has a hydrogen-absorbing alloy for the negative electrode instead of cadmium.\nLithium-ion battery: The choice in many consumer electronics and have one of the best energy-to-mass ratios and a very slow self-discharge when not in use.\nLithium-ion polymer battery: These batteries are light in weight and can be made in any shape desired.\nAluminium-sulfur battery with rock salt crystals as electrolyte: aluminium and sulfur are Earth-abundant materials and are much more cheaper than traditional Lithium.\n\n\n===== Flow battery =====\n\nA flow battery works by passing a solution over a membrane where ions are exchanged to charge or discharge the cell. Cell voltage is chemically determined by the Nernst equation and ranges, in practical applications, from 1.0 V to 2.2 V. Storage capacity depends on the volume of solution. A flow battery is technically akin both to a fuel cell and an electrochemical accumulator cell. Commercial applications are for long half-cycle storage such as backup grid power.\n\n\n==== Supercapacitor ====\n\nSupercapacitors, also called electric double-layer capacitors (EDLC) or ultracapacitors, are a family of electrochemical capacitors that do not have conventional solid dielectrics. Capacitance is determined by two storage principles, double-layer capacitance and pseudocapacitance.Supercapacitors bridge the gap between conventional capacitors and rechargeable batteries. They store the most energy per unit volume or mass (energy density) among capacitors. They support up to 10,000 farads/1.2 Volt, up to 10,000 times that of electrolytic capacitors, but deliver or accept less than half as much power per unit time (power density).While supercapacitors have specific energy and energy densities that are approximately 10% of batteries, their power density is generally 10 to 100 times greater. This results in much shorter charge/discharge cycles. Also, they tolerate many more charge-discharge cycles than batteries.\nSupercapacitors have many applications, including:\n\nLow supply current for memory backup in static random-access memory (SRAM)\nPower for cars, buses, trains, cranes and elevators, including energy recovery from braking, short-term energy storage and burst-mode power delivery\n\n\n=== Chemical ===\n\n\n==== Power to gas ====\n\nPower to gas is the conversion of electricity to a gaseous fuel such as hydrogen or methane. The three commercial methods use electricity to reduce water into hydrogen and oxygen by means of electrolysis.\nIn the first method, hydrogen is injected into the natural gas grid or is used for transportation. The second method is to combine the hydrogen with carbon dioxide to produce methane using a methanation reaction such as the Sabatier reaction, or biological methanation, resulting in an extra energy conversion loss of 8%. The methane may then be fed into the natural gas grid. The third method uses the output gas of a wood gas generator or a biogas plant, after the biogas upgrader is mixed with the hydrogen from the electrolyzer, to upgrade the quality of the biogas.\n\n\n===== Hydrogen =====\n\nThe element hydrogen can be a form of stored energy. Hydrogen can produce electricity via a hydrogen fuel cell. Green hydrogen, from electrolysis of water, is a more economical means of long-term renewable energy storage in terms of capital expenditures than pumped-storage hydroelectricity or batteries.At penetrations below 20% of the grid demand, renewables do not severely change the economics; but beyond about 20% of the total demand, external storage becomes important. If these sources are used to make ionic hydrogen, they can be freely expanded. A 5-year community-based pilot program using wind turbines and hydrogen generators began in 2007 in the remote community of Ramea, Newfoundland and Labrador. A similar project began in 2004 on Utsira, a small Norwegian island.\nEnergy losses involved in the hydrogen storage cycle come from the electrolysis of water, liquification or compression of the hydrogen and conversion to electricity.About 50 kW\u00b7h (180 MJ) of solar energy is required to produce a kilogram of hydrogen, so the cost of the electricity is crucial. At $0.03/kWh, a common off-peak high-voltage line rate in the United States, hydrogen costs $1.50 per kilogram for the electricity, equivalent to $1.50/gallon for gasoline. Other costs include the electrolyzer plant, hydrogen compressors or liquefaction, storage and transportation.Hydrogen can also be produced from aluminum and water by stripping aluminum's naturally-occurring aluminum oxide barrier and introducing it to water. This method is beneficial because recycled aluminum cans can be used to generate hydrogen, however systems to harness this option have not been commercially developed and are much more complex than electrolysis systems. Common methods to strip the oxide layer include caustic catalysts such as sodium hydroxide and alloys with gallium, mercury and other metals.Underground hydrogen storage is the practice of hydrogen storage in caverns, salt domes and depleted oil and gas fields. Large quantities of gaseous hydrogen have been stored in caverns by Imperial Chemical Industries for many years without any difficulties. The European Hyunder project indicated in 2013 that storage of wind and solar energy using underground hydrogen would require 85 caverns.Powerpaste is a magnesium and hydrogen -based fluid gel that releases hydrogen when reacting with water. It was invented, patented and is being developed by the Fraunhofer Institute for Manufacturing Technology and Advanced Materials (IFAM) of the Fraunhofer-Gesellschaft.  Powerpaste is made by combining magnesium powder with hydrogen to form magnesium hydride in a process conducted at 350 \u00b0C and five to six times atmospheric pressure. An ester and a metal salt are then added to make the finished product.  Fraunhofer states that they are building a production plant slated to start production in 2021, which will produce 4 tons of Powerpaste annually. Fraunhofer has patented their invention in the United States and EU. Fraunhofer claims that Powerpaste is able to store hydrogen energy at 10 times the energy density of a lithium battery of a similar dimension and is safe and convenient for automotive situations.\n\n\n===== Methane =====\n\nMethane is the simplest hydrocarbon with the molecular formula CH4. Methane is more easily stored and transported than hydrogen. Storage and combustion infrastructure (pipelines, gasometers, power plants) are mature.\nSynthetic natural gas (syngas or SNG) can be created in a multi-step process, starting with hydrogen and oxygen. Hydrogen is then reacted with carbon dioxide in a Sabatier process, producing methane and water. Methane can be stored and later used to produce electricity. The resulting water is recycled, reducing the need for water. In the electrolysis stage, oxygen is stored for methane combustion in a pure oxygen environment at an adjacent power plant, eliminating nitrogen oxides.\nMethane combustion produces carbon dioxide (CO2) and water. The carbon dioxide can be recycled to boost the Sabatier process and water can be recycled for further electrolysis. Methane production, storage and combustion recycles the reaction products.\nThe CO2 has economic value as a component of an energy storage vector, not a cost as in carbon capture and storage.\n\n\n==== Power to liquid ====\nPower to liquid is similar to power to gas except that the hydrogen is converted into liquids such as methanol or ammonia. These are easier to handle than gases, and require fewer safety precautions than hydrogen. They can be used for transportation, including aircraft, but also for industrial purposes or in the power sector.\n\n\n==== Biofuels ====\n\nVarious biofuels such as biodiesel, vegetable oil, alcohol fuels, or biomass can replace fossil fuels. Various chemical processes can convert the carbon and hydrogen in coal, natural gas, plant and animal biomass and organic wastes into short hydrocarbons suitable as replacements for existing hydrocarbon fuels. Examples are Fischer\u2013Tropsch diesel, methanol, dimethyl ether and syngas. This diesel source was used extensively in World War II in Germany, which faced limited access to crude oil supplies. South Africa produces most of the country's diesel from coal for similar reasons. A long term oil price above US$35/bbl may make such large scale synthetic liquid fuels economical.\n\n\n===== Aluminum =====\nAluminum has been proposed as an energy store by a number of researchers. Its electrochemical equivalent (8.04 Ah/cm3) is nearly four times greater than that of lithium (2.06 Ah/cm3). Energy can be extracted from aluminum by reacting it with water to generate hydrogen. However, it must first be stripped of its natural oxide layer, a process which requires pulverization, chemical reactions with caustic substances, or alloys. The byproduct of the reaction to create hydrogen is aluminum oxide, which can be recycled into aluminum with the Hall\u2013H\u00e9roult process, making the reaction theoretically renewable. If the Hall-Heroult Process is run using solar or wind power, aluminum could be used to store the energy produced at higher efficiency than direct solar electrolysis.\n\n\n==== Boron, silicon, and zinc ====\nBoron, silicon, and zinc have been proposed as energy storage solutions.\n\n\n==== Other chemical ====\nThe organic compound norbornadiene converts to quadricyclane upon exposure to light, storing solar energy as the energy of chemical bonds. A working system has been developed in Sweden as a molecular solar thermal system.\n\n\n=== Electrical methods ===\n\n\n==== Capacitor ====\n\nA capacitor (originally known as a 'condenser') is a passive two-terminal electrical component used to store energy electrostatically. Practical capacitors vary widely, but all contain at least two electrical conductors (plates) separated by a dielectric (i.e., insulator). A capacitor can store electric energy when disconnected from its charging circuit, so it can be used like a temporary battery, or like other types of rechargeable energy storage system. Capacitors are commonly used in electronic devices to maintain power supply while batteries change. (This prevents loss of information in volatile memory.) Conventional capacitors provide less than 360 joules per kilogram, while a conventional alkaline battery has a density of 590 kJ/kg.\nCapacitors store energy in an electrostatic field between their plates. Given a potential difference across the conductors (e.g., when a capacitor is attached across a battery), an electric field develops across the dielectric, causing positive charge (+Q) to collect on one plate and negative charge (-Q) to collect on the other plate. If a battery is attached to a capacitor for a sufficient amount of time, no current can flow through the capacitor. However, if an accelerating or alternating voltage is applied across the leads of the capacitor, a displacement current can flow. Besides capacitor plates, charge can also be stored in a dielectric layer.Capacitance is greater given a narrower separation between conductors and when the conductors have a larger surface area. In practice, the dielectric between the plates emits a small amount of leakage current and has an electric field strength limit, known as the breakdown voltage. However, the effect of recovery of a dielectric after a high-voltage breakdown holds promise for a new generation of self-healing capacitors. The conductors and leads introduce undesired inductance and resistance.\nResearch is assessing the quantum effects of nanoscale capacitors for digital quantum batteries.\n\n\n==== Superconducting magnetics ====\n\nSuperconducting magnetic energy storage (SMES) systems store energy in a magnetic field created by the flow of direct current in a superconducting coil that has been cooled to a temperature below its superconducting critical temperature. A typical SMES system includes a superconducting coil, power conditioning system and refrigerator. Once the superconducting coil is charged, the current does not decay and the magnetic energy can be stored indefinitely.The stored energy can be released to the network by discharging the coil. The associated inverter/rectifier accounts for about 2\u20133% energy loss in each direction. SMES loses the least amount of electricity in the energy storage process compared to other methods of storing energy. SMES systems offer round-trip efficiency greater than 95%.Due to the energy requirements of refrigeration and the cost of superconducting wire, SMES is used for short duration storage such as improving power quality. It also has applications in grid balancing.\n\n\n== Applications ==\n\n\n=== Mills ===\nThe classic application before the industrial revolution was the control of waterways to drive water mills for processing grain or powering machinery. Complex systems of reservoirs and dams were constructed to store and release water (and the potential energy it contained) when required.\n\n\n=== Homes ===\n\nHome energy storage is expected to become increasingly common given the growing importance of distributed generation of renewable energies (especially photovoltaics) and the important share of energy consumption in buildings. To exceed a self-sufficiency of 40% in a household equipped with photovoltaics, energy storage is needed. Multiple manufacturers produce rechargeable battery systems for storing energy, generally to hold surplus energy from home solar or wind generation. Today, for home energy storage, Li-ion batteries are preferable to lead-acid ones given their similar cost but much better performance.Tesla Motors produces two models of the Tesla Powerwall. One is a 10 kWh weekly cycle version for backup applications and the other is a 7 kWh version for daily cycle applications. In 2016, a limited version of the Tesla Powerpack 2 cost $398(US)/kWh to store electricity worth 12.5 cents/kWh (US average grid price) making a positive return on investment doubtful unless electricity prices are higher than 30 cents/kWh.RoseWater Energy produces two models of the \"Energy & Storage System\", the HUB 120 and SB20. Both versions provide 28.8 kWh of output, enabling it to run larger houses or light commercial premises, and protecting custom installations. The system provides five key elements into one system, including providing a clean 60 Hz Sine wave, zero transfer time, industrial-grade surge protection, renewable energy grid sell-back (optional), and battery backup.Enphase Energy announced an integrated system that allows home users to store, monitor and manage electricity. The system stores 1.2 kWh of energy and 275W/500W power output.Storing wind or solar energy using thermal energy storage though less flexible, is considerably cheaper than batteries. A simple 52-gallon electric water heater can store roughly 12 kWh of energy for supplementing hot water or space heating.For purely financial purposes in areas where net metering is available, home generated electricity may be sold to the grid through a grid-tie inverter without the use of batteries for storage.\n\n\n=== Grid electricity and power stations ===\n\n\n==== Renewable energy ====\nThe largest source and the greatest store of renewable energy is provided by hydroelectric dams. A large reservoir behind a dam can store enough water to average the annual flow of a river between dry and wet seasons, and a very large reservoir can store enough water to average the flow of a river between dry and wet years. While a hydroelectric dam does not directly store energy from intermittent sources, it does balance the grid by lowering its output and retaining its water when power is generated by solar or wind. If wind or solar generation exceeds the region's hydroelectric capacity, then some additional source of energy is needed.\nMany renewable energy sources (notably solar and wind) produce variable power. Storage systems can level out the imbalances between supply and demand that this causes. Electricity must be used as it is generated or converted immediately into storable forms.The main method of electrical grid storage is pumped-storage hydroelectricity. Areas of the world such as Norway, Wales, Japan and the US have used elevated geographic features for reservoirs, using electrically powered pumps to fill them. When needed, the water passes through generators and converts the gravitational potential of the falling water into electricity. Pumped storage in Norway, which gets almost all its electricity from hydro, has currently a capacity of 1.4 GW but since the total installed capacity is nearly 32 GW and 75% of that is regulable, it can be expanded significantly.Some forms of storage that produce electricity include pumped-storage hydroelectric dams, rechargeable batteries, thermal storage including molten salts which can efficiently store and release very large quantities of heat energy, and compressed air energy storage, flywheels, cryogenic systems and superconducting magnetic coils.\nSurplus power can also be converted into methane (sabatier process) with stockage in the natural gas network.In 2011, the Bonneville Power Administration in the northwestern United States created an experimental program to absorb excess wind and hydro power generated at night or during stormy periods that are accompanied by high winds. Under central control, home appliances absorb surplus energy by heating ceramic bricks in special space heaters to hundreds of degrees and by boosting the temperature of modified hot water heater tanks. After charging, the appliances provide home heating and hot water as needed. The experimental system was created as a result of a severe 2010 storm that overproduced renewable energy to the extent that all conventional power sources were shut down, or in the case of a nuclear power plant, reduced to its lowest possible operating level, leaving a large area running almost completely on renewable energy.Another advanced method used at the former Solar Two project in the United States and the Solar Tres Power Tower in Spain uses molten salt to store thermal energy captured from the sun and then convert it and dispatch it as electrical power. The system pumps molten salt through a tower or other special conduits to be heated by the sun. Insulated tanks store the solution. Electricity is produced by turning water to steam that is fed to turbines.\nSince the early 21st century batteries have been applied to utility scale load-leveling and frequency regulation capabilities.In vehicle-to-grid storage, electric vehicles that are plugged into the energy grid can deliver stored electrical energy from their batteries into the grid when needed.\n\n\n=== Air conditioning ===\n\nThermal energy storage (TES) can be used for air conditioning. It is most widely used for cooling single large buildings and/or groups of smaller buildings. Commercial air conditioning systems are the biggest contributors to peak electrical loads. In 2009, thermal storage was used in over 3,300 buildings in over 35 countries. It works by chilling material at night and using the chilled material for cooling during the hotter daytime periods.The most popular technique is ice storage, which requires less space than water and is cheaper than fuel cells or flywheels. In this application, a standard chiller runs at night to produce an ice pile. Water circulates through the pile during the day to chill water that would normally be the chiller's daytime output.\nA partial storage system minimizes capital investment by running the chillers nearly 24 hours a day. At night, they produce ice for storage and during the day they chill water. Water circulating through the melting ice augments the production of chilled water. Such a system makes ice for 16 to 18 hours a day and melts ice for six hours a day. Capital expenditures are reduced because the chillers can be just 40% - 50% of the size needed for a conventional, no-storage design. Storage sufficient to store half a day's available heat is usually adequate.\nA full storage system shuts off the chillers during peak load hours. Capital costs are higher, as such a system requires larger chillers and a larger ice storage system.\nThis ice is produced when electrical utility rates are lower. Off-peak cooling systems can lower energy costs. The U.S. Green Building Council has developed the Leadership in Energy and Environmental Design (LEED) program to encourage the design of reduced-environmental impact buildings. Off-peak cooling may help toward LEED Certification.Thermal storage for heating is less common than for cooling. An example of thermal storage is storing solar heat to be used for heating at night.\nLatent heat can also be stored in technical phase change materials (PCMs). These can be encapsulated in wall and ceiling panels, to moderate room temperatures.\n\n\n=== Transport ===\nLiquid hydrocarbon fuels are the most commonly used forms of energy storage for use in transportation, followed by a growing use of Battery Electric Vehicles and Hybrid Electric Vehicles. Other energy carriers such as hydrogen can be used to avoid producing greenhouse gases.\nPublic transport systems like trams and trolleybuses require electricity, but due to their variability in movement, a steady supply of electricity via renewable energy is challenging. Photovoltaic systems installed on the roofs of buildings can be used to power public transportation systems during periods in which there is increased demand for electricity and access to other forms of energy are not readily available. Upcoming transitions in the transportation system also include e.g. ferries and airplanes, where electric power supply is investigated as an interesting alternative.\n\n\n=== Electronics ===\nCapacitors are widely used in electronic circuits for blocking direct current while allowing alternating current to pass. In analog filter networks, they smooth the output of power supplies. In resonant circuits they tune radios to particular frequencies. In electric power transmission systems they stabilize voltage and power flow.\n\n\n== Use cases ==\n\nThe United States Department of Energy International Energy Storage Database (IESDB), is a free-access database of energy storage projects and policies funded by the United States Department of Energy Office of Electricity and Sandia National Labs.\n\n\n== Capacity ==\nStorage capacity is the amount of energy extracted from an energy storage device or system; usually measured in joules or kilowatt-hours and their multiples, it may be given in number of hours of electricity production at power plant nameplate capacity; when storage is of primary type (i.e., thermal or pumped-water), output is sourced only with the power plant embedded storage system.\n\n\n== Economics ==\nThe economics of energy storage strictly depends on the reserve service requested, and several uncertainty factors affect the profitability of energy storage. Therefore, not every storage method is technically and economically suitable for the storage of several MWh, and the optimal size of the energy storage is market and location dependent.Moreover, ESS are affected by several risks, e.g.:\nTechno-economic risks, which are related to the specific technology;\nMarket risks, which are the factors that affect the electricity supply system;\nRegulation and policy risks.Therefore, traditional techniques based on deterministic Discounted Cash Flow (DCF) for the investment appraisal are not fully adequate to evaluate these risks and uncertainties and the investor's flexibility to deal with them. Hence, the literature recommends to assess the value of risks and uncertainties through the Real Option Analysis (ROA), which is a valuable method in uncertain contexts.The economic valuation of large-scale applications (including pumped hydro storage and compressed air) considers benefits including: curtailment avoidance, grid congestion avoidance, price arbitrage and carbon-free energy delivery. In one technical assessment by the Carnegie Mellon Electricity Industry Centre, economic goals could be met using batteries if their capital cost was $30 to $50 per kilowatt-hour.A metric of energy efficiency of storage is energy storage on energy invested (ESOI), which is the amount of energy that can be stored by a technology, divided by the amount of energy required to build that technology. The higher the ESOI, the better the storage technology is energetically. For lithium-ion batteries this is around 10, and for lead acid batteries it is about 2. Other forms of storage such as pumped hydroelectric storage generally have higher ESOI, such as 210.Pumped-storage hydroelectricity is by far the largest storage technology used globally. However, the usage of conventional pumped-hydro storage is limited because it requires terrain with elevation differences and also has a very high land use for relatively small power. In locations without suitable natural geography, underground pumped-hydro storage could also be used. High costs and limited life still make batteries a \"weak substitute\" for dispatchable power sources, and are unable to cover for variable renewable power gaps lasting for days, weeks or months. In grid models with high VRE share, the excessive cost of storage tends to dominate the costs of the whole grid \u2014 for example, in California alone 80% share of VRE would require 9.6 TWh of storage but 100% would require 36.3 TWh. As of 2018 the state only had 150 GWh of storage, primarily in pumped storage and a small fraction in batteries. According to another study, supplying 80% of US demand from VRE would require a smart grid covering the whole country or battery storage capable to supply the whole system for 12 hours, both at cost estimated at $2.5 trillion. Similarly, several studies have found that relying only on VRE and energy storage would cost about 30-50% more than a comparable system that combines VRE with nuclear plants or plants with carbon capture and storage instead of energy storage.\n\n\n== Research ==\n\n\n=== Germany ===\nIn 2013, the German government allocated \u20ac200M (approximately US$270M) for research, and another \u20ac50M to subsidize battery storage in residential rooftop solar panels, according to a representative of the German Energy Storage Association.Siemens AG commissioned a production-research plant to open in 2015 at the Zentrum f\u00fcr Sonnenenergie und Wasserstoff (ZSW, the German Center for Solar Energy and Hydrogen Research in the State of Baden-W\u00fcrttemberg), a university/industry collaboration in Stuttgart, Ulm and Widderstall, staffed by approximately 350 scientists, researchers, engineers, and technicians. The plant develops new near-production manufacturing materials and processes (NPMM&P) using a computerized Supervisory Control and Data Acquisition (SCADA) system. It aims to enable the expansion of rechargeable battery production with increased quality and lower cost.\n\n\n=== United States ===\nIn 2014, research and test centers opened to evaluate energy storage technologies. Among them was the Advanced Systems Test Laboratory at the University of Wisconsin at Madison in Wisconsin State, which partnered with battery manufacturer Johnson Controls. The laboratory was created as part of the university's newly opened Wisconsin Energy Institute. Their goals include the evaluation of state-of-the-art and next generation electric vehicle batteries, including their use as grid supplements.The State of New York unveiled its New York Battery and Energy Storage Technology (NY-BEST) Test and Commercialization Center at Eastman Business Park in Rochester, New York, at a cost of $23 million for its almost 1,700 m2 laboratory. The center includes the Center for Future Energy Systems, a collaboration between Cornell University of Ithaca, New York and the Rensselaer Polytechnic Institute in Troy, New York. NY-BEST tests, validates and independently certifies diverse forms of energy storage intended for commercial use.On September 27, 2017, Senators Al Franken of Minnesota and Martin Heinrich of New Mexico introduced Advancing Grid Storage Act (AGSA), which would devote more than $1 billion in research, technical assistance and grants to encourage energy storage in the United States.In grid models with high VRE share, the excessive cost of storage tends to dominate the costs of the whole grid \u2014 for example, in California alone 80% share of VRE would require 9.6 TWh of storage but 100% would require 36.3 TWh. According to another study, supplying 80% of US demand from VRE would require a smart grid covering the whole country or battery storage capable to supply the whole system for 12 hours, both at cost estimated at $2.5 trillion.\n\n\n=== United Kingdom ===\nIn the United Kingdom, some 14 industry and government agencies allied with seven British universities in May 2014 to create the SUPERGEN Energy Storage Hub in order to assist in the coordination of energy storage technology research and development.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n Journals and papers \n\nChen, Haisheng; Thang Ngoc Cong; Wei Yang; Chunqing Tan; Yongliang Li; Yulong Ding. Progress in electrical energy storage system: A critical review, Progress in Natural Science, accepted July 2, 2008, published in Vol. 19, 2009, pp. 291\u2013312, doi: 10.1016/j.pnsc.2008.07.014. Sourced from the National Natural Science Foundation of China and the Chinese Academy of Sciences. Published by Elsevier and Science in China Press. Synopsis: a review of electrical energy storage technologies for stationary applications. Retrieved from ac.els-cdn.com on May 13, 2014. (PDF)\nCorum, Lyn. The New Core Technology: Energy storage is part of the smart grid evolution, The Journal of Energy Efficiency and Reliability, December 31, 2009. Discusses: Anaheim Public Utilities Department, lithium ion energy storage, iCel Systems, Beacon Power, Electric Power Research Institute (EPRI), ICEL, Self Generation Incentive Program, ICE Energy, vanadium redox flow, lithium Ion, regenerative fuel cell, ZBB, VRB, lead acid, CAES, and Thermal Energy Storage. (PDF)\nde Oliveira e Silva, G.; Hendrick, P. (2016). \"Lead-acid batteries coupled with photovoltaics for increased electricity self-sufficiency in households\". Applied Energy. 178: 856\u2013867. doi:10.1016/j.apenergy.2016.06.003.\nWhittingham, M. Stanley. History, Evolution, and Future Status of Energy Storage, Proceedings of the IEEE, manuscript accepted February 20, 2012, date of publication April 16, 2012; date of current version May 10, 2012, published in Proceedings of the IEEE, Vol. 100, May 13, 2012, 0018\u20139219, pp. 1518\u20131534, doi: 10.1109/JPROC.2012.219017. Retrieved from ieeexplore.ieee.org May 13, 2014. Synopsis: A discussion of the important aspects of energy storage including emerging battery technologies and the importance of storage systems in key application areas, including electronic devices, transportation, and the utility grid. (PDF) Books \n\nGA Mansoori, N Enayati, LB Agyarko (2016), Energy: Sources, Utilization, Legislation, Sustainability, Illinois as Model State, World Sci. Pub. Co., ISBN 978-981-4704-00-7\nD\u00edaz-Gonz\u00e1lez, Franscisco (2016). Energy storage in power systems. United Kingdom: John Wiley & Sons. ISBN 9781118971321.\n\n\n== External links ==\n\nU.S. Dept of Energy - Energy Storage Systems Government research center on energy storage technology.\nU.S. Dept of Energy - International Energy Storage Database Archived November 13, 2013, at the Wayback Machine The DOE International Energy Storage Database provides free, up-to-date information on grid-connected energy storage projects and relevant state and federal policies.\nIEEE Special Issue on Massive Energy Storage\nIEA-ECES - International Energy Agency - Energy Conservation through Energy Conservation programme.\nEnergy Information Administration Glossary\nEnergy Storage Project Regeneration."}, {"id": 79, "title": "Deutsch\u2013Jozsa algorithm", "content": "The Deutsch\u2013Jozsa algorithm is a deterministic quantum algorithm proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm.The Deutsch\u2013Jozsa problem is specifically designed to be easy for a quantum algorithm and hard for any deterministic classical algorithm. It is a black box problem that can be solved efficiently by a quantum computer with no error, whereas a deterministic classical computer would need a exponential number of queries to the black box to solve the problem. More formally, it yields an oracle relative to which EQP, the class of problems that can be solved exactly in polynomial time on a quantum computer, and P are different.Since the problem is easy to solve on a probabilistic classical computer, it does not yield an oracle separation with BPP, the class of problems that can be solved with bounded error in polynomial time on a probabilistic classical computer. Simon's problem is an example of a problem that yields an oracle separation between BQP and BPP.\n\n\n== Problem statement ==\nIn the Deutsch\u2013Jozsa problem, we are given a black box quantum computer known as an oracle that implements some function: \n\n  \n    \n      \n        f\n        :\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        \u2192\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle f\\colon \\{0,1\\}^{n}\\rightarrow \\{0,1\\}}\n   \nThe function takes n-bit binary values as input and produces either a 0 or a 1 as output for each such value. We are promised that the function is either constant (0 on all inputs or 1 on all inputs) or balanced (1 for exactly half of the input domain and 0 for the other half). The task then is to determine if \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is constant or balanced by using the oracle.\n\n\n== Classical solution ==\nFor a conventional deterministic algorithm where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the number of bits, \n  \n    \n      \n        \n          2\n          \n            n\n            \u2212\n            1\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle 2^{n-1}+1}\n   evaluations of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   will be required in the worst case. To prove that \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is constant, just over half the set of inputs must be evaluated and their outputs found to be identical (because the function is guaranteed to be either balanced or constant, not somewhere in between). The best case occurs where the function is balanced and the first two output values are different. For a conventional randomized algorithm, a constant \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   evaluations of the function suffices to produce the correct answer with a high probability (failing with probability \n  \n    \n      \n        \u03f5\n        \u2264\n        1\n        \n          /\n        \n        \n          2\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\epsilon \\leq 1/2^{k}}\n   with \n  \n    \n      \n        k\n        \u2265\n        1\n      \n    \n    {\\displaystyle k\\geq 1}\n  ). However, \n  \n    \n      \n        k\n        =\n        \n          2\n          \n            n\n            \u2212\n            1\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle k=2^{n-1}+1}\n   evaluations are still required if we want an answer that has no possibility of error. The Deutsch-Jozsa quantum algorithm produces an answer that is always correct with a single evaluation of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  .\n\n\n== History ==\nThe Deutsch\u2013Jozsa algorithm generalizes earlier (1985) work by David Deutsch, which provided a solution for the simple case where \n  \n    \n      \n        n\n        =\n        1\n      \n    \n    {\\displaystyle n=1}\n  . \nSpecifically, given a boolean function whose input is one bit, \n  \n    \n      \n        f\n        :\n        {\n        0\n        ,\n        1\n        }\n        \u2192\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle f:\\{0,1\\}\\rightarrow \\{0,1\\}}\n  , is it constant?The algorithm, as Deutsch had originally proposed it, was not deterministic. The algorithm was successful with a probability of one half. \nIn 1992, Deutsch and Jozsa produced a deterministic algorithm which was generalized to a function which takes \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   bits for its input. Unlike Deutsch's algorithm, this algorithm required two function evaluations instead of only one.\nFurther improvements to the Deutsch\u2013Jozsa algorithm were made by Cleve et al., resulting in an algorithm that is both deterministic and requires only a single query of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . This algorithm is still referred to as Deutsch\u2013Jozsa algorithm in honour of the groundbreaking techniques they employed.\n\n\n== Algorithm ==\nFor the Deutsch\u2013Jozsa algorithm to work, the oracle computing \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   from \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   must be a quantum oracle which does not decohere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  . It also must not make a copy of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , because that would violate the no cloning theorem.\n\nThe algorithm begins with the \n  \n    \n      \n        n\n        +\n        1\n      \n    \n    {\\displaystyle n+1}\n   bit state \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            n\n          \n        \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes n}|1\\rangle }\n  . That is, the first n bits are each in the state \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle }\n   and the final bit is \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |1\\rangle }\n  . A Hadamard transform is applied to each bit to obtain the state\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                  +\n                  1\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2^{n+1}}}}\\sum _{x=0}^{2^{n}-1}|x\\rangle (|0\\rangle -|1\\rangle )}\n  ,where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   runs over all \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -bit strings. We have the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   implemented as a quantum oracle. The oracle maps its input-state \n  \n    \n      \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u27e9\n      \n    \n    {\\displaystyle |x\\rangle |y\\rangle }\n   to \n  \n    \n      \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u2295\n        f\n        (\n        x\n        )\n        \u27e9\n      \n    \n    {\\displaystyle |x\\rangle |y\\oplus f(x)\\rangle }\n  , where \n  \n    \n      \n        \u2295\n      \n    \n    {\\displaystyle \\oplus }\n   denotes addition modulo 2. Applying the quantum oracle gives;\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                  +\n                  1\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        (\n        \n          |\n        \n        0\n        \u2295\n        f\n        (\n        x\n        )\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u2295\n        f\n        (\n        x\n        )\n        \u27e9\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2^{n+1}}}}\\sum _{x=0}^{2^{n}-1}|x\\rangle (|0\\oplus f(x)\\rangle -|1\\oplus f(x)\\rangle )}\n  .For each \n  \n    \n      \n        x\n        ,\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle x,f(x)}\n   is either 0 or 1. Testing these two possibilities, we see the above state is equal to\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                  +\n                  1\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2^{n+1}}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}|x\\rangle (|0\\rangle -|1\\rangle )}\n  .At this point the last qubit \n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              0\n              \u27e9\n              \u2212\n              \n                |\n              \n              1\n              \u27e9\n            \n            \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {|0\\rangle -|1\\rangle }{\\sqrt {2}}}}\n   may be ignored and the following remains:\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2^{n}}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}|x\\rangle }\n  .Next, we will apply the Hadamard transform\n\n  \n    \n      \n        \n          H\n          \n            \u2297\n            n\n          \n        \n        \n          |\n        \n        k\n        \u27e9\n        =\n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            j\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        (\n        \u2212\n        1\n        \n          )\n          \n            k\n            \u22c5\n            j\n          \n        \n        \n          |\n        \n        j\n        \u27e9\n      \n    \n    {\\displaystyle H^{\\otimes n}|k\\rangle ={\\frac {1}{\\sqrt {2^{n}}}}\\sum _{j=0}^{2^{n}-1}(-1)^{k\\cdot j}|j\\rangle }\n  (\n  \n    \n      \n        j\n        \u22c5\n        k\n        =\n        \n          j\n          \n            0\n          \n        \n        \n          k\n          \n            0\n          \n        \n        \u2295\n        \n          j\n          \n            1\n          \n        \n        \n          k\n          \n            1\n          \n        \n        \u2295\n        \u22ef\n        \u2295\n        \n          j\n          \n            n\n            \u2212\n            1\n          \n        \n        \n          k\n          \n            n\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle j\\cdot k=j_{0}k_{0}\\oplus j_{1}k_{1}\\oplus \\cdots \\oplus j_{n-1}k_{n-1}}\n   is the sum of the bitwise product, where \n  \n    \n      \n        \u2295\n      \n    \n    {\\displaystyle \\oplus }\n   is addition modulo 2) over the first register to obtain\n\n  \n    \n      \n        \n          \n            1\n            \n              \n                2\n                \n                  n\n                \n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          [\n          \n            \n              \n                1\n                \n                  \n                    2\n                    \n                      n\n                    \n                  \n                \n              \n            \n            \n              \u2211\n              \n                y\n                =\n                0\n              \n              \n                \n                  2\n                  \n                    n\n                  \n                \n                \u2212\n                1\n              \n            \n            (\n            \u2212\n            1\n            \n              )\n              \n                x\n                \u22c5\n                y\n              \n            \n            \n              |\n            \n            y\n            \u27e9\n          \n          ]\n        \n        =\n        \n          \u2211\n          \n            y\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        \n          [\n          \n            \n              \n                1\n                \n                  2\n                  \n                    n\n                  \n                \n              \n            \n            \n              \u2211\n              \n                x\n                =\n                0\n              \n              \n                \n                  2\n                  \n                    n\n                  \n                \n                \u2212\n                1\n              \n            \n            (\n            \u2212\n            1\n            \n              )\n              \n                f\n                (\n                x\n                )\n              \n            \n            (\n            \u2212\n            1\n            \n              )\n              \n                x\n                \u22c5\n                y\n              \n            \n          \n          ]\n        \n        \n          |\n        \n        y\n        \u27e9\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2^{n}}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}\\left[{\\frac {1}{\\sqrt {2^{n}}}}\\sum _{y=0}^{2^{n}-1}(-1)^{x\\cdot y}|y\\rangle \\right]=\\sum _{y=0}^{2^{n}-1}\\left[{\\frac {1}{2^{n}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}(-1)^{x\\cdot y}\\right]|y\\rangle }\n  From this, we can see that the probability for a state \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   to be measured is\n\n  \n    \n      \n        \n          \n            |\n            \n              \n                \n                  1\n                  \n                    2\n                    \n                      n\n                    \n                  \n                \n              \n              \n                \u2211\n                \n                  x\n                  =\n                  0\n                \n                \n                  \n                    2\n                    \n                      n\n                    \n                  \n                  \u2212\n                  1\n                \n              \n              (\n              \u2212\n              1\n              \n                )\n                \n                  f\n                  (\n                  x\n                  )\n                \n              \n              (\n              \u2212\n              1\n              \n                )\n                \n                  x\n                  \u22c5\n                  k\n                \n              \n            \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\left|{\\frac {1}{2^{n}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}(-1)^{x\\cdot k}\\right|^{2}}\n  The probability of measuring \n  \n    \n      \n        k\n        =\n        0\n      \n    \n    {\\displaystyle k=0}\n  , corresponding to \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            n\n          \n        \n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes n}}\n  , is\n\n  \n    \n      \n        \n          \n            |\n          \n        \n        \n          \n            1\n            \n              2\n              \n                n\n              \n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            \n              2\n              \n                n\n              \n            \n            \u2212\n            1\n          \n        \n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          \n            \n              |\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\bigg |}{\\frac {1}{2^{n}}}\\sum _{x=0}^{2^{n}-1}(-1)^{f(x)}{\\bigg |}^{2}}\n  which evaluates to 1 if \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is constant (constructive interference) and 0 if \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is balanced (destructive interference). In other words, the final measurement will be \n  \n    \n      \n        \n          |\n        \n        0\n        \n          \u27e9\n          \n            \u2297\n            n\n          \n        \n      \n    \n    {\\displaystyle |0\\rangle ^{\\otimes n}}\n   (all zeros) if and only if \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is constant and will yield some other state if \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is balanced.\n\n\n== Deutsch's algorithm ==\nDeutsch's algorithm is a special case of the general Deutsch\u2013Jozsa algorithm where n = 1 in \n  \n    \n      \n        f\n        :\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        \u2192\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle f\\colon \\{0,1\\}^{n}\\rightarrow \\{0,1\\}}\n  . We need to check the condition \n  \n    \n      \n        f\n        (\n        0\n        )\n        =\n        f\n        (\n        1\n        )\n      \n    \n    {\\displaystyle f(0)=f(1)}\n  . It is equivalent to check \n  \n    \n      \n        f\n        (\n        0\n        )\n        \u2295\n        f\n        (\n        1\n        )\n      \n    \n    {\\displaystyle f(0)\\oplus f(1)}\n   (where \n  \n    \n      \n        \u2295\n      \n    \n    {\\displaystyle \\oplus }\n   is addition modulo 2, which can also be viewed as a quantum XOR gate implemented as a Controlled NOT gate), if zero, then \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is constant, otherwise \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is not constant.\nWe begin with the two-qubit state \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle |1\\rangle }\n   and apply a Hadamard transform to each qubit. This yields\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        (\n        \n          |\n        \n        0\n        \u27e9\n        +\n        \n          |\n        \n        1\n        \u27e9\n        )\n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {1}{2}}(|0\\rangle +|1\\rangle )(|0\\rangle -|1\\rangle ).}\n  We are given a quantum implementation of the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   that maps \n  \n    \n      \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u27e9\n      \n    \n    {\\displaystyle |x\\rangle |y\\rangle }\n   to \n  \n    \n      \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        f\n        (\n        x\n        )\n        \u2295\n        y\n        \u27e9\n      \n    \n    {\\displaystyle |x\\rangle |f(x)\\oplus y\\rangle }\n  . Applying this function to our current state we obtain\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        (\n        \n          |\n        \n        0\n        \u27e9\n        (\n        \n          |\n        \n        f\n        (\n        0\n        )\n        \u2295\n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        f\n        (\n        0\n        )\n        \u2295\n        1\n        \u27e9\n        )\n        +\n        \n          |\n        \n        1\n        \u27e9\n        (\n        \n          |\n        \n        f\n        (\n        1\n        )\n        \u2295\n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        f\n        (\n        1\n        )\n        \u2295\n        1\n        \u27e9\n        )\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{2}}(|0\\rangle (|f(0)\\oplus 0\\rangle -|f(0)\\oplus 1\\rangle )+|1\\rangle (|f(1)\\oplus 0\\rangle -|f(1)\\oplus 1\\rangle ))}\n  \n\n  \n    \n      \n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n          \n        \n        \n          |\n        \n        0\n        \u27e9\n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n        +\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            1\n            )\n          \n        \n        \n          |\n        \n        1\n        \u27e9\n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n        )\n      \n    \n    {\\displaystyle ={\\frac {1}{2}}((-1)^{f(0)}|0\\rangle (|0\\rangle -|1\\rangle )+(-1)^{f(1)}|1\\rangle (|0\\rangle -|1\\rangle ))}\n  \n\n  \n    \n      \n        =\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n          \n        \n        \n          \n            1\n            2\n          \n        \n        \n          (\n          \n            \n              |\n            \n            0\n            \u27e9\n            +\n            (\n            \u2212\n            1\n            \n              )\n              \n                f\n                (\n                0\n                )\n                \u2295\n                f\n                (\n                1\n                )\n              \n            \n            \n              |\n            \n            1\n            \u27e9\n          \n          )\n        \n        (\n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        )\n        .\n      \n    \n    {\\displaystyle =(-1)^{f(0)}{\\frac {1}{2}}\\left(|0\\rangle +(-1)^{f(0)\\oplus f(1)}|1\\rangle \\right)(|0\\rangle -|1\\rangle ).}\n  We ignore the last bit and the global phase and therefore have the state\n\n  \n    \n      \n        \n          \n            1\n            \n              2\n            \n          \n        \n        (\n        \n          |\n        \n        0\n        \u27e9\n        +\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n            \u2295\n            f\n            (\n            1\n            )\n          \n        \n        \n          |\n        \n        1\n        \u27e9\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {1}{\\sqrt {2}}}(|0\\rangle +(-1)^{f(0)\\oplus f(1)}|1\\rangle ).}\n  Applying a Hadamard transform to this state we have\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        (\n        \n          |\n        \n        0\n        \u27e9\n        +\n        \n          |\n        \n        1\n        \u27e9\n        +\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n            \u2295\n            f\n            (\n            1\n            )\n          \n        \n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n            \u2295\n            f\n            (\n            1\n            )\n          \n        \n        \n          |\n        \n        1\n        \u27e9\n        )\n      \n    \n    {\\displaystyle {\\frac {1}{2}}(|0\\rangle +|1\\rangle +(-1)^{f(0)\\oplus f(1)}|0\\rangle -(-1)^{f(0)\\oplus f(1)}|1\\rangle )}\n  \n\n  \n    \n      \n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        (\n        1\n        +\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n            \u2295\n            f\n            (\n            1\n            )\n          \n        \n        )\n        \n          |\n        \n        0\n        \u27e9\n        +\n        (\n        1\n        \u2212\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            0\n            )\n            \u2295\n            f\n            (\n            1\n            )\n          \n        \n        )\n        \n          |\n        \n        1\n        \u27e9\n        )\n        .\n      \n    \n    {\\displaystyle ={\\frac {1}{2}}((1+(-1)^{f(0)\\oplus f(1)})|0\\rangle +(1-(-1)^{f(0)\\oplus f(1)})|1\\rangle ).}\n  \n  \n    \n      \n        f\n        (\n        0\n        )\n        \u2295\n        f\n        (\n        1\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(0)\\oplus f(1)=0}\n   if and only if we measure \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n      \n    \n    {\\displaystyle |0\\rangle }\n   and \n  \n    \n      \n        f\n        (\n        0\n        )\n        \u2295\n        f\n        (\n        1\n        )\n        =\n        1\n      \n    \n    {\\displaystyle f(0)\\oplus f(1)=1}\n   if and only if we measure \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |1\\rangle }\n  . So with certainty we know whether \n  \n    \n      \n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)}\n   is constant or balanced.\n\n\n== See also ==\nBernstein\u2013Vazirani algorithm\n\n\n== References ==\n\n\n== External links ==\nDeutsch's lecture about the Deutsch-Jozsa algorithm"}, {"id": 80, "title": "Storage room", "content": "A storeage room or storeroom is a room in a building for storing objects. They are not designed for permanent residence, and are often small and without windows. Such rooms often have more lenient requirements for fire protection, daylight entry and emergency exits compared to rooms intended for permanent residence.\nIn businesses, the storage is a place where the employees can put their goods and then take them out when the store starts to become empty or when there is a high demand.\nIn dwelling, storage rooms are used to store less used tools or items that are not used on a daily basis. The term shed is often used for separate small independent buildings for storing food, equipment and the like,  for example storage sheds, toolsheds or woodsheds. Historically, storage rooms in homes have often been narrow, dark and inconspicuous, and places on floors other than the main floors of the building, such as in a basement or an attic.\nA storage room can be lockable, and can be located in a housing unit or a common area, indoors or outdoors.\n\n\n== Rental of storage ==\nThere are companies that rent out storage space for self storage, where individuals and companies can rent storage rooms.\n\n\n== Television programs ==\nSheds, garages and other storage rooms can become overcrowded and cluttered with items that are not in use, or old scrap that has neither been thrown away nor repaired yet, things that one is unable to get rid of or have big plans for. The value of the mess is often small, especially if the people who live there have a compulsive hoarding problem and if the objects are stored in such a way that the condition becomes very poor. The TV show Hoarders is one of several TV shows that try to help people with such problems.\nIn some cases, there may be valuable antiques that have been stored and forgotten. The TV program American Pickers is a show where the hosts go through old collections in search of valuable antiques.\nStorage Wars is a TV series where the contents of storage lockers are auctioned off to customers who hasn't paid their rent, without the bidders being allowed to enter and have a close look on what is inside except for a quick peek from the outside.\n\n\n== See also ==\nGarage (residential), a storage area usually used to store cars\nManual handling of loads\nOverhead storage\nPantry\nUtility room\nWarehouse, a building used to store goods\nWine room\n\n\n== References =="}, {"id": 81, "title": "Classical mechanics", "content": "Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility).\nThe \"classical\" in \"classical mechanics\" does not refer to classical antiquity, as it might in, say, classical architecture. On the contrary, the development of classical mechanics involved substantial change in the methods and philosophy of physics. Instead, the qualifier distinguishes classical mechanics from physics developed after the revolutions of the early 20th century, which revealed limitations of classical mechanics.The earliest formulation of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts based on foundational works of Sir Isaac Newton, and the mathematical methods invented by Gottfried Wilhelm Leibniz, Joseph-Louis Lagrange, Leonhard Euler, and other contemporaries in the 17th century to describe the motion of bodies under the influence of forces. Later, more abstract methods were developed, leading to the reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances, made predominantly in the 18th and 19th centuries, extend substantially beyond earlier works, particularly through their use of analytical mechanics. They are, with some modification, also used in all areas of modern physics.\nClassical mechanics provides accurate results when studying large objects that are not extremely massive and speeds not approaching the speed of light. When the objects being examined have about the size of an atom diameter, it becomes necessary to introduce the other major sub-field of mechanics: quantum mechanics. To describe velocities that are not small compared to the speed of light, special relativity is needed. In cases where objects become extremely massive, general relativity becomes applicable. However, a number of modern sources do include relativistic mechanics in classical physics, which in their view represents classical mechanics in its most developed and accurate form.\n\n\n== Description of the theory ==\nThe following ideas introduce the basic concepts of classical mechanics. For simplicity, it often models real-world objects as point particles (objects with negligible size). The motion of a point particle is determined by a small number of parameters: its position, mass, and the forces applied to it. \nIn reality, the kind of objects that classical mechanics can describe always have a non-zero size. (The behavior of very small particles, such as the electron, is more accurately described by quantum mechanics.) Objects with non-zero size have more complicated behavior than hypothetical point particles, because of the additional degrees of freedom, e.g., a baseball can spin while it is moving. However, the results for point particles can be used to study such objects by treating them as composite objects, made of a large number of collectively acting point particles. The center of mass of a composite object behaves like a point particle.\nClassical mechanics assumes that matter and energy have definite, knowable attributes such as location in space and speed. Non-relativistic mechanics also assumes that forces act instantaneously (see also Action at a distance).\n\n\n=== Position and its derivatives ===\n\nThe position of a point particle is defined in relation to a coordinate system centered on an arbitrary fixed reference point in space called the origin O. A simple coordinate system might describe the position of a particle P with a vector notated by an arrow labeled r that points from the origin O to point P. In general, the point particle does not need to be stationary relative to O. In cases where P is moving relative to O, r is defined as a function of t, time. In pre-Einstein relativity (known as Galilean relativity), time is considered an absolute, i.e., the time interval that is observed to elapse between any given pair of events is the same for all observers. In addition to relying on absolute time, classical mechanics assumes Euclidean geometry for the structure of space.\n\n\n==== Velocity and speed ====\n\nThe velocity, or the rate of change of displacement with time, is defined as the derivative of the position with respect to time:\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {v} ={\\mathrm {d} \\mathbf {r}  \\over \\mathrm {d} t}\\,\\!}\n  .In classical mechanics, velocities are directly additive and subtractive. For example, if one car travels east at 60 km/h and passes another car traveling in the same direction at 50 km/h, the slower car perceives the faster car as traveling east at 60 \u2212 50 = 10 km/h. However, from the perspective of the faster car, the slower car is moving 10 km/h to the west, often denoted as \u221210 km/h where the sign implies opposite direction. Velocities are directly additive as vector quantities; they must be dealt with using vector analysis.\nMathematically, if the velocity of the first object in the previous discussion is denoted by the vector u = ud and the velocity of the second object by the vector v = ve, where u is the speed of the first object, v is the speed of the second object, and d and e are unit vectors in the directions of motion of each object respectively, then the velocity of the first object as seen by the second object is:\n\n  \n    \n      \n        \n          \n            u\n          \n          \u2032\n        \n        =\n        \n          u\n        \n        \u2212\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=\\mathbf {u} -\\mathbf {v} \\,.}\n  Similarly, the first object sees the velocity of the second object as:\n\n  \n    \n      \n        \n          \n            v\n            \u2032\n          \n        \n        =\n        \n          v\n        \n        \u2212\n        \n          u\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v'} =\\mathbf {v} -\\mathbf {u} \\,.}\n  When both objects are moving in the same direction, this equation can be simplified to:\n\n  \n    \n      \n        \n          \n            u\n          \n          \u2032\n        \n        =\n        (\n        u\n        \u2212\n        v\n        )\n        \n          d\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=(u-v)\\mathbf {d} \\,.}\n  Or, by ignoring direction, the difference can be given in terms of speed only:\n\n  \n    \n      \n        \n          u\n          \u2032\n        \n        =\n        u\n        \u2212\n        v\n        \n        .\n      \n    \n    {\\displaystyle u'=u-v\\,.}\n  \n\n\n==== Acceleration ====\n\nThe acceleration, or rate of change of velocity, is the derivative of the velocity with respect to time (the second derivative of the position with respect to time):\n\n  \n    \n      \n        \n          a\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  d\n                  \n                    2\n                  \n                \n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {a} ={\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}={\\mathrm {d^{2}} \\mathbf {r}  \\over \\mathrm {d} t^{2}}.}\n  Acceleration represents the velocity's change over time.  Velocity can change in magnitude, direction, or both. Occasionally, a decrease in the magnitude of velocity \"v\" is referred to as deceleration, but generally any change in the velocity over time, including deceleration, is referred to as acceleration.\n\n\n==== Frames of reference ====\n\nWhile the position, velocity and acceleration of a particle can be described with respect to any observer in any state of motion, classical mechanics assumes the existence of a special family of reference frames in which the mechanical laws of nature take a comparatively simple form. These special reference frames are called inertial frames. An inertial frame is an idealized frame of reference within which an object with zero net force acting upon it moves with a constant velocity; that is, it is either at rest or moving uniformly in a straight line. In an inertial frame Newton's law of motion, \n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  , is valid.:\u200a185\u200aNon-inertial reference frames accelerate in relation to another inertial frame. A body rotating with respect to an inertial frame is not an inertial frame. When viewed from an inertial frame, particles in the non-inertial frame appear to move in ways not explained by forces from existing fields in the reference frame. Hence, it appears that there are other forces that enter the equations of motion solely as a result of the relative acceleration. These forces are referred to as fictitious forces, inertia forces, or pseudo-forces.\nConsider two reference frames S and S'. For observers in each of the reference frames an event has space-time coordinates of (x,y,z,t) in frame S and (x',y',z',t') in frame S'. Assuming time is measured the same in all reference frames, if we require x = x' when t = 0, then the relation between the space-time coordinates of the same event observed from the reference frames S' and S, which are moving at a relative velocity u in the x direction, is:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  \u2032\n                \n              \n              \n                \n                =\n                x\n                \u2212\n                t\n                u\n                ,\n              \n            \n            \n              \n                \n                  y\n                  \u2032\n                \n              \n              \n                \n                =\n                y\n                ,\n              \n            \n            \n              \n                \n                  z\n                  \u2032\n                \n              \n              \n                \n                =\n                z\n                ,\n              \n            \n            \n              \n                \n                  t\n                  \u2032\n                \n              \n              \n                \n                =\n                t\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x'&=x-tu,\\\\y'&=y,\\\\z'&=z,\\\\t'&=t.\\end{aligned}}}\n  This set of formulas defines a group transformation known as the Galilean transformation (informally, the Galilean transform). This group is a limiting case of the Poincar\u00e9 group used in special relativity. The limiting case applies when the velocity u is very small compared to c, the speed of light.\nThe transformations have the following consequences:\n\nv\u2032 = v \u2212 u (the velocity v\u2032 of a particle from the perspective of S\u2032 is slower by u than its velocity v from the perspective of S)\na\u2032 = a (the acceleration of a particle is the same in any inertial reference frame)\nF\u2032 = F (the force on a particle is the same in any inertial reference frame)\nthe  speed of light is not a constant in classical mechanics, nor does the special position given to the speed of light in relativistic mechanics have a counterpart in classical mechanics.For some problems, it is convenient to use rotating coordinates (reference frames). Thereby one can either keep a mapping to a convenient inertial frame, or introduce additionally a fictitious centrifugal force and Coriolis force.\n\n\n=== Forces and Newton's second law ===\n\nA force in physics is any action that causes an object's velocity to change; that is, to accelerate.  A force originates from within a field, such as an electro-static field (caused by static electrical charges), electro-magnetic field (caused by moving charges), or gravitational field (caused by mass), among others.\nNewton was the first to mathematically express the relationship between force and momentum. Some physicists interpret Newton's second law of motion as a definition of force and mass, while others consider it a fundamental postulate, a law of nature. Either interpretation has the same mathematical consequences, historically known as \"Newton's Second Law\":\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                d\n              \n              (\n              m\n              \n                v\n              \n              )\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\mathrm {d} \\mathbf {p}  \\over \\mathrm {d} t}={\\mathrm {d} (m\\mathbf {v} ) \\over \\mathrm {d} t}.}\n  The quantity mv is called the (canonical) momentum. The net force on a particle is thus equal to the rate of change of the momentum of the particle with time. Since the definition of acceleration is a = dv/dt, the second law can be written in the simplified and more familiar form:\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          a\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m\\mathbf {a} \\,.}\n  So long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.\nAs an example, assume that friction is the only force acting on the particle, and that it may be modeled as a function of the velocity of the particle, for example:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              R\n            \n          \n        \n        =\n        \u2212\n        \u03bb\n        \n          v\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\rm {R}}=-\\lambda \\mathbf {v} \\,,}\n  where \u03bb is a positive constant, the negative sign states that the force is opposite the sense of the velocity. Then the equation of motion is\n\n  \n    \n      \n        \u2212\n        \u03bb\n        \n          v\n        \n        =\n        m\n        \n          a\n        \n        =\n        m\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle -\\lambda \\mathbf {v} =m\\mathbf {a} =m{\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}\\,.}\n  This can be integrated to obtain\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            v\n          \n          \n            0\n          \n        \n        \n          e\n          \n            \n              \u2212\n              \u03bb\n              t\n            \n            \n              /\n            \n            \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} =\\mathbf {v} _{0}e^{{-\\lambda t}/{m}}}\n  where v0 is the initial velocity. This means that the velocity of this particle decays exponentially to zero as time progresses. In this case, an equivalent viewpoint is that the kinetic energy of the particle is absorbed by friction (which converts it to heat energy in accordance with the conservation of energy), and the particle is slowing down. This expression can be further integrated to obtain the position r of the particle as a function of time.\nImportant forces include the gravitational force and the Lorentz force for electromagnetism. In addition, Newton's third law can sometimes be used to deduce the forces acting on a particle: if it is known that particle A exerts a force F on another particle B, it follows that B must exert an equal and opposite reaction force, \u2212F, on A. The strong form of Newton's third law requires that F and \u2212F act along the line connecting A and B, while the weak form does not. Illustrations of the weak form of Newton's third law are often found for magnetic forces.\n\n\n=== Work and energy ===\n\nIf a constant force F is applied to a particle that makes a displacement \u0394r, the work done by the force is defined as the scalar product of the force and displacement vectors:\n\n  \n    \n      \n        W\n        =\n        \n          F\n        \n        \u22c5\n        \u0394\n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\mathbf {F} \\cdot \\Delta \\mathbf {r} \\,.}\n  More generally, if the force varies as a function of position as the particle moves from r1 to r2 along a path C, the work done on the particle is given by the line integral\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            C\n          \n        \n        \n          F\n        \n        (\n        \n          r\n        \n        )\n        \u22c5\n        \n          d\n        \n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} (\\mathbf {r} )\\cdot \\mathrm {d} \\mathbf {r} \\,.}\n  If the work done in moving the particle from r1 to r2 is the same no matter what path is taken, the force is said to be conservative. Gravity is a conservative force, as is the force due to an idealized spring, as given by Hooke's law. The force due to friction is non-conservative.\nThe kinetic energy Ek of a particle of mass m travelling at speed v is given by\n\n  \n    \n      \n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle E_{\\mathrm {k} }={\\tfrac {1}{2}}mv^{2}\\,.}\n  For extended objects composed of many particles, the kinetic energy of the composite body is the sum of the kinetic energies of the particles.\nThe work\u2013energy theorem states that for a particle of constant mass m, the total work W done on the particle as it moves from position r1 to r2 is equal to the change in kinetic energy Ek of the particle:\n\n  \n    \n      \n        W\n        =\n        \u0394\n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          E\n          \n            \n              \n                k\n                \n                  2\n                \n              \n            \n          \n        \n        \u2212\n        \n          E\n          \n            \n              \n                k\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          (\n          \n            \n              v\n              \n                2\n              \n              \n                \n                2\n              \n            \n            \u2212\n            \n              v\n              \n                1\n              \n              \n                \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle W=\\Delta E_{\\mathrm {k} }=E_{\\mathrm {k_{2}} }-E_{\\mathrm {k_{1}} }={\\tfrac {1}{2}}m\\left(v_{2}^{\\,2}-v_{1}^{\\,2}\\right).}\n  Conservative forces can be expressed as the gradient of a scalar function, known as the potential energy and denoted Ep:\n\n  \n    \n      \n        \n          F\n        \n        =\n        \u2212\n        \n          \u2207\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\,.}\n  If all the forces acting on a particle are conservative, and Ep is the total potential energy (which is defined as a work of involved forces to rearrange mutual positions of bodies), obtained by summing the potential energies corresponding to each force\n\n  \n    \n      \n        \n          F\n        \n        \u22c5\n        \u0394\n        \n          r\n        \n        =\n        \u2212\n        \n          \u2207\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        \u22c5\n        \u0394\n        \n          r\n        \n        =\n        \u2212\n        \u0394\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} \\cdot \\Delta \\mathbf {r} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\cdot \\Delta \\mathbf {r} =-\\Delta E_{\\mathrm {p} }\\,.}\n  The decrease in the potential energy is equal to the increase in the kinetic energy\n\n  \n    \n      \n        \u2212\n        \u0394\n        \n          E\n          \n            \n              p\n            \n          \n        \n        =\n        \u0394\n        \n          E\n          \n            \n              k\n            \n          \n        \n        \u21d2\n        \u0394\n        (\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        )\n        =\n        0\n        \n        .\n      \n    \n    {\\displaystyle -\\Delta E_{\\mathrm {p} }=\\Delta E_{\\mathrm {k} }\\Rightarrow \\Delta (E_{\\mathrm {k} }+E_{\\mathrm {p} })=0\\,.}\n  This result is known as conservation of energy and states that the total energy,\n\n  \n    \n      \n        \u2211\n        E\n        =\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\sum E=E_{\\mathrm {k} }+E_{\\mathrm {p} }\\,,}\n  is constant in time. It is often useful, because many commonly encountered forces are conservative.\n\n\n=== Beyond Newton's laws ===\nClassical mechanics also describes the more complex motions of extended non-pointlike objects. Euler's laws provide extensions to Newton's laws in this area. The concepts of angular momentum rely on the same calculus used to describe one-dimensional motion. The rocket equation extends the notion of rate of change of an object's momentum to include the effects of an object \"losing mass\".\n(These generalizations/extensions are derived from Newton's laws, say, by decomposing a solid body into a collection of points.)\nThere are two important alternative formulations of classical mechanics: Lagrangian mechanics and Hamiltonian mechanics. These, and other modern formulations, usually bypass the concept of \"force\", instead referring to other physical quantities, such as energy, speed and momentum, for describing mechanical systems in generalized coordinates. These are basically mathematical rewritings of Newton's laws, but complicated mechanical problems are much easier to solve in these forms. Also, analogy with quantum mechanics is more explicit in Hamiltonian formalism. \nThe expressions given above for momentum and kinetic energy are only valid when there is no significant electromagnetic contribution. In electromagnetism, Newton's second law for current-carrying wires breaks down unless one includes the electromagnetic field contribution to the momentum of the system as expressed by the Poynting vector divided by c2, where c is the speed of light in free space.\n\n\n== Limits of validity ==\nMany branches of classical mechanics are simplifications or approximations of more accurate forms; two of the most accurate being general relativity and relativistic statistical mechanics. Geometric optics is an approximation to the quantum theory of light, and does not have a superior \"classical\" form.\nWhen both quantum mechanics and classical mechanics cannot apply, such as at the quantum level with many degrees of freedom, quantum field theory (QFT) is of use. QFT deals with small distances, and large speeds with many degrees of freedom as well as the possibility of any change in the number of particles throughout the interaction. When treating large degrees of freedom at the macroscopic level, statistical mechanics becomes useful. Statistical mechanics describes the behavior of large (but countable) numbers of particles and their interactions as a whole at the macroscopic level. Statistical mechanics is mainly used in thermodynamics for systems that lie outside the bounds of the assumptions of classical thermodynamics. In the case of high velocity objects approaching the speed of light, classical mechanics is enhanced by special relativity. In case that objects become extremely heavy (i.e., their Schwarzschild radius is not negligibly small for a given application), deviations from Newtonian mechanics become apparent and can be quantified by using the parameterized post-Newtonian formalism. In that case, general relativity (GR) becomes applicable. However, until now there is no theory of quantum gravity unifying GR and QFT in the sense that it could be used when objects become extremely small and heavy.[4][5]\n\n\n=== The Newtonian approximation to special relativity ===\nIn special relativity, the momentum of a particle is given by\n\n  \n    \n      \n        \n          p\n        \n        =\n        \n          \n            \n              m\n              \n                v\n              \n            \n            \n              1\n              \u2212\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} ={\\frac {m\\mathbf {v} }{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}\\,,}\n  where m is the particle's rest mass, v its velocity, v is the modulus of v, and c is the speed of light.\nIf v is very small compared to c, v2/c2 is approximately zero, and so\n\n  \n    \n      \n        \n          p\n        \n        \u2248\n        m\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} \\approx m\\mathbf {v} \\,.}\n  Thus the Newtonian equation p = mv is an approximation of the relativistic equation for bodies moving with low speeds compared to the speed of light.\nFor example, the relativistic cyclotron frequency of a cyclotron, gyrotron, or high voltage magnetron is given by\n\n  \n    \n      \n        f\n        =\n        \n          f\n          \n            \n              c\n            \n          \n        \n        \n          \n            \n              m\n              \n                0\n              \n            \n            \n              \n                m\n                \n                  0\n                \n              \n              +\n              \n                \n                  T\n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle f=f_{\\mathrm {c} }{\\frac {m_{0}}{m_{0}+{\\frac {T}{c^{2}}}}}\\,,}\n  where fc is the classical frequency of an electron (or other charged particle) with kinetic energy T and (rest) mass m0 circling in a magnetic field. The (rest) mass of an electron is 511 keV. So the frequency correction is 1% for a magnetic vacuum tube with a 5.11 kV direct current accelerating voltage.\n\n\n=== The classical approximation to quantum mechanics ===\nThe ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is\n\n  \n    \n      \n        \u03bb\n        =\n        \n          \n            h\n            p\n          \n        \n      \n    \n    {\\displaystyle \\lambda ={\\frac {h}{p}}}\n  where h is Planck's constant and p is the momentum.\nAgain, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.\nMore practical examples of the failure of classical mechanics on an engineering scale are conduction by quantum tunneling in tunnel diodes and very narrow transistor gates in integrated circuits.\nClassical mechanics is the same extreme high frequency approximation as geometric optics. It is more often accurate because it describes particles and bodies with rest mass. These have more momentum and therefore shorter De Broglie wavelengths than massless particles, such as light, with the same kinetic energies.\n\n\n== History ==\n\nThe study of the motion of bodies is an ancient one, making classical mechanics one of the oldest and largest subjects in science, engineering, and technology.\nSome Greek philosophers of antiquity, among them Aristotle, founder of Aristotelian physics, may have been the first to maintain the idea that \"everything happens for a reason\" and that theoretical principles can assist in the understanding of nature. While to a modern reader, many of these preserved ideas come forth as eminently reasonable, there is a conspicuous lack of both mathematical theory and controlled experiment, as we know it. These later became decisive factors in forming modern science, and their early application came to be known as classical mechanics. In his Elementa super demonstrationem ponderum, medieval mathematician Jordanus de Nemore introduced the concept of \"positional gravity\" and the use of component forces.\n\nThe first published causal explanation of the motions of planets was Johannes Kepler's Astronomia nova, published in 1609. He concluded, based on Tycho Brahe's observations on the orbit of Mars, that the planet's orbits were ellipses. This break with ancient thought was happening around the same time that Galileo was proposing abstract mathematical laws for the motion of objects. He may (or may not) have performed the famous experiment of dropping two cannonballs of different weights from the tower of Pisa, showing that they both hit the ground at the same time. The reality of that particular experiment is disputed, but he did carry out quantitative experiments by rolling balls on an inclined plane. His theory of accelerated motion was derived from the results of such experiments and forms a cornerstone of classical mechanics. In 1673 Christiaan Huygens described in his Horologium Oscillatorium the first two laws of motion. The work is also the first modern treatise in which a physical problem (the accelerated motion of a falling body) is idealized by a set of parameters then analyzed mathematically and constitutes one of the seminal works of applied mathematics.\nNewton founded his principles of natural philosophy on three proposed laws of motion: the law of inertia, his second law of acceleration (mentioned above), and the law of action and reaction; and hence laid the foundations for classical mechanics. Both Newton's second and third laws were given the proper scientific and mathematical treatment in Newton's Philosophi\u00e6 Naturalis Principia Mathematica. Here they are distinguished from earlier attempts at explaining similar phenomena, which were either incomplete, incorrect, or given little accurate mathematical expression. Newton also enunciated the principles of conservation of momentum and angular momentum. In mechanics, Newton was also the first to provide the first correct scientific and mathematical formulation of gravity in Newton's law of universal gravitation. The combination of Newton's laws of motion and gravitation provides the fullest and most accurate description of classical mechanics. He demonstrated that these laws apply to everyday objects as well as to celestial objects. In particular, he obtained a theoretical explanation of Kepler's laws of motion of the planets.\nNewton had previously invented the calculus, of mathematics, and used it to perform the mathematical calculations. For acceptability, his book, the Principia, was formulated entirely in terms of the long-established geometric methods, which were soon eclipsed by his calculus. However, it was Leibniz who developed the notation of the derivative and integral preferred today. Newton, and most of his contemporaries, with the notable exception of Huygens, worked on the assumption that classical mechanics would be able to explain all phenomena, including light, in the form of geometric optics. Even when discovering the so-called Newton's rings (a wave interference phenomenon) he maintained his own corpuscular theory of light.\n\nAfter Newton, classical mechanics became a principal field of study in mathematics as well as physics. Mathematical formulations progressively allowed finding solutions to a far greater number of problems. The first notable mathematical treatment was in 1788 by Joseph Louis Lagrange. Lagrangian mechanics was in turn re-formulated in 1833 by William Rowan Hamilton.\n\nSome difficulties were discovered in the late 19th century that could only be resolved by more modern physics. Some of these difficulties related to compatibility with electromagnetic theory, and the famous Michelson\u2013Morley experiment. The resolution of these problems led to the special theory of relativity, often still considered a part of classical mechanics.\nA second set of difficulties were related to thermodynamics. When combined with thermodynamics, classical mechanics leads to the Gibbs paradox of classical statistical mechanics, in which entropy is not a well-defined quantity. Black-body radiation was not explained without the introduction of quanta. As experiments reached the atomic level, classical mechanics failed to explain, even approximately, such basic things as the energy levels and sizes of atoms and the photo-electric effect. The effort at resolving these problems led to the development of quantum mechanics.\nSince the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard Model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields. Also, it has been extended into the complex domain where complex classical mechanics exhibits behaviors very similar to quantum mechanics.\n\n\n== Branches ==\nClassical mechanics was traditionally divided into three main branches:\n\nStatics, the study of equilibrium and its relation to forces\nDynamics, the study of motion and its relation to forces\nKinematics, dealing with the implications of observed motions without regard for circumstances causing themAnother division is based on the choice of mathematical formalism:\n\nNewtonian mechanics\nLagrangian mechanics\nHamiltonian mechanicsAlternatively, a division can be made by region of application:\n\nCelestial mechanics, relating to stars, planets and other celestial bodies\nContinuum mechanics, for materials modelled as a continuum, e.g., solids and fluids (i.e., liquids and gases).\nRelativistic mechanics (i.e. including the special and general theories of relativity), for bodies whose speed is close to the speed of light.\nStatistical mechanics, which provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk thermodynamic properties of materials.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nAlonso, M.; Finn, J. (1992). Fundamental University Physics. Addison-Wesley.\nFeynman, Richard (1999). The Feynman Lectures on Physics. Perseus Publishing. ISBN 978-0-7382-0092-7.\nFeynman, Richard; Phillips, Richard (1998). Six Easy Pieces. Perseus Publishing. ISBN 978-0-201-32841-7.\nGoldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd ed.). Addison Wesley. ISBN 978-0-201-65702-9.\nKibble, Tom W.B.; Berkshire, Frank H. (2004). Classical Mechanics (5th ed.). Imperial College Press. ISBN 978-1-86094-424-6.\nKleppner, D.; Kolenkow, R.J. (1973). An Introduction to Mechanics. McGraw-Hill. ISBN 978-0-07-035048-9.\nLandau, L.D.; Lifshitz, E.M. (1972). Course of Theoretical Physics, Vol. 1 \u2013 Mechanics. Franklin Book Company. ISBN 978-0-08-016739-8.\nMorin, David (2008). Introduction to Classical Mechanics: With Problems and Solutions (1st ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-87622-3.\nGerald Jay Sussman; Jack Wisdom (2001). Structure and Interpretation of Classical Mechanics. MIT Press. ISBN 978-0-262-19455-6.\nO'Donnell, Peter J. (2015). Essential Dynamics and Relativity. CRC Press. ISBN 978-1-4665-8839-4.\nThornton, Stephen T.; Marion, Jerry B. (2003). Classical Dynamics of Particles and Systems (5th ed.). Brooks Cole. ISBN 978-0-534-40896-1.\n\n\n== External links ==\n\nCrowell, Benjamin. Light and Matter (an introductory text, uses algebra with optional sections involving calculus)\nFitzpatrick, Richard. Classical Mechanics (uses calculus)\nHoiland, Paul (2004). Preferred Frames of Reference & Relativity\nHorbatsch, Marko, \"Classical Mechanics Course Notes\".\nRosu, Haret C., \"Classical Mechanics\". Physics Education. 1999. [arxiv.org : physics/9909035]\nShapiro, Joel A. (2003). Classical Mechanics\nSussman, Gerald Jay & Wisdom, Jack &  Mayer, Meinhard E. (2001). Structure and Interpretation of Classical Mechanics\nTong, David. Classical Dynamics (Cambridge lecture notes on Lagrangian and Hamiltonian formalism)\nKinematic Models for Design Digital Library (KMODDL) Movies and photos of hundreds of working mechanical-systems models at Cornell University. Also includes an e-book library of classic texts on mechanical design and engineering.\nMIT OpenCourseWare 8.01: Classical Mechanics Free videos of actual course lectures with links to lecture notes, assignments and exams.\nAlejandro A. Torassa, On Classical Mechanics"}, {"id": 82, "title": "Quantum computing", "content": "A quantum computer is a computer that takes advantage of quantum mechanical phenomena.\nAt small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior, specifically quantum superposition and entanglement, using specialized hardware that supports the preparation and manipulation of quantum states.\nClassical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster (with respect to input size scaling) than any modern \"classical\" computer. In particular, a large-scale quantum computer could break widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications. Moreover, scalable quantum computers do not hold promise for many practical tasks, and for many important tasks quantum speedups are proven impossible.\nThe basic unit of information in quantum computing is the qubit, similar to the bit in traditional digital electronics. Unlike a classical bit, a qubit can exist in a superposition of its two \"basis\" states, which loosely means that it is in both states simultaneously. When measuring a qubit, the result is a probabilistic output of a classical bit, therefore making quantum computers nondeterministic in general. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\nPhysically engineering high-quality qubits has proven challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. Paradoxically, perfectly isolating qubits is also undesirable because quantum computations typically need to initialize qubits, perform controlled qubit interactions, and measure the resulting quantum states. Each of those operations introduces errors and suffers from noise, and such inaccuracies accumulate.\nNational governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. Two of the most promising technologies are superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single ion using electromagnetic fields).\nIn principle, a non-quantum (classical) computer can solve the same computational problems as a quantum computer, given enough time. Quantum advantage comes in the form of time complexity rather than computability, and quantum complexity theory shows that some quantum algorithms for carefully selected tasks require exponentially fewer computational steps than the best known non-quantum algorithms. Such tasks can in theory be solved on a large-scale quantum computer whereas classical computers would not finish computations in any reasonable amount of time. However, quantum speedup is not universal or even typical across computational tasks, since basic tasks such as sorting are proven to not allow any asymptotic quantum speedup. Claims of quantum supremacy have drawn significant attention to the discipline, but are demonstrated on contrived tasks, while near-term practical use cases remain limited. \nOptimism about quantum computing is fueled by a broad range of new theoretical hardware possibilities facilitated by quantum physics, but the improving understanding of quantum computing limitations counterbalances this optimism. In particular, quantum speedups have been traditionally estimated for noiseless quantum computers, whereas the impact of noise and the use of quantum error-correction can undermine low-polynomial speedups.\n\n\n== History ==\n\nFor many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory developed in the 1920s to explain the wave\u2013particle duality observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for the nuclear physics used in the Manhattan Project.As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge.\nIn 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.\nQuantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein\u2013Vazirani algorithm in 1993, and Simon's algorithm in 1994.\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.Peter Shor built on these results with his 1994 algorithms for breaking the widely used RSA and Diffie\u2013Hellman encryption protocols, which drew significant attention to the field of quantum computing.\nIn 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture.Over the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates.\nIn 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer. However, the validity of this claim is still being actively researched.The threshold theorem shows how increasing the number of qubits can mitigate errors, yet fully fault-tolerant quantum computing remains \"a rather distant dream\". According to some researchers, noisy intermediate-scale quantum (NISQ) machines may have specialized uses in the near future, but noise in quantum gates limits their reliability.Investment in quantum computing research has increased in the public and private sectors.\nAs one consulting firm summarized,\n... investment dollars are pouring in, and quantum-computing start-ups are proliferating. ... While quantum computing promises to help businesses solve problems that are beyond the reach and speed of conventional high-performance computers, use cases are largely experimental and hypothetical at this early stage.\nWith focus on business management\u2019s point of view, the potential applications of quantum computing into four major categories are cybersecurity, data analytics and artificial intelligence, optimization and simulation, and data management and searching.In December 2023, physicists, for the first time, report the entanglement of individual molecules, which may have significant applications in quantum computing.\n\n\n== Quantum information processing ==\n\nComputer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\nQuantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\nAs physicist Charlie Bennett describes the relationship between quantum and classical computers,\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\n\n\n=== Quantum information ===\nThe qubit serves as the basic unit of quantum information.\nIt represents a two-state system, just like a classical bit, except that it can exist in a superposition of its two states.\nIn one sense, a superposition is like a probability distribution over the two values.\nHowever, a quantum computation can be influenced by both values at once, inexplicable by either state individually.\nIn this sense, a \"superposed\" qubit stores both values simultaneously.A two-dimensional vector mathematically represents a qubit state. Physicists typically use Dirac notation for quantum mechanical linear algebra, writing |\u03c8\u27e9 'ket psi' for a vector labeled \u03c8. Because a qubit is a two-state system, any qubit state takes the form \u03b1|0\u27e9 + \u03b2|1\u27e9, where |0\u27e9 and |1\u27e9 are the standard basis states, and \u03b1 and \u03b2 are the probability amplitudes. If either \u03b1 or \u03b2 is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a quantum state vector acts similarly to a (classical) probability vector, with one key difference: unlike probabilities, probability amplitudes are not necessarily positive numbers. Negative amplitudes allow for destructive wave interference.When a qubit is measured in the standard basis, the result is a classical bit.\nThe Born rule describes the norm-squared correspondence between amplitudes and probabilities\u2014when measuring a qubit \u03b1|0\u27e9 + \u03b2|1\u27e9, the state collapses to |0\u27e9 with probability |\u03b1|2, or to |1\u27e9 with probability |\u03b2|2.\nAny valid qubit state has coefficients \u03b1 and \u03b2 such that |\u03b1|2 + |\u03b2|2 = 1.\nAs an example, measuring the qubit 1/\u221a2|0\u27e9 + 1/\u221a2|1\u27e9 would produce either |0\u27e9 or |1\u27e9 with equal probability.\nEach additional qubit doubles the dimension of the state space.\nAs an example, the vector 1/\u221a2|00\u27e9 + 1/\u221a2|01\u27e9 represents a two-qubit state, a tensor product of the qubit |0\u27e9 with the qubit 1/\u221a2|0\u27e9 + 1/\u221a2|1\u27e9.\nThis vector inhabits a four-dimensional vector space spanned by the basis vectors |00\u27e9, |01\u27e9, |10\u27e9, and |11\u27e9.\nThe Bell state 1/\u221a2|00\u27e9 + 1/\u221a2|11\u27e9 is impossible to decompose into the tensor product of two individual qubits\u2014the two qubits are entangled because their probability amplitudes are correlated.\nIn general, the vector space for an n-qubit system is 2n-dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2100 classical values.\n\n\n=== Unitary operators ===\n\nThe state of this one-qubit quantum memory can be manipulated by applying quantum logic gates, analogous to how classical memory can be manipulated with classical logic gates. One important gate for both classical and quantum computation is the NOT gate, which can be represented by a matrix\n\nMathematically, the application of such a logic gate to a quantum state vector is modelled with matrix multiplication. Thus\n\n  \n    \n      \n        X\n        \n          |\n        \n        0\n        \u27e9\n        =\n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle X|0\\rangle =|1\\rangle }\n   and \n  \n    \n      \n        X\n        \n          |\n        \n        1\n        \u27e9\n        =\n        \n          |\n        \n        0\n        \u27e9\n      \n    \n    {\\displaystyle X|1\\rangle =|0\\rangle }\n  .The mathematics of single qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are\n\nThe controlled NOT (CNOT) gate can then be represented using the following matrix:\n\nAs a mathematical consequence of this definition, \n  \n    \n      \n        CNOT\n        \u2061\n        \n          |\n        \n        00\n        \u27e9\n        =\n        \n          |\n        \n        00\n        \u27e9\n      \n    \n    {\\textstyle \\operatorname {CNOT} |00\\rangle =|00\\rangle }\n  , \n  \n    \n      \n        CNOT\n        \u2061\n        \n          |\n        \n        01\n        \u27e9\n        =\n        \n          |\n        \n        01\n        \u27e9\n      \n    \n    {\\textstyle \\operatorname {CNOT} |01\\rangle =|01\\rangle }\n  , \n  \n    \n      \n        CNOT\n        \u2061\n        \n          |\n        \n        10\n        \u27e9\n        =\n        \n          |\n        \n        11\n        \u27e9\n      \n    \n    {\\textstyle \\operatorname {CNOT} |10\\rangle =|11\\rangle }\n  , and \n  \n    \n      \n        CNOT\n        \u2061\n        \n          |\n        \n        11\n        \u27e9\n        =\n        \n          |\n        \n        10\n        \u27e9\n      \n    \n    {\\textstyle \\operatorname {CNOT} |11\\rangle =|10\\rangle }\n  . In other words, the CNOT applies a NOT gate (\n  \n    \n      \n        X\n      \n    \n    {\\textstyle X}\n   from before) to the second qubit if and only if the first qubit is in the state \n  \n    \n      \n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\textstyle |1\\rangle }\n  . If the first qubit is \n  \n    \n      \n        \n          |\n        \n        0\n        \u27e9\n      \n    \n    {\\textstyle |0\\rangle }\n  , nothing is done to either qubit.\nIn summary, quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\n\n\n=== Quantum parallelism ===\nQuantum parallelism refers to the ability of quantum computers to evaluate a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a superposition of input states, and applying a unitary transformation that encodes the function to be evaluated. The resulting state encodes the function's output values for all input values in the superposition, allowing for the computation of multiple outputs simultaneously. This property is key to the speedup of many quantum algorithms.\n\n\n=== Quantum programming ===\n\nThere are a number of models of computation for quantum computing, distinguished by the basic elements in which the computation is decomposed.\n\n\n==== Gate array ====\nA quantum gate array decomposes computation into a sequence of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\nAny quantum computation (which is, in the above formalism, any unitary matrix of size \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n        \u00d7\n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}\\times 2^{n}}\n   over \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as a universal gate set, since a computer that can run such circuits is a universal quantum computer. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to the Solovay-Kitaev theorem.\n\n\n==== Measurement-based quantum computing ====\nA measurement-based quantum computer decomposes computation into a sequence of Bell state measurements and single-qubit quantum gates applied to a highly entangled initial state (a cluster state), using a technique called quantum gate teleportation.\n\n\n==== Adiabatic quantum computing ====\nAn adiabatic quantum computer, based on quantum annealing, decomposes computation into a slow continuous transformation of an initial Hamiltonian into a final Hamiltonian, whose ground states contain the solution.\n\n\n==== Topological quantum computing ====\nA topological quantum computer decomposes computation into the braiding of anyons in a 2D lattice.\n\n\n==== Quantum Turing machine ====\nA quantum Turing machine is the quantum analog of a Turing machine. All of these models of computation\u2014quantum circuits, one-way quantum computation, adiabatic quantum computation, and topological quantum computation\u2014have been shown to be equivalent to the quantum Turing machine; given a perfect implementation of one such quantum computer, it can simulate all the others with no more than polynomial overhead. This equivalence need not hold for practical quantum computers, since the overhead of simulation may be too large to be practical.\n\n\n== Communication ==\n\nQuantum cryptography enables new ways to transmit data securely; for example, quantum key distribution uses entangled quantum states to establish secure cryptographic keys. When a sender and receiver exchange quantum states, they can guarantee that an adversary does not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change. With appropriate cryptographic protocols, the sender and receiver can thus establish shared private information resistant to eavesdropping.Modern fiber-optic cables can transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distance quantum networks with end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhanced quantum sensing.\n\n\n== Algorithms ==\nProgress in finding quantum algorithms typically focuses on this quantum circuit model, though exceptions like the quantum adiabatic algorithm exist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms.Quantum algorithms that offer more than a polynomial speedup over the best-known classical algorithm include Shor's algorithm for factoring and the related quantum algorithms for computing discrete logarithms, solving Pell's equation, and more generally solving the hidden subgroup problem for abelian finite groups. These algorithms depend on the primitive of the quantum Fourier transform. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, but evidence suggests that this is unlikely. Certain oracle problems like Simon's problem and the Bernstein\u2013Vazirani problem do give provable speedups, though this is in the quantum query model, which is a restricted model where lower bounds are much easier to prove and doesn't necessarily translate to speedups for practical problems.\nOther problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certain Jones polynomials, and the quantum algorithm for linear systems of equations have quantum algorithms appearing to give super-polynomial speedups and are BQP-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that no quantum algorithm gives a super-polynomial speedup, which is believed to be unlikely.Some quantum algorithms, like Grover's algorithm and amplitude amplification, give polynomial speedups over corresponding classical algorithms. Though these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems.\n\n\n=== Simulation of quantum systems ===\n\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, quantum simulation may be an important application of quantum computing. Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider. In June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer.About 2% of the annual global energy output is used for nitrogen fixation to produce ammonia for the Haber process in the agricultural fertilizer industry (even though naturally occurring organisms also produce ammonia). Quantum simulations might be used to understand this process and increase the energy efficiency of production. It is expected that an early use of quantum computing will be modeling that improves the efficiency of the Haber\u2013Bosch process by the mid 2020s although some have predicted it will take longer.\n\n\n=== Post-quantum cryptography ===\n\nA notable application of quantum computation is for attacks on cryptographic systems that are currently in use. Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes). By comparison, a quantum computer could solve this problem exponentially faster using Shor's algorithm to find its factors. This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular, the RSA, Diffie\u2013Hellman, and elliptic curve Diffie\u2013Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\nIdentifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field of post-quantum cryptography. Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like the McEliece cryptosystem based on a problem in coding theory. Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem. It has been proven that applying Grover's algorithm to break a symmetric (secret key) algorithm by brute force requires time equal to roughly 2n/2 invocations of the underlying cryptographic algorithm, compared with roughly 2n in the classical case, meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (see Key size).\n\n\n=== Search problems ===\n\nThe most well-known example of a problem that allows for a polynomial quantum speedup is unstructured search, which involves finding a marked item out of a list of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   items in a database. This can be solved by Grover's algorithm using \n  \n    \n      \n        O\n        (\n        \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {n}})}\n   queries to the database, quadratically fewer than the \n  \n    \n      \n        \u03a9\n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\Omega (n)}\n   queries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Many examples of provable quantum speedups for query problems are based on Grover's algorithm, including Brassard, H\u00f8yer, and Tapp's algorithm for finding collisions in two-to-one functions, and Farhi, Goldstone, and Gutmann's algorithm for evaluating NAND trees.Problems that can be efficiently addressed with Grover's algorithm have the following properties:\nThere is no searchable structure in the collection of possible answers,\nThe number of possible answers to check is the same as the number of inputs to the algorithm, and\nThere exists a boolean function that evaluates each input and determines whether it is the correct answer.For problems with all these properties, the running time of Grover's algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be applied is a Boolean satisfiability problem, where the database through which the algorithm iterates is that of all possible answers. An example and possible application of this is a password cracker that attempts to guess a password. Breaking symmetric ciphers with this algorithm is of interest to government agencies.\n\n\n=== Quantum annealing ===\nQuantum annealing relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which slowly evolves to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process. Adiabatic optimization may be helpful for solving computational biology problems.\n\n\n=== Machine learning ===\n\nSince quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed up machine learning tasks.For example, the quantum algorithm for linear systems of equations, or \"HHL Algorithm\", named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts. Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.\nDeep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems and thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative models including quantum GANs may eventually be developed into ultimate generative chemistry algorithms.\n\n\n== Engineering ==\nAs of 2023, classical computers outperform quantum computers for all real-world applications. While current quantum computers may speed up solutions to particular mathematical problems, they give no computational advantage for practical tasks. For many tasks there is no promise of useful quantum speedup, and some tasks provably prohibit any quantum speedup in the sense that any speedup is ruled out by proved theorems. Scientists and engineers are exploring multiple technologies for quantum computing hardware and hope to develop scalable quantum architectures, but serious obstacles remain.\n\n\n=== Challenges ===\nThere are a number of technical challenges in building a large-scale quantum computer. Physicist David DiVincenzo has listed these requirements for a practical quantum computer:\nPhysically scalable to increase the number of qubits\nQubits that can be initialized to arbitrary values\nQuantum gates that are faster than decoherence time\nUniversal gate set\nQubits that can be read easily.Sourcing parts for quantum computers is also very difficult. Superconducting quantum computers, like those constructed by Google and IBM, need helium-3, a nuclear research byproduct, and special superconducting cables made only by the Japanese company Coax Co.The control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development of quantum controllers that enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge.\n\n\n==== Decoherence ====\nOne of the greatest challenges involved with constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 (for NMR and MRI technology, also called the dephasing time), typically range between nanoseconds and seconds at low temperature. Currently, some quantum computers require their qubits to be cooled to 20 millikelvin (usually using a dilution refrigerator) in order to prevent significant decoherence. A 2020 study argues that ionizing radiation such as cosmic rays can nevertheless cause certain systems to decohere within milliseconds.As a result, time-consuming tasks may render some quantum algorithms inoperable, as attempting to maintain the state of qubits for a long enough duration will eventually corrupt the superpositions.These issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time, hence any operation must be completed much more quickly than the decoherence time.\nAs described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often-cited figure for the required error rate in each gate for fault-tolerant computation is 10\u22123, assuming the noise is depolarizing.\nMeeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between L and L2, where L is the number of digits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of L. For a 1000-bit number, this implies a need for about 104 bits without error correction. With error correction, the figure would rise to about 107 bits. Computation time is about L2 or about 107 steps and at 1 MHz, about 10 seconds. However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude. Careful estimates show that at least 3 million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer. In terms of the number of physical qubits, to date, this remains the lowest estimate for practically useful integer factorization problem sizing 1,024-bit or larger. \nAnother approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads, and relying on braid theory to form stable logic gates.\n\n\n=== Quantum supremacy ===\nPhysicist John Preskill coined the term quantum supremacy to describe the engineering feat of demonstrating that a programmable quantum device can solve a problem beyond the capabilities of state-of-the-art classical computers. The problem need not be useful, so some view the quantum supremacy test only as a potential future benchmark.In October 2019, Google AI Quantum, with the help of NASA, became the first to claim to have achieved quantum supremacy by performing calculations on the Sycamore quantum computer more than 3,000,000 times faster than they could be done on Summit, generally considered the world's fastest computer. This claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed, and researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputers and even beating it.In December 2020, a group at USTC implemented a type of Boson sampling on 76 photons with a photonic quantum computer, Jiuzhang, to demonstrate quantum supremacy. The authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds.Claims of quantum supremacy have generated hype around quantum computing, but they are based on contrived benchmark tasks that do not directly imply useful real-world applications.\n\n\n=== Skepticism ===\nDespite high hopes for quantum computing, significant progress in hardware, and optimism about future applications, a 2023 Nature spotlight article summarised current quantum computers as being \"For now, [good for] absolutely nothing\". The article elaborated that quantum computers are yet to be more useful or efficient than conventional computers in any case, though it also argued that in the long term such computers are likely to be useful. A 2023 Communications of the ACM article found that current quantum computing algorithms are \"insufficient for practical quantum advantage without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will not achieve quantum advantage with current quantum algorithms in the foreseeable future\", and it identified I/O constraints that make speedup unlikely for \"big data problems, unstructured linear systems, and database search based on Grover's algorithm\".\nThis state of affairs can be traced to several current and long-term considerations. \n\nConventional computer hardware and algorithms are not only optimized for practical tasks, but are still improving rapidly, particularly GPU accelerators.\nCurrent quantum computing hardware generates only a limited amount of entanglement before getting overwhelmed by noise and does not rule out practical simulation on conventional computers, possibly except for contrived cases.\nQuantum algorithms provide speedup over conventional algorithms only for some tasks, and matching these tasks with practical applications proved challenging. Some promising tasks and applications require resources far beyond those available today. In particular, processing large amounts of non-quantum data is a challenge for quantum computers.\nSome promising algorithms have been \"dequantized\", i.e., their non-quantum analogues with similar complexity have been found.\nIf quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine speedup offered by many quantum algorithms.\nComplexity analysis of algorithms sometimes makes abstract assumptions that do not hold in applications. For example, input data may not already be available encoded in quantum states, and \"oracle functions\" used in Grover's algorithm often have internal structure that can be exploited for faster algorithms.In particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain sufficiently high degree of entanglement for long time. When trying to outperform conventional computers, quantum computing researchers often look for new tasks that can be solved on quantum computers, but this leaves the possibility that efficient non-quantum techniques will be developed in response, as seen for Quantum supremacy demonstrations. Therefore, it is desirable to prove lower bounds on the complexity of best possible non-quantum algorithms (which may be unknown) and show that some quantum algorithms asymptomatically improve upon those bounds.\nSome researchers have expressed skepticism that scalable quantum computers could ever be built, typically because of the issue of maintaining coherence at large scales, but also for other reasons.\nBill Unruh doubted the practicality of quantum computers in a paper published in 1994. Paul Davies argued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle. Skeptics like Gil Kalai doubt that quantum supremacy will ever be achieved. Physicist Mikhail Dyakonov has expressed skepticism of quantum computing as follows:\n\n\"So the number of continuous parameters describing the state of such a useful quantum computer at any given moment must be... about 10300... Could we ever learn to control the more than 10300 continuously variable parameters defining the quantum state of such a system? My answer is simple. No, never.\"\n\n\n=== Candidates for physical realizations ===\n\nA practical quantum computer must use a physical system as a programmable quantum register. Researchers are exploring several technologies as candidates for reliable qubit implementations. Superconductors and trapped ions are some of the most developed proposals, but experimentalists are considering other hardware possibilities as well.\n\n\n== Theory ==\n\n\n=== Computability ===\n\nAny computational problem solvable by a classical computer is also solvable by a quantum computer. Intuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described using quantum mechanics, which underlies the operation of quantum computers.\nConversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer can be simulated by a Turing machine. In other words, quantum computers provide no additional power over classical computers in terms of computability. This means that quantum computers cannot solve undecidable problems like the halting problem, and the existence of quantum computers does not disprove the Church\u2013Turing thesis.\n\n\n=== Complexity ===\n\nWhile quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficiently factor integers, while this is not believed to be the case for classical computers.\nThe class of problems that can be efficiently solved by a quantum computer with bounded error is called BQP, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart to BPP (\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-time probabilistic Turing machines with bounded error. It is known that \n  \n    \n      \n        \n          \n            B\n            P\n            P\n            \u2286\n            B\n            Q\n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {BPP\\subseteq BQP}}}\n   and is widely suspected that \n  \n    \n      \n        \n          \n            B\n            Q\n            P\n            \u228a\n            B\n            P\n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}}\n  , which intuitively would mean that quantum computers are more powerful than classical computers in terms of time complexity.\nThe exact relationship of BQP to P, NP, and PSPACE is not known. However, it is known that \n  \n    \n      \n        \n          \n            P\n            \u2286\n            B\n            Q\n            P\n            \u2286\n            P\n            S\n            P\n            A\n            C\n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {P\\subseteq BQP\\subseteq PSPACE}}}\n  ; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning there are problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and the discrete logarithm problem are known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected that \n  \n    \n      \n        \n          \n            N\n            P\n            \u2288\n            B\n            Q\n            P\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {NP\\nsubseteq BQP}}}\n  ; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of NP-complete problems (if an NP-complete problem were in BQP, then it would follow from NP-hardness that all problems in NP are in BQP).\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Quantum computer at Wikimedia Commons\n Learning materials related to Quantum computing at Wikiversity\nStanford Encyclopedia of Philosophy: \"Quantum Computing\" by Amit Hagar and Michael E. Cuffaro.\n\"Quantum computation, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nQuantum computing for the very curious by Andy Matuschak and Michael NielsenLecturesQuantum computing for the determined \u2013 22 video lectures by Michael Nielsen\nVideo Lectures by David Deutsch\nLectures at the Institut Henri Poincar\u00e9 (slides and videos)\nOnline lecture on An Introduction to Quantum Computing, Edward Gerjuoy (2008)\nLomonaco, Sam. Four Lectures on Quantum Computing given at Oxford University in July 2006"}, {"id": 83, "title": "Peter Shore", "content": "Peter David Shore, Baron Shore of Stepney,  (20 May 1924 \u2013 24 September 2001) was a British Labour Party politician and former Cabinet Minister, noted in part for his opposition to the United Kingdom's entry into the European Economic Community.\nHis idiosyncratic left-wing nationalism led to comparison with the French politician Jean-Pierre Chev\u00e8nement. He was described in an obituary by the Conservative journalist Patrick Cosgrave as \"Between Harold Wilson and Tony Blair, the only possible Labour Party leader of whom a Conservative leader had cause to walk in fear\" and, along with Enoch Powell, \"the most captivating rhetorician of the age\".\n\n\n== Early life ==\nBorn in Great Yarmouth, Norfolk, Shore was the son of a Merchant Navy captain and was brought up in a middle-class environment. He attended Quarry Bank High School in Liverpool and, from there, went to King's College, Cambridge, to read History as an exhibitioner, where he was a member of the Cambridge Apostles, a secret society with an elite membership. During the later stages of World War II he served in the Royal Air Force, spending most of his time in India.\n\n\n== Political career ==\nHe had specialised in political economy during part of his degree and joined the Labour Party in 1948. He spent the 1950s working for the party and, after two unsuccessful Parliamentary contests at St Ives in 1950 and Halifax in 1959, he was appointed as Head of the Labour Party's Research Department and took charge of the renewal of party policy following its third successive defeat in 1959. Shore was only briefly a follower of Hugh Gaitskell; his adherence to the Campaign for Nuclear Disarmament from 1958 led to a breach in relations for several years.He became close to Harold Wilson after Wilson had been elected as party leader, and was the main author of the Labour Party manifesto for the 1964 general election. At the last minute, he was selected to fight the safe seat of Stepney in the election, which he easily won.After only a short spell on the backbenches, Wilson chose Shore to be Parliamentary Private Secretary, responsible for liaising between the Prime Minister and Labour MPs, though Denis Healey termed him \"Harold's lapdog\". Shore was responsible for drafting the 1966 and 1970 election manifestos. Shore's job as Wilson's PPS kept them in close contact and in August 1967, Shore became a member of the Cabinet as Secretary of State for Economic Affairs.\n\n\n=== In government ===\nThis Department had been created by Wilson to undertake long-term planning of the economy. Shore declared immediately his belief in state-controlled economic planning, together with the regulation of prices and wages. Early in 1968, the responsibility for prices and incomes was transferred to another department. The Treasury had never approved of the creation of the Department for Economic Affairs and began reasserting its influence, depriving it of any significant power. The department was wound up in October 1969. At the same time, Shore sided with those in cabinet who were opposed to Barbara Castle's White Paper, In Place of Strife. In a conversation with Richard Crossman at the time, Wilson was frustrated with Shore: \"I over-promoted him. He's no good\".Shore was retained in the Cabinet as a Minister without Portfolio and Deputy Leader of the House of Commons. He played a key part, behind the scenes, in planning the Labour Party's unsuccessful 1970 general election campaign. In opposition, Shore was appointed as spokesman on Europe, taking the lead in opposing Edward Heath's application to join the European Economic Community. Shore had already become convinced that membership of the EEC would be a disaster because it would stop the British government from taking necessary economic action. However, due to organisation by pro-EEC Labour backbenchers, Heath was able to steer his policy successfully through Parliament.\n\n\n=== EEC ===\nWhen Wilson returned to government in 1974, Shore was appointed as Secretary of State for Trade. His term in office was dominated by the renegotiation of the terms of British membership of the EEC, a pledge contained in the Labour manifesto as a preparation for a national referendum on membership; this compromise had reunited the Labour Party on the issue. Shore participated in the discussions without believing that any new terms would be acceptable, and during the referendum he joined with other anti-EEC politicians in opposing membership.\nThe results of the 1975 Referendum, giving a two-to-one majority in favour of remaining a member of the EEC, damaged Shore along with the other 'dissenting ministers'. His inclination to support an autarkic economy ruled him out of consideration as a new Chancellor of the Exchequer, but Shore was moved to Secretary of State for the Environment by new Prime Minister James Callaghan in 1976. This move was a promotion but involved him in considerable political controversy. He called on local authorities to cut spending and waste, and criticised the trade unions representing local authority staff for failure to support modernisation. Shore also launched a campaign to revitalise the inner cities of Britain.\n\n\n=== Nuclear deterrent ===\nShore became a fervent advocate of the British nuclear deterrent for the last three decades of his life, but in 1958 he had been an active member of CND. In his 1966 book Entitled to Know, he was critical of the Nassau Agreement with the United States under which Britain's nuclear submarines were, except in a national emergency, permanently assigned to NATO. Regarding dependence on NATO as limiting Britain's freedom of action, Shore negatively compared Britain's nuclear strategy to that of France:\n\nFor if such a policy is like General de Gaulle's, based upon a deliberate and far-reaching politico-military strategy of national independence, past disengagement from NATO and d\u00e9tente in Europe, it merits the most careful examination. But, of these broader aims, there was not a whisper or suggestion from Tory Ministers.\n...after the cancellation of Blue Streak...that, failing the development of a major new British weapons system, we hadn't, and could not in future possess, a genuine independent nuclear capacity.\nShore had always been implacably opposed to any suggestion of British participation in the Vietnam war, both as PPS and in Cabinet he had encouraged Wilson to distance himself more explicitly from American foreign policy. By the mid-1970s, while continuing to condemn American foreign policy in Vietnam and Chile, he had become more supportive of NATO and the United States.\n\n\n=== Labour leadership candidate ===\nWhen the Labour Party went into opposition in 1979, Shore was made Shadow Foreign Secretary, having recanted on his previous support for CND. He was persuaded to stand as a candidate in the election of a new party leader in November 1980 by Michael Foot who thought he was the best-placed soft-left candidate to defeat Denis Healey. However, Shore came bottom of the poll with 32 votes when Foot was himself persuaded to stand. Foot then made him Shadow Chancellor where his support for interventionist measures met with Foot's approval; party policy also became opposed to EEC membership, which suited Shore well. In the early 1980s, Shore's patriotic tendencies were again evident when he first of all strongly opposed the Conservative Government's attempts to hand over the Falkland Islands to Argentina, then supported Margaret Thatcher over the Falklands War of 1982.\n\n\n=== Shadow Cabinet ===\nHe fought for the leadership again after Foot resigned, but obtained a dismal vote of 3%, being unsupported by any Constituency Labour Party. Shore served as Shadow Leader of the House of Commons for four years under Neil Kinnock but his influence with the leadership was negligible and he was not re-elected to the Shadow cabinet in 1985. He stood down from the front bench in 1987 and thereafter served on the Foreign Affairs Select Committee, devoting himself to European Union questions. Edward Pearce wrote in his Guardian obituary of Shore that \"he had now become a right-wing figure, cluckingly approved of by Conservatives\".Tony Blair selected him as a senior Labour statesman as his nominee for the Committee on Standards in Public Life when it was set up in 1994.\n\n\n=== Backbenches, retirement and death ===\nAfter several attempts in his constituency party to deselect him, he finally stood down from the House of Commons at the 1997 general election, and in the dissolution honours he was made a life peer, being created Baron Shore of Stepney, of Stepney in the London Borough of Tower Hamlets on 5 June 1997.In contrast to Pearce's assertion that Shore had become a \"right-wing figure\", Chris Mullin quoted Shore in 1997 as saying: \"I still believe in state intervention, a good measure of equality, full employment.\" Mullin described Shore as alienated from New Labour, and quoted his criticism: \"I like Tony Blair. I think he is probably right about wanting to put a certain distance between the party and the unions, but I\u2019m offended by New Labour\u2019s constant repudiation of our past.\"His book Separate Ways (2000) advocated a multi-speed Europe, with some countries as merely associate members, so as to allow the centre to forge a political union at its own pace. He died in 2001, aged 77.\n\n\n== Family ==\nOn 27 September 1948, Shore married Dr Elizabeth Catherine Wrong, daughter of the historian Edward Murray Wrong. Known as Liz, she was the Deputy Chief Medical Officer of England from 1977 to 1985, and in this role and later positions she championed women's career progression in medicine. They had two daughters, Thomasina and Tacy, both retired teachers, and two sons, Crispin, who is Professor of Social Anthropology at Goldsmiths, University of London, and Piers, who died in 1977. Elizabeth Catherine Wrong Shore died in 2022.\n\n\n== References ==\n\n\n== Bibliography ==\nEntitled to Know, MacGibbon & Kee (1966) ISBN 978-0-2616-3132-8\nEurope: the way back, Fabian Society (1973)\nLeading the Left, Weidenfeld & Nicolson (1993) ISBN 978-0-2978-1096-4\nSeparate Ways, Duckworth (2000) ISBN 978-0-7156-2972-7\n\n\n== Archives ==\nCatalogue of the Shore papers at the Archives Division of the London School of Economics.\n\n\n== External links ==\n Quotations related to Peter Shore at Wikiquote\nHansard 1803\u20132005: contributions in Parliament by Peter Shore"}, {"id": 84, "title": "Energy", "content": "In physics, energy (from Ancient Greek  \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 (en\u00e9rgeia) 'activity') is the quantitative property that is transferred to a body or to a physical system, recognizable in the performance of work and in the form of heat and light. Energy is a conserved quantity\u2014the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The unit of measurement for energy in the International System of Units (SI) is the joule (J).\nCommon forms of energy include the kinetic energy of a moving object, the potential energy stored by an object (for instance due to its position in a field), the elastic energy stored in a solid object, chemical energy associated with chemical reactions, the radiant energy carried by electromagnetic radiation, and the internal energy contained within a thermodynamic system. All living organisms constantly take in and release energy.\nDue to mass\u2013energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. \nHuman civilization requires energy to function, which it gets from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The Earth's climate and ecosystems processes are driven by the energy the planet receives from the Sun (although a small amount is also contributed by geothermal energy).\n\n\n== Forms ==\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object \u2013 or the composite motion of the components of an object \u2013 and potential energy reflects the potential of an object to have motion, and generally is a function of the position of an object within a field or may be stored in the field itself.\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, the sum of translational and rotational kinetic and potential energy within a system is referred to as mechanical energy, whereas nuclear energy refers to the combined potentials within an atomic nucleus from either the nuclear force or the weak force, among other examples.\n\n\n== History ==\n\nThe word energy derives from the Ancient Greek: \u1f10\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1, romanized: energeia, lit.\u2009'activity, operation', which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\nIn the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the motions of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two. Writing in the early 18th century, \u00c9milie du Ch\u00e2telet proposed the concept of conservation of energy in the marginalia of her French language translation of Newton's Principia Mathematica, which represented the first formulation of a conserved measurable quantity that was distinct from momentum, and which would later be called \"energy\".\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jo\u017eef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\n\n\n== Units of measure ==\n\nIn 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the \"Joule apparatus\": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.\nIn the International System of Units (SI), the unit of energy is the joule, named after Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British thermal units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.\nThe SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.\n\n\n== Scientific use ==\n\n\n=== Classical mechanics ===\n\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\nWork, a function of energy, is force times distance.\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            C\n          \n        \n        \n          F\n        \n        \u22c5\n        \n          d\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} \\cdot \\mathrm {d} \\mathbf {s} }\n  This says that the work (\n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  ) is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\n\n=== Chemistry ===\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular, or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is usually accompanied by a decrease, and sometimes an increase, of the total energy of the substances involved. Some energy may be transferred between the surroundings and the reactants in the form of heat or light; thus the products of a reaction have sometimes more but usually less energy than the reactants. A reaction is said to be exothermic or exergonic if the final state is lower on the energy scale than the initial state; in the less common case of endothermic reactions the situation is the reverse. Chemical reactions are usually not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at a given temperature T) is related to the activation energy E by the Boltzmann's population factor e\u2212E/kT; that is, the probability of a molecule to have energy greater than or equal to E at a given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction can be provided in the form of thermal energy.\n\n\n=== Biology ===\n\nIn biology, energy is an attribute of all biological systems, from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or organelle of a biological organism. Energy used in respiration is stored in substances such as carbohydrates (including sugars), lipids, and proteins stored by cells. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, using as a standard an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 \u00f7 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.Sunlight's radiant energy is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into carbohydrates, lipids, proteins and oxygen. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark in a forest fire, or it may be made available more slowly for animal or human metabolism when organic molecules are ingested and catabolism is triggered by enzyme action.\nAll living creatures rely on an external source of energy to be able to grow and reproduce \u2013 radiant energy from the Sun in the case of green plants and chemical energy (in some form) in the case of animals. The daily 1500\u20132000 Calories (6\u20138 MJ) recommended for a human adult are taken as food molecules, mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidized to carbon dioxide and water in the mitochondria\n\nand some of the energy is used to convert ADP into ATP:\n\nThe rest of the chemical energy of the carbohydrate or fat are converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:\ngain in kinetic energy of a sprinter during a 100 m race: 4 kJ\ngain in gravitational potential energy of a 150 kg weight lifted through 2 metres: 3 kJ\nDaily food intake of a normal adult: 6\u20138 MJIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy); most machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology. As an example, to take just the first step in the food chain: of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\n\n\n=== Earth sciences ===\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations in our atmosphere brought about by solar energy.\nSunlight is the main input to Earth's energy budget which accounts for its temperature and climate stability. Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example when) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives most weather phenomena, save a few exceptions, like those generated by volcanic events for example. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, suddenly give up some of their thermal energy to power a few days of violent air movement.\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may later be transformed into active kinetic energy during landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars (which created these atoms).\n\n\n=== Cosmology ===\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\n\n\n=== Quantum mechanics ===\n\nIn quantum mechanics, energy is defined in terms of the energy operator\n(Hamiltonian) as a time derivative of the wave function. The Schr\u00f6dinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schr\u00f6dinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schr\u00f6dinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: \n  \n    \n      \n        E\n        =\n        h\n        \u03bd\n      \n    \n    {\\displaystyle E=h\\nu }\n   (where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is the Planck constant and \n  \n    \n      \n        \u03bd\n      \n    \n    {\\displaystyle \\nu }\n   the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.\n\n\n=== Relativity ===\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically \u2013 using Lorentz transformations instead of Newtonian mechanics \u2013 Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\n\nwhere\n\nm0 is the rest mass of the body,\nc is the speed of light in vacuum,\n\n  \n    \n      \n        \n          E\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle E_{0}}\n   is the rest energy.For example, consider electron\u2013positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process \u2013 the inverse process is called pair creation \u2013 in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.\nIn general relativity, the stress\u2013energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.Energy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy\u2013momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of spacetime (= boosts).\n\n\n== Transformation ==\n\nEnergy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery (from chemical energy to electric energy), a dam (from gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator), and a heat engine (from heat to work).\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. The Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that itself (since it still contains the same total energy even in different forms) but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\nEnergy transformations in the universe over time are characterized by various kinds of potential energy, that has been available since the Big Bang, being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae to \"store\" energy in the creation of heavy isotopes (such as uranium and thorium), and nuclear decay, a process in which energy is released that was originally stored in these heavy elements, before they were incorporated into the Solar System and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic and thermal energy in a very short time.\nYet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at its maximum. At its lowest point the kinetic energy is at its maximum and is equal to the decrease in potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.\nEnergy is also transferred from potential energy (\n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle E_{p}}\n  ) to kinetic energy (\n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle E_{k}}\n  ) and then back to potential energy constantly. This is referred to as conservation of energy. In this isolated system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n\nThe equation can then be simplified further since \n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n        =\n        m\n        g\n        h\n      \n    \n    {\\displaystyle E_{p}=mgh}\n   (mass times acceleration due to gravity times the height) and \n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n      \n    \n    {\\textstyle E_{k}={\\frac {1}{2}}mv^{2}}\n   (half mass times velocity squared). Then the total amount of energy can be found by adding \n  \n    \n      \n        \n          E\n          \n            p\n          \n        \n        +\n        \n          E\n          \n            k\n          \n        \n        =\n        \n          E\n          \n            total\n          \n        \n      \n    \n    {\\displaystyle E_{p}+E_{k}=E_{\\text{total}}}\n  .\n\n\n=== Conservation of energy and mass in transformation ===\nEnergy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass\u2013energy equivalence. The formula E = mc\u00b2, derived by Albert Einstein (1905) quantifies the relationship between relativistic mass and energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J.J. Thomson (1881), Henri Poincar\u00e9 (1900), Friedrich Hasen\u00f6hrl (1904) and others (see Mass\u2013energy equivalence#History for further information).\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle c^{2}}\n   is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass (for example, 1 kg) from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy (~\n  \n    \n      \n        9\n        \u00d7\n        \n          10\n          \n            16\n          \n        \n      \n    \n    {\\displaystyle 9\\times 10^{16}}\n   joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of an everyday amount energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure on a weighing scale, unless the energy loss is very large. Examples of large transformations between rest energy (of matter) and other forms of energy (e.g., kinetic energy into particles with rest mass) are found in nuclear physics and particle physics. Often, however, the complete conversion of matter (such as atoms) to non-matter (such as photons) is forbidden by conservation laws.\n\n\n=== Reversible and non-reversible transformations ===\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as thermal energy and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomization in a crystal).\nAs the universe evolves with time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or as other kinds of increases in disorder). This has led to the hypothesis of the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), continues to decrease.\n\n\n== Conservation of energy ==\n\nThe fact that energy can be neither created nor destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out as work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\nRichard Feynman said during a 1961 lecture:\nThere is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law \u2013 it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle \u2013 it is impossible to define the exact amount of energy during any definite time interval (though this is practically significant only for very short time intervals). The uncertainty principle should not be confused with energy conservation \u2013 rather it provides mathematical limits to which energy can in principle be defined and measured.\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appear as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by\n\n  \n    \n      \n        \u0394\n        E\n        \u0394\n        t\n        \u2265\n        \n          \n            \u210f\n            2\n          \n        \n      \n    \n    {\\displaystyle \\Delta E\\Delta t\\geq {\\frac {\\hbar }{2}}}\n  which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics).\nIn particle physics, this inequality permits a qualitative understanding of virtual particles, which carry momentum. The exchange of virtual particles with real particles is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons are also responsible for the electrostatic interaction between electric charges (which results in Coulomb's law), for spontaneous radiative decay of excited atomic and nuclear states, for the Casimir force, for the Van der Waals force and some other observable phenomena.\n\n\n== Energy transfer ==\n\n\n=== Closed systems ===\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, tidal interactions, and the conductive transfer of thermal energy.\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n\nwhere \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   is the amount of energy transferred, \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n    represents the work done on or by the system, and \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n   represents the heat flow into or out of the system. As a simplification, the heat term, \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  , can sometimes be ignored, especially for fast processes involving gases, which are poor conductors of heat, or when the thermal efficiency of the transfer is high. For such adiabatic processes,\n\nThis simplified equation is the one used to define the joule, for example.\n\n\n=== Open systems ===\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (this process is illustrated by injection of an air-fuel mixture into a car engine, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by \n  \n    \n      \n        \n          E\n          \n            matter\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{matter}}}\n  , one may write\n\n\n== Thermodynamics ==\n\n\n=== Internal energy ===\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\n\n\n=== First law of thermodynamics ===\nThe first law of thermodynamics asserts that the total energy of a system and its surroundings (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as\n\n  \n    \n      \n        \n          d\n        \n        E\n        =\n        T\n        \n          d\n        \n        S\n        \u2212\n        P\n        \n          d\n        \n        V\n        \n      \n    \n    {\\displaystyle \\mathrm {d} E=T\\mathrm {d} S-P\\mathrm {d} V\\,}\n  ,where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and its change dS is positive when heat is added to the system), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system).\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and PV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by\n\n  \n    \n      \n        \n          d\n        \n        E\n        =\n        \u03b4\n        Q\n        +\n        \u03b4\n        W\n      \n    \n    {\\displaystyle \\mathrm {d} E=\\delta Q+\\delta W}\n  where \n  \n    \n      \n        \u03b4\n        Q\n      \n    \n    {\\displaystyle \\delta Q}\n   is the heat supplied to the system and \n  \n    \n      \n        \u03b4\n        W\n      \n    \n    {\\displaystyle \\delta W}\n   is the work applied to the system.\n\n\n=== Equipartition of energy ===\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternately kinetic and potential energy. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over a whole cycle, or over many cycles, average energy is equally split between kinetic and potential. This is an example of the equipartition principle: the total energy of a system with many degrees of freedom is equally split among all available degrees of freedom, on average.\nThis principle is vitally important to understanding the behavior of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is part of the second law of thermodynamics. The second law of thermodynamics is simple only for systems which are near or in a physical equilibrium state. For non-equilibrium systems, the laws governing the systems' behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way as to maximize their entropy production.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Journals ===\nThe Journal of Energy History / Revue d'histoire de l'\u00e9nergie (JEHRHE), 2018\u2013 \n\n\n== External links ==\n\nEnergy at Curlie\nDifferences between Heat and Thermal energy (Archived 2016-08-27 at the Wayback Machine) \u2013 BioCab"}, {"id": 85, "title": "Grover\u2019s algorithm", "content": "In quantum computing, Grover's algorithm, also known as the quantum search algorithm, is a quantum algorithm for unstructured search that finds with high probability the unique input to a black box function that produces a particular output value, using just \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   evaluations of the function, where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the size of the function's domain. It was devised by Lov Grover in 1996.The analogous problem in classical computation cannot be solved in fewer than \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n   evaluations (because, on average, one has to check half of the domain to get a 50% chance of finding the right input). Charles H. Bennett, Ethan Bernstein, Gilles Brassard, and Umesh Vazirani proved that any quantum solution to the problem needs to evaluate the function \n  \n    \n      \n        \u03a9\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Omega ({\\sqrt {N}})}\n   times, so Grover's algorithm is asymptotically optimal. Since classical algorithms for NP-complete problems require exponentially many steps, and Grover's algorithm provides at most a quadratic speedup over the classical solution for unstructured search, this suggests that Grover's algorithm by itself will not provide polynomial-time solutions for NP-complete problems (as the square root of an exponential function is an exponential, not polynomial, function).Unlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is large, and Grover's algorithm can be applied to speed up broad classes of algorithms. Grover's algorithm could brute-force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. It may not be the case that Grover's algorithm poses a significantly increased risk to encryption over existing classical algorithms, however.\n\n\n== Applications and limitations ==\nGrover's algorithm, along with variants like amplitude amplification, can be used to speed up a broad range of algorithms. In particular, algorithms for NP-complete problems generally contain exhaustive search as a subroutine, which can be sped up by Grover's algorithm. The current best algorithm for 3SAT is one such example. Generic constraint satisfaction problems also see quadratic speedups with Grover. These algorithms do not require that the input be given in the form of an oracle, since Grover's algorithm is being applied with an explicit function, e.g. the function checking that a set of bits satisfies a 3SAT instance.\nGrover's algorithm can also give provable speedups for black-box problems in quantum query complexity, including element distinctness and the collision problem (solved with the Brassard\u2013H\u00f8yer\u2013Tapp algorithm). In these types of problems, one treats the oracle function f as a database, and the goal is to use the quantum query to this function as few times as possible.\n\n\n=== Cryptography ===\nGrover's algorithm essentially solves the task of function inversion. Roughly speaking, if we have a function \n  \n    \n      \n        y\n        =\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle y=f(x)}\n   that can be evaluated on a quantum computer, Grover's algorithm allows us to calculate \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   when given \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  . Consequently, Grover's algorithm gives broad asymptotic speed-ups to many kinds of brute-force attacks on symmetric-key cryptography, including collision attacks and pre-image attacks. However, this may not necessarily be the most efficient algorithm since, for example, the parallel rho algorithm is able to find a collision in SHA2 more efficiently than Grover's algorithm.\n\n\n=== Limitations ===\nGrover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input. However, this database is not represented explicitly. Instead, an oracle is invoked to evaluate an item by its index. Reading a full database item by item and converting it into such a representation may take a lot longer than Grover's search. To account for such effects, Grover's algorithm can be viewed as solving an equation or satisfying a constraint. In such applications, the oracle is a way to check the constraint and is not related to the search algorithm. This separation usually prevents algorithmic optimizations, whereas conventional search algorithms often rely on such optimizations and avoid exhaustive search. Fortunately, fast Grover's oracle implementation is possible for many constraint satisfaction and optimization problems.The major barrier to instantiating a speedup from Grover's algorithm is that the quadratic speedup achieved is too modest to overcome the large overhead of near-term quantum computers. However, later generations of fault-tolerant quantum computers with better hardware performance may be able to realize these speedups for practical instances of data.\n\n\n== Problem description ==\nAs input for Grover's algorithm, suppose we have a function \n  \n    \n      \n        f\n        :\n        {\n        0\n        ,\n        1\n        ,\n        \u2026\n        ,\n        N\n        \u2212\n        1\n        }\n        \u2192\n        {\n        0\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle f\\colon \\{0,1,\\ldots ,N-1\\}\\to \\{0,1\\}}\n  . In the \"unstructured database\" analogy, the domain represent indices to a database, and f(x) = 1 if and only if the data that x points to satisfies the search criterion. We additionally assume that only one index satisfies f(x) = 1, and we call this index \u03c9. Our goal is to identify \u03c9.\nWe can access f with a subroutine (sometimes called an oracle) in the form of a unitary operator U\u03c9 that acts as follows:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    U\n                    \n                      \u03c9\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  =\n                  \u2212\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  =\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  1\n                  ,\n                \n              \n              \n                \n                  \n                    U\n                    \n                      \u03c9\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  \u2260\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  0.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}U_{\\omega }|x\\rangle =-|x\\rangle &{\\text{for }}x=\\omega {\\text{, that is, }}f(x)=1,\\\\U_{\\omega }|x\\rangle =|x\\rangle &{\\text{for }}x\\neq \\omega {\\text{, that is, }}f(x)=0.\\end{cases}}}\n  This uses the \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  -dimensional state space \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  , which is supplied by a register with \n  \n    \n      \n        n\n        =\n        \u2308\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        N\n        \u2309\n      \n    \n    {\\displaystyle n=\\lceil \\log _{2}N\\rceil }\n   qubits.\nThis is often written as\n\n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        =\n        (\n        \u2212\n        1\n        \n          )\n          \n            f\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        .\n      \n    \n    {\\displaystyle U_{\\omega }|x\\rangle =(-1)^{f(x)}|x\\rangle .}\n  Grover's algorithm outputs \u03c9 with probability at least 1/2 using \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   applications of U\u03c9. This probability can be made arbitrarily large by running Grover's algorithm multiple times. If one runs Grover's algorithm until \u03c9 is found, the expected number of applications is still \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n  , since it will only be run twice on average.\n\n\n=== Alternative oracle definition ===\nThis section compares the above oracle \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   with an oracle \n  \n    \n      \n        \n          U\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle U_{f}}\n  .\nU\u03c9 is different from the standard quantum oracle for a function f. This standard oracle, denoted here as Uf, uses an ancillary qubit system. The operation then represents an inversion (NOT gate) on the main system conditioned by the value of f(x) from the ancillary system:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    U\n                    \n                      f\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  \u00ac\n                  y\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  =\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  1\n                  ,\n                \n              \n              \n                \n                  \n                    U\n                    \n                      f\n                    \n                  \n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                  =\n                  \n                    |\n                  \n                  x\n                  \u27e9\n                  \n                    |\n                  \n                  y\n                  \u27e9\n                \n                \n                  \n                    for \n                  \n                  x\n                  \u2260\n                  \u03c9\n                  \n                    , that is, \n                  \n                  f\n                  (\n                  x\n                  )\n                  =\n                  0\n                  ,\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}U_{f}|x\\rangle |y\\rangle =|x\\rangle |\\neg y\\rangle &{\\text{for }}x=\\omega {\\text{, that is, }}f(x)=1,\\\\U_{f}|x\\rangle |y\\rangle =|x\\rangle |y\\rangle &{\\text{for }}x\\neq \\omega {\\text{, that is, }}f(x)=0,\\end{cases}}}\n  or briefly,\n\n  \n    \n      \n        \n          U\n          \n            f\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u27e9\n        =\n        \n          |\n        \n        x\n        \u27e9\n        \n          |\n        \n        y\n        \u2295\n        f\n        (\n        x\n        )\n        \u27e9\n        .\n      \n    \n    {\\displaystyle U_{f}|x\\rangle |y\\rangle =|x\\rangle |y\\oplus f(x)\\rangle .}\n  These oracles are typically realized using uncomputation.\nIf we are given Uf as our oracle, then we can also implement U\u03c9, since U\u03c9 is Uf when the ancillary qubit is in the state \n  \n    \n      \n        \n          |\n        \n        \u2212\n        \u27e9\n        =\n        \n          \n            1\n            \n              2\n            \n          \n        \n        \n          \n            (\n          \n        \n        \n          |\n        \n        0\n        \u27e9\n        \u2212\n        \n          |\n        \n        1\n        \u27e9\n        \n          \n            )\n          \n        \n        =\n        H\n        \n          |\n        \n        1\n        \u27e9\n      \n    \n    {\\displaystyle |-\\rangle ={\\frac {1}{\\sqrt {2}}}{\\big (}|0\\rangle -|1\\rangle {\\big )}=H|1\\rangle }\n  :\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  U\n                  \n                    f\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  |\n                \n                x\n                \u27e9\n                \u2297\n                \n                  |\n                \n                \u2212\n                \u27e9\n                \n                  \n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      U\n                      \n                        f\n                      \n                    \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    0\n                    \u27e9\n                    \u2212\n                    \n                      U\n                      \n                        f\n                      \n                    \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    1\n                    \u27e9\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    1\n                    \n                      2\n                    \n                  \n                \n                \n                  (\n                  \n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    f\n                    (\n                    x\n                    )\n                    \u27e9\n                    \u2212\n                    \n                      |\n                    \n                    x\n                    \u27e9\n                    \n                      |\n                    \n                    1\n                    \u2295\n                    f\n                    (\n                    x\n                    )\n                    \u27e9\n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    {\n                    \n                      \n                        \n                          \n                            \n                              1\n                              \n                                2\n                              \n                            \n                          \n                          \n                            (\n                            \n                              \u2212\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              0\n                              \u27e9\n                              +\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              1\n                              \u27e9\n                            \n                            )\n                          \n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          =\n                          1\n                          ,\n                        \n                      \n                      \n                        \n                          \n                            \n                              1\n                              \n                                2\n                              \n                            \n                          \n                          \n                            (\n                            \n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              0\n                              \u27e9\n                              \u2212\n                              \n                                |\n                              \n                              x\n                              \u27e9\n                              \n                                |\n                              \n                              1\n                              \u27e9\n                            \n                            )\n                          \n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          =\n                          0\n                        \n                      \n                    \n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                (\n                \n                  U\n                  \n                    \u03c9\n                  \n                \n                \n                  |\n                \n                x\n                \u27e9\n                )\n                \u2297\n                \n                  |\n                \n                \u2212\n                \u27e9\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}U_{f}{\\big (}|x\\rangle \\otimes |-\\rangle {\\big )}&={\\frac {1}{\\sqrt {2}}}\\left(U_{f}|x\\rangle |0\\rangle -U_{f}|x\\rangle |1\\rangle \\right)\\\\&={\\frac {1}{\\sqrt {2}}}\\left(|x\\rangle |f(x)\\rangle -|x\\rangle |1\\oplus f(x)\\rangle \\right)\\\\&={\\begin{cases}{\\frac {1}{\\sqrt {2}}}\\left(-|x\\rangle |0\\rangle +|x\\rangle |1\\rangle \\right)&{\\text{if }}f(x)=1,\\\\{\\frac {1}{\\sqrt {2}}}\\left(|x\\rangle |0\\rangle -|x\\rangle |1\\rangle \\right)&{\\text{if }}f(x)=0\\end{cases}}\\\\&=(U_{\\omega }|x\\rangle )\\otimes |-\\rangle \\end{aligned}}}\n  So, Grover's algorithm can be run regardless of which oracle is given. If Uf is given, then we must maintain an additional qubit in the state \n  \n    \n      \n        \n          |\n        \n        \u2212\n        \u27e9\n      \n    \n    {\\displaystyle |-\\rangle }\n   and apply Uf in place of U\u03c9.\n\n\n== Algorithm ==\nThe steps of Grover's algorithm are given as follows:\n\nInitialize the system to the uniform superposition over all states\n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n        =\n        \n          \n            1\n            \n              N\n            \n          \n        \n        \n          \u2211\n          \n            x\n            =\n            0\n          \n          \n            N\n            \u2212\n            1\n          \n        \n        \n          |\n        \n        x\n        \u27e9\n        .\n      \n    \n    {\\displaystyle |s\\rangle ={\\frac {1}{\\sqrt {N}}}\\sum _{x=0}^{N-1}|x\\rangle .}\n  \nPerform the following \"Grover iteration\" \n  \n    \n      \n        r\n        (\n        N\n        )\n      \n    \n    {\\displaystyle r(N)}\n   times:\nApply the operator \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n  \nApply the Grover diffusion operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        =\n        2\n        \n          |\n          s\n          \u27e9\n        \n        \n          \u27e8\n          s\n          |\n        \n        \u2212\n        I\n      \n    \n    {\\displaystyle U_{s}=2\\left|s\\right\\rangle \\left\\langle s\\right|-I}\n  \nMeasure the resulting quantum state in the computational basis.For the correctly chosen value of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , the output will be \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   with probability approaching 1 for N \u226b 1. Analysis shows that this eventual value for \n  \n    \n      \n        r\n        (\n        N\n        )\n      \n    \n    {\\displaystyle r(N)}\n   satisfies \n  \n    \n      \n        r\n        (\n        N\n        )\n        \u2264\n        \n          \n            \u2308\n          \n        \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n          \n        \n        \n          \n            \u2309\n          \n        \n      \n    \n    {\\displaystyle r(N)\\leq {\\Big \\lceil }{\\frac {\\pi }{4}}{\\sqrt {N}}{\\Big \\rceil }}\n  .\nImplementing the steps for this algorithm can be done using a number of gates linear in the number of qubits. Thus, the gate complexity of this algorithm is \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        (\n        N\n        )\n        r\n        (\n        N\n        )\n        )\n      \n    \n    {\\displaystyle O(\\log(N)r(N))}\n  , or \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        (\n        N\n        )\n        )\n      \n    \n    {\\displaystyle O(\\log(N))}\n   per iteration.\n\n\n== Geometric proof of correctness ==\nThere is a geometric interpretation of Grover's algorithm, following from the observation that the quantum state of Grover's algorithm stays in a two-dimensional subspace after each step. Consider the plane spanned by \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  ; equivalently, the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   and the perpendicular ket \n  \n    \n      \n        \n          \n            |\n          \n          \n            s\n            \u2032\n          \n          \u27e9\n          =\n          \n            \n              1\n              \n                N\n                \u2212\n                1\n              \n            \n          \n          \n            \u2211\n            \n              x\n              \u2260\n              \u03c9\n            \n          \n          \n            |\n          \n          x\n          \u27e9\n        \n      \n    \n    {\\displaystyle \\textstyle |s'\\rangle ={\\frac {1}{\\sqrt {N-1}}}\\sum _{x\\neq \\omega }|x\\rangle }\n  .\nGrover's algorithm begins with the initial ket \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n  , which lies in the subspace. The operator \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   is a reflection at the hyperplane orthogonal to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   for vectors in the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  , i.e. it acts as a reflection across \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n  . This can be seen by writing \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   in the form of a Householder reflection:\n\n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        I\n        \u2212\n        2\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \u27e8\n        \u03c9\n        \n          |\n        \n        .\n      \n    \n    {\\displaystyle U_{\\omega }=I-2|\\omega \\rangle \\langle \\omega |.}\n  The operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        =\n        2\n        \n          |\n        \n        s\n        \u27e9\n        \u27e8\n        s\n        \n          |\n        \n        \u2212\n        I\n      \n    \n    {\\displaystyle U_{s}=2|s\\rangle \\langle s|-I}\n   is a reflection through \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n  . Both operators \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   and \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   take states in the plane spanned by \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n   to states in the plane. Therefore, Grover's algorithm stays in this plane for the entire algorithm.\nIt is straightforward to check that the operator \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n   of each Grover iteration step rotates the state vector by an angle of \n  \n    \n      \n        \u03b8\n        =\n        2\n        arcsin\n        \u2061\n        \n          \n            \n              1\n              \n                N\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\theta =2\\arcsin {\\tfrac {1}{\\sqrt {N}}}}\n  .\nSo, with enough iterations, one can rotate from the initial state \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   to the desired output state \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  . The initial ket is close to the state orthogonal to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  :\n\n  \n    \n      \n        \u27e8\n        \n          s\n          \u2032\n        \n        \n          |\n        \n        s\n        \u27e9\n        =\n        \n          \n            \n              \n                N\n                \u2212\n                1\n              \n              N\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\langle s'|s\\rangle ={\\sqrt {\\frac {N-1}{N}}}.}\n  In geometric terms, the angle \n  \n    \n      \n        \u03b8\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle \\theta /2}\n   between \n  \n    \n      \n        \n          |\n        \n        s\n        \u27e9\n      \n    \n    {\\displaystyle |s\\rangle }\n   and \n  \n    \n      \n        \n          |\n        \n        \n          s\n          \u2032\n        \n        \u27e9\n      \n    \n    {\\displaystyle |s'\\rangle }\n   is given by\n\n  \n    \n      \n        sin\n        \u2061\n        \n          \n            \u03b8\n            2\n          \n        \n        =\n        \n          \n            1\n            \n              N\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sin {\\frac {\\theta }{2}}={\\frac {1}{\\sqrt {N}}}.}\n  We need to stop when the state vector passes close to \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  ; after this, subsequent iterations rotate the state vector away from \n  \n    \n      \n        \n          |\n        \n        \u03c9\n        \u27e9\n      \n    \n    {\\displaystyle |\\omega \\rangle }\n  , reducing the probability of obtaining the correct answer. The exact probability of measuring the correct answer is\n\n  \n    \n      \n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        \n          (\n          \n            \n              \n                (\n              \n            \n            r\n            +\n            \n              \n                1\n                2\n              \n            \n            \n              \n                )\n              \n            \n            \u03b8\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\sin ^{2}\\left({\\Big (}r+{\\frac {1}{2}}{\\Big )}\\theta \\right),}\n  where r is the (integer) number of Grover iterations. The earliest time that we get a near-optimal measurement is therefore \n  \n    \n      \n        r\n        \u2248\n        \u03c0\n        \n          \n            N\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle r\\approx \\pi {\\sqrt {N}}/4}\n  .\n\n\n== Algebraic proof of correctness ==\nTo complete the algebraic analysis, we need to find out what happens when we repeatedly apply \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n  . A natural way to do this is by eigenvalue analysis of a matrix. Notice that during the entire computation, the state of the algorithm is a linear combination of \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and \n  \n    \n      \n        \u03c9\n      \n    \n    {\\displaystyle \\omega }\n  . We can write the action of \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   and \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   in the space spanned by \n  \n    \n      \n        {\n        \n          |\n        \n        s\n        \u27e9\n        ,\n        \n          |\n        \n        \u03c9\n        \u27e9\n        }\n      \n    \n    {\\displaystyle \\{|s\\rangle ,|\\omega \\rangle \\}}\n   as:\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        :\n        a\n        \n          |\n        \n        \u03c9\n        \u27e9\n        +\n        b\n        \n          |\n        \n        s\n        \u27e9\n        \u21a6\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  a\n                \n              \n              \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{s}:a|\\omega \\rangle +b|s\\rangle \\mapsto [|\\omega \\rangle \\,|s\\rangle ]{\\begin{bmatrix}-1&0\\\\2/{\\sqrt {N}}&1\\end{bmatrix}}{\\begin{bmatrix}a\\\\b\\end{bmatrix}}.}\n  \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n        :\n        a\n        \n          |\n        \n        \u03c9\n        \u27e9\n        +\n        b\n        \n          |\n        \n        s\n        \u27e9\n        \u21a6\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  a\n                \n              \n              \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{\\omega }:a|\\omega \\rangle +b|s\\rangle \\mapsto [|\\omega \\rangle \\,|s\\rangle ]{\\begin{bmatrix}-1&-2/{\\sqrt {N}}\\\\0&1\\end{bmatrix}}{\\begin{bmatrix}a\\\\b\\end{bmatrix}}.}\n  So in the basis \n  \n    \n      \n        {\n        \n          |\n        \n        \u03c9\n        \u27e9\n        ,\n        \n          |\n        \n        s\n        \u27e9\n        }\n      \n    \n    {\\displaystyle \\{|\\omega \\rangle ,|s\\rangle \\}}\n   (which is neither orthogonal nor a basis of the whole space) the action \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }}\n   of applying \n  \n    \n      \n        \n          U\n          \n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle U_{\\omega }}\n   followed by \n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle U_{s}}\n   is given by the matrix\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  1\n                \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n              \n              \n                \n                  \u2212\n                  2\n                  \n                    /\n                  \n                  \n                    \n                      N\n                    \n                  \n                \n                \n                  1\n                  \u2212\n                  4\n                  \n                    /\n                  \n                  N\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle U_{s}U_{\\omega }={\\begin{bmatrix}-1&0\\\\2/{\\sqrt {N}}&1\\end{bmatrix}}{\\begin{bmatrix}-1&-2/{\\sqrt {N}}\\\\0&1\\end{bmatrix}}={\\begin{bmatrix}1&2/{\\sqrt {N}}\\\\-2/{\\sqrt {N}}&1-4/N\\end{bmatrix}}.}\n  This matrix happens to have a very convenient Jordan form. If we define \n  \n    \n      \n        t\n        =\n        arcsin\n        \u2061\n        (\n        1\n        \n          /\n        \n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle t=\\arcsin(1/{\\sqrt {N}})}\n  , it is\n\n  \n    \n      \n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        =\n        M\n        \n          \n            [\n            \n              \n                \n                  \n                    e\n                    \n                      2\n                      i\n                      t\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      2\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle U_{s}U_{\\omega }=M{\\begin{bmatrix}e^{2it}&0\\\\0&e^{-2it}\\end{bmatrix}}M^{-1}}\n   where \n  \n    \n      \n        M\n        =\n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  i\n                \n                \n                  i\n                \n              \n              \n                \n                  \n                    e\n                    \n                      i\n                      t\n                    \n                  \n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle M={\\begin{bmatrix}-i&i\\\\e^{it}&e^{-it}\\end{bmatrix}}.}\n  It follows that r-th power of the matrix (corresponding to r iterations) is \n\n  \n    \n      \n        (\n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          )\n          \n            r\n          \n        \n        =\n        M\n        \n          \n            [\n            \n              \n                \n                  \n                    e\n                    \n                      2\n                      r\n                      i\n                      t\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    e\n                    \n                      \u2212\n                      2\n                      r\n                      i\n                      t\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle (U_{s}U_{\\omega })^{r}=M{\\begin{bmatrix}e^{2rit}&0\\\\0&e^{-2rit}\\end{bmatrix}}M^{-1}.}\n  Using this form, we can use trigonometric identities to compute the probability of observing \u03c9 after r iterations mentioned in the previous section, \n\n  \n    \n      \n        \n          \n            |\n            \n              \n                \n                  [\n                  \n                    \n                      \n                        \u27e8\n                        \u03c9\n                        \n                          |\n                        \n                        \u03c9\n                        \u27e9\n                      \n                      \n                        \u27e8\n                        \u03c9\n                        \n                          |\n                        \n                        s\n                        \u27e9\n                      \n                    \n                  \n                  ]\n                \n              \n              (\n              \n                U\n                \n                  s\n                \n              \n              \n                U\n                \n                  \u03c9\n                \n              \n              \n                )\n                \n                  r\n                \n              \n              \n                \n                  [\n                  \n                    \n                      \n                        0\n                      \n                    \n                    \n                      \n                        1\n                      \n                    \n                  \n                  ]\n                \n              \n            \n            |\n          \n          \n            2\n          \n        \n        =\n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        \n          (\n          \n            (\n            2\n            r\n            +\n            1\n            )\n            t\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\left|{\\begin{bmatrix}\\langle \\omega |\\omega \\rangle &\\langle \\omega |s\\rangle \\end{bmatrix}}(U_{s}U_{\\omega })^{r}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}\\right|^{2}=\\sin ^{2}\\left((2r+1)t\\right).}\n  Alternatively, one might reasonably imagine that a near-optimal time to distinguish would be when the angles 2rt and \u22122rt are as far apart as possible, which corresponds to \n  \n    \n      \n        2\n        r\n        t\n        \u2248\n        \u03c0\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 2rt\\approx \\pi /2}\n  , or \n  \n    \n      \n        r\n        =\n        \u03c0\n        \n          /\n        \n        4\n        t\n        =\n        \u03c0\n        \n          /\n        \n        4\n        arcsin\n        \u2061\n        (\n        1\n        \n          /\n        \n        \n          \n            N\n          \n        \n        )\n        \u2248\n        \u03c0\n        \n          \n            N\n          \n        \n        \n          /\n        \n        4\n      \n    \n    {\\displaystyle r=\\pi /4t=\\pi /4\\arcsin(1/{\\sqrt {N}})\\approx \\pi {\\sqrt {N}}/4}\n  . Then the system is in state\n\n  \n    \n      \n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        (\n        \n          U\n          \n            s\n          \n        \n        \n          U\n          \n            \u03c9\n          \n        \n        \n          )\n          \n            r\n          \n        \n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \u2248\n        [\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n        \n          |\n        \n        s\n        \u27e9\n        ]\n        M\n        \n          \n            [\n            \n              \n                \n                  i\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \u2212\n                  i\n                \n              \n            \n            ]\n          \n        \n        \n          M\n          \n            \u2212\n            1\n          \n        \n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          |\n        \n        \u03c9\n        \u27e9\n        \n          \n            1\n            \n              cos\n              \u2061\n              (\n              t\n              )\n            \n          \n        \n        \u2212\n        \n          |\n        \n        s\n        \u27e9\n        \n          \n            \n              sin\n              \u2061\n              (\n              t\n              )\n            \n            \n              cos\n              \u2061\n              (\n              t\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle [|\\omega \\rangle \\,|s\\rangle ](U_{s}U_{\\omega })^{r}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}\\approx [|\\omega \\rangle \\,|s\\rangle ]M{\\begin{bmatrix}i&0\\\\0&-i\\end{bmatrix}}M^{-1}{\\begin{bmatrix}0\\\\1\\end{bmatrix}}=|\\omega \\rangle {\\frac {1}{\\cos(t)}}-|s\\rangle {\\frac {\\sin(t)}{\\cos(t)}}.}\n  A short calculation now shows that the observation yields the correct answer \u03c9 with error \n  \n    \n      \n        O\n        \n          (\n          \n            \n              1\n              N\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle O\\left({\\frac {1}{N}}\\right)}\n  .\n\n\n== Extensions and variants ==\n\n\n=== Multiple matching entries ===\n\nIf, instead of 1 matching entry, there are k matching entries, the same algorithm works, but the number of iterations must be \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            \n              (\n              \n                \n                  N\n                  k\n                \n              \n              )\n            \n            \n              1\n              \n                /\n              \n              2\n            \n          \n        \n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{\\left({\\frac {N}{k}}\\right)^{1/2}}}\n  instead of \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n            \n              1\n              \n                /\n              \n              2\n            \n          \n        \n        .\n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{N^{1/2}}.}\n  \nThere are several ways to handle the case if k is unknown. A simple solution performs optimally up to a constant factor: run Grover's algorithm repeatedly for increasingly small values of k, e.g., taking k = N, N/2, N/4, ..., and so on, taking \n  \n    \n      \n        k\n        =\n        N\n        \n          /\n        \n        \n          2\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle k=N/2^{t}}\n   for iteration t until a matching entry is found.\nWith sufficiently high probability, a marked entry will be found by iteration \n  \n    \n      \n        t\n        =\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        N\n        \n          /\n        \n        k\n        )\n        +\n        c\n      \n    \n    {\\displaystyle t=\\log _{2}(N/k)+c}\n   for some constant c. Thus, the total number of iterations taken is at most\n\n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            (\n          \n        \n        1\n        +\n        \n          \n            2\n          \n        \n        +\n        \n          \n            4\n          \n        \n        +\n        \u22ef\n        +\n        \n          \n            \n              N\n              \n                k\n                \n                  2\n                  \n                    c\n                  \n                \n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        =\n        O\n        \n          \n            (\n          \n        \n        \n          \n            N\n            \n              /\n            \n            k\n          \n        \n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\pi }{4}}{\\Big (}1+{\\sqrt {2}}+{\\sqrt {4}}+\\cdots +{\\sqrt {\\frac {N}{k2^{c}}}}{\\Big )}=O{\\big (}{\\sqrt {N/k}}{\\big )}.}\n  Another approach if k is unknown is to derive it via the quantum counting algorithm prior. \nIf \n  \n    \n      \n        k\n        =\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k=N/2}\n   (or the traditional one marked state Grover's Algorithm if run with \n  \n    \n      \n        N\n        =\n        2\n      \n    \n    {\\displaystyle N=2}\n  ), the algorithm will provide no amplification. If \n  \n    \n      \n        k\n        >\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k>N/2}\n  , increasing k will begin to increase the number of iterations necessary to obtain a solution. On the other hand, if \n  \n    \n      \n        k\n        \u2265\n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle k\\geq N/2}\n  , a classical running of the checking oracle on a single random choice of input will more likely than not give a correct solution.\nA version of this algorithm is used in order to solve the collision problem.\n\n\n=== Quantum partial search ===\nA  modification of Grover's algorithm called quantum partial search was described by Grover and Radhakrishnan in 2004. In partial search, one is not interested in finding the exact address of the target item, only the first few digits of the address. Equivalently, we can think of \"chunking\" the search space into blocks, and then asking \"in which block is the target item?\". In many applications, such a search yields enough information if the target address contains the information wanted. For instance, to use the example given by L. K. Grover, if one has a list of students organized by class rank, we may only be interested in whether a student is in the lower 25%, 25\u201350%, 50\u201375% or 75\u2013100% percentile.\nTo describe partial search, we consider a database separated into \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   blocks, each of size \n  \n    \n      \n        b\n        =\n        N\n        \n          /\n        \n        K\n      \n    \n    {\\displaystyle b=N/K}\n  . The partial search problem is easier. Consider the approach we would take classically \u2013 we pick one block at random, and then perform a normal search through the rest of the blocks (in set theory language, the complement). If we don't find the target, then we know it's in the block we didn't search. The average number of iterations drops from \n  \n    \n      \n        N\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle N/2}\n   to \n  \n    \n      \n        (\n        N\n        \u2212\n        b\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle (N-b)/2}\n  .\nGrover's algorithm requires \n  \n    \n      \n        \n          \n            \u03c0\n            4\n          \n        \n        \n          \n            N\n          \n        \n      \n    \n    {\\textstyle {\\frac {\\pi }{4}}{\\sqrt {N}}}\n   iterations. Partial search will be faster by a numerical factor that depends on the number of blocks \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  . Partial search uses \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n_{1}}\n   global iterations and \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{2}}\n   local iterations. The global Grover operator is designated \n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle G_{1}}\n   and the local Grover operator is designated \n  \n    \n      \n        \n          G\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle G_{2}}\n  .\nThe global Grover operator acts on the blocks. Essentially, it is given as follows:\n\nPerform \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle j_{1}}\n   standard Grover iterations on the entire database.\nPerform \n  \n    \n      \n        \n          j\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j_{2}}\n   local Grover iterations. A local Grover iteration is a direct sum of Grover iterations over each block.\nPerform one standard Grover iteration.The optimal values of \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle j_{1}}\n   and \n  \n    \n      \n        \n          j\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle j_{2}}\n   are discussed in the paper by Grover and Radhakrishnan. One might also wonder what happens if one applies successive partial searches at different levels of \"resolution\". This idea was studied in detail by Vladimir Korepin and Xu, who called it binary quantum search. They proved that it is not in fact any faster than performing a single partial search.\n\n\n== Optimality ==\nGrover's algorithm is optimal up to sub-constant factors. That is, any algorithm that accesses the database only by using the operator U\u03c9 must apply U\u03c9 at least a \n  \n    \n      \n        1\n        \u2212\n        o\n        (\n        1\n        )\n      \n    \n    {\\displaystyle 1-o(1)}\n   fraction as many times as Grover's algorithm. The extension of Grover's algorithm to k matching entries, \u03c0(N/k)1/2/4, is also optimal. This result is important in understanding the limits of quantum computation.\nIf the Grover's search problem was solvable with logc N applications of U\u03c9, that would imply that NP is contained in BQP, by transforming problems in NP into Grover-type search problems. The optimality of Grover's algorithm suggests that quantum computers cannot solve NP-Complete problems in polynomial time, and thus NP is not contained in BQP.\nIt has been shown that a class of non-local hidden variable quantum computers could implement a search of an \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  -item database in at most \n  \n    \n      \n        O\n        (\n        \n          \n            N\n            \n              3\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt[{3}]{N}})}\n   steps. This is faster than the \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   steps taken by Grover's algorithm.\n\n\n== See also ==\nAmplitude amplification\nBrassard\u2013H\u00f8yer\u2013Tapp algorithm (for solving the collision problem)\nShor's algorithm (for factorization)\nQuantum walk search\n\n\n== Notes ==\n\n\n== References ==\nGrover L.K.: A fast quantum mechanical algorithm for database search, Proceedings, 28th Annual ACM Symposium on the Theory of Computing, (May 1996) p. 212\nGrover L.K.: From Schr\u00f6dinger's equation to quantum search algorithm, American Journal of Physics, 69(7): 769\u2013777, 2001. Pedagogical review of the algorithm and its history.\nGrover L.K.: QUANTUM COMPUTING: How the weird logic of the subatomic world could make it possible for machines to calculate millions of times faster than they do today The Sciences, July/August 1999, pp. 24\u201330.\nNielsen, M.A. and Chuang, I.L. Quantum computation and quantum information. Cambridge University Press, 2000. Chapter 6.\nWhat's a Quantum Phone Book?, Lov Grover, Lucent Technologies\n\n\n== External links ==\n\nDavy Wybiral. \"Quantum Circuit Simulator\". Archived from the original on 2017-01-16. Retrieved 2017-01-13.\nCraig Gidney (2013-03-05). \"Grover's Quantum Search Algorithm\".\nFran\u00e7ois Schwarzentruber (2013-05-18). \"Grover's algorithm\".\nAlexander Prokopenya. \"Quantum Circuit Implementing Grover's Search Algorithm\". Wolfram Alpha.\n\"Quantum computation, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nRoberto Maestre (2018-05-11). \"Grover's Algorithm implemented in R and C\". GitHub.\nBernhard \u00d6mer. \"QCL - A Programming Language for Quantum Computers\". Retrieved 2022-04-30. Implemented in /qcl-0.6.4/lib/grover.qcl"}, {"id": 86, "title": "Regulation of sport", "content": "The regulation of sport is usually done by a sport governing body for each sport, resulting in a core of relatively invariant, agreed rules. People responsible for leisure activities often seek recognition and respectability as sports by joining sports federations such as the International Olympic Committee, or by forming their own regulatory body.  In this way sports evolve from leisure activity to more formal sports: relatively recent newcomers are BMX cycling, snowboarding, wrestling,  etc. Some of these activities have been popular but uncodified pursuits for different lengths of time. Indeed, the formal regulation of sport is a relatively modern and increasing development. This method promotes a sport globally, in a very successful way. It also promotes the universality of each sport, by ensuring that the same gameplay rules are being practiced worldwide, using a standardized/homogenous international gameplay rule system (sanctioned by the respective international sports governing bodies) that is applied uniformly on all member associations and recognized leagues. Examples are FIFA in association football and FIBA in basketball, which have regulated international gameplay rules that are even practiced within US sports leagues today, despite not practicing them historically (which therefore meant that many US sports leagues weren't recognized by international governing bodies in the past, until they began to adopt international rules). In the sport of basketball, the defender/defense cannot call foul.\nFormula One motor racing is an example of strict and changing regulation, where the regulating body appears to control rather than to simply define the sport. There have been major changes in the rules of F1 recently, almost on an annual basis, and more are planned. Sometimes this is done for safety reasons, sometimes to make the racing more interesting as a spectator sport, and sometimes to promote competition through involvement of smaller teams. Some changes make overtaking more probable for example or reduce the probability of an overwhelming technical advantage by any one team. Although heavily regulated, most people agree that the sport has thereby greatly benefitted, not least through dramatic leaps in safety.\nThe degree of organisation can vary from national or worldwide  competitions for the sport, or it can occur in a  purely ad hoc, spontaneous way. A sport may be played individually (e.g. time trialling in cycling) or in a team,  or just for recreation and well being (e.g. swimming).\nSome challenging situations have had to be dealt with when  there is an overlap of the regulation of the sport  with other forms of regulation, e.g. safety (There have been serious losses of life in football audiences, through stand collapses or poor crowd management), or simple laws of the land (Some inadvertent or otherwise physical interchanges occur between participants: when is it acceptable for the sport regulating authority alone to investigate and if necessary punish these? Can there be economic or public relations pressures affecting these issues?)\nThe broadcasting of sports events is also highly regulated, with contracts limiting who can show footage.\n\n\n== See also ==\nOutline of sports\nFairness in Women's Sports Act\n\n\n== References ==\n\nSporting regulation lessons at SportsProMedia"}, {"id": 87, "title": "List of Presidents of the United States by Other Offices Held", "content": "This is a list of presidents of the United States by other offices (either elected or appointed) held. Every president of the United States except Donald Trump has served as at least one of the following:\n\nVice President of the United States\na member of Congress (either U.S. senator or representative)\na governor of a state\na Cabinet secretary\na general of the United States Army\n\n\n== Federal government ==\n\n\n=== Executive branch ===\n\n\n==== Vice presidents ====\n13 former vice presidents (R. Johnson, Breckinridge, Morton, Stevenson, Fairbanks, Garner, Wallace, Barkley, Nixon, Humphrey, Mondale, Quayle, and Gore) all made failed runs for the presidency. Humphrey, Mondale, and Gore received their party's nominations and Nixon received his party's nomination. Nixon would later be elected in a second run for the presidency.\n\n\n==== Cabinet secretaries ====\nCalvin Coolidge (as the vice president) and Herbert Hoover both served in the Cabinet of Warren G. Harding.\n\n\n==== Ambassadors ====\n\n\n==== Other federal appointees ====\n\n\n=== Judicial branch ===\n\n\n==== Chief Justice of the United States ====\n\n\n==== Other federal judges ====\n\n\n=== Legislative branch ===\n\n\n==== Senators ====\nA number of future presidents served together while in the Senate:\n\nMonroe served under Vice President Adams (1790\u20131794).\nJackson served under Vice President Jefferson (1797\u20131798). Jackson later served with Van Buren (1823\u20131825). Van Buren also served with W.H. Harrison (1825\u20131828) and Tyler (1827\u20131828). Buchanan also served with Tyler (1834\u20131836) and later served with Pierce (1837\u20131842). Both Buchanan and Tyler served under Vice President Van Buren (1833\u20131837), while Buchanan and Pierce later served under Vice President Tyler (1841).\nB. Harrison briefly served under Vice President Arthur (1881).\nL. Johnson served with both Nixon (1950\u20131953) and Kennedy (1953\u20131960). L. Johnson and Kennedy both served under Vice President Nixon (1953\u20131961).\nBiden served under vice presidents Ford (1973\u20131974) and Bush (1981\u20131989) and later served with Obama (2005\u20132008).James A. Garfield was elected senator for Ohio in 1880, but he did not take up the office due to being elected President later that year.\nSeven former senators (Monroe, Adams, Jackson, W.H. Harrison, Pierce, Buchanan, and B. Harrison) were elected to the presidency without ever serving as the vice president between their departure from the Senate and the beginning of their presidencies.\n\n\n==== Members of the House of Representatives ====\nA number of future and former presidents served in the House together:\n\nJackson served with Madison (1796\u20131797).\nW.H. Harrison served with Tyler (1816\u20131819).\nBuchanan served with Polk (1825\u20131831). Polk also served with J. Q. Adams (1831\u20131839). J. Q. Adams later served with Fillmore (1833\u20131835; 1837\u20131843), Pierce (1833\u20131837), A. Johnson (1843\u20131848), and Lincoln (1847\u20131848). A. Johnson and Lincoln would continue to serve alongside each other (1848\u20131849).\nGarfield served with both Hayes (1865\u20131867) and McKinley (1877\u20131880).\nNixon served with L. Johnson (1947\u20131949), Kennedy (1947\u20131950), and Ford (1949\u20131950). Ford, who continued to serve alongside Kennedy (1950\u20131953), later served with G. H. W. Bush (1967\u20131971).\n\n\n=== Continental Congress ===\n\n\n== State and territorial government ==\n\n\n=== Governors ===\n\n\n=== State legislators ===\nSee below for information about pre-1776 colonial offices held.\n\n\n=== Other statewide offices ===\n\n\n== Local government ==\n\n\n== Presidents who had not previously held elective office ==\n\n\n=== With previous experience in government ===\n\n\n=== With previous experience in the military ===\n\n\n=== Without previous experience in government or the military ===\n\n\n== Colonial governments ==\n\n\n=== Colonial and confederate legislators ===\n\n\n== See also ==\nList of former presidents of the United States who ran for office"}, {"id": 88, "title": "Stadium", "content": "A stadium (pl.: stadiums or stadia) is a place or venue for (mostly) outdoor sports, concerts, or other events and consists of a field or stage either partly or completely surrounded by a tiered structure designed to allow spectators to stand or sit and view the event.Pausanias noted that for about half a century the only event at the ancient Greek Olympic festival was the race that comprised one length of the stadion at Olympia, where the word \"stadium\" originated.Most of the stadiums with a capacity of at least 10,000 are used for association football. Other popular stadium sports include gridiron football, baseball, cricket, the various codes of rugby, field lacrosse, bandy, and bullfighting. Many large sports venues are also used for concerts.\n\n\n== Etymology ==\n\"Stadium\" is the Latin form of the Greek word \"stadion\" (\u03c3\u03c4\u03ac\u03b4\u03b9\u03bf\u03bd), a measure of length equalling the length of 600 human feet. As feet are of variable length the exact length of a stadion depends on the exact length adopted for 1 foot at a given place and time. Although in modern terms 1 stadion = 600 ft (180 m), in a given historical context it may actually signify a length up to 15% larger or smaller.The equivalent Roman measure, the stadium, had a similar length \u2013 about 185 m (607 ft) \u2013 but instead of being defined in feet was defined using the Roman standard passus to be a distance of 125 pass\u016bs (double-paces).\nThe English use of stadium comes from the tiered infrastructure surrounding a Roman track of such length.\nMost dictionaries provide for both stadiums and stadia as valid English plurals.\n\n\n== History ==\nThe oldest known stadium is the Stadium at Olympia in Greece, where the ancient Olympic Games were held from 776 BC. Initially the Games consisted of a single event, a sprint along the length of the stadium.\nGreek and Roman stadiums have been found in numerous ancient cities, perhaps the most famous being the Stadium of Domitian, in Rome.\nThe excavated and refurbished ancient Panathenaic Stadium hosted attempted revivals of the Olympic Games in 1870 and 1875 before hosting the first modern Olympics in 1896, the 1906 Intercalated Games, and some events of the 2004 Summer Olympics. The excavation and refurbishment of the stadium was part of the legacy of the Greek national benefactor Evangelos Zappas, and it was the first ancient stadium to be used in modern times.\n\n\n=== Antiquity ===\nStadiums in ancient Greece and Rome were built for different purposes, and at first only the Greeks built structures called \"stadium\"; Romans built structures called \"circus\". Greek stadia were for foot races, whereas the Roman circus was for horse races. Both had similar shapes and bowl-like areas around them for spectators. The Greeks also developed the theatre, with its seating arrangements foreshadowing those of modern stadiums. The Romans copied the theatre, then expanded it to accommodate larger crowds and more elaborate settings. The Romans also developed the double-sized round theatre called amphitheatre, seating crowds in the tens of thousands for gladiatorial combats and beast shows. The Greek stadium and theatre and the Roman circus and amphitheatre are all ancestral to the modern stadium.\n\n\n==== Examples ====\n\n\n=== Modernity ===\nThe first stadiums to be built in the modern era were basic facilities, designed for the single purpose of fitting as many spectators in as possible. With tremendous growth in the popularity of organised sport in the late Victorian era, especially association football in the United Kingdom and baseball in the United States, the first such structures were built. One such early stadium was the Lansdowne Road Stadium, the brainchild of Henry Dunlop, who organised the first All Ireland Athletics Championships. Banned from locating sporting events at Trinity College, Dunlop built the stadium in 1872. \"I laid down a cinder running path of a quarter-mile, laid down the present Lansdowne Tennis Club ground with my own theodolite, started a Lansdowne archery club, a Lansdowne cricket club, and last, but not least, the Lansdowne Rugby Football Club \u2013 colours red, black and yellow.\" Some 300 cartloads of soil from a trench beneath the railway were used to raise the ground, allowing Dunlop to use his engineering expertise to create a pitch envied around Ireland.\nOther early stadiums from this period in the UK include the Stamford Bridge stadium (opened in 1877 for the London Athletic Club) and Anfield stadium (1884 as a venue for Everton F.C.).\nIn the U.S., many professional baseball teams built large stadiums mainly out of wood, with the first such venue being the South End Grounds in Boston, opened in 1871 for the team then known as the Boston Beaneaters (now the Atlanta Braves). Many of these parks caught fire, and those that did not proved inadequate for a growing game. All of the 19th-century wooden parks were replaced, some after a few years, and none survive today.\nGoodison Park was the first purpose-built association football stadium in the world. Walton-based building firm Kelly brothers were instructed to erect two uncovered stands that could each accommodate 4,000 spectators. A third covered stand accommodating 3,000 spectators was also requested. Everton officials were impressed with the builder's workmanship and agreed two further contracts: exterior hoardings were constructed at a cost of \u00a3150 and 12 turnstiles were installed at a cost of \u00a37 each. The stadium was officially opened on 24 August 1892 by Lord Kinnaird and Frederick Wall of the Football Association. No football was played; instead the 12,000 crowd watched a short track and field event followed by music and a fireworks display. Upon its completion the stadium was the first joint purpose-built football stadium in the world.The architect Archibald Leitch brought his experience with the construction of industrial buildings to bear on the design of functional stadiums up and down the country. His work encompassed the first 40 years of the 20th century. One of his most notable designs was Old Trafford in Manchester. The ground was originally designed with a capacity of 100,000 spectators and featured seating in the south stand under cover, while the remaining three stands were left as terraces and uncovered. It was the first stadium to feature continuous seating along the contours of the stadium.These early venues, originally designed to host football matches, were adopted for use by the Olympic Games, the first one being held in 1896 in Athens, Greece. The White City Stadium, built for the 1908 Summer Olympics in London is often cited as the first modern seater stadium, at least in the UK. Designed by the engineer J.J. Webster and completed in 10 months by George Wimpey, on the site of the Franco-British Exhibition, this stadium with a seating capacity of 68,000 was opened by King Edward VII on 27 April 1908. Upon completion, the stadium had a running track 24 ft wide (7.3 m) and three laps to the mile (536 m); outside there was a 35-foot-wide (11 m), 660-yard (600 m) cycle track. The infield included a swimming and diving pool. The London Highbury Stadium, built in 1913, was the first stadium in the UK to feature a two-tiered seating arrangement when it was redesigned in the Art Deco style in 1936.During these decades, parallel stadium developments were taking place in the U.S. The Baker Bowl, a baseball park in Philadelphia that opened in its original form in 1887 but was completely rebuilt in 1895, broke new ground in stadium construction in two major ways. The stadium's second incarnation featured the world's first cantilevered second deck (tier) in a sports venue, and was the first baseball park to use steel and brick for the majority of its construction. Another influential venue was Boston's Harvard Stadium, built in 1903 by Harvard University for its American football team and track and field program. It was the world's first stadium to use concrete-and-steel construction. In 1909, concrete-and-steel construction came to baseball with the opening of Shibe Park in Philadelphia and, a few months later, Forbes Field in Pittsburgh. The latter was the world's first three-tiered sporting venue. The opening of these parks marked the start of the \"jewel box\" era of park construction. The largest stadium crowd ever was 199,854 people watching the final match of the 1950 World Cup at Rio de Janeiro's Maracan\u00e3 on 16 July 1950.\n\n\n== Types ==\nDomed stadiums are distinguished from conventional stadiums by their enclosing roofs. Many of these are not actually domes in the pure architectural sense, some being better described as vaults, some having truss-supported roofs and others having more exotic designs such as a tensegrity structure. But, in the context of sports stadiums, the term \"dome\" has become standard for all covered stadiums, particularly because the first such enclosed stadium, the Houston Astrodome, was built with an actual dome-shaped roof. Some stadiums have partial roofs, and a few have even been designed to have moveable fields as part of the infrastructure. The Caesars Superdome in New Orleans is a true dome structure made of a lamellar multi-ringed frame and has a diameter of 680 feet (210 m). It is the largest fixed domed structure in the world.Even though enclosed, dome stadiums are called stadiums because they are large enough for, and designed for, what are generally considered to be outdoor sports such as athletics, American football, association football, rugby, and baseball. Those designed for what are usually indoor sports like basketball, ice hockey and volleyball are generally called arenas. Exceptions include:\n\nCameron Indoor Stadium, home to Duke University's Blue Devils men's and women's basketball programs.\nRed Bull Arena, an open-air venue that is home to Major League Soccer's New York Red Bulls and NJ/NY Gotham FC of the National Women's Soccer League.\nParis La D\u00e9fense Arena, a domed stadium that is home to the rugby union club Racing 92. It has a movable seating block that allows a configuration appropriate for indoor court sports.\nChicago Stadium (demolished), former home to the National Hockey League's Chicago Blackhawks and the National Basketball Association's Chicago Bulls.\n\n\n== Design issues ==\n\nDifferent sports require different playing surfaces of various size and shape. Some stadiums are designed primarily for a single sport while others can accommodate different events, particularly ones with retractable seating. Stadiums built specifically for association football are common in Europe; Gaelic games stadiums, such as Croke Park, are common in Ireland, while stadiums built specifically for baseball or American football are common in the United States. The most common multiple use design combines a football pitch with a running track, although certain compromises must be made. The major drawback is that the stands are necessarily set back a good distance from the pitch, especially at the ends of the pitch. In the case of some smaller stadiums, there are not stands at the ends. When there are stands all the way around, the stadium takes on an oval shape. When one end is open, the stadium has a horseshoe shape. All three configurations (open, oval and horseshoe) are common, especially in the case of American college football stadiums. Rectangular stadiums are more common in Europe, especially for football where many stadiums have four often distinct and very different stands on the four sides of the stadium. These are often all of different sizes and designs and have been erected at different periods in the stadium's history. The vastly differing character of European football stadiums has led to the growing hobby of ground hopping where spectators make a journey to visit the stadium for itself rather than for the event held there. In recent years the trend of building completely new oval stadiums in Europe has led to traditionalists criticising the designs as bland and lacking in the character of the old stadiums they replace.\nIn North America, where baseball and American football are the two most popular outdoor spectator sports, a number of football/baseball multi-use stadiums were built, especially during the 1960s, and some of them were successful.\nSince the requirements for baseball and football are significantly different, the trend has been toward the construction of single-purpose stadiums, beginning with Kansas City in 1972\u20131973 and accelerating in the 1990s. In several cases, an American football stadium has been constructed adjacent to a baseball park, to allow for the sharing of mutual parking lots and other amenities. With the rise of MLS, the construction of soccer-specific stadiums has also increased since the late 1990s to better fit the needs of that sport. In many cases, earlier baseball stadiums were constructed to fit into a particular land area or city block. This resulted in asymmetrical dimensions for many baseball fields. Yankee Stadium, for example, was built on a triangular city block in The Bronx, New York City. This resulted in a large left field dimension but a small right field dimension.\nBefore more modern football stadiums were built in the United States, many baseball parks, including Fenway Park, the Polo Grounds, Wrigley Field, Comiskey Park, Tiger Stadium, Griffith Stadium, Milwaukee County Stadium, Shibe Park, Forbes Field, Yankee Stadium, and Sportsman's Park were used by the National Football League or the American Football League. (To a certain extent, this continues in lower football leagues as well, with the venue now known as Charles Schwab Field Omaha being used as the home stadium of the United Football League's Omaha Nighthawks.) Along with today's single use stadiums is the trend for retro-style ballparks closer to downtown areas. Oriole Park at Camden Yards was the first such ballpark for Major League Baseball to be built, using early-20th-century styling with 21st-century amenities.\nThere is a solar-powered stadium in Taiwan that produces as much energy as it needs to function.Stadium designers often study acoustics to increase noise caused by fans' voices, aiming to create a lively atmosphere.\n\n\n=== Lighting ===\nUntil the advent of floodlights, most games played on large areas had to rely on natural lighting.\nBramall Lane was reportedly the first floodlit stadium. Floodlighting in association football dates as far back as 1878, when there were floodlit experimental matches at Bramall Lane, Sheffield during the dark winter afternoons. With no national grid, lights were powered by batteries and dynamoes, and were unreliable.\nSince the development of electrical grids, lighting has been an important element in stadium design, allowing games to be played after sundown, and in covered, or partly covered stadiums that allow less natural light, but provide more shelter for the public.\n\n\n== Spectator areas and seating ==\nAn \"all-seater\" stadium has seats for all spectators. Other stadiums are designed so that all or some spectators stand to view the event. The term \"all-seater\" is not common in the U.S., as very few American stadiums have sizeable standing-only sections. Poor stadium design has contributed to disasters, such as the Hillsborough disaster and the Heysel Stadium disaster. Since these, all Premier League, UEFA European Championship and FIFA World Cup qualifying matches require all spectators to be seated.\nSeating areas may be known as terraces, tiers, or decks. Originally set out for standing room only, they are now usually equipped with seating. Another term used in the US is bleachers, which is mostly used for seating areas with bench seats as opposed to individual seats, and which often are uncovered; the name refers to the bleaching effect direct, unshaded  sunlight has on the benches and patrons in those sections.\nMany stadiums make luxury suites or boxes available to patrons at high prices. These suites can accommodate ten to thirty people, depending on the venue. Luxury suites at events such as the Super Bowl can cost hundreds of thousands of dollars.\n\n\n=== Safety and security ===\n\nDue to the number of people congregating in stadiums and the frequency of events, many notable accidents have occurred in the past, some causing injury and death. For example, the Hillsborough disaster was a human crush at Hillsborough Stadium in Sheffield, England on 15 April 1989. The resulting 97 deaths and 765 injuries makes this the worst disaster in British sporting history.\nMuch effort has been spent to avoid the recurrence of such events, both in design and legislation. Especially where there is a perceived risk of terrorism or violence attention remains high to prevent human death and keep stadiums as places where families can enjoy a public event together.\nIn Europe and South America, during the twentieth century, it was common for violent bands of supporters to fight inside or close to association football stadiums. In the United Kingdom they are known as hooligans.\nStructural features that increase safety include separate entry and exit accesses for each spectator area, especially separating accesses for home and visitor supporters, dividing walls, glass parapets, vibration attenuation and sprinkler systems.\nSecurity features that have been adopted include armed surveillance, Identity document checks, video surveillance, metal detectors and security searches to enforce rules that forbid spectators to carry dangerous or potentially dangerous items.\n\n\n== Political and economic issues ==\n\nModern stadiums, especially the largest among them, are megaprojects that can only be afforded by the largest corporations, wealthiest individuals, or government. Sports fans have a deep emotional attachment to their teams. In North America, with its closed-league \"franchise\" system, there are fewer teams than cities which would like them. This creates tremendous bargaining power for the owners of teams, whereby owners can threaten to relocate teams to other cities unless governments subsidize the construction of new facilities. In Europe and Latin America, where there are multiple association football clubs in any given city, and several leagues in each country, no such monopoly power exists, and stadiums are built primarily with private money. Outside professional sports, governments are also involved through the intense competition for the right to host major sporting events, primarily the Summer Olympics and the FIFA World Cup (of association football), during which cities often pledge to build new stadiums in order to satisfy the International Olympic Committee (IOC) or FIFA.\n\n\n=== Corporate naming ===\n\nIn recent decades, to help take the burden of the massive expense of building and maintaining a stadium, many American and European sports teams have sold the rights to the name of the facility. This trend, which began in the 1970s, but accelerated greatly in the 1990s, has led to sponsors' names being affixed to both established stadiums and new ones. In some cases, the corporate name replaces (with varying degrees of success) the name by which the venue has been known for many years. But many of the more recently built stadiums, like the Volkswagen Arena in Wolfsburg, Germany, have never been known by a non-corporate name. The sponsorship phenomenon has since spread worldwide. There remain a few municipally owned stadiums, which are often known by a name that is significant to their area (for example, Boston's Fenway Park). In recent years, some government-owned stadiums have also been subject to naming-rights agreements, with some or all of the revenue often going to the team(s) that play there.\nOne consequence of corporate naming has been an increase in stadium name changes, when the namesake corporation changes its name, or if it is the naming agreement simply expires. Phoenix's Chase Field, for example, was previously known as Bank One Ballpark, but was renamed to reflect the takeover of the latter corporation. San Francisco's historic Candlestick Park was renamed as 3Com Park for several years, but the name was dropped when the sponsorship agreement expired, and it was another two years before the new name of Monster Cable Products' Monster Park was applied. Local opposition to the corporate naming of that particular stadium led San Francisco's city council to permanently restore the Candlestick Park name once the Monster contract expired. More recently, in Ireland, there has been huge opposition to the renaming of Dublin's historic Lansdowne Road as the Aviva Stadium. Lansdowne was redeveloped as the Aviva, opening in May 2010.\nOn the other hand, Los Angeles' Great Western Forum, one of the earliest examples of corporate renaming, retained its name for many years, even after the namesake bank no longer existed, the corporate name being dropped only after the building later changed ownership. This practice has typically been less common in countries outside the United States. A notable exception is the Nippon Professional Baseball league of Japan, in which many of the teams are themselves named after their parent corporations. Also, many newer European football stadiums, such as the University of Bolton and Emirates Stadiums in England and Signal Iduna Park and Allianz Arena in Germany have been corporately named.\nThis new trend in corporate naming (or renaming) is distinguishable from names of some older venues, such as Crosley Field, Wrigley Field, and the first and second Busch Stadiums, in that the parks were named by and for the club's owner, which also happened to be the name of the company owned by those clubowners. (The current Busch Stadium received its name via a modern naming rights agreement.)\nDuring the 2006 FIFA World Cup in Germany, some stadiums were temporarily renamed because FIFA prohibits sponsorship of stadiums. For example, the Allianz Arena in Munich was called the FIFA World Cup Stadium, Munich during the tournament. Likewise, the same stadium will be known as the \"M\u00fcnchen Arena\" during the European Competitions. Similar rules affect the Imtech Arena and Veltins-Arena. This rule applies even if the stadium sponsor is an official FIFA sponsor\u2014the Johannesburg stadium then commercially known as \"Coca-Cola Park\", bearing the name of one of FIFA's major sponsors, was known by its historic name of Ellis Park Stadium during the 2010 FIFA World Cup. Corporate names are also temporarily replaced during the Olympics.\n\n\n== Environmental issues ==\nModern stadiums bring several negative environmental issues with their construction. They require thousands of tons of materials to build, they greatly increase traffic in the area around the stadium, as well as maintaining the stadium. The increased traffic around modern stadiums has led to create exposure zones says the Health Effect Institute, exposing 30-40% of people living around the stadium to potential health issues. Many stadiums are attempting to counteract these issues by implementing solar panels, and high efficiency lighting, to reduce their own carbon footprint.\n\n\n== Music venues ==\n\nAlthough concerts, such as classical music, had been presented in them for decades, beginning in  the 1960s stadiums began to be used as live venues for popular music, giving rise to the term \"stadium rock\", particularly for forms of hard rock and progressive rock. The origins of stadium rock are sometimes dated to when the Beatles played Shea Stadium in New York in 1965. Also important was the use of large stadiums for American tours by bands in the later 1960s, such as the Rolling Stones, Grand Funk Railroad and Led Zeppelin. The tendency developed in the mid-1970s as the increased power of amplification and sound systems allowed the use of larger and larger venues. Smoke, fireworks and sophisticated lighting shows became staples of arena rock performances. Key acts from this era included Journey, REO Speedwagon, Boston, Foreigner, Styx, Kiss, Peter Frampton and Queen. In the 1980s arena rock became dominated by glam metal bands, following the lead of Aerosmith and including M\u00f6tley Cr\u00fce, Quiet Riot, W.A.S.P. and Ratt. Since the 1980s, rock, pop and folk stars, including the Grateful Dead, Madonna, Beyonc\u00e9, Lady Gaga and Taylor Swift, have undertaken large-scale stadium based concert tours.\n\n\n== See also ==\nArchitectural structure\nList of nonbuilding structure types\nAmphitheatre\nJumbotron\nPerforming arts center\nSport venue\nSports complex\nTheater\nList of indoor arenas\nList of sports attendance figures\nLists of stadiums\n\n\n== References ==\n\n\n== Bibliography ==\nJohn, Geraint; Rod Sheard; Ben Vickery (2007). Stadia: A Design and Development Guide (4th ed.). Amsterdam: Elsevier/Architectural Press. ISBN 978-0-7506-6844-6.\nLisle, Benjamin D. (2017). Modern Coliseum: Stadiums and American Culture. Philadelphia: U of Pennsylvania Press. p. 321.\nSerby, Myron W. (1930). The Stadium; A Treatise on the Design of Stadiums and Their Equipment. New York, Cleveland: American Institute of Steel, inc. OCLC 23706869.\n\n\n== External links ==\n\n\"Stadium Guide\".\n\"Stadium Journey\"."}, {"id": 89, "title": "Credit risk", "content": "Credit risk is the possibility of losing a lender holds due to a risk of default on a debt that may arise from a borrower failing to make required payments. In the first resort, the risk is that of the lender and includes lost principal and interest, disruption to cash flows, and increased collection costs. The loss may be complete or partial. In an efficient market, higher levels of credit risk will be associated with higher borrowing costs.  Because of this, measures of borrowing costs such as yield spreads can be used to infer credit risk levels based on assessments by market participants.\nLosses can arise in a number of circumstances, for example:\n\nA consumer may fail to make a payment due on a mortgage loan, credit card, line of credit, or other loan.\nA company is unable to repay asset-secured fixed or floating charge debt.\nA business or consumer does not pay a trade invoice when due.\nA business does not pay an employee's earned wages when due.\nA business or government bond issuer does not make a payment on a coupon or principal payment when due.\nAn insolvent insurance company does not pay a policy obligation.\nAn insolvent bank will not return funds to a depositor.\nA government grants bankruptcy protection to an insolvent consumer or business.To reduce the lender's credit risk, the lender may perform a credit check on the prospective borrower, may require the borrower to take out appropriate insurance, such as mortgage insurance, or seek security over some assets of the borrower or a guarantee from a third party. The lender can also take out insurance against the risk or on-sell the debt to another company. In general, the higher the risk, the higher will be the interest rate that the debtor will be asked to pay on the debt.\nCredit risk mainly arises when borrowers are unable or unwilling to pay.\n\n\n== Types ==\nA credit risk can be of the following types:\nCredit default risk \u2013 The risk of loss arising from a debtor being unlikely to pay its loan obligations in full or the debtor is more than 90 days past due on any material credit obligation; default risk may impact all credit-sensitive transactions, including loans, securities and derivatives.\nConcentration risk \u2013 The risk associated with any single exposure or group of exposures with the potential to produce large enough losses to threaten a bank's core operations. It may arise in the form of single-name concentration or industry concentration.\nCountry risk \u2013 The risk of loss arising from a sovereign state freezing foreign currency payments (transfer/conversion risk) or when it defaults on its obligations (sovereign risk); this type of risk is prominently associated with the country's macroeconomic performance and its political stability.\n\n\n== Assessment ==\n\nSignificant resources and sophisticated programs are used to analyze and manage risk. Some companies run a credit risk department whose job is to assess the financial health of their customers, and extend credit (or not) accordingly. They may use in-house programs to advise on avoiding, reducing and transferring risk. They also use the third party provided intelligence. Nationally recognized statistical rating organizations provide such information for a fee.\nFor large companies with liquidly traded corporate bonds or Credit Default Swaps, bond yield spreads and credit default swap spreads indicate market participants assessments of credit risk and may be used as a reference point to price loans or trigger collateral calls.\nMost lenders employ their models (credit scorecards) to rank potential and existing customers according to risk, and then apply appropriate strategies. With products such as unsecured personal loans or mortgages, lenders charge a higher price for higher-risk customers and vice versa. With revolving products such as credit cards and overdrafts, the risk is controlled through the setting of credit limits. Some products also require collateral, usually an asset that is pledged to secure the repayment of the loan.Credit scoring models also form part of the framework used by banks or lending institutions to grant credit to clients. For corporate and commercial borrowers, these models generally have qualitative and quantitative sections outlining various aspects of the risk including, but not limited to, operating experience, management expertise, asset quality, and leverage and liquidity ratios, respectively. Once this information has been fully reviewed by credit officers and credit committees, the lender provides the funds subject to the terms and conditions presented within the contract (as outlined above).\n\n\n=== Sovereign risk ===\nSovereign credit risk is the risk of a government being unwilling or unable to meet its loan obligations, or reneging on loans it guarantees. Many countries have faced sovereign risk in the late-2000s global recession. The existence of such risk means that creditors should take a two-stage decision process when deciding to lend to a firm based in a foreign country. Firstly one should consider the sovereign risk quality of the country and then consider the firm's credit quality.Five macroeconomic variables that affect the probability of sovereign debt rescheduling are:\nDebt service ratio\nImport ratio\nInvestment ratio\nVariance of export revenue\nDomestic money supply growthThe probability of rescheduling is an increasing function of debt service ratio, import ratio, the variance of export revenue and domestic money supply growth. The likelihood of rescheduling is a decreasing function of investment ratio due to future economic productivity gains. Debt rescheduling likelihood can increase if the investment ratio rises as the foreign country could become less dependent on its external creditors and so be less concerned about receiving credit from these countries/investors.\n\n\n=== Counterparty risk ===\nA counterparty risk, also known as a settlement risk or counterparty credit risk (CCR), is a risk that a counterparty will not pay as obligated on a bond, derivative, insurance policy, or other contract. \nFinancial institutions or other transaction counterparties may hedge or take out credit insurance or, particularly in the context of derivatives, require the posting of collateral. \nOffsetting counterparty risk is not always possible, e.g. because of temporary liquidity issues or longer-term systemic reasons. \nFurther, counterparty risk increases due to positively correlated risk factors; accounting for this correlation between portfolio risk factors and counterparty default in risk management methodology is not trivial.The capital requirement here is calculated using SA-CCR, the standardized approach for counterparty credit risk. This framework replaced both non-internal model approaches  -  Current Exposure Method (CEM) and Standardised Method (SM). It is a \"risk-sensitive methodology\", i.e. conscious of asset class and hedging, that differentiates between margined and non-margined trades and recognizes netting benefits; issues insufficiently addressed under the preceding frameworks.\n\n\n== Mitigation ==\nLenders mitigate credit risk in a number of ways, including:\n\nRisk-based pricing \u2013 Lenders may charge a higher interest rate to borrowers who are more likely to default, a practice called risk-based pricing. Lenders consider factors relating to the loan such as loan purpose, credit rating, and loan-to-value ratio and estimates the effect on yield (credit spread).\nCovenants \u2013 Lenders may write stipulations on the borrower, called covenants, into loan agreements, such as:Periodically report its financial condition,\nRefrain from paying dividends, repurchasing shares, borrowing further, or other specific, voluntary actions that negatively affect the company's financial position, and\nRepay the loan in full, at the lender's request, in certain events such as changes in the borrower's debt-to-equity ratio or interest coverage ratio.\nCredit insurance and credit derivatives \u2013 Lenders and bond holders may hedge their credit risk by purchasing credit insurance or credit derivatives. These contracts transfer the risk from the lender to the seller (insurer) in exchange for payment. The most common credit derivative is the credit default swap.\nTightening \u2013 Lenders can reduce credit risk by reducing the amount of credit extended, either in total or to certain borrowers. For example, a distributor selling its products to a troubled retailer may attempt to lessen credit risk by reducing payment terms from net 30  to net 15.\nDiversification \u2013 Lenders to a small number of borrowers (or kinds of borrower) face a high degree of unsystematic credit risk, called concentration risk. Lenders reduce this risk by diversifying the borrower pool.\nDeposit insurance \u2013 Governments may establish deposit insurance to guarantee bank deposits in the event of insolvency and to encourage consumers to hold their savings in the banking system instead of in cash.\n\n\n== Related  Initialisms ==\nACPM Active credit portfolio management \nCCR Counterparty Credit Risk\nCE Credit Exposure\nCVA Credit valuation adjustment\nDVA  Debit Valuation Adjustment \u2013 see XVA\nEAD Exposure at default\nEE Expected Exposure\nEL Expected loss\nLGD Loss given default\nPD Probability of default\nPFE Potential future exposure\nSA-CCR The Standardised Approach to Counterparty Credit Risk\nVAR Value at risk\n\n\n== See also ==\nCredit (finance)\nDefault (finance)\nDistressed securities\nJarrow\u2013Turnbull model\nKMV model\nMerton model\nCriticism of credit scoring systems in the United States\n\n\n== References ==\n\n\n== Further reading ==\nBluhm, Christian; Ludger Overbeck & Christoph Wagner (2002). An Introduction to Credit Risk Modeling. Chapman & Hall/CRC. ISBN 978-1-58488-326-5.\nDamiano Brigo and Massimo Masetti (2006). Risk Neutral Pricing of Counterparty Risk, in: Pykhtin, M. (Editor), Counterparty Credit Risk Modeling: Risk Management, Pricing and Regulation. Risk Books. ISBN 978-1-904339-76-2.\nOrlando, Giuseppe; Bufalo Michele; Penikas Henry; Zurlo Concetta (2022). Modern Financial Engineering: Counterparty, Credit, Portfolio and Systemic Risks. World Scientific. ISBN 978-981-125-235-8.\nde Servigny, Arnaud; Olivier Renault (2004). The Standard & Poor's Guide to Measuring and Managing Credit Risk. McGraw-Hill. ISBN 978-0-07-141755-6.\nDarrell Duffie and Kenneth J. Singleton (2003). Credit Risk: Pricing, Measurement, and Management. Princeton University Press. ISBN 978-0-691-09046-7.\nPrinciples for the management of credit risk from the Bank for International Settlements\n\n\n== External links ==\nBank Management and Control, Springer Nature \u2013 Management for Professionals, 2020\nCredit Risk Modelling, - information on credit risk modelling and decision analytics\nA Guide to Modeling Counterparty Credit Risk \u2013  SSRN Research Paper, July 2007\nDefaultrisk.com \u2013 research and white papers on credit risk modelling\nThe Journal of Credit Risk publishes research on credit risk theory and practice.\nSoft Data Modeling Via Type 2 Fuzzy Distributions for Corporate Credit Risk Assessment in Commercial Banking SSRN Research Paper, July 2018"}, {"id": 90, "title": "History of film technology", "content": "The history of film technology traces the development of techniques for the recording, construction and presentation of motion pictures. When the film medium came about in the 19th century, there already was a centuries old tradition of screening moving images through shadow play and the magic lantern that were very popular with audiences in many parts of the world. Especially the magic lantern influenced much of the projection technology, exhibition practices and cultural implementation of film. Between 1825 and 1840, the relevant technologies of stroboscopic animation, photography and stereoscopy were introduced. For much of the rest of the century, many engineers and inventors tried to combine all these new technologies and the much older technique of projection to create a complete illusion or a complete documentation of reality. Colour photography was usually included in these ambitions and the introduction of the phonograph in 1877 seemed to promise the addition of synchronized sound recordings. Between 1887 and 1894, the first successful short cinematographic presentations  were established. The biggest popular breakthrough of the technology came in 1895 with the first projected movies that lasted longer than 10 seconds. During the first years after this breakthrough, most motion pictures lasted about 50 seconds, lacked synchronized sound and natural colour, and were mainly exhibited as novelty attractions. In the first decades of the 20th century, movies grew much longer and the medium quickly developed into one of the most important tools of communication and entertainment. The breakthrough of synchronized sound occurred at the end of the 1920s and that of full color motion picture film in the 1930s (although black and white films remained very common for several decades). By the start of the 21st century, physical film stock was being replaced with digital film technologies at both ends of the production chain by digital image sensors and projectors.\n3D film technologies have been around from the beginning, but only became a standard option in most movie theatres during the first decades of the 21st century.\nTelevision, video and video games are closely related technologies, but are traditionally seen as different media. Historically, they were often interpreted as threats to the movie industry that had to be countered with innovations in movie theatre screenings, such as colour, widescreen formats and 3D.\nThe rise of new media and digitization have caused many aspects of different media to overlap with film, resulting in shifts in ideas about the definition of film. To differentiate film from television: a film is usually not transmitted live and is commonly a standalone release, or at least not part of a very regular ongoing schedule. Unlike computer games, a film is rarely interactive. The difference between video and film used to be obvious from the medium and the mechanism used to record and present the images, but both have evolved into digital techniques and few technological differences remain. Regardless of its medium, the term \"film\" mostly refers to relatively long and big productions that can be best enjoyed by large audiences on a large screen in a movie theatre, usually relating a story full of emotions, while the term \"video\" is mostly used for shorter, small-scale productions that seem to be intended for home viewing, or for instructional presentations to smaller groups.\n\n\n== From ancient times to 1894: motion picture technologies before film ==\n\nThe technology of film emerged mostly from developments and achievements in the fields of projection, lenses, photography and optics.\nEarly techniques that involve moving pictures and/or projection include:\n\nShadowgraphy (probably in practice since prehistoric times)\nCamera obscura (a natural phenomenon that has possibly been used as an artistic aid since prehistoric times)\nShadow puppetry (possibly originated around 200 BCE in Central Asia, India, Indonesia or China)\nMagic lantern (developed in the 1650s, preceded by some incidental and/or inferior projectors)\nstroboscopic \"persistence of vision\" animation devices (ph\u00e9nakisticope since 1833, zoetrope since 1866, flip book since 1868)Live projection of moving images occurs in the camera obscura (also known as \"pinhole image\"), a natural phenomenon that may have been used artistically since prehistory. Very occasionally, the camera obscura was used to project theatrical spectacles to entertain small audiences. It is believed that the technique was more commonly used by charlatans, priests and wizards to conjure up magical, religious and necromantic appearances, for instance of spiritual beings like ghosts, gods or demons. The use of a lens in a camera obscura has been dated back to 1550. In the 17th century, the camera obscura was a popular drawing aid and commonly turned into a mobile device, first as tents and not much later as portable wooden boxes. Starting around 1790 with the experiments of Thomas Wedgwood, the box-type camera obscura would be adapted into a photographic still camera by capturing the projected images on plates or sheets that were treated with light-sensitive chemicals.\nAround 1659 the magic lantern was developed by Christiaan Huygens. It projected slides that were usually painted in color on glass. A sketch by Huygens believed to have been made in 1659, indicates that moving images from mechanical slides may have been part of the earliest screenings. Around 1790, the magic lantern became an important instrument in the multi-media phantasmagoria spectacles. Rear projection, animated slides, multiple projectors (superimposition), mobile projectors (on tracks or handheld), projection on smoke, sounds, odors and even electric shocks were used to frighten audiences in dedicated theatres with a convincing ghost horror experience. In the 19th century, several other popular magic lantern techniques were developed, including dissolving views and new types of mechanical slides that created dazzling abstract effects (chromatrope, et cetera) or that depicted, for instance, falling snow or the planets and their moons revolving.\nMany aspects of cinema are closely related to theatre. The term \"photoplay\", commonly used in the early days of cinema, reflects the idea of motion pictures as filmed plays. Technologies used for the theatre, such as stage lighting and all kinds of special effects were automatically adopted for use in front of cameras.\n\n\n=== 1831\u20131848: Early stroboscopic animation ===\nOn 21 January 1831, Michael Faraday introduced an experiment with a rotating cardboard disc with concentric series of apertures that represented cogwheels of different sizes and different amounts of cogs. When looking at a mirror through the holes of one series of apertures, that \"wheel\" seemed to stand still while the others would appear to move around with different velocities or in opposite directions.In January 1833, Joseph Plateau, who had been working on similar experiments for years, published a letter about his recently discovered slotted disc inspired by Faraday's input. The illustrated example of a pirouetting dancer demonstrated that if drawings of successive phases of a scene or object in motion replaced the apertures in Faraday's experiment, they would give the impression of fluent motion when viewed in the mirror through the slots. Plateau's Fantascope became better known as the Ph\u00e9nakisticope and its principle would form the basis for many later motion picture technologies (including cinematography). The possibilities of the Fantascope were limited to the short loops of images that could be drawn or printed on a cardboard circle, but Plateau suggested in a letter to Faraday that the principle might find modified applications in, for instance, phantasmagoria.In May 1833, Simon Stampfer published his Stroboscopische Scheiben that were very similar to Plateau's Fantascope discs. In a booklet issued later that year, he explained the stroboscopic animation principles and stated to have discovered the technique in December 1832 soon after repeating Faraday's experiments. Stampfer also suggested several variations, including a cylinder (similar to the later zoetrope), a long paper or canvas strip looped around two parallel rollers to enable longer theatre scenes (somewhat similar to film) and a theater-like frame (much like the later praxinoscope theatre). Because most movements in nature could not be \"fixed in their individual moments\", Stampfer promoted careful analysis of motion and strict division into regular phases for accurate motion designs. In the April 1833 patent application for the stroboscope discs, Stampfer and publisher Matthias Trentsensky had also suggested stroboscopic presentation of transparent pictures (which were commonly used for magic lantern projection).\nThe earliest known public screening of projected stroboscopic animation was presented by Austrian magician Ludwig D\u00f6bler on 15 January 1847 at the Josephstadt Theatre in Vienna, with his patented Phantaskop. The spectacle was well-received with sold-out shows in several European cities during a tour that lasted until the spring of 1848, although one critic complained about the flickering quality of the stroboscopic images.\n\n\n=== 1849\u20131870: Photography in motion ===\nWhen photography was introduced in 1839, long exposure times seemed to prohibit a combination with stroboscopic animation. In 1849, Joseph Plateau published about improvements for his Fantascope, including a suggestion by Charles Wheatstone to combine it with his invention of the stereoscope and with inspiration from Wheatstone's early stereoscopic photography. Plateau proposed a stop motion technique avant la lettre with stereoscopic recordings of plaster models in different positions. He never executed the elaborate plan, probably because he had turned blind by this time. Stereoscopic photography became very popular in the early 1850s with David Brewster and Jules Duboscq's new portable viewer with lenses.\nStereoscopy inspired hope that photography could also be augmented with colour and motion for a more complete illusion of reality, and several pioneers started to experiment with these goals in mind.\nAntoine Claudet claimed in March 1851 to have exhibited a self portrait that showed 12 sides of his face at the French Industrial Exposition of 1844. These were probably not meant as a representation of different phases of a motion, but as an overview of different camera angles. However, Claudet got interested in animating stereoscopic photography and in November 1851 he claimed to have created a stereo viewer that showed people in motion. It could show a motion of two phases repetitively and Claudet worked on a camera that would record stereoscopic pairs for four different poses (patented in 1853). Although Claudet was not satisfied with the stereoscopic effect in this device, he believed the illusion of motion was successful.On 12 November 1852, Jules Duboscq (who published Plateau's Fantascope in France and also manufactured Wheatstone stereoscopes) added a \"St\u00e9r\u00e9oscope-fantascope, ou B\u00efoscope\" variation to his patent for a stereoscope. Basically a combination of Plateau's Fantascope and the stereoscope, it used two small mirrors in different angles next to each other that reflected stereoscopic image pairs (printed above each other on the stroboscopic disc). Of three planned variations only one was actually produced, without commercial success. The only known extant Bioscope disc has stereoscopic sets of a sequence of photographic images of a machine in action. No original viewing device has resurfaced, but parts of it are known from an illustration in an 1853 advertisement.\n\nOther concepts for stereoscopic viewers include a double-phenakistiscope version that one F. Wenham (possibly Francis Herbert Wenham) in 1895 claimed to have made in 1852, a similar idea and a proto-zoetrope by Johann Nepomuk Czermak published in 1855, an 1858 stroboscopic-stereoscopic projection system by Joseph-Charles d'Almeida that he wanted to combine with the principles of the ph\u00e9nakisticope, and Coleman Sellers II's kinematoscope patented in 1861.On 27 February 1860 Peter Hubert Desvignes received British patent no. 537 for 28 monocular and stereoscopic variations of cylindrical stroboscopic devices. This included a version that used an endless band of pictures running between two spools that was intermittently lit by an electric spark. Desvignes' Mimoscope, similar to Czermak's Stereophoroskop, received an Honourable Mention \"for ingenuity of construction\" at the 1862 International Exhibition in London. It could \"exhibit drawings, models, single or stereoscopic photographs, so as to animate animal movements, or that of machinery, showing various other illusions.\"During the 1850s the first examples of instantaneous photography had appeared, which furthered hope for the possibilities of motion photography. In 1860, John Herschel figured it was or would soon be possible to take ten stereoscopic snap-shots in one second that could then be combined with the phenakisticope. He also had high hopes for the development of colour photography, since he himself had already obtained promising results.On 5 February 1870, Philadelphia engineer Henry Renno Heyl projected three moving picture scenes with his Phasmatrope to 1500 persons at a church entertainment evening at the Philadelphia Academy of Music. Each scene was projected from its own intermittent spur-geared rotating disk with 16 photographic images. The only known extant disk repeated four images of a waltzing couple four times and was screened with an appropriate musical accompaniment by a 40-person orchestra. The presentation of a disk depicting a Brother Jonathan speech (considered lost) was voiced live by an actor.\n\n\n=== 1874: Janssen's photographic revolver ===\n\nJules Janssen developed a large photographic revolver that was used to document the stages of the transit of Venus in 1874 at different geographic points, in an early form of time-lapse photography. Several discs with images have been preserved, but research concluded that all of the known discs contained test recordings of a model in front of a circular light source (or brightly lit surface). The photographs were most likely never intended to be presented as motion pictures, but much later images of one disc were transferred and animated into a very short stop motion film. In 1875 and 1876, Janssen suggested that the revolver could also be used to document animal locomotion, especially birds, since they would be hard to photograph by other means.\n\n\n=== 1876\u20131878: Donisthorpe's early film concepts ===\nOn 9 November 1876, Wordsworth Donisthorpe filed a patent application for \"an apparatus for taking and exhibiting photographs\" that would \"facilitate the taking of a succession of photographic pictures at equal intervals of time, in order to record the changes taking place in or the movements of the object being photographed, and also by means of succession of pictures so taken of any moving object to give to the eye a presentation of the object in continuous movement as it appeared when being photographed\". The camera would have a mechanism to move photographic plates one by one past a lens and shutter to be exposed for the necessary time and then dropped or carried into a receiver. The recorded images would be printed at equal distances apart on a strip of paper. The strip was to be wound between cylinders and carried past the eye of the observer, with a stroboscopic device to expose each picture momentarily. Such photographic strips only became commercially available several years later and Donisthorpe seems to have been unable to produce motion pictures at this stage.Thomas Edison demonstrated his phonograph on 29 November 1877. An article in Scientific American concluded: \"It is already possible, by ingenious optical contrivances, to throw stereoscopic photographs of people on screens in full view of an audience. Add the talking phonograph to counterfeit their voices and it would be difficult to carry the illusion of real presence much further\". Donisthorpe announced in the 24 January 1878 edition of Nature that he would advance that conception: \"By combining the phonograph with the Kinesigraph I will undertake not only to produce a talking picture of Mr. Gladstone which, with motionless lips and unchanged expression shall positively recite his latest anti-Turkish speech in his own voice and tone. Not only this, but the life size photograph itself shall move and gesticulate precisely as he did when making the speech, the words and gestures corresponding as in real life.\" A Dr. Phipson repeated this idea in a French photography magazine, but renamed the device \"Kin\u00e9tiscope\" to reflect the viewing purpose rather than the recording option. This was picked up in the United States and discussed in an interview with Edison later in the year.\n\n\n=== 1878\u20131881: Muybridge and the horse in motion ===\n\nIn June 1878, Eadweard Muybridge made several sequential series of photographs of Leland Stanford's horses in motion with a line of cameras along the race track. Results were soon after published as The Horse in Motion and the achievement received worldwide praise (as well as astonishment about the relatively \"ungraceful\" positions of the legs of the horses). By January 1879 at the latest, people placed Muybridge's sequential pictures in zoetropes to watch them in motion. These were probably the very first viewings of photographic motion pictures that were recorded in real-time. The quality of the small pictures was limited and the figures were mostly seen as silhouettes, in some cases furthered by retouching of the pictures to get rid of photographic irregularities. From 1879 to 1893 Muybridge gave lectures in which he projected silhouettes of his pictures with a device he eventually called the Zoopraxiscope. It used slightly anamorphic pictures traced from his photographs and painted onto glass discs, in an early type of rotoscoping. One disc had anamorphic chronophotographs of the skeleton of a horse posed in the different positions of a stride, as recorded in 1881. Muybridge continued his locomotion studies of different animals and of people until 1886.\n\n\n=== 1882\u20131890s: Marey and chronophotography ===\n\nMany others would follow Muybridge's example and started making sequential photograph series, a method dubbed \"Chronophotographie\" by French scientist \u00c9tienne-Jules Marey.\n\u00c9tienne-Jules Marey had already been researching and graphically recording animal locomotion for years. His book The animal machine, terrestrial and aerial locomotion (French edition 1873, English edition 1874) had inspired Leland Stanford to look for a way to correctly visualize the strides of horses. In 1882, Marey started using his chronophotographic gun for scientific study of animal locomotion. It was capable of taking 12 consecutive frames a second through a single lens.\nMarey had relatively little interest in moving imagery and preferred to study stills. Many of his later chronophotographic studies recorded the sequential images on a single plate, with a newly-developed chronophotographic camera system.\nPrompted by the much publicized successes of Muybridge's photographic sequences and other chronophotographic achievements, inventors in the late 19th century began to realize that the making and showing of photographic 'moving pictures' of a more useful or even indefinite length was a practical possibility. Many people working in the field followed the international developments closely through information in periodicals, patent filings, personal contact with colleagues or by getting their hands on new equipment.\n\n\n=== 1886\u20131895: Ansch\u00fctz' Electrotachyscope ===\n\nBetween 1886 and 1894 Ottomar Ansch\u00fctz developed several versions of his \"elektrische Schnellseher\", or Electrotachyscope. His first machine had 24 chronophotographic 9x12 centimeter glass plate images on a rotating disk, illuminated from behind by synchronized stroboscopic flashes from a Geissler tube. In very successful presentations between 1887 and 1890, four to seven spectators at a time would watch the images on a 12.5 centimeter wide milk-glass screen in a window in a wall of a small darkened room. In 1890, Ansch\u00fctz introduced a long cylindrical automated version with six small screens. In 1891, Siemens & Halske started manufacture of circa 152 copies of Ansch\u00fctz' coin-operated peep-box Electrotachyscope-automat, that was successfully distributed internationally. On 25 November 1894, Ansch\u00fctz introduced his patented projector with two intermittently rotating large disks and continuous light to project images on a 6 by 8 meter screen for 300-seat audiences.\n\n\n== 1884-1900: paper and gelatin films ==\nAnsch\u00fctz' successful presentations and projections of cinematography were technologically based on rotating discs or drums and the repeating loops never contained more than 24 images.\nAlthough Simon Stampfer had already suggested rolls of paper or canvas as a means to present stroboscopic animation in 1833, the idea never caught on. Donisthorpe's 1876 patent had suggested the uses of paper film rolls, but had not resulted in any satisfying recordings or presentations.\nIn 1884, George Eastman patented his ideas for photographic film. The first rolls of Eastman film used gelatin with a paper backing as a flexible support for a light-sensitive layer of chemicals.\nSeveral motion picture pioneers discovered the possibilities to record and present their chronophotographic work on rolls of film. \u00c9mile Reynaud seems to have been the first to present motion pictures through the projection of long strips of transparent images.\n\n\n=== 1886\u20131889: Le Prince's \"animated pictures of natural scenery and life\" ===\n\nThe oldest known functional motion picture cameras were developed by Louis Le Prince in the 1880s. On 2 November 1886, he applied for a US patent for a \"Method of and apparatus for producing animated pictures of natural scenery and life\", which was granted on 10 January 1888. It described a multi-lens camera in detail and also provided some information on a projector, but construction details for the projector were planned to be patented separately. The idea for a two-fold apparatus was also included. The camera could be fitted with three, four, eight, nine, sixteen or more lenses and was illustrated with sixteen lenses in the patent documents. The images were to be recorded as negatives on a pair of sensitive films, stored on two lower drums and mechanically transported without interruption to two upper drums, past lenses and successively operated shutters. The sensitive film could be \"an endless sheet of insoluble gelatine coated with bromide emulsion or any convenient ready-made quick-acting paper, such as Eastman's paper film\". For longer recordings, the receiver could be suited with extra supply boxes after the first boxes were exhausted. With sixteen lenses the camera could record 960 images per minute (16 per second). The projector would have positive transparencies on flexible material, \"such as gelatine, mica, horn &c\" to be \"adjusted on a pair of endless metallic ribbons accurately punctured with small round holes\" and guided past the lenses and shutters by pins on drums. Shorter sequences could be projected from glass discs instead of the films on drums. Le Prince intended the pictures to \"pass through the hands of artists\" to be suitably colored. Despite similarities in terminology in Le Prince and Donisthorpe's patents and the fact that they lived and worked on similar projects in the same town, it remains uncertain whether Le Prince was directly inspired by Donisthorpe's work. Before the application was granted, it was criticized for possible infringements of the patent of Du Mont (1861 and 1865) and Muybridge (1883). In the meantime, Le Prince kept experimenting with different techniques and materials for years and applied for additional patents in many countries.A sequence of 16 frames of recordings of a man walking around a corner has been preserved and seems to have been shot on one glass plate with the 16-lens camera by Le Prince in Paris around August 1887. Only 12 frames contain complete and clear images. On 18 August 1887, while in Paris, Le Prince sent his wife 8 gelatin pictures showing his mechanic running. Le Prince claimed he was able to record 32 images per second and mentioned that he wanted to show her his progress, but was keeping the better results for his own use. He also thought about reverting to his original plan of using \"a special light for each image\". It is believed that the preserved images are from experiments relating to the ones mentioned in the letter, and were shot at the corner of Rue Bochard-de-Saron (where Le Prince was living) and Avenue Trudaine.In May 1887, after much trial and error, Le Prince was able to develop and patent his first single-lensed motion picture camera. He later used it to shoot the film that became known as Roundhay Garden Scene, a short test photographed on October 14, 1888 in Roundhay, Leeds. Le Prince also recorded trams and the horse-drawn and pedestrian traffic on Leeds Bridge, and a few more short films.\nLe Prince used paper-backed gelatin films for the negatives, from which the paper could be peeled off after filming. He also investigated the possibilities of celluloid film and obtained long lengths from the Lumiere factory in Lyon.In 1889, Le Prince developed a single-lensed projector with an arc lamp to project his films onto a white screen.Le Prince didn't publish about his inventions. His wife arranged a demonstration at the Morris\u2013Jumel Mansion in Manhattan in 1890, but Le Prince vanished after boarding a train on 16 September 1890.\n\n\n=== 1888\u20131900: Reynaud's Th\u00e9\u00e2tre Optique ===\n\n\u00c9mile Reynaud already mentioned the possibility of projecting moving images in his 1877 patent application for the Praxinoscope. He presented a praxinoscope projection device at the Soci\u00e9t\u00e9 fran\u00e7aise de photographie on 4 June 1880, but did not market his Praxinoscope \u00e0 projection before 1882. He then further developed the device into the Th\u00e9\u00e2tre Optique, which could project longer sequences with separate backgrounds, and patented the machine in 1888. He created several Pantomimes Illumineuses for this optical theatre by painting colourful images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. The perforated strip of images could be manually transported past a lens and mirror projection system. Many simple actions of the figures, for example one figure hitting another, were repeated several times by rewinding and forwarding certain sequences. Some sound effects were synchronized by electro-magnetic devices, triggered by metal parts on the strip, while a score with some songs was performed live. From 28 October 1892 to March 1900, Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Mus\u00e9e Gr\u00e9vin in Paris.\n\n\n== 1889\u20131896: early celluloid films ==\nCelluloid photographic film was commercially introduced in 1889.\nWilliam Friese-Greene reportedly used oiled paper as a medium for displaying motion pictures in 1885, but by 1887 would have started working with celluloid. In 1889, Friese-Greene took out a patent for a chronophotographic camera. This was capable of taking up to ten photographs per second using perforated celluloid film. A report on the camera was published in the British Photographic News on February 28, 1890. He showed his negative film strips and a projection device at the Photographic Convention held at the Town Hall, Chester, in late June 1890, but was unable to demonstrate the projector, supposedly because it had suffered some derangement during transport. Instead he seems to have used a phenakisticope-based device built for him by John Arthur Roebuck Rudge, which could only show a looping short sequence, possibly photographed as a series of posed shots.Le Prince investigated the possibilities of celluloid film and obtained long lengths from the Lumi\u00e8re factory in Lyon.Donisthorpe's interest in moving pictures was revived when he heard about the successful experiments of Louis Le Prince, who was then working in Donsithorpe's home town of Leeds. In 1889, Donisthorpe took out a patent, jointly with William Carr Crofts, for a camera using celluloid roll film and a projector system; they then made a short film of the bustling traffic in London's Trafalgar Square.The Pleograph, invented by Polish emigre Kazimierz Pr\u00f3szy\u0144ski in 1894 was another early camera. It also doubled as a projector. The apparatus used a rectangle of celluloid with perforations between several parallel rows of images. Using an improved pleograph, Pr\u00f3szynski shot short films showing scenes of life in Warsaw, such as people skating in the park.\n\n\n=== 1891\u20131896: The Kinetoscope ===\n\nSoon after he introduced his phonograph in 1877, Thomas Edison was confronted with ideas to combine it with moving images. He never showed much interest, but eventually he registered a caveat for \"an instrument which does for the Eye what the phonograph does for the Ear\" in October 1888. A meeting with Muybridge for a possible collaboration, in February 1888, seems to have triggered the action. Edison employee W. K. L. Dickson got the job for the development of the technology. Initially, experiments focused on an apparatus that would have 42,000 microscopic pinhole photographs on a celluloid sheet wrapped around a cylinder, similar to phonograph cylinders, to be viewed through a magnifying lens at the end of a conical tube. This concept was abandoned by the end of 1889 and a system based on Ansch\u00fctz's rotating disc Electrotachyscope was investigated for a short while. After Edison had visited \u00c9tienne-Jules Marey, further experiments concentrated on 3/4 inch strips, much like Marey was using in his chronophotography cameras at the time. The Edison company added sprocket holes, possibly inspired by Reynaud's Th\u00e9\u00e2tre Optique. A prototype of the Kinetoscope was demonstrated to a convention of the National Federation of Women's Clubs visiting the Edison studio on 20 May 1891, with the short demo film Dickson Greeting, leading to much press coverage. Later machines would have 35mm films in a coin-operated peep-box. The device was first publicly demonstrated at the Brooklyn Institute of Arts and Sciences on 9 May 1893.\nCommercial exploitation began with a first Kinetoscope parlor opening on 14 April 1894, soon followed by many others across the United States and in Europe. Edison never attempted to patent these instruments outside the US, since they relied so greatly on technologies that were well-known and often patented in other countries.\n\n\n=== 1894-1896: Early film screenings ===\nAfter Ansch\u00fctz's Electrotachyscopes and Edison's Kinetoscopes were presented publicly and the underlying technique was described in magazines, many engineers would try their hand at the projection of moving photographic pictures on a large screen. D\u00f6bler, Muybridge and Reynaud had already been quite successful with their projections of animated pictures, and stereopticons already enabled large photographic projections, but it took a while before anybody managed to publicly reproduce live-action recordings on a large screen. Newspapers and magazines often reported on such developments, but few had the knowledge necessary to properly describe true technological advancements. Many \"inventors\" promoted their results as groundbreaking endeavors even if they had (knowingly or unknowingly) only copied previously existing technology, claimed inventions before they were fully realized, or claimed priority with vague evidence after others had introduced similar advancements. Even if chauvinistic motives were put aside, many publications on the history of film have favored one or just a few inventors and presented those as the very first geniuses to introduce movies. From narrow teleological viewpoints, historians would often ignore pioneering technology if it didn't resemble the movie apparatus that they knew best (for instance the use of stroboscopic flashtubes instead of shutter blades).The Eidoloscope, devised by Eugene Augustin Lauste for the Latham family, was demonstrated for members of the press on April 21, 1895 and opened to the paying public on May 20, in a lower Broadway store with films of the Griffo-Barnett prize boxing fight, taken from Madison Square Garden's roof on May 4. The special Latham loop allowed extended recording and reproduction of moving images. The Griffo-Barnett films lasted 12 minutes and the machine reportedly could work for hours, with up to 40 frames per second.Max and Emil Skladanowsky screened short motion pictures with their \"Bioscop\", a flickerfree duplex construction, starting as part of a popular variety program at the Berlin Wintergarten theatre from 1 to 31 November 1895. On 21 december, their movies were screened as a single event in Hamburg. When they arrived in Paris, they caught the second screening of the Lumi\u00e8re Cin\u00e9matographe on 29 december 1895 and consequently the booked Bioskop screenings at the Folies Berg\u00e8re for january 1896 were cancelled. The brothers took their Bioscop on tour throughout Germany, The Netherlands and Scandinavia, but they struggled financially and quit the Bioscop screenings in 1897. Max Skladanowsky continued with related enterprises, like sales of flip books and amateur cameras.\nIn Lyon, Louis and Auguste Lumi\u00e8re developed the Cin\u00e9matographe, an apparatus that took, printed, and projected film. On 27 december 1895 in Paris, father Antoine Lumi\u00e8re began exhibitions of projected films before the paying public. The Lumi\u00e8re company quickly became Europe's main producers with their actualit\u00e9s like Workers Leaving the Lumi\u00e8re Factory and a few comic vignettes like The Sprinkler Sprinkled (both 1895).\nIn Britain, Robert W. Paul and Birt Acres both independently developed their own systems for projecting a moving image on to a screen. Acres presented his in January 1896, and Paul unveiled his more influential Theatrograph shortly after on 20 February, on exactly the same day the Lumieres' films would first be projected in London. The Theatrograph pioneered the \u2018Maltese cross\u2019 system that drove sprocket rollers to provide intermittent motion. After some demonstrations before scientific groups, he was asked to supply a projector and staff to the Alhambra Music Hall in Leicester Square, and he presented his first theatrical programme on 25 March 1896. His device was the prototype for the modern film projector and was sold across Europe.By 1896, it had dawned on the Edison company that more money could be made by showing motion picture films with a projector to a large audience than exhibiting them in peep-show machines. The Edison company took up a projector developed by Armat and Jenkins, the \"Phantoscope\", which was renamed the Vitascope, and it joined various projecting machines made by other people to show the films made by the Edison company and others in France and the UK.\n\n\n== 1896-1910s: Early movie industry ==\nInitially, a lack of standardization meant that film producers used a variety of different film widths and projection speeds, but after a few years the 35-mm wide Edison film, and the 16-frames-per-second projection speed of the Lumi\u00e8re Cin\u00e9matographe became the standard.By 1898, Georges M\u00e9li\u00e8s was the largest producer of fiction films in France, and from this point onwards his output consisted almost entirely of films featuring trick effects, which were very successful in all markets. The special popularity of his longer films, which were several minutes long from 1899 onwards (while most other films were still only a minute long), led other makers to start producing longer films.\n\n\n== Flicker problem and solutions ==\nThe quality of the experience of films was often troubled by an obvious flicker in the projected image. Many of the systems in use featured intermittent transport of the film strip in order to avoid motion blur, while a shutter blocked projection for each advancement of the film frames. Intermittently blocking the light was also necessary for the stroboscopic effect that was widely known from the ph\u00e9nakisticope and zoetrope. The strain of starting and stopping also often caused damage to the film strip and could cause the system to jam (often with the result of burning the combustible film material as it was exposed to the heat of the lamp for too long). Eventually the solution was found in a three-bladed shutter that not just blocked the light intermittently during film transport, but more often and also during projection. The first three-bladed shutter was developed by Theodor P\u00e4tzold and went in production with Messter in 1902.\nOther systems used a continuous feed of film and projected the images intermittently by reflections from a mirror carousel, similar to the principle applied in Reynaud's Praxinoscope.\n\n\n== Color films ==\n\n\n=== Additive process ===\nThe first person to demonstrate a natural-color motion picture system was British inventor Edward Raymond Turner, who applied for his patent in 1899, received it in 1900, and was able to show promising but very mechanically defective results in 1902.Turner's camera used a rotating disk of three color filters to photograph color separations on one roll of black-and-white film. A red, green or blue-filtered image was recorded on each successive frame of film. The finished film print was projected, three frames at a time, through the corresponding color filters.:\u200a42\u200aWhen Turner died in 1903, his financial backer at that time, pioneering film producer Charles Urban, passed on the development of the process to George Albert Smith, who by 1906 had developed a simplified version that he later named Kinemacolor. The Kinemacolor camera had red and green filters in the apertures of its rotating shutter, so that alternating red-filtered and green-filtered views of the subject were recorded on consecutive frames of the panchromatic black-and-white film. The Kinemacolor projector did the same thing in reverse, projecting the frames of the black-and-white print alternately through the red and green filters in its rotating shutter.\nBoth devices were operated at twice the usual frame rate to reduce the color flicker (technically known as \"color bombardment\") produced by non-simultaneous projection of the two color components, a defect which some viewers barely noticed but which others found obtrusive and headache-inducing. A related defect was the most obvious shortcoming of this process: because the two components had not been photographed at the same time, as pairs of frames, rapidly moving subjects did not adequately match up from one frame to the next when projected on the screen, resulting in color \"fringes\" or in extreme cases vividly colored \"ghosts\". A white dog wagging its tail in front of a dark background could appear to have several tails, variously red, green and white.\nKinemacolor motion pictures were first shown in 1908. The general public first saw Kinemacolor in a program of 21 short films shown on 26 February 1909 at the Palace Theatre in London. On 6 July 1909, George Albert Smith presented a programme of 11 Kinemacolor films at Knowsley Hall before King Edward VII and Queen Alexandra. The films included military subjects as well as a party at Knowsley Hall and the King himself. Edward was pleased with the films.The process was first seen in the US on 11 December 1909, at an exhibition staged by Smith and Urban at Madison Square Garden in New York.The Natural Color Kinematograph Company, founded by Urban in 1909, released the first drama filmed in the Kinemacolor, By The Order of Napoleon and the first newsreel in colour, The Funeral of King Edward VII, both in 1910, and the first feature-length documentary, With Our King and Queen Through India, in 1912. \nKinemacolor projectors were installed in some 300 cinemas in Britain, and 54 dramatic films were produced. Four dramatic short films were made in Kinemacolor in the US in 1912\u20131913, and one in Japan in 1914. Kinemacolor was popular with members of the British royal family, and both Emperor Taish\u014d and Pope Pius X saw Kinemacolor films in 1913. However, the company was not a success, partly due to the expense of installing the special Kinemacolor projectors.\nA variant method was promoted by William Friese-Greene. He called his additive color system \"Biocolour\". It differed from Kinemacolor only in that the need for a filter-equipped projector was eliminated by staining alternate frames of the film itself with red and green dyes. An ordinary projector could therefore be used, if it would bear being cranked at a sufficient rate. Like Kinemacolor, Biocolour suffered from noticeable color flicker and from red and green fringing when the subject was in rapid motion.\n\nIn 1912, French film entrepreneur and inventor L\u00e9on Gaumont unveiled Chronochrome, a full-color additive system. The camera used three lenses with color filters to photograph red, green and blue color components simultaneously on consecutive frames of one strip of 35 mm black-and-white film. The projector had a corresponding triad of lenses. To reduce the strain imposed on the film as the mechanism in each device pulled it down three frames at a time, frame height was reduced from the usual four film perforations to three, resulting in a widescreen image format identical with the modern 16:9 aspect ratio.\nChronochrome's color quality was impressive, as surviving specimens attest, and because the three frames were exposed and projected simultaneously, Kinemacolor's color bombardment and color fringes around moving objects were avoided. However, because the camera's three lenses could not all photograph the scene from exactly the same viewpoint, subjects that were too near the camera would exhibit color fringes if the registration of the three projected images was optimized for the background of the scene, and vice versa. A method of notching the prints to trigger automatic adjustment of the projection optics was invented, but expert supervision of the presentation was still a requisite. Light loss due to the color filters and the constrained dimensions of the projection lenses resulted in an image that was too dim for showing in a large auditorium unless a highly reflective metalized screen or rear-projection onto a translucent screen was used, and either solution created a \"hot spot\" that made the views from the side sections of the auditorium very unsatisfactory. The films were seldom screened outside of Gaumont's own cinemas and the system soon fell into disuse.\n\n\n=== Technicolor ===\nAfter experimenting from 1915 to 1921 with additive color systems that filmed and projected the two color components simultaneously, rather than in rapid alternation (thereby eliminating Kinemacolor's color flicker and false color fringes around rapidly moving objects), the Technicolor Motion Picture Corporation developed a subtractive color print process. As in its last additive system, the camera had only one lens but used a beam splitter that allowed red and green-filtered images to be photographed simultaneously on adjacent frames of a single strip of black-and-white 35 mm film, which ran through the camera at twice the normal rate. By skip-frame printing from the negative, two prints were made, on film stock with half the normal base thickness. They were chemically toned (i.e., the silver particles forming the black-and-white images were proportionally replaced by coloring matter) to colors roughly complementary to the filter colors (red for the green-filtered images and vice versa), as subtractive color reproduction requires. They were then cemented together, base to base, into a single strip of film. No special projection equipment was needed.\n\nThe first publicly shown film using this process was The Toll of the Sea (1922) starring Anna May Wong. Perhaps the most ambitious all-Technicolor feature was The Black Pirate (1926), starring and produced by Douglas Fairbanks.\nIn 1928, the system was refined by the adoption of dye imbibition, which allowed for the transferring of dyes from both color matrices into a single one-sided print, thus eliminating the complication of attaching two prints back to back and allowing multiple prints to be created from a single pair of matrices.Technicolor's system was popular for a number of years, but it was an expensive process: shooting cost three times as much as black-and-white photography and printing costs were also much higher. By 1932, color photography in general had nearly been abandoned by the major studios, but then Technicolor introduced a new process which recorded all three primary colors. Utilizing a dichroic beam splitter sandwiched between two 45-degree prisms in the form of a cube, light from the lens was split into two paths to expose three black-and-white films (two of them in bipack), one each to record the densities for red, green and blue.The three negatives were printed to gelatin matrix films, which were processed with a selectively hardening developer, treated to remove the silver, and hot-washed to leave only a gelatin relief of the images. A receiver print, consisting of a 50% density silver print of the black-and-white negative for the green component, and including the soundtrack and frame lines, was made and treated with dye mordants to aid in the imbibition process (the inclusion of a \"black\" image was discontinued in the early 1940s). The matrix for each color was soaked in its complementary dye (yellow, cyan, or magenta), then each in succession was brought into high-pressure contact with the receiver, which imbibed and held the dyes, thus reproducing a nearly complete spectrum of color, unlike previous two-color processes. The first animation film to use the three-color (also called three-strip) system was Walt Disney's Flowers and Trees (1932), which introduced it to an enthusiastic public. The first short live-action film was La Cucaracha (1934), and the first all-color feature in \"New Technicolor\" was Becky Sharp (1935).The proliferation of television in the early 1950s contributed to a heavy mid-century push for color within the film industry. In 1947, only 12 percent of American films were made in color. By 1954, that number had risen to over 50 percent. The color boom was aided by the breakup of Technicolor's near-monopoly on the medium. The last stand of black-and-white films made by or released through the major Hollywood studios came in the mid-1960s, after which the use of color film for all productions was effectively mandatory and exceptions were only rarely and grudgingly made.\n\n\n== Synchronized sound ==\n\nIn July 1879, Fairman Rogers published an article about his and Thomas Eakins' experiments with Muybridge's horse pictures in a special zoetrope. He mentioned working on the addition of a system which would give a sharp tap of a small hammer when each of the horse's feet appeared to strike the ground.Edison's phonograph had inspired more interest in recording motion pictures to accompany the new medium, but when motion picture systems were developed, synchronization turned out to be much more of a technical challenge than imagined. Edison started the exploitation of the Kinetoscope without the expected accompaniment of sound. His 1895 Kinetophone version provided earphones for sound coming from a phonograph hidden in the same cabinet, but without serious effort to synchronize the sound to the images.\nHowever, there was still significant interest in motion pictures for films to be produced without sound. To enhance the viewers' experience, silent films were commonly accompanied by live musicians, sometimes sound effects and/or commentary spoken by the showman or projectionist. In most countries, intertitles came to be used to provide dialogue and narration for the film.\nExperimentation with sound film technology, both for recording and playback, was virtually constant throughout the silent era, but the twin problems of accurate synchronization and sufficient amplification had been difficult to overcome (Eyman, 1997). In 1926, Hollywood studio Warner Bros. introduced the \"Vitaphone\" system, producing short films of live entertainment acts and public figures and adding recorded sound effects and orchestral scores to some of its major features.\nDuring late 1927, Warners released The Jazz Singer, which was mostly silent but contained what is generally regarded as the first synchronized dialogue (and singing) in a feature film. The early sound-on-disc processes such as Vitaphone were soon superseded by sound-on-film methods such as Fox Movietone, DeForest Phonofilm, and RCA Photophone. The trend convinced the largely reluctant industrialists that \"talking pictures\", or \"talkies\", were the future. A lot of attempts were made before the success of The Jazz Singer, that can be seen in the List of film sound systems.\nMany improvements in sound recording and reproduction have been developed for theatrical movies, including stereophonic sound and surround sound (as Fantasound for Disney's Fantasia (1940)).\n\n\n== 3D ==\n\nThe popularity of stereoscopic photography in the 1850s boosted interest in the creation of a medium that would be able to give a more complete illusion of reality through the addition of motion (and colour). Although stereoscopy was part of almost every attempt to record and/or display photography in motion until the mid 1880s, the influential early practical results were not stereoscopic.\nSeveral cinematic 3D systems were developed and sometimes even reached theatres throughout the first 50 years after the breakthrough of cinema, but none had much impact until anaglyphic films became popular for a while in the 1950s. 3D cinema technology originally began with a method of utilizing two cameras filming the same thing. The content was then placed over each other, and while wearing light filtering glasses, the images would appear to project itself offscreen. Interest in theatrical 3D movies dwindled during the following decades, but they started to get exploited as (part of) special attractions, such as 4D simulator rides and Imax theatres.\nIn the early 2000s, digital cinema began to takeover and polarized 3D movies became popular. Movies were no longer created on film. They were no longer shipped to theaters film canisters, spliced together and threaded through the projector, creating the movies we watched on screen. They were digitized, delivered on hard drives or via satellite. During this time, 3D films peaked in popularity. Avatar was one of the first major 3D motion pictures that changed 3D features as we know it. Avatar focused on 3D CG, using motion capture to make these CG characters look real. This film used real characters alongside of its CG characters, developing this incredible world, creating a visually stunning 3D spectacle. 3D films are often thought of as immersive experiences, allowing the audience to feel like they can reach out and grab what's displayed in front of them. With the rise of this new technology, most films were releasing into cinemas in both standard 2D and 3D options. Cinemas all over began turning over to completely digital, so that 3D content could be offered in multiple theaters. In order for the experience to feel immersive, audience members need to feel like they existed in this world they are overcome by. Being present in this fantasy world is not only created by 3D technology, but also how well people relate to the story. 3D films certainly aren\u2019t for everyone. Some people suffer from motion sickness and headaches while watching 3D features. It\u2019s not uncommon for those who suffer from migraines to trigger a headache while experiencing a 3D movie.Creating immersive experiences doesn\u2019t always come easy. 3D films aren\u2019t all as captivating as Avatar or Gravity, and it has become increasingly difficult to create such visually stunning films that take audiences on a journey throughout. The 3D effects became less of an immersive experience and left audiences questioning whether or not the film was 3D enough. Though Hollywood marketed this as must see attractions, the content became less planned out and more forced, with movies converting to 3D after they were filmed.\nWhile it isn\u2019t always easy for live action 3D films to succeed, 3D animation is always able to take different leaps with this technology. Animation can look really good and, at the same time, show the audiences new visual technologies. They can take different risks, utilizing their artistic skills and computer generated worlds to create 3D spectacles for both children and adults.\n3D films aren\u2019t always made for 3D leaving some to be converted after the fact. When that happens, the image displayed on screen can be dull and not nearly as bright as a film should be. A normal reduction in light should be expected, as a 3D lens covers the lens of the projector, but also moviegoers must wear 3D glasses. Knowing that the loss of light will occur, it\u2019s important for filmmakers to know whether or not they want the film to be offered in 3D, so that they can make the necessary changes, to avoid converting it later.3D features require the use of polarized 3D glasses so that viewers can see the images on screen in 3D. Many look at the glasses as more of a hassle. It\u2019s not the easiest for those with prescription glasses to wear them and also proves to be difficult for young children to keep them on. Simply touching the lens of the glasses can mess them up to the point that you can no longer see the 3D images. Automultiscopic display technology is a new film technology looking to enhance the moviegoing experience. Glasses would no longer be required, but instead allowing multiple images from various angles to be displayed on screen. With this new technology, it would be imperative that people sit a certain distance away from the screen, which could present many challenges. Either way, new developments in 3D film technology could have us sitting in the theatre, experiencing 3D, without the need for 3D glasses.\nWhile 3D features aren\u2019t nearly as popular as they once were, they still are immersive experiences leaving the guests in awe of the feature in front of them. RealD 3D movies bring a realism to films making it seem as though you are in fact in the movie. Filmmakers and cinemas alike are far from giving up on this technology, only looking to figure out new ways to create experiences leaving audience members in awe. Although audience interest declined after a few years, many cinemas continue to offer 3D screenings. Recently technology has changed, giving moviegoers the opportunity to watch screenX films, where audience members are surrounded by the image. 4DX is also a popular new technology, attempting to provide incredibly immersive experiences, allowing audience members to smell scents, feel rain and wind, experience vibrations and see a cars headlights flash across their face as though they are driving. Motion pictures continue to adapt to the ever changing technologies.\n\n\n== 4D ==\n\nIn 1962, Morton Heilig received a patent for his Sensorama simulator. He built a prototype with a motional chair and 5 different stereoscopic films, while fans and odor emitters delivered additional sensations. Heilig couldn't find funding to exploit or further develop this project\n4D film has become a regular theme park attraction since the 1980s and became a common screening option for high-budget action movies in an increasing amount of theatres since the introduction of the 4DX technology in 2009.\n\n\n== Interactive film and virtual reality ==\n\nCoin-operated movie viewers like the Electrotachyscope, the Kinetoscope, and the Mutoscope were closely related to other amusement arcade machines that came about at the turn of the 20th century, but the machines in the arcades that would remain popular were interactive games and motion pictures proved to be much more popular in theatres. Although cinema and interactive (electronic-mechanical) games would mostly exist as separate media throughout the 20th century, there were occasional systems that would combine both and the digital revolution ensured that video games could start to appear cinematic. Attempts to exploit the idea of theatrical interactive cinema, with for instance I'm Your Man (1992), were less successful. Virtual reality is sometimes regarded as a technique that enables a more effective combination of interactivity and cinema.\nThe basic idea of interactive film arose soon after the introduction of the cinematograph. A technology for a shooting gallery with magic lantern or film projections was patented in 1901. An early successful example of cinematic shooting galleries appeared in the UK around 1912 under the title Life Targets. This type of game, often including footage of safari animals, was popular in Greta Britain for a short while.Auto Test (1954) was a operated driving training arcade game in which a player has to match his actions with the gas medal, brake and steering action to POV film footage of a car ride projected on a small screen.\nThe early virtual reality system The Sword of Damocles was created in 1968, with the perspective of the stereoscopic view of cg wireframe rooms depending on mechanical head tracking.\nIn the 1970s and 1980s, virtual reality (VR) was mainly applied in industrial, medical and military simulations. During the 1990s, VR head sets became commercially available, mainly intended for video games. VR has increasingly been used for cinematic experiences, with for instance its own section at the Venice Film Festival since 2017.\n\n\n== Widescreen and 360\u00b0 film ==\n\nSince several people developed film systems independently, frames sizes and projection ratios varied (although 35mm movie film had become standard early on). The 1.37:1 Academy ratio became a standard in the 1930s.\nAfter the advent of television, widescreen movies became one of the more popular solutions to keep theatrical movies more interesting since the 1950s. Anamorphic formats were developed to enable filming for widescreen formats on standard 35mm film.\nScreen projections encircling the audience were developed by Disney as a Disneyland attraction called Circarama in 1955 and was replaced by Circle-Vision 360\u00b0 a few years later.\n360-degree video has become common since YouTube and Facebook added support for publishing and viewing on their platforms in 2015.\n\n\n== Digital film ==\n\nDigital cinematography, the process of capturing film images using digital image sensors rather than through film stock, has largely replaced analog film technology. As digital technology has improved in recent years, this practice has become dominant. Since the mid 2010s, most of the movies across the world are captured as well as distributed digitally.Many vendors have brought products to market, including traditional film camera vendors like Arri and Panavision, as well as new vendors like RED, Blackmagic, Silicon Imaging, Vision Research and companies which have traditionally focused on consumer and broadcast video equipment, like Sony, GoPro, and Panasonic.\nCurrent digital film cameras with 4k output are approximately equal to 35mm film in their resolution and dynamic range capacity, however, digital film still has a slightly different look to analog film. Some filmmakers and photographers still prefer to use analogue film to achieve the desired results.\nDigital cinema, the use of digital technology to distribute or project motion pictures has also largely replaced the historical use of reels of motion picture film, such as 35 mm film. Whereas traditional film reels had to be shipped to movie theaters,  a digital movie can be distributed to cinemas in a number of ways: over the Internet or dedicated satellite links or by sending hard drives or optical discs such as Blu-ray discs. Digital movies are projected using a digital projector instead of a conventional film projector. Digital cinema is distinct from high-definition television and is not dependent on using television or high-definition video standards, aspect ratios, or frame rates. In digital cinema, resolutions are represented by the horizontal pixel count, usually 2K (2048\u00d71080 or 2.2 megapixels) or 4K (4096\u00d72160 or 8.8 megapixels). As digital cinema technology improved in the early 2010s, most of the theaters across the world converted to digital.\n\n\n== See also ==\nList of cinematic firsts\nList of color film systems\nList of motion picture film formats\nNewsreel\nSilent film\nSound film\nThe Story of Film: An Odyssey, 2011 documentary\n\n\n== References ==\n\n\n== Further reading ==\nMunslow., Alun (December 2007). \"Film and history: Robert A. Rosenstone and History on Film/Film on History\". Rethinking History. 4 (11): 565\u2013575. doi:10.1080/13642520701652103. S2CID 145006358.\nAbel, Richard. The Cine Goes to Town: French Cinema 1896\u20131914University of California Press, 1998.\nAcker, Ally. Reel Women: Pioneers of the Cinema, 1896 to the Present. London: B.T. Batsford, 1991.\nBarnes, John. The Cinema in England: 1894\u20131901 (5 Volumes) University of Exeter Press, 1997.\nBasten, Fred E. Glorious Technicolor: The Movies' Magic Rainbow. AS Barnes & Company, 1980.\nBowser, Eileen. The Transformation of Cinema 1907\u20131915 (History of the American Cinema, Vol. 2) Charles Scribner's Sons, 1990.\nRawlence, Christopher (1990). The Missing Reel: The Untold Story of the Lost Inventor of Moving Pictures. Charles Atheneum. ISBN 978-0689120688.\nCousins, Mark. The Story of Film: A Worldwide History, New York: Thunder's Mouth press, 2006.\nDixon, Wheeler Winston and Gwendolyn Audrey Foster. A Short History of Film, 2nd edition. New Brunswick: Rutgers University Press, 2013.\nKing, Geoff. New Hollywood Cinema: An Introduction. New York: Columbia University Press, 2002.\nMerritt, Greg. Celluloid Mavericks: A History of American Independent Film. Thunder's Mouth Press, 2001.\nMusser, Charles (1990). The Emergence of Cinema: The American Screen to 1907. New York: Charles Scribner's Sons. ISBN 0-684-18413-3.\nNowell-Smith, Geoffrey, ed. The Oxford History of World Cinema. Oxford University Press, 1999.\nParkinson, David. History of Film. New York: Thames & Hudson, 1995. ISBN 0-500-20277-X\nRocchio, Vincent F. Reel Racism. Confronting Hollywood's Construction of Afro-American Culture. Westview Press, 2000.\nSalt, Barry. Film Style and Technology: History and Analysis 2nd Ed. Starword, 1992.\nSalt, Barry. Moving Into Pictures Starword, 2001.\nUsai, P.C. & Codelli, L. (editors) Before Caligari: German Cinema, 1895\u20131920 Edizioni Biblioteca dell'Immagine, 1990.\n\n\n== External links ==\n\nView inside an ancient film camera *popup warning, possible vanity site*\nHistory exhibit of filmmaking in Florida, presented by the State Archives of Florida\nAmerican Cinematographer \u2013 January, 1930, THE EARLY HISTORY OF WIDE FILMS\nHistory of Film Formats\nTechnicolor History\nWhat is a Camera Obscura?\nFilm Sound History at FilmSound.org\nAn Introduction to Early cinema\nList of Early Sound Films 1894\u20131929 at Silent Era website\nOfficial Web Site of Film Historian/Oral Historian Scott Feinberg\nReality Film\nFilm History by Decade *popup warning*\nProject \"Westphalian History in the film\"\nCinema: From 1890 To Now"}, {"id": 91, "title": "History of film", "content": "The history of film chronicles the development of a visual art form created using film technologies that began in the late 19th century. The advent of film as an artistic medium is not clearly defined. However, the commercial, public screening of ten of the Lumi\u00e8re brothers' short films in Paris on 28 December 1895, can be regarded as the breakthrough of projected cinematographic motion pictures. There had been earlier cinematographic results and screenings by others, like the Skladanowsky brothers, who used their self-made Bioscop to display the first moving picture show to a paying audience on 1 November 1895, in Berlin, but they had neither the quality, financial backing, stamina, or luck to find the momentum that propelled the cin\u00e9matographe Lumi\u00e8re into worldwide success. Those earliest films were in black and white, under a minute long, without recorded sound, and consisted of a single shot from a steady camera. The first decade of motion pictures saw film move from a novelty to an established mass entertainment industry, with film production companies and studios established all over the world.\nConventions toward a general cinematic language also developed, with film editing camera movements and other cinematic techniques contributing specific roles in the narrative of films.\nPopular new media, including television (mainstream since the 1950s), home video (mainstream since the 1980s), and the internet (mainstream since the 1990s), influenced the distribution and consumption of films. Film production usually responded with content to fit the new media, and with technical innovations (including widescreen (mainstream since the 1950s), 3D, and 4D film) and more spectacular films to keep theatrical screenings attractive.\nSystems that were cheaper and more easily handled (including 8mm film, video, and smartphone cameras) allowed for an increasing number of people to create films of varying qualities, for any purpose (including home movies and video art). The technical quality was usually lower than that of professional movies, but improved with digital video and affordable, high-quality digital cameras.\nImproving over time, digital production methods became more and more popular during the 1990s, resulting in increasingly realistic visual effects and popular feature-length computer animations.\nVarious film genres emerged and enjoyed variable degrees of success over time, with huge differences among, for instance, horror.\n\n\n== Precursors ==\n\nThe use of film as an art form traces its origins to several earlier traditions in the arts such as (oral) storytelling, literature, theatre and visual arts. Cantastoria and similar ancient traditions combined storytelling with series of images that were shown or indicated one after the other. Predecessors to film that had already used light and shadows to create art before the advent of modern film technology include shadowgraphy, shadow puppetry, camera obscura, and the magic lantern.\nShadowgraphy and shadow puppetry represent early examples of the intent to use moving imagery for entertainment and storytelling. Thought to have originated in the Far East, the art form used shadows cast by hands or objects to assist in the creation of narratives. Shadow puppetry enjoyed popularity for centuries around Asia, notably in Java, and eventually spread to Europe during the Age of Enlightenment.By the 16th century, entertainers often conjured images of ghostly apparitions, using techniques such as camera obscura and other forms of projection to enhance their performances. Magic lantern shows developed in the latter half of the 17th century seem to have continued this tradition with images of death, monsters and other scary figures. Around 1790, this practice was developed into a type of multimedia ghost show known as phantasmagoria. These popular shows entertained audiences using mechanical slides, rear projection, mobile projectors, superimposition, dissolves, live actors, smoke (on which projections may have been cast), odors, sounds and even electric shocks. While many first magic lantern shows were intended to frighten viewers, advances by projectionists allowed for creative and even educational storytelling that could appeal to wider family audiences. Newly pioneered techniques such as the use of dissolving views and the chromatrope allowed for smoother transitions between two projected images and aided in providing stronger narratives.In 1833, scientific study of a stroboscopic illusion in spoked wheels by Joseph Plateau, Michael Faraday and Simon Stampfer led to the invention of the Fantascope, also known as the stroboscopic disk or the phenakistiscope, which was popular in several European countries for a while. Plateau thought it could be further developed for use in phantasmagoria and Stampfer imagined a system for longer scenes with strips on rollers, as well as a transparent version (probably intended for projection). Plateau, Charles Wheatstone, Antoine Claudet and others tried to combine the technique with the stereoscope (introduced in 1838) and photography (introduced in 1839) for a more complete illusion of reality, but for decades such experiments were mostly hindered by the need for long exposure times, with motion blur around objects that moved while the reflected light fell on the photo-sensitive chemicals. A few people managed to get decent results from stop motion techniques, but these were only very rarely marketed and no form of animated photography had much cultural impact before the advent of chronophotography.\nMost early photographic sequences, known as chronophotography, were not initially intended to be viewed in motion and were typically presented as a serious, even scientific, method of studying locomotion. The sequences almost exclusively involved humans or animals performing a simple movement in front of the camera. Starting in 1878 with the publication of The Horse in Motion cabinet cards, photographer Eadweard Muybridge began making hundreds of chronophotographic studies of the motion of animals and humans in real-time. He was soon followed by other chronophotographers like \u00c9tienne-Jules Marey, Georges Demen\u00ff, Albert Londe and Ottomar Ansch\u00fctz. In 1879, Muybridge started lecturing on animal locomotion and used his Zoopraxiscope to project animations of the contours of his recordings, traced onto glass discs.In 1887, the German inventor and photographer Ottomar Ansch\u00fctz started presenting his chronophotographic recordings in motion, using a device he called the Elektrischen Schnellseher (also known as the Electrotachyscope), which displayed short loops on a small milk glass screen. By 1891, he had started mass production of a more economical, coin-operated peep-box viewing device of the same name that was exhibited at international exhibitions and fairs. Some machines were installed for longer periods, including some at The Crystal Palace in London, and in several U.S. stores. Shifting the focus of the medium from technical and scientific interest in motion to entertainment for the masses, he recorded wrestlers, dancers, acrobats, and scenes of everyday life. Nearly 34,000 people paid to see his shows at the Berlin Exhibition Park in summer 1892. Others saw it in London or at the 1893 Chicago World's Fair.Though little evidence remains for most of these recordings, some scenes probably depicted staged comical scenes. Extant records suggest some of his output directly influenced later works by the Edison Company, such as the 1894 film Fred Ott's Sneeze.Advances towards motion picture projection technologies were based on the popularity of magic lanterns, chronophotographic demonstrations, and other closely related forms of projected entertainment such as illustrated songs. From October 1892 to March 1900, inventor \u00c9mile Reynaud exhibited his Th\u00e9\u00e2tre Optique (\"Optical Theatre\") film system at the Mus\u00e9e Gr\u00e9vin in Paris. Reynaud's device, which projected a series of animated stories such as Pauvre Pierrot and Autour d'une cabine, was displayed to over 500,000 visitors over the course of 12,800 shows. On 25, 29 and 30 November 1894, Ottomar Ansch\u00fctz projected moving images from Electrotachyscope discs on a large screen in the darkened Grand Auditorium of a Post Office Building in Berlin. From 22 February to 30 March 1895, a commercial 1.5-hour program of 40 different scenes was screened for audiences of 300 people at the old Reichstag and received circa 4,000 visitors.\n\n\n== Novelty era (1890s \u2013 early 1900s) ==\n\n\n=== Advances towards projection ===\nIn June 1889, American inventor Thomas Edison assigned a lab assistant, William Kennedy Dickson, to help develop a device that could produce visuals to accompany the sounds produced from the phonograph. Building upon previous machines by Muybridge, Marey, Ansch\u00fctz and others, Dickson and his team created the Kinetoscope peep-box viewer, with celluloid loops containing about half a minute of motion picture entertainment. After an early preview on 20 May 1891, Edison introduced the machine in 1893. Many of the movies presented on the Kinetoscope showcased well-known vaudeville acts performing in Edison's Black Maria studio. The Kinetoscope quickly became a global sensation with multiple viewing parlors across major cities by 1895. As the initial novelty of the images wore off, the Edison Company was slow to diversify their repertoire of films and waning public interest caused business to slow by Spring 1895. To remedy declining profits, experiments, such as The Dickson Experimental Sound Film, were conducted in an attempt to achieve the device's original goal of providing visual accompaniment for sound recordings. Limitations in syncing the sound to the visuals, however, prevented widespread application. During that same period, inventors began advancing technologies towards film projection that would eventually overtake Edison's peep-box format.Multiple inventors including Wordsworth Donisthorpe, Louis Le Prince, and William Friese-Greene experimented with prototype motion picture projection devices in the pursuit of creating and displaying films. The scenes in these experiments were usually filmed with family, friends or passing traffic as the moving subjects. Most of these films never passed the experimental stage and their efforts garnered little public attention until after cinema had become successful.\nIn the latter half of 1895, brothers Auguste and Louis Lumi\u00e8re filmed a number of short scenes with their invention, the Cin\u00e9matographe. On 28 December 1895, the brothers gave their first commercial screening in Paris (though evidence exists of demonstrations of the device to small audiences as early as October 1895). The screening consisted of ten films and lasted roughly 20 minutes. The program consisted mainly of actuality films such as Workers Leaving the Lumi\u00e8re Factory as truthful documents of the world, but the show also included the staged comedy L'Arroseur Arros\u00e9. The most advanced demonstration of film projection thus far, the Cin\u00e9matographe was an instant success, bringing in an average of 2,500 to 3,000 francs daily by the end of January 1896. Following the first screening, the order and selection of films were changed often.The Lumi\u00e8re brothers' primary business interests were in selling cameras and film equipment to exhibitors, not the actual production of films. Despite this, filmmakers across the world were inspired by the potential of film as exhibitors brought their shows to new countries. This era of filmmaking, dubbed by film historian Tom Gunning as \"the cinema of attractions\", offered a relatively cheap and simple way of providing entertainment to the masses. Rather than focusing on stories, Gunning argues, filmmakers mainly relied on the ability to delight audiences through the \"illusory power\" of viewing sequences in motion, much as they did in the Kinetoscope era that preceded it. Despite this, early experimentation with fiction filmmaking (both in actuality film and other genres) did occur. Films were mostly screened inside temporary storefront spaces, in tents of traveling exhibitors at fairs, or as \"dumb\" acts in vaudeville programs. During this period, before the process of post-production was clearly defined, exhibitors were allowed to exercise their creative freedom in their presentations. To enhance the viewers' experience, some showings were accompanied by live musicians in an orchestra, a theatre organ, live sound effects and commentary spoken by the showman or projectionist.Experiments in film editing, special effects, narrative construction, and camera movement during this period by filmmakers in France, England, and the United States became influential in establishing an identity for film going forward. At both the Edison and Lumi\u00e8re studios, loose narratives such as the 1895 Edison film, Washday Troubles, established short relationship dynamics and simple storylines. In 1896, La F\u00e9e aux Choux (The Fairy of the Cabbages) was first released. Directed and edited by Alice Guy, the story is arguably the earliest narrative film in history, as well as the first film to be directed by a woman. That same year, the Edison Manufacturing Company released The May Irwin Kiss in May to widespread financial success. The film, which featured the first kiss in cinematic history, led to the earliest known calls for film censorship.Another early film producer was Australia's Limelight Department. Commencing in 1898, it was operated by The Salvation Army in Melbourne, Australia. The Limelight Department produced evangelistic material for use by the Salvation Army, including lantern slides as early as 1891, as well as private and government contracts. In its nineteen years of operation, the Limelight Department produced about 300 films of various lengths, making it one of largest film producers of its time. The Limelight Department made a 1904 film by Joseph Perry called Bushranging in North Queensland, which is believed to be the first ever film about bushrangers.\n\n\n=== Proliferation of actualities and newsreels ===\n\nIn its infancy, film was rarely recognized as an art form by presenters or audiences. Regarded by the upper class as a \"vulgar\" and \"lowbrow\" form of cheap entertainment, films largely appealed to the working class and were often too short to hold any strong narrative potential. Initial advertisements promoted the technologies used to screen films rather than the films themselves. As the devices became more familiar to audiences, their potential for capturing and recreating events was exploited primarily in the form of newsreels and actualities. During the creation of these films, cinematographers often drew upon aesthetic values established by past art forms such as framing and the intentional placement of the camera in the composition of their image. In a 1955 article for The Quarterly of Film Radio and television, film producer and historian Kenneth Macgowan asserted that the intentional staging and recreation of events for newsreels \"brought storytelling to the screen\".With the advertisement of film technologies over content, actualities initially began as a \"series of views\" that often contained shots of beautiful and lively places or performance acts. Following the success of their 1895 screening, The Lumi\u00e8re brothers established a company and sent cameramen across the world to capture new subjects for presentation. After the cinematographer shot scenes, they often exhibited their recordings locally and then sent them back to the company factory in Lyon to make duplicate prints for sale to whoever wanted them. In the process of filming actualities, especially those of real events, filmmakers discovered and experimented with multiple camera techniques to accommodate for their unpredictable nature. Due to the short length (often only one shot) of many actualities, catalogue records indicate that production companies marketed to exhibitors by promoting multiple actualities with related subject matters that could be purchased to complement each other. Exhibitors who bought the films often presented them in a program and would provide spoken accompaniment to explain the action on screen to audiences.The first paying audience for a motion picture gathered at Madison Square Garden to see a staged actuality that purported itself to be a boxing fight filmed by Woodville Latham using a device called the Eidoloscope on May 20, 1895. Commissioned by Latham, the French inventor Eugene Augustin Lauste created the device with additional expertise from William Kennedy Dickson and crafted a mechanism that came to be known as the Latham loop, which allowed for longer continuous runtimes and was less abrasive on the celluloid film.In subsequent years, screenings of actualities and newsreels proved to be profitable. In 1897, The Corbett-Fitzsimmons Fight was released. The film was a complete recording of a heavyweight world championship boxing match at Carson City, Nevada. It generated more income in box office than in live gate receipts and was the longest film produced at the time. Audiences had probably been drawn to the Corbett-Fitzsimmons film en masse because James J. Corbett (a.k.a. Gentleman Jim) had become a matinee idol since he had played a fictionalized version of himself in a stage play.From 1910 on, regular newsreels were exhibited and soon became a popular way of discovering the news before the advent of television \u2013  the British Antarctic Expedition to the South Pole was filmed for the newsreels as were the suffragette demonstrations that were happening at the same time. F. Percy Smith was an early nature documentary pioneer working for Charles Urban when he pioneered the use of time lapse and micro cinematography in his 1910 documentary on the growth of flowers.\n\n\n=== Experimentation with narrative filmmaking ===\n\n\n==== France: Georges M\u00e9li\u00e8s, Path\u00e9 Fr\u00e8res, Gaumont Film Company ====\nFollowing the successful exhibition of the Cin\u00e9matographe, development of a motion picture industry rapidly accelerated in France. Multiple filmmakers experimented with the technology as they worked to attain the same success that the Lumi\u00e8re brothers had with their screening. These filmmakers established new companies such as the Star Film Company, Path\u00e9 Fr\u00e8res, and the Gaumont Film Company.\nThe most widely cited progenitor of narrative filmmaking is the French filmmaker, Georges M\u00e9li\u00e8s. M\u00e9li\u00e8s was an illusionist who had previously used magic lantern projections to enhance his magic act. In 1895, M\u00e9li\u00e8s attended the demonstration of the Cinematographe and recognized the potential of the device to aid his act. He attempted to buy a device from the Lumi\u00e8re brothers, but they refused. Months later, he bought a camera from Robert W. Paul and began experiments with the device by creating actualities. During this period of experimentation, M\u00e9li\u00e8s discovered and implemented various special effects including the stop trick, the multiple exposure, and the use of dissolves in his films. At the end of 1896, M\u00e9li\u00e8s established the Star Film Company and started producing, directing, and distributing a body of work that would eventually contain over 500 short films. Recognizing the narrative potential afforded by combining his theater background with the newly discovered effects for the camera, M\u00e9li\u00e8s designed an elaborate stage that contained trapdoors and a fly system. The stage construction and editing techniques allowed for the development of more complex stories, such as the 1896 film, Le Manoir du Diable (The House of the Devil), regarded as a first in the horror film genre, and the 1899 film Cendrillon (Cinderella). In M\u00e9li\u00e8s' films, he based the placement of the camera on the theatrical construct of proscenium framing, the metaphorical plane or fourth wall that divides the actors and the audience. Throughout his career, M\u00e9li\u00e8s consistently placed the camera in a fixed position and eventually fell out of favor with audiences as other filmmakers experimented with more complex and creative techniques. M\u00e9li\u00e8s is most widely known today for his 1902 film, Le Voyage Dans La Lune (A Trip to the Moon), where he used his expertise in effects and narrative construction to create the first science fiction film.In 1900, Charles Path\u00e9 began film production under the Path\u00e9-Fr\u00e8res brand, with Ferdinand Zecca hired to lead the creative process. Prior to this focus on production, Path\u00e9 had become involved with the industry by exhibiting and selling what were likely counterfeit versions of the Kinetoscope in his phonograph shop. With the creative leadership of Zecca and the capability to mass-produce copies of the films through a partnership with a French toolmaking company, Charles Path\u00e9 sought to make Path\u00e9-Fr\u00e8res the leading film producer in the country. Within the next few years, Path\u00e9-Fr\u00e8res became the largest film studio in the world, with satellite offices in major cities and an expanding selection of films available for presentation. The company's films were varied in content, with directors specializing in various genres for fairground presentations throughout the early 1900s.The Gaumont Film Company was the main regional rival of Path\u00e9-Fr\u00e8res. Founded in 1895 by L\u00e9on Gaumont, the firm initially sold photographic equipment and began film production in 1897, under the direction of Alice Guy, the industry's first female director. Her earlier films share many characteristics and themes with her contemporary competitors, such as the Lumi\u00e8res and M\u00e9li\u00e8s. She explored dance and travel films, often combining the two, such as Le Bol\u00e9ro performed by Miss Saharet (1905) and Tango (1905). Many of Guy's early dance films were popular in music-hall attractions such as the serpentine dance films \u2013 also a staple of the Lumi\u00e8res and Thomas Edison film catalogs. In 1906, she made The Life of Christ, a big-budget production for the time, which included 300 extras.\n\n\n==== England: Robert W Paul, Cecil Hepworth, The Brighton School ====\nBoth Cecil Hepworth and Robert W. Paul experimented with the use of different camera techniques in their films. Paul's 'Cinematograph Camera No. 1' of 1895 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times, thereby creating multiple exposures. This technique was first used in his 1901 film Scrooge, or, Marley's Ghost. Both filmmakers experimented with the speeds of the camera to generate new effects. Paul shot scenes from On a Runaway Motor Car through Piccadilly Circus (1899) by cranking the camera apparatus very slowly. When the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Hepworth used the opposite effect in The Indian Chief and the Seidlitz Powder (1901). The Chief's movements are sped up by cranking the camera much faster than 16 frames per second, producing what modern audiences would call a \"slow motion\" effect.The first films to move from single shots to successive scenes began around the turn of the 20th century. Due to the loss of many early films, a conclusive shift from static singular shots to a series of scenes can be hard to determine. Despite these limitations, Michael Brooke of the British Film Institute attributes real film continuity, involving action moving from one sequence into another, to Robert W. Paul's 1898 film, Come Along, Do!. Only a still from the second shot remains extant today. Released in 1901, the British film Attack on a China Mission was one of the first films to show a continuity of action across multiple scenes. The use of the intertitle to explain actions and dialogue on screen began in the early 1900s. Filmed intertitles were first used in Robert W. Paul's film, Scrooge, or Marley's Ghost. In most countries, intertitles gradually came to be used to provide dialogue and narration for the film, thus dispensing the need for narration provided by exhibitors.\n\nDevelopment of continuous action across multiple shots was furthered in England by a loosely associated group of film pioneers collectively termed \"the Brighton School\". These filmmakers included George Albert Smith and James Williamson, among others. Smith and Williamson experimented with action continuity and were likely the first to incorporate the use of inserts and close-ups between shots. A basic technique for trick cinematography was the double exposure of the film in the camera. The effect was pioneered by Smith in the 1898 film, Photographing a Ghost. According to Smith's catalogue records, the (now lost) film chronicles a photographer's struggle to capture a ghost on camera. Using the double exposure of the film, Smith overlaid a transparent ghostly figure onto the background in a comical manner to taunt the photographer. Smith's The Corsican Brothers was described in the catalogue of the Warwick Trading Company in 1900: \"By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow.\" Smith also initiated the special effects technique of reverse motion. He did this by repeating the action a second time, while filming it with an inverted camera, and then joining the tail of the second negative to that of the first. The first films made using this device were Tipsy, Topsy, Turvy and The Awkward Sign Painter. The earliest surviving example of this technique is Smith's The House That Jack Built, made before September 1900. Cecil Hepworth took this technique further by printing the negatives of the forward motion in reverse frame by frame, producing a print in which the original action was exactly reversed. To do this he built a special printer in which the negative running through a projector was projected into the gate of a camera through a special lens giving a same-size image. This arrangement came to be called a \"projection printer\", and eventually an \"optical printer\". In 1898, George Albert Smith experimented with close-ups, filming shots of a man drinking beer and a woman using sniffing tobacco. The following year, Smith made The Kiss in the Tunnel, a sequence consisting of three shots: a train enters a tunnel; a man and a woman exchange a brief kiss in the darkness and then return to their seats; the train exits the tunnel. Smith created the scenario in response to the success of a genre known as a phantom ride. In a phantom ride film, cameras would capture the motion and surroundings from the front of a moving train. The separate shots, when edited together, formed a distinct sequence of events and established causality from one shot to the next. Following The Kiss in the Tunnel, Smith more definitively experimented with continuity of action across successive shots and began using inserts in his films, such as Grandma's Reading Glass and Mary Jane's Mishap. In 1900, Smith made As Seen Through a Telescope. The main shot shows a street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene.James Williamson perfected narrative building techniques in his 1900 film, Attack on a China Mission. The film, which film historian John Barnes later described as having \"the most fully developed narrative of any film made in England up to that time\", opens as the first shot shows Chinese Boxer rebels at the gate; it then cuts to the missionary family in the garden, where a fight ensues. The wife signals to British sailors from the balcony, who come and rescue them. The film also used the first \"reverse angle\" cut in film history. The following year, Williamson created The Big Swallow. In the film. a man becomes irritated by the presence of the filmmaker and \"swallows\" the camera and its operator through the use of interpolated close-up shots. He combined these effects, along with superimpositions, use of wipe transitions to denote a scene change, and other techniques to create a film language, or \"film grammar\". James Williamson's use of continuous action in his 1901 film, Stop Thief! stimulated a film genre known as the \"chase film.\" In the film, a tramp steals a leg of mutton from a butcher's boy in the first shot, is chased by the butcher's boy and assorted dogs in the following shot, and is finally caught by the dogs in the third shot.\n\n\n==== United States: The Edison Company and Edwin S. Porter ====\nThe Execution of Mary Stuart, produced in 1895 by the Edison Company for viewing with the Kinetoscope, showed Mary Queen of Scots being executed in full view of the camera. The effect, known as the stop trick, was achieved by replacing the actor with a dummy for the final shot. The technique used in the film is seen as one of the earliest known uses of special effects in film.The American filmmaker Edwin S. Porter started making films for the Edison Company in 1901. A former projectionist hired by Thomas Edison to develop his new projection model known as the Vitascope, Porter was inspired in part by the works of M\u00e9li\u00e8s, Smith, and Williamson and drew upon their newly crafted techniques to further the development of continuous narrative through editing. When he began making longer films in 1902, he put a dissolve between every shot, just as Georges M\u00e9li\u00e8s was already doing, and he frequently had the same action repeated across the dissolves.\nIn 1902, Porter shot Life of an American Fireman for the Edison Manufacturing Company and distributed the film the following year. In the film, Porter combined stock footage from previous Edison films with newly shot footage and spliced them together to convey a dramatic story of the rescue of a woman and her child by heroic firemen.Porter's film, The Great Train Robbery (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. The film is seen as a first in the Western film genre and is significant for the use of shots suggesting simultaneous action occurring at different locations. Porter's use of both staged and real outdoor environments helped to create a sense of space while the placement of the camera in a wider shot established depth and allowed for an extended duration of motion on screen. The Great Train Robbery served as one of the vehicles that would launch the film medium into mass popularity. That same year, the Miles Brothers opened the first film exchange in the country, which allowed permanent exhibitors to rent films from the company at a lower cost than the producers that sold their films outright.John P. Harris opened the first permanent theater devoted exclusively to the presentation of films, the nickelodeon, in 1905 in Pittsburgh, Pennsylvania. The idea rapidly took off and by 1908, there were around 8,000 nickelodeon theaters across the country. With the arrival of the nickelodeon, audience demand for a larger quantity of story films with a variety of subjects and locations led to a need to hire more creative talent and caused studios to invest in more elaborate stage designs.In 1908, Thomas Edison spearheaded the creation of a corporate trust between the major film companies in America known as the Motion Picture Patents Company (MPPC) to limit infringement on his patents. Members of the trust controlled every aspect of the filmmaking process from the creation of film stock, the production of films, and the distribution to cinemas through licensing arrangements. The trust lead to increased quality filmmaking spurred by internal competition and placed limits on the amount of foreign films to encourage the growth of the American film industry, but it also discouraged the creation of feature films. By 1915, the MPPC had lost most of its hold on the film industry as the companies moved towards the wider production of feature films.\n\n\n== Continued international growth (1900s\u20131910s) ==\n\n\n=== New film producing countries ===\nWith the worldwide film boom, more countries now joined Britain, France, Germany and the United States in serious film production. In Italy, production was spread over several centers, Turin was the first major film production centre, and Milan and Naples gave birth to the first film magazines. In Turin, Ambrosio was the first company in the field in 1905, and remained the largest in the country through this period. Its most substantial rival was Cines in Rome, which started producing in 1906. The great strength of the Italian industry was historical epics, with large casts and massive scenery. As early as 1911, Giovanni Pastrone's two-reel La Caduta di Troia (The Fall of Troy) made a big impression worldwide, and it was followed by even bigger productions like Quo Vadis? (1912), which ran for 90 minutes, and Pastrone's Cabiria of 1914, which ran for two and a half hours.Italian companies also had a strong line in slapstick comedy, with actors like Andr\u00e9 Deed, known locally as \"Cretinetti\", and elsewhere as \"Foolshead\" and \"Gribouille\", achieving worldwide fame with his almost surrealistic gags.\nThe most important film-producing country in Northern Europe up until the First World War was Denmark. The Nordisk company was set up there in 1906 by Ole Olsen, a fairground showman, and after a brief period imitating the successes of French and British filmmakers, in 1907 he produced 67 films, most directed by Viggo Larsen, with sensational subjects like Den hvide Slavinde (The White Slave), Isbj\u00f8rnejagt (Polar Bear Hunt) and L\u00f8vejagten (The Lion Hunt). By 1910, new smaller Danish companies began joining the business, and besides making more films about the white slave trade, they contributed other new subjects. The most important of these finds was Asta Nielsen in Afgrunden (The Abyss), directed by Urban Gad for Kosmorama, This combined the circus, sex, jealousy and murder, all put over with great conviction, and pushed the other Danish filmmakers further in this direction. By 1912, the Danish film companies were multiplying rapidly.The Swedish film industry was smaller and slower to get started than the Danish industry. Here, Charles Magnusson, a newsreel cameraman for the Svenskabiografteatern cinema chain, started fiction film production for them in 1909, directing a number of the films himself. Production increased in 1912, when the company engaged Victor Sj\u00f6str\u00f6m and Mauritz Stiller as directors. They started out by imitating the subjects favoured by the Danish film industry, but by 1913 they were producing their own strikingly original work, which sold very well.Russia began its film industry in 1908 with Path\u00e9 shooting some fiction subjects there, and then the creation of real Russian film companies by Aleksandr Drankov and Aleksandr Khanzhonkov. The Khanzhonkov company quickly became much the largest Russian film company, and remained so until 1918.In Germany, Oskar Messter had been involved in film-making from 1896, but did not make a significant number of films per year until 1910. When the worldwide film boom started, he, and the few other people in the German film business, continued to sell prints of their own films outright, which put them at a disadvantage. It was only when Paul Davidson, the owner of a chain of cinemas, brought Asta Nielsen and Urban Gad to Germany from Denmark in 1911, and set up a production company, Projektions-AG \"Union\" (PAGU), that a change-over to renting prints began. Messter replied with a series of longer films starring Henny Porten, but although these did well in the German-speaking world, they were not particularly successful internationally, unlike the Asta Nielsen films. Another of the growing German film producers just before World War I was the German branch of the French \u00c9clair company, Deutsche \u00c9clair. This was expropriated by the German government, and turned into DECLA when the war started. But altogether, German producers only had a minor part of the German market in 1914.Overall, from about 1910, American films had the largest share of the market in all European countries except France, and even in France, the American films had just pushed the local production out of first place on the eve of World War I. Path\u00e9 Fr\u00e8res expanded and significantly shaped the American film business, creating many \"firsts\" in the film industry, such as adding titles and subtitles to films for the first time, releasing scrolls for the first time, introducing film posters for the first time, producing color pictures for the first time, taking out commercial bills for the first time, contacting exhibitors and studying their needs for the first time. The world's largest film supplier, Path\u00e9, is limited to the U.S. market, which has reached a saturation level, so the U.S. seeks additional profits from foreign markets. Movies are defined as \"pure\" American phenomenon in the United States.\n\n\n=== Film technique ===\nNew film techniques that were introduced in this period include the use of artificial lighting, fire effects and low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes.Continuity of action from shot to shot was also refined, such as in Path\u00e9's le Cheval emball\u00e9 (The Runaway Horse) (1907) where cross-cutting between parallel actions is used. D. W. Griffith also began using cross-cutting in the film The Fatal Hour, made in July 1908. Another development was the use of the point of view shot, first used in 1910 in Vitagraph's Back to Nature. Insert shots were also used for artistic purposes; the Italian film La mala planta (The Evil Plant), directed by Mario Caserini had an insert shot of a snake slithering over the \"Evil Plant\". By 1914 it was widely held in the American film industry that cross-cutting was most generally useful because it made possible the elimination of uninteresting parts of the action that play no part in advancing the drama.In 1909, 35mm became the internationally recognized theatrical film gauge.As films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel. Genres began to be used as categories; the main division was into comedy and drama, but these categories were further subdivided.Intertitles containing lines of dialogue began to be used consistently from 1908 onwards, such as in Vitagraph's An Auto Heroine; or, The Race for the Vitagraph Cup and How It Was Won. The dialogue was eventually inserted into the middle of the scene and became commonplace by 1912. The introduction of dialogue titles transformed the nature of film narrative. When dialogue titles came to be always cut into a scene just after a character starts speaking, and then left with a cut to the character just before they finish speaking, then one had something that was effectively the equivalent of a present-day sound film.\n\n\n=== During World War I and industry ===\nThe years of the First World War were a complex transitional period for the film industry. The exhibition of films changed from short one-reel programmes to feature films. Exhibition venues became larger and began charging higher prices.In the United States, these changes brought destruction to many film companies, the Vitagraph company being an exception. Film production began to shift to Los Angeles during World War I. The Universal Film Manufacturing Company was formed in 1912 as an umbrella company. New entrants included the Jesse Lasky Feature Play Company, and Famous Players, both formed in 1913, and later amalgamated into Famous Players\u2013Lasky. The biggest success of these years was David Wark Griffith's The Birth of a Nation (1915). Griffith followed this up with the even bigger Intolerance (1916), but, due to the high quality of film produced in the US, the market for their films was high.In France, film production shut down due to the general military mobilization of the country at the start of the war. Although film production began again in 1915, it was on a reduced scale, and the biggest companies gradually retired from production. Italian film production held up better, although so called \"diva films\", starring anguished female leads were a commercial failure. In Denmark, the Nordisk company increased its production so much in 1915 and 1916 that it could not sell all its films, which led to a very sharp decline in Danish production, and the end of Denmark's importance on the world film scene.The German film industry was seriously weakened by the war. The most important of the new film producers at the time was Joe May, who made a series of thrillers and adventure films through the war years, but Ernst Lubitsch also came into prominence with a series of very successful comedies and dramas.\n\n\n=== New techniques ===\nAt this time, studios were blacked out to allow shooting to be unaffected by changing sunlight. This was replaced with floodlights and spotlights. The widespread adoption of irising-in and out to begin and end scenes caught on in this period. This is the revelation of a film shot in a circular mask, which gradually gets larger until it expands beyond the frame. Other shaped slits were used, including vertical and diagonal apertures.A new idea taken over from still photography was \"soft focus\". This began in 1915, with some shots being intentionally thrown out of focus for expressive effect, as in Mary Pickford starrer Fanchon the Cricket.It was during this period that camera effects intended to convey the subjective feelings of characters in a film really began to be established. These could now be done as Point of View (POV) shots, as in Sidney Drew's The Story of the Glove (1915), where a wobbly hand-held shot of a door and its keyhole represents the POV of a drunken man. The use of anamorphic (in the general sense of distorted shape) images first appears in these years when Abel Gance directed la Folie du Docteur Tube (The Madness of Dr. Tube). In this film the effect of a drug administered to a group of people was suggested by shooting the scenes reflected in a distorting mirror of the fair-ground type.Symbolic effects taken over from conventional literary and artistic tradition continued to make some appearances in films during these years. In D. W. Griffith's The Avenging Conscience (1914), the title \"The birth of the evil thought\" precedes a series of three shots of the protagonist looking at a spider, and ants eating an insect. Symbolist art and literature from the turn of the century also had a more general effect on a small number of films made in Italy and Russia. The supine acceptance of death resulting from passion and forbidden longings was a major feature of this art, and states of delirium dwelt on at length were important as well.\nThe use of insert shots, i.e. close-ups of objects other than faces, had already been established by the Brighton school, but were infrequently used before 1914. It is really only with Griffith's The Avenging Conscience that a new phase in the use of the Insert Shot starts. As well as the symbolic inserts already mentioned, the film also made extensive use of large numbers of Big Close Up shots of clutching hands and tapping feet as a means of emphasizing those parts of the body as indicators of psychological tension.Atmospheric inserts were developed in Europe in the late 1910s. This kind of shot is one in a scene which neither contains any of the characters in the story, nor is a Point of View shot seen by one of them. An early example is when Maurice Tourneur directed The Pride of the Clan (1917), in which there is a series of shots of waves beating on a rocky shore to demonstrate the harsh lives of the fishing folk. Maurice Elvey's Nelson; The Story of England's Immortal Naval Hero (1919) has a symbolic sequence dissolving from a picture of Kaiser Wilhelm II to a peacock, and then to a battleship.By 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another. Cutting to different angles within a scene also became well-established as a technique for dissecting a scene into shots in American films. If the direction of the shot changes by more than ninety degrees, it is called a reverse-angle cutting. The leading figure in the full development of reverse-angle cutting was Ralph Ince in his films, such as The Right Girl and His Phantom Sweetheart.The use of flash-back structures continued to develop in this period, with the usual way of entering and leaving a flash-back being through a dissolve. The Vitagraph Company's The Man That Might Have Been (William J. Humphrey, 1914), is even more complex, with a series of reveries and flash-backs that contrast the protagonist's real passage through life with what might have been, if his son had not died.\nAfter 1914, cross cutting between parallel actions came to be used \u2013  more so in American films than in European ones. Cross-cutting was used to get new effects of contrast, such as the cross-cut sequence in Cecil B. DeMille's The Whispering Chorus (1918), in which a supposedly dead husband is having a liaison with a Chinese prostitute in an opium den, while simultaneously his unknowing wife is being remarried in church.Silent film tinting, too, gained popularity during these periods. Amber tinting meant daytime, or vividly-lit nighttime, blue tints meant dawn or dimly-lit night, red tinting represented fire scenes, green tinting meant a mysterious atmosphere, and brown tints (aka sepia toning) were used usually for full-length films instead of individual scenes. D.W. Griffiths' ground-breaking epic, The Birth of a Nation, the famous 1920 film Dr. Jekyll and Mr. Hyde, and the Robert Wiene epic from the same year, The Cabinet of Dr. Caligari, are some notable examples of tinted silent films.The Photo-Drama of Creation, first shown to audiences in 1914, was the first major screenplay to incorporate synchronized sound, moving film, and color slides. Until 1927, most motion pictures were produced without sound. This period is commonly referred to as the silent era of film.\n\n\n=== Film art ===\nThe general trend in the development of cinema, led from the United States, was towards using the newly developed specifically filmic devices for expression of the narrative content of film stories, and combining this with the standard dramatic structures already in use in commercial theatre. D. W. Griffith had the highest standing among American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. Cecil B. DeMille's The Cheat (1915), brought out the moral dilemmas facing their characters in a more subtle way than Griffith. DeMille was also in closer touch with the reality of contemporary American life. Maurice Tourneur was also highly ranked for the pictorial beauties of his films, together with the subtlety of his handling of fantasy, while at the same time he was capable of getting greater naturalism from his actors at appropriate moments, as in A Girl's Folly (1917).Sidney Drew was the leader in developing \"polite comedy\", while slapstick was refined by Fatty Arbuckle and Charles Chaplin, who both started with Mack Sennett's Keystone company. They reduced the usual frenetic pace of Sennett's films to give the audience a chance to appreciate the subtlety and finesse of their movement, and the cleverness of their gags. By 1917 Chaplin was also introducing more dramatic plot into his films, and mixing the comedy with sentiment.In Russia, Yevgeni Bauer put a slow intensity of acting combined with Symbolist overtones onto film in a unique way.In Sweden, Victor Sj\u00f6str\u00f6m made a series of films that combined the realities of people's lives with their surroundings in a striking manner, while Mauritz Stiller developed sophisticated comedy to a new level.In Germany, Ernst Lubitsch got his inspiration from the stage work of Max Reinhardt, both in bourgeois comedy and in spectacle, and applied this to his films, culminating in his die Puppe (The Doll), die Austernprinzessin (The Oyster Princess) and Madame DuBarry.\n\n\n== 1920s ==\n\n\n=== Golden years of German cinema, Hollywood triumphant ===\nAt the start of the First World War, French and Italian cinema had been the most globally popular. The war came as a devastating interruption to European film industries.\nThroughout the early 20th century, screen artists continued to learn how to work with cameras and create illusions using space and time in their shots. This newly introduced form of creativity made way for a whole new group of people to be introduced to stardom, including David W. Griffith, who made a name for himself with his 1915 film, The Birth of a Nation. In 1920, there were two major changes to the film industry: the introduction of sound and the creation of studio systems. In the 1920s, talent who had been working independently began joining studios and working with other actors and directors. In 1927, The Jazz Singer was released, bringing sound to the motion picture industry.\nThe German cinema, marked by those times, saw the era of the German Expressionist film movement. Berlin was its center with the Filmstudio Babelsberg, which is the oldest large-scale film studio in the world. The first Expressionist films made up for a lack of lavish budgets by using set designs with wildly non-realistic, geometrically absurd angles, along with designs painted on walls and floors to represent lights, shadows, and objects. The plots and stories of the Expressionist films often dealt with madness, insanity, betrayal and other \"intellectual\" topics triggered by the experiences of World War I. Films like The Cabinet of Dr. Caligari (1920), Nosferatu (1922) and M (1931), similar to the movement they were part of, had a historic impact on film itself.Movies like Metropolis (1927) and Woman in the Moon (1929) partly created the genre of science fiction films and Lotte Reiniger became a pioneer in animation, producing animated feature films like The Adventures of Prince Achmed, the oldest surviving and oldest European made animated movie.\nMany German and German-based directors, actors, writers and others emigrated to the US when the Nazis gained power, giving Hollywood and the American film industry the final edge in its competition with other movie producing countries.The American industry, or \"Hollywood\", as it was becoming known after its new geographical center in California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries on earth.\nBy the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 feature films annually, or 82% of the global total (Eyman, 1997). The comedies of Charlie Chaplin and Buster Keaton, the swashbuckling adventures of Douglas Fairbanks and the romances of Clara Bow, to cite just a few examples, made these performers' faces well known on every continent. The Western visual norm that would become classical continuity editing was developed and exported \u2013 although its adoption was slower in some non-Western countries without strong realist traditions in art and drama, such as Japan.\nThis development was contemporary with the growth of the studio system and its greatest publicity method, the star system, which characterized American film for decades to come and provided models for other film industries. The studios' efficient, top-down control over all stages of their product enabled a new and ever-growing level of lavish production and technical sophistication. At the same time, the system's commercial regimentation and focus on glamorous escapism discouraged daring and ambition beyond a certain degree, a prime example being the brief but still legendary directing career of the iconoclastic Erich von Stroheim in the late teens and the 1920s.\nIn 1924, Sam Goldwyn, Louis B. Mayer, and the Metro Pictures Corporation create MGM.\n\n\n== 1930s ==\n\n\n=== Sound era ===\n\nDuring late 1927, Warners released The Jazz Singer, which was mostly silent but contained what is generally regarded as the first synchronized dialogue (and singing) in a feature film; but this process was actually accomplished first by Charles Taze Russell in 1914 with the lengthy film The Photo-Drama of Creation. This drama consisted of picture slides and moving pictures synchronized with phonograph records of talks and music. The early sound-on-disc processes such as Vitaphone were soon superseded by sound-on-film methods like Fox Movietone, DeForest Phonofilm, and RCA Photophone. The trend convinced the largely reluctant industrialists that \"talking pictures\", or \"talkies\", were the future. A lot of attempts were made before the success of The Jazz Singer, that can be seen in the List of film sound systems. And in 1926, Warner Bros. Debuts the film Don Juan with synchronized sound effects and music.The change was remarkably swift. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Total changeover was slightly slower in the rest of the world, principally for economic reasons. Cultural reasons were also a factor in countries like China and Japan, where silents co-existed successfully with sound well into the 1930s, indeed producing what would be some of the most revered classics in those countries, like Wu Yonggang's The Goddess (China, 1934) and Yasujir\u014d Ozu's I Was Born, But... (Japan, 1932). But even in Japan, a figure such as the benshi, the live narrator who was a major part of Japanese silent cinema, found his acting career was ending.\nSound further tightened the grip of major studios in numerous countries: the vast expense of the transition overwhelmed smaller competitors, while the novelty of sound lured vastly larger audiences for those producers that remained. In the case of the U.S., some historians credit sound with saving the Hollywood studio system in the face of the Great Depression (Parkinson, 1995). Thus began what is now often called \"The Golden Age of Hollywood\", which refers roughly to the period beginning with the introduction of sound until the late 1940s. The American cinema reached its peak of efficiently manufactured glamour and global appeal during this period. The top actors of the era are now thought of as the classic film stars, such as Clark Gable, Katharine Hepburn, Humphrey Bogart, Greta Garbo, and the greatest box office draw of the 1930s, child performer Shirley Temple.\n\n\n=== Creative impact of sound ===\nCreatively, however, the rapid transition was a difficult one, and in some ways, film briefly reverted to the conditions of its earliest days. The late '20s were full of static, stagey talkies as artists in front of and behind the camera struggled with the stringent limitations of the early sound equipment and their own uncertainty as to how to use the new medium. Many stage performers, directors and writers were introduced to cinema as producers sought personnel experienced in dialogue-based storytelling. Many major silent filmmakers and actors were unable to adjust and found their careers severely curtailed or even ended.\nThis awkward period was fairly short-lived. 1929 was a watershed year: William Wellman with Chinatown Nights and The Man I Love, Rouben Mamoulian with Applause, Alfred Hitchcock with Blackmail (Britain's first sound feature), were among the directors to bring greater fluidity to talkies and experiment with the expressive use of sound (Eyman, 1997). In this, they both benefited from, and pushed further, technical advances in microphones and cameras, and capabilities for editing and post-synchronizing sound (rather than recording all sound directly at the time of filming).\n\nSound films emphasized black history, and benefited different genres to a greater extent than silents did. Most obviously, the musical film was born; the first classic-style Hollywood musical was The Broadway Melody (1929), and the form would find its first major creator in choreographer/director Busby Berkeley (42nd Street, 1933, Dames, 1934). In France, avant-garde director Ren\u00e9 Clair made surreal use of song and dance in comedies like Under the Roofs of Paris (1930) and Le Million (1931). Universal Pictures began releasing gothic horror films like Dracula and Frankenstein (both 1931). In 1933, RKO Pictures released Merian C. Cooper's classic \"giant monster\" film King Kong. The trend thrived best in India, where the influence of the country's traditional song-and-dance drama made the musical the basic form of most sound films (Cook, 1990); virtually unnoticed by the Western world for decades, this Indian popular cinema would nevertheless become the world's most prolific. (See also Bollywood.)\nAt this time, American gangster films like Little Caesar and Wellman's The Public Enemy (both 1931) became popular. Dialogue now took precedence over slapstick in Hollywood comedies: the fast-paced, witty banter of The Front Page (1931) or It Happened One Night (1934), the sexual double entendres of Mae West (She Done Him Wrong, 1933), or the often subversively anarchic nonsense talk of the Marx Brothers (Duck Soup, 1933). Walt Disney, who had previously been in the short cartoon business, stepped into feature films with the first English-speaking animated feature Snow White and the Seven Dwarfs, released by RKO Pictures in 1937. 1939, a major year for American cinema, brought such films as The Wizard of Oz and Gone with The Wind.\n\n\n=== Color in cinema ===\n\nCirca 80 percent of the films of the 1890s to the 1920s had colours. Many made use of monochromatic film tinting dye baths, some had the frames painted in multiple transparent colours by hand, and since 1905 there was a mechanized stencil-process (Path\u00e9color).\nKinemacolor, the first commercially successful cinematographic colour process, produced films in two colours (red and cyan) from 1908 to 1914.\nTechnicolor's natural three-strip colour process was very successfully introduced in 1932 with Walt Disney's animated Academy Award-winning short \"Flowers and Trees\", directed by Burt Gillett. Technicolor was initially used mainly for musicals like \"The Wizard of Oz\" (1939), in costume films such as \"The Adventures of Robin Hood\", and in animation. Not long after television became prevalent in the early 1950s, colour became more or less standard for theatrical movies.\n\n\n== 1940s ==\n\n\n=== World War II and its aftermath ===\n\nThe desire for wartime propaganda against the opposition created a renaissance in the film industry in Britain, with realistic war dramas like 49th Parallel (1941), Went the Day Well? (1942), The Way Ahead (1944) and No\u00ebl Coward and David Lean's celebrated naval film In Which We Serve in 1942, which won a special Academy Award. These existed alongside more flamboyant films like Michael Powell and Emeric Pressburger's The Life and Death of Colonel Blimp (1943), A Canterbury Tale (1944) and A Matter of Life and Death (1946), as well as Laurence Olivier's 1944 film Henry V, based on the Shakespearean history Henry V. The success of Snow White and the Seven Dwarfs allowed Disney to make more animated features like Pinocchio (1940), Fantasia (1940), Dumbo (1941) and Bambi (1942).\nThe onset of US involvement in World War II also brought a proliferation of films as both patriotism and propaganda. American propaganda films included Desperate Journey (1942), Mrs. Miniver (1942), Forever and a Day (1943) and Objective, Burma! (1945). Notable American films from the war years include the anti-Nazi Watch on the Rhine (1943), scripted by Dashiell Hammett; Shadow of a Doubt (1943), Hitchcock's direction of a script by Thornton Wilder; the George M. Cohan biographical film, Yankee Doodle Dandy (1942), starring James Cagney, and the immensely popular Casablanca, with Humphrey Bogart. Bogart would star in 36 films between 1934 and 1942 including John Huston's The Maltese Falcon (1941), one of the first films now considered a classic film noir. In 1941, RKO Pictures released Citizen Kane made by Orson Welles. It is often considered the greatest film of all time. It would set the stage for the modern motion picture, as it revolutionized film story telling.\nThe strictures of wartime also brought an interest in more fantastical subjects. These included Britain's Gainsborough melodramas (including The Man in Grey and The Wicked Lady), and films like Here Comes Mr. Jordan, Heaven Can Wait, I Married a Witch and Blithe Spirit. Val Lewton also produced a series of atmospheric and influential small-budget horror films, some of the more famous examples being Cat People, Isle of the Dead and The Body Snatcher. The decade probably also saw the so-called \"women's pictures\", such as Now, Voyager, Random Harvest and Mildred Pierce at the peak of their popularity.\n1946 saw RKO Radio releasing It's a Wonderful Life directed by Italian-born filmmaker Frank Capra. Soldiers returning from the war would provide the inspiration for films like The Best Years of Our Lives, and many of those in the film industry had served in some capacity during the war. Samuel Fuller's experiences in World War II would influence his largely autobiographical films of later decades such as The Big Red One. The Actors Studio was founded in October 1947 by Elia Kazan, Robert Lewis, and Cheryl Crawford, and the same year Oskar Fischinger filmed Motion Painting No. 1.\n\nIn 1943, Ossessione was screened in Italy, marking the beginning of Italian neorealism. Major films of this type during the 1940s included Bicycle Thieves, Rome, Open City, and La Terra Trema. In 1952 Umberto D was released, usually considered the last film of this type.\nIn the late 1940s, in Britain, Ealing Studios embarked on their series of celebrated comedies, including Whisky Galore!, Passport to Pimlico, Kind Hearts and Coronets and The Man in the White Suit, and Carol Reed directed his influential thrillers Odd Man Out, The Fallen Idol and The Third Man. David Lean was also rapidly becoming a force in world cinema with Brief Encounter and his Dickens adaptations Great Expectations and Oliver Twist, and Michael Powell and Emeric Pressburger would experience the best of their creative partnership with films like Black Narcissus and The Red Shoes.\n\n\n== 1950s ==\n\nThe House Un-American Activities Committee investigated Hollywood in the early 1950s. Protested by the Hollywood Ten before the committee, the hearings resulted in the blacklisting of many actors, writers and directors, including Chayefsky, Charlie Chaplin, and Dalton Trumbo, and many of these fled to Europe, especially the United Kingdom.\nThe Cold War era zeitgeist translated into a type of near-paranoia manifested in themes such as invading armies of evil aliens (Invasion of the Body Snatchers, The War of the Worlds) and communist fifth columnists (The Manchurian Candidate).\nDuring the immediate post-war years the cinematic industry was also threatened by television, and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The demise of the \"studio system\" spurred the self-commentary of films like Sunset Boulevard (1950) and The Bad and the Beautiful (1952).\nIn 1950, the Lettrists avante-gardists caused riots at the Cannes Film Festival, when Isidore Isou's Treatise on Slime and Eternity was screened. After their criticism of Charlie Chaplin and split with the movement, the Ultra-Lettrists continued to cause disruptions when they showed their new hypergraphical techniques.\nThe most notorious film is Guy Debord's Howls for Sade of 1952.\nDistressed by the increasing number of closed theatres, studios and companies would find new and innovative ways to bring audiences back. These included attempts to widen their appeal with new screen formats. Cinemascope, which would remain a 20th Century Fox distinction until 1967, was announced with 1953's The Robe. VistaVision, Cinerama, and Todd-AO boasted a \"bigger is better\" approach to marketing films to a dwindling US audience. This resulted in the revival of epic films to take advantage of the new big screen formats. Some of the most successful examples of these Biblical and historical spectaculars include The Ten Commandments (1956), The Vikings (1958), Ben-Hur (1959), Spartacus (1960) and El Cid (1961). Also during this period a number of other significant films were produced in Todd-AO, developed by Mike Todd shortly before his death, including Oklahoma! (1955), Around the World in 80 Days (1956), South Pacific (1958) and Cleopatra (1963) plus many more.\nGimmicks also proliferated to lure in audiences. The fad for 3-D film would last for only two years, 1952\u20131954, and helped sell House of Wax and Creature from the Black Lagoon. Producer William Castle would tout films featuring \"Emergo\" \"Percepto\", the first of a series of gimmicks that would remain popular marketing tools for Castle and others throughout the 1960s.\nIn 1954, Dorothy Dandridge was nominated as the best actress at the Oscar for her role in the film Carman Jones. She became the first black woman to be nominated for this award.In the U.S., a post-WW2 tendency toward questioning the establishment and societal norms and the early activism of the civil rights movement was reflected in Hollywood films such as Blackboard Jungle (1955), On the Waterfront (1954), Paddy Chayefsky's Marty and Reginald Rose's 12 Angry Men (1957). Disney continued making animated films, notably; Cinderella (1950), Peter Pan (1953), Lady and the Tramp (1955), and Sleeping Beauty (1959). He began, however, getting more involved in live action films, producing classics like 20,000 Leagues Under the Sea (1954), and Old Yeller (1957). Television began competing seriously with films projected in theatres, but surprisingly it promoted more filmgoing rather than curtailing it.\nLimelight is probably a unique film in at least one interesting respect. Its two leads, Charlie Chaplin and Claire Bloom, were in the industry in no less than three different centuries. In the 19th century, Chaplin made his theatrical debut at the age of eight, in 1897, in a clog dancing troupe, The Eight Lancaster Lads. In the 21st century, Bloom is still enjoying a full and productive career, having appeared in dozens of films and television series produced up to and including 2022. She received particular acclaim for her role in The King's Speech (2010).\n\n\n=== Golden age of Asian cinema ===\n\nFollowing the end of World War II in the 1940s, the following decade, the 1950s, marked a 'golden age' for non-English world cinema, especially for Asian cinema. Many of the most critically acclaimed Asian films of all time were produced during this decade, including Yasujir\u014d Ozu's Tokyo Story (1953), Satyajit Ray's The Apu Trilogy (1955\u20131959) and Jalsaghar (1958), Kenji Mizoguchi's Ugetsu (1954) and Sansho the Bailiff (1954), Raj Kapoor's Awaara (1951), Mikio Naruse's Floating Clouds (1955), Guru Dutt's Pyaasa (1957) and Kaagaz Ke Phool (1959), and the Akira Kurosawa films Rashomon (1950), Ikiru (1952), Seven Samurai (1954) and Throne of Blood (1957).During Japanese cinema's 'Golden Age' of the 1950s, successful films included Rashomon (1950), Seven Samurai (1954) and The Hidden Fortress (1958) by Akira Kurosawa, as well as Yasujir\u014d Ozu's Tokyo Story (1953) and Ishir\u014d Honda's Godzilla (1954). These films have had a profound influence on world cinema. In particular, Kurosawa's Seven Samurai has been remade several times as Western films, such as The Magnificent Seven (1960) and Battle Beyond the Stars (1980), and has also inspired several Bollywood films, such as Sholay (1975) and China Gate (1998). Rashomon was also remade as The Outrage (1964), and inspired films with \"Rashomon effect\" storytelling methods, such as Andha Naal (1954), The Usual Suspects (1995) and Hero (2002). The Hidden Fortress was also an inspiration behind George Lucas' Star Wars (1977). Other famous Japanese filmmakers from this period include Kenji Mizoguchi, Mikio Naruse, Hiroshi Inagaki and Nagisa Oshima. Japanese cinema later became one of the main inspirations behind the New Hollywood movement of the 1960s to 1980s.\nDuring Indian cinema's 'Golden Age' of the 1950s, it was producing 200 films annually, while Indian independent films gained greater recognition through international film festivals. One of the most famous was The Apu Trilogy (1955\u20131959) from critically acclaimed Bengali film director Satyajit Ray, whose films had a profound influence on world cinema, with directors such as Akira Kurosawa, Martin Scorsese, James Ivory, Abbas Kiarostami, Elia Kazan, Fran\u00e7ois Truffaut, Steven Spielberg, Carlos Saura, Jean-Luc Godard, Isao Takahata, Gregory Nava, Ira Sachs, Wes Anderson and Danny Boyle being influenced by his cinematic style. According to Michael Sragow of The Atlantic Monthly, the \"youthful coming-of-age dramas that have flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy\". Subrata Mitra's cinematographic technique of bounce lighting also originates from The Apu Trilogy. Other famous Indian filmmakers from this period include Guru Dutt, Ritwik Ghatak, Mrinal Sen, Raj Kapoor, Bimal Roy, K. Asif and Mehboob Khan.The cinema of South Korea also experienced a 'Golden Age' in the 1950s, beginning with director Lee Kyu-hwan's tremendously successful remake of Chunhyang-jon (1955). That year also saw the release of Yangsan Province by the renowned director, Kim Ki-young, marking the beginning of his productive career. Both the quality and quantity of filmmaking had increased rapidly by the end of the 1950s. South Korean films, such as Lee Byeong-il's 1956 comedy Sijibganeun nal (The Wedding Day), had begun winning international awards. In contrast to the beginning of the 1950s, when only 5 films were made per year, 111 films were produced in South Korea in 1959.The 1950s was also a 'Golden Age' for Philippine cinema, with the emergence of more artistic and mature films, and significant improvement in cinematic techniques among filmmakers. The studio system produced frenetic activity in the local film industry as many films were made annually and several local talents started to earn recognition abroad. The premiere Philippine directors of the era included Gerardo de Leon, Gregorio Fern\u00e1ndez, Eddie Romero, Lamberto Avellana, and Cirio Santiago.\n\n\n== 1960s ==\n\nDuring the 1960s, the studio system in Hollywood declined, because many films were now being made on location in other countries, or using studio facilities abroad, such as Pinewood in the UK and Cinecitt\u00e0 in Rome. \"Hollywood\" films were still largely aimed at family audiences, and it was often the more old-fashioned films that produced the studios' biggest successes. Productions like Mary Poppins (1964), My Fair Lady (1964) and The Sound of Music (1965) were among the biggest money-makers of the decade. The growth in independent producers and production companies, and the increase in the power of individual actors also contributed to the decline of traditional Hollywood studio production.\nThere was also an increasing awareness of foreign language cinema in America during this period. During the late 1950s and 1960s, the French New Wave directors such as Fran\u00e7ois Truffaut and Jean-Luc Godard produced films such as Les quatre cents coups, Breathless and Jules et Jim which broke the rules of Hollywood cinema's narrative structure. As well, audiences were becoming aware of Italian films like Federico Fellini's La Dolce Vita (1960), 8\u00bd (1963) and the stark dramas of Sweden's Ingmar Bergman.\nIn Britain, the \"Free Cinema\" of Lindsay Anderson, Tony Richardson and others lead to a group of realistic and innovative dramas including Saturday Night and Sunday Morning, A Kind of Loving and This Sporting Life. Other British films such as Repulsion, Darling, Alfie, Blowup and Georgy Girl (all in 1965\u20131966) helped to reduce prohibitions of sex and nudity on screen, while the casual sex and violence of the James Bond films, beginning with Dr. No in 1962 would render the series popular worldwide.\nDuring the 1960s, Ousmane Semb\u00e8ne produced several French- and Wolof-language films and became the \"father\" of African Cinema. In Latin America, the dominance of the \"Hollywood\" model was challenged by many film makers. Fernando Solanas and Octavio Getino called for a politically engaged Third Cinema in contrast to Hollywood and the European auteur cinema.\nIn Egypt, the golden age of Egyptian cinema continued in the 1960s at the hands of many directors, and Egyptian cinema greatly appreciated women at that time, such as Soad Hosny. The Zulfikar brothers; Ezz El-Dine Zulfikar, Salah Zulfikar and Mahmoud Zulfikar were on a date with many productions, including Ezz El Dine Zulfikar's The River of Love (1960), Mahmoud Zulfikar's Soft Hands (1964), and Dearer Than My Life (1965) starring Salah Zulfikar and Salah Zulfikar Films production; My Wife, the Director General (1966) as well as Youssef Chahine's Saladin (1963).Further, the nuclear paranoia of the age, and the threat of an apocalyptic nuclear exchange (like the 1962 close-call with the USSR during the Cuban Missile Crisis) prompted a reaction within the film community as well. Films like Stanley Kubrick's Dr. Strangelove and Fail Safe with Henry Fonda were produced in a Hollywood that was once known for its overt patriotism and wartime propaganda.\nIn documentary film the sixties saw the blossoming of Direct Cinema, an observational style of film making as well as the advent of more overtly partisan films like In the Year of the Pig about the Vietnam War by Emile de Antonio. By the late 1960s however, Hollywood filmmakers were beginning to create more innovative and ground-breaking films that reflected the social revolution taken over much of the western world such as Bonnie and Clyde (1967), The Graduate (1967), 2001: A Space Odyssey (1968), Rosemary's Baby (1968), Midnight Cowboy (1969), Easy Rider (1969) and The Wild Bunch (1969). Bonnie and Clyde is often considered the beginning of the so-called New Hollywood.\nIn Japanese cinema, Academy Award-winning director Akira Kurosawa produced Yojimbo (1961), which like his previous films also had a profound influence around the world. The influence of this film is most apparent in Sergio Leone's A Fistful of Dollars (1964) and Walter Hill's Last Man Standing (1996). Yojimbo was also the origin of the \"Man with No Name\" trend.\n\n\n== 1970s ==\n\nThe New Hollywood was the period following the decline of the studio system during the 1950s and 1960s and the end of the production code, (which was replaced in 1968 by the MPAA film rating system). During the 1970s, filmmakers increasingly depicted explicit sexual content and showed gunfight and battle scenes that included graphic images of bloody deaths \u2013  a notable example of this is Wes Craven's The Last House on the Left (1972).\nPost-classical cinema is the changing methods of storytelling of the New Hollywood producers. The new methods of drama and characterization played upon audience expectations acquired during the classical/Golden Age period: story chronology may be scrambled, storylines may feature unsettling \"twist endings\", main characters may behave in a morally ambiguous fashion, and the lines between the antagonist and protagonist may be blurred. The beginnings of post-classical storytelling may be seen in 1940s and 1950s film noir films, in films such as Rebel Without a Cause (1955), and in Hitchcock's Psycho. 1971 marked the release of controversial films like Straw Dogs, A Clockwork Orange, The French Connection and Dirty Harry. This sparked heated controversy over the perceived escalation of violence in cinema.\nDuring the 1970s, a new group of American filmmakers emerged, such as Martin Scorsese, Francis Ford Coppola, George Lucas, Woody Allen, Terrence Malick, and Robert Altman. This coincided with the increasing popularity of the auteur theory in film literature and the media, which posited that a film director's films express their personal vision and creative insights. The development of the auteur style of filmmaking helped to give these directors far greater control over their projects than would have been possible in earlier eras. This led to some great critical and commercial successes, like Scorsese's Taxi Driver, Coppola's The Godfather films, William Friedkin's The Exorcist, Altman's Nashville, Allen's Annie Hall and Manhattan, Malick's Badlands and Days of Heaven, and Polish immigrant Roman Polanski's Chinatown. It also, however, resulted in some failures, including Peter Bogdanovich's At Long Last Love and Michael Cimino's hugely expensive Western epic Heaven's Gate, which helped to bring about the demise of its backer, United Artists.\nThe financial disaster of Heaven's Gate marked the end of the visionary \"auteur\" directors of the \"New Hollywood\", who had unrestrained creative and financial freedom to develop films. The phenomenal success in the 1970s of Spielberg's Jaws originated the concept of the modern \"blockbuster\". However, the enormous success of George Lucas' 1977 film Star Wars led to much more than just the popularization of blockbuster filmmaking. The film's revolutionary use of special effects, sound editing and music had led it to become widely regarded as one of the single most important films in the medium's history, as well as the most influential film of the 1970s. Hollywood studios increasingly focused on producing a smaller number of very large budget films with massive marketing and promotional campaigns. This trend had already been foreshadowed by the commercial success of disaster films such as The Poseidon Adventure and The Towering Inferno.\nDuring the mid-1970s, more pornographic theatres, euphemistically called \"adult cinemas\", were established, and the legal production of hardcore pornographic films began. Porn films such as Deep Throat and its star Linda Lovelace became something of a popular culture phenomenon and resulted in a spate of similar sex films. The porn cinemas finally died out during the 1980s, when the popularization of the home VCR and pornography videotapes allowed audiences to watch sex films at home. In the early 1970s, English-language audiences became more aware of the new West German cinema, with Werner Herzog, Rainer Werner Fassbinder and Wim Wenders among its leading exponents.\nIn world cinema, the 1970s saw a dramatic increase in the popularity of martial arts films, largely due to its reinvention by Bruce Lee, who departed from the artistic style of traditional Chinese martial arts films and added a much greater sense of realism to them with his Jeet Kune Do style. This began with The Big Boss (1971), which was a major success across Asia. However, he did not gain fame in the Western world until shortly after his death in 1973, when Enter the Dragon was released. The film went on to become the most successful martial arts film in cinematic history, popularized the martial arts film genre across the world, and cemented Bruce Lee's status as a cultural icon. Hong Kong action cinema, however, was in decline due to a wave of \"Bruceploitation\" films. This trend eventually came to an end in 1978 with the martial arts comedy films, Snake in the Eagle's Shadow and Drunken Master, directed by Yuen Woo-ping and starring Jackie Chan, laying the foundations for the rise of Hong Kong action cinema in the 1980s.\nWhile the musical film genre had declined in Hollywood by this time, musical films were quickly gaining popularity in the cinema of India, where the term \"Bollywood\" was coined for the growing Hindi film industry in Bombay (now Mumbai) that ended up dominating South Asian cinema, overtaking the more critically acclaimed Bengali film industry in popularity. Hindi filmmakers combined the Hollywood musical formula with the conventions of ancient Indian theatre to create a new film genre called \"Masala\", which dominated Indian cinema throughout the late 20th century. These \"Masala\" films portrayed action, comedy, drama, romance and melodrama all at once, with \"filmi\" song and dance routines thrown in. This trend began with films directed by Manmohan Desai and starring Amitabh Bachchan, who remains one of the most popular film stars in South Asia. The most popular Indian film of all time was Sholay (1975), a \"Masala\" film inspired by a real-life dacoit as well as Kurosawa's Seven Samurai and the Spaghetti Westerns.\nThe end of the decade saw the first major international marketing of Australian cinema, as Peter Weir's films Picnic at Hanging Rock and The Last Wave and Fred Schepisi's The Chant of Jimmie Blacksmith gained critical acclaim. In 1979, Australian filmmaker George Miller also garnered international attention for his violent, low-budget action film Mad Max.\n\n\n== 1980s ==\n\nDuring the 1980s, audiences began increasingly watching films on their home VCRs. In the early part of that decade, the film studios tried legal action to ban home ownership of VCRs as a violation of copyright, which proved unsuccessful. Eventually, the sale and rental of films on home video became a significant \"second venue\" for exhibition of films, and an additional source of revenue for the film industries. Direct-to-video (niche) markets usually offered lower quality, cheap productions that were not deemed very suitable for the general audiences of television and theatrical releases.\nThe Lucas\u2013Spielberg combine would dominate \"Hollywood\" cinema for much of the 1980s, and lead to much imitation. Two follow-ups to Star Wars, three to Jaws, and three Indiana Jones films helped to make sequels of successful films more of an expectation than ever before. Lucas also launched THX Ltd, a division of Lucasfilm in 1982, while Spielberg enjoyed one of the decade's greatest successes in E.T. the Extra-Terrestrial the same year. 1982 also saw the release of Disney's Tron which was one of the first films from a major studio to use computer graphics extensively. American independent cinema struggled more during the decade, although Martin Scorsese's Raging Bull (1980), After Hours (1985), and The King of Comedy (1983) helped to establish him as one of the most critically acclaimed American film makers of the era. Also during 1983 Scarface was released, which was very profitable and resulted in even greater fame for its leading actor Al Pacino. Probably the most successful film commercially was Tim Burton's 1989 version of Bob Kane's creation, Batman, which broke box-office records. Jack Nicholson's portrayal of the demented Joker earned him a total of $60,000,000 after figuring in his percentage of the gross.British cinema was given a boost during the early 1980s by the arrival of David Puttnam's company Goldcrest Films. The films Chariots of Fire, Gandhi, The Killing Fields and A Room with a View appealed to a \"middlebrow\" audience which was increasingly being ignored by the major Hollywood studios. While the films of the 1970s had helped to define modern blockbuster motion pictures, the way \"Hollywood\" released its films would now change. Films, for the most part, would premiere in a wider number of theatres, although, to this day, some films still premiere using the route of the limited/roadshow release system. Against some expectations, the rise of the multiplex cinema did not allow less mainstream films to be shown, but simply allowed the major blockbusters to be given an even greater number of screenings. However, films that had been overlooked in cinemas were increasingly being given a second chance on home video.\nDuring the 1980s, Japanese cinema experienced a revival, largely due to the success of anime films. At the beginning of the 1980s, Space Battleship Yamato (1973) and Mobile Suit Gundam (1979), both of which were unsuccessful as television series, were remade as films and became hugely successful in Japan. In particular, Mobile Suit Gundam sparked the Gundam franchise of Real Robot mecha anime. The success of Macross: Do You Remember Love? also sparked a Macross franchise of mecha anime. This was also the decade when Studio Ghibli was founded. The studio produced Hayao Miyazaki's first fantasy films, Nausica\u00e4 of the Valley of the Wind (1984) and Castle in the Sky (1986), as well as Isao Takahata's Grave of the Fireflies (1988), all of which were very successful in Japan and received worldwide critical acclaim. Original video animation (OVA) films also began during this decade; the most influential of these early OVA films was Noboru Ishiguro's cyberpunk film Megazone 23 (1985). The most famous anime film of this decade was Katsuhiro Otomo's cyberpunk film Akira (1988), which although initially unsuccessful at Japanese theaters, went on to become an international success.\nHong Kong action cinema, which was in a state of decline due to endless Bruceploitation films after the death of Bruce Lee, also experienced a revival in the 1980s, largely due to the reinvention of the action film genre by Jackie Chan. He had previously combined the comedy film and martial arts film genres successfully in the 1978 films Snake in the Eagle's Shadow and Drunken Master. The next step he took was in combining this comedy martial arts genre with a new emphasis on elaborate and highly dangerous stunts, reminiscent of the silent film era. The first film in this new style of action cinema was Project A (1983), which saw the formation of the Jackie Chan Stunt Team as well as the \"Three Brothers\" (Chan, Sammo Hung and Yuen Biao). The film added elaborate, dangerous stunts to the fights and slapstick humor, and became a huge success throughout the Far East. As a result, Chan continued this trend with martial arts action films containing even more elaborate and dangerous stunts, including Wheels on Meals (1984), Police Story (1985), Armour of God (1986), Project A Part II (1987), Police Story 2 (1988), and Dragons Forever (1988). Other new trends which began in the 1980s were the \"girls with guns\" subgenre, for which Michelle Yeoh gained fame; and especially the \"heroic bloodshed\" genre, revolving around Triads, largely pioneered by John Woo and for which Chow Yun-fat became famous. These Hong Kong action trends were later adopted by many Hollywood action films in the 1990s and 2000s.\n\n\n== 1990s ==\n\nThe early 1990s saw the development of a commercially successful independent cinema in the United States. Although cinema was increasingly dominated by special-effects films such as Terminator 2: Judgment Day (1991), Jurassic Park (1993) and Titanic (1997), the latter of which became the highest-grossing film of all time at the time up until Avatar (2009), also directed by James Cameron, independent films like Steven Soderbergh's Sex, Lies, and Videotape (1989) and Quentin Tarantino's Reservoir Dogs (1992) had significant commercial success both at the cinema and on home video.\nFilmmakers associated with the Danish film movement Dogme 95 introduced a manifesto aimed to purify filmmaking. Its first few films gained worldwide critical acclaim, after which the movement slowly faded out.\nScorsese's Goodfellas was released in 1990. It is considered by many as one of the greatest movies to be made, particularly in the gangster genre. It is said to be the highest point of Scorsese's career.\n\nMajor American studios began to create their own \"independent\" production companies to finance and produce non-mainstream fare. One of the most successful independents of the 1990s, Miramax Films, was bought by Disney the year before the release of Tarantino's runaway hit Pulp Fiction in 1994. The same year marked the beginning of film and video distribution online. Animated films aimed at family audiences also regained their popularity, with Disney's Beauty and the Beast (1991), Aladdin (1992), and The Lion King (1994). During 1995, the first feature-length computer-animated feature, Toy Story, was produced by Pixar Animation Studios and released by Disney. After the success of Toy Story, computer animation would grow to become the dominant technique for feature-length animation, which would allow competing film companies such as DreamWorks, 20th Century Fox and Warner Bros. to effectively compete with Disney with successful films of their own. During the late 1990s, another cinematic transition began, from physical film stock to digital cinema technology. Meanwhile, DVDs became the new standard for consumer video, replacing VHS tapes.\n\n\n== 2000s ==\n\nSince the late 2000s streaming media platforms like YouTube provided means for anyone with access to internet and cameras (a standard feature of smartphones) to publish videos to the world. Also competing with the increasing popularity of video games and other forms of home entertainment, the industry once again started to make theatrical releases more attractive, with new 3D technologies and epic (fantasy and superhero) films becoming a mainstay in cinemas.\nThe documentary film also rose as a commercial genre for perhaps the first time, with the success of films such as March of the Penguins and Michael Moore's Bowling for Columbine and Fahrenheit 9/11. A new genre was created with Martin Kunert and Eric Manes' Voices of Iraq, when 150 inexpensive DV cameras were distributed across Iraq, transforming ordinary people into collaborative filmmakers. The success of Gladiator led to a revival of interest in epic cinema, and Moulin Rouge! renewed interest in musical cinema. Home theatre systems became increasingly sophisticated, as did some of the special edition DVDs designed to be shown on them. The Lord of the Rings trilogy was released on DVD in both the theatrical version and in a special extended version intended only for home cinema audiences.\nIn 2001, the Harry Potter film series began, and by its end in 2011, it had become the highest-grossing film franchise of all time until the Marvel Cinematic Universe passed it in 2015.\nMore films were also being released simultaneously to IMAX cinema, the first was in 2002's Disney animation Treasure Planet; and the first live action was in 2003's The Matrix Revolutions and a re-release of The Matrix Reloaded. Later in the decade, The Dark Knight was the first major feature film to have been at least partially shot in IMAX technology.\nThere has been an increasing globalization of cinema during this decade, with foreign-language films gaining popularity in English-speaking markets. Examples of such films include Crouching Tiger, Hidden Dragon (Mandarin), Am\u00e9lie (French), Lagaan (Hindi), Spirited Away (Japanese), City of God (Brazilian Portuguese), The Passion of the Christ (Aramaic), Apocalypto (Mayan) and Inglourious Basterds (multiple European languages). Italy is the most awarded country at the Academy Award for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations.\nIn 2003, there was a revival in 3D film popularity the first being James Cameron's Ghosts of the Abyss which was released as the first full-length 3-D IMAX feature filmed with the Reality Camera System. This camera system used the latest HD video cameras, not film, and was built for Cameron by Emmy nominated Director of Photography Vince Pace, to his specifications. The same camera system was used to film Spy Kids 3D: Game Over (2003), Aliens of the Deep IMAX (2005), and The Adventures of Sharkboy and Lavagirl in 3-D (2005).\nAfter James Cameron's 3D film Avatar became the highest-grossing film of all time, 3D films gained brief popularity with many other films being released in 3D, with the best critical and financial successes being in the field of feature film animation such as Universal Pictures/Illumination Entertainment's Despicable Me and DreamWorks Animation's How To Train Your Dragon, Shrek Forever After and Megamind. Avatar is also note-worthy for pioneering highly sophisticated use of motion capture technology and influencing several other films such as Rise of the Planet of the Apes.\n\n\n== 2010s ==\n\nAs of 2011, the largest film industries by number of feature films produced were those of India, the United States, China, Nigeria, and Japan.In 2010, the first woman to win the Best Director Award in Oscar history appeared. Katherine Bigelow's The Hurt Locker won six awards.In Hollywood, superhero films have greatly increased in popularity and financial success, with films based on Marvel and DC Comics regularly being released every year up to the present. As of 2019, the superhero genre has been the most dominant as far as American box office receipts are concerned. The 2019 superhero film Avengers: Endgame was the most successful movie of all-time at the box office.\nIn 2020, Parasite became the first international film to win the Academy Award for Best Picture.\n\n\n== 2020s ==\n\n\n=== COVID-19 pandemic ===\n\nThe COVID-19 pandemic resulted in the closure of film theaters around the world in response to regional and national lockdowns. Many films slated to release in the early 2020s faced delays in development, production, and distribution, with others being released on streaming services with little or no theatrical window.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nMunslow, Alun (December 2007). \"Film and history: Robert A. Rosenstone and History on Film/Film on History\". Rethinking History. 4 (11): 565\u2013575. doi:10.1080/13642520701652103. S2CID 145006358.\nAbel, Richard. The Cine Goes to Town: French Cinema 1896\u20131914University of California Press, 1998.\nAcker, Ally. Reel Women: Pioneers of the Cinema, 1896 to the Present. London: B.T. Batsford, 1991.\nRobert C. Allen, Douglas Gomery: Film History. Theory and Practice, New York: Alfred Knopf, 1985\nBarr, Charles. All our yesterdays: 90 years of British cinema (British Film Institute, 1986).\nBasten, Fred E. Glorious Technicolor: The Movies' Magic Rainbow. AS Barnes & Company, 1980.\nBowser, Eileen. The Transformation of Cinema 1907\u20131915 (History of the American Cinema, Vol. 2) Charles Scribner's Sons, 1990.\nRawlence, Christopher (1990). The Missing Reel: The Untold Story of the Lost Inventor of Moving Pictures. Charles Atheneum. ISBN 978-0689120688.\nCook, David A. A History of Narrative Film, 2nd edition. New York: W. W. Norton, 1990.\nCousins, Mark. The Story of Film: A Worldwide History, New York: Thunder's Mouth press, 2006.\nDixon, Wheeler Winston and Gwendolyn Audrey Foster. A Short History of Film, 2nd edition. New Brunswick: Rutgers University Press, 2013.\nHennefeld, Maggie (December 2016). \"Death from Laughter, Female Hysteria, and Early Cinema\". differences: A Journal of Feminist Cultural Studies. Duke University Press. 27 (3): 45\u201392. doi:10.1215/10407391-3696631.\nKing, Geoff. New Hollywood Cinema: An Introduction. New York: Columbia University Press, 2002.\nKolker, Robert Phillip (2009). The Altering Eye: Contemporary International Cinema. Cambridge: Open Book Publishers. doi:10.11647/OBP.0002. ISBN 9781906924034.\nLandry, Marcia. British Genres: Cinema and Society, 1930\u20131960 (1991)\nMerritt, Greg. Celluloid Mavericks: A History of American Independent Film. Thunder's Mouth Press, 2001.\nMusser, Charles (1990). The Emergence of Cinema: The American Screen to 1907. New York: Charles Scribner's Sons. ISBN 0-684-18413-3.\nNowell-Smith, Geoffrey, ed. The Oxford History of World Cinema. Oxford University Press, 1999.\nParkinson, David. History of Film. New York: Thames & Hudson, 1995. ISBN 0-500-20277-X\nRocchio, Vincent F. Reel Racism. Confronting Hollywood's Construction of Afro-American Culture. Westview Press, 2000.\nSargeant, Amy. British Cinema: A Critical History (2008).\nSchrader, Paul. \"Notes on Film Noir\". Film Comment, 1984.\nSteele, Asa (February 1911). \"The Moving-Picture Show: ... How The Films Are Made, Who Writes The 'Plots', Who Censors The Plays, And What It All Costs\". The World's Work: A History of Our Time. XXI: 14018\u201314032. Retrieved 10 July 2009.\nTsivian, Yuri. Silent Witnesses: Russian Films 1908\u20131919 British Film Institute, 1989.\nUnterburger, Amy L. The St. James Women Filmmakers Encyclopedia: Women on the Other Side of the Camera. Visible Ink Press, 1999.\nUsai, P.C. & Codelli, L. (editors) Before Caligari: German Cinema, 1895\u20131920 Edizioni Biblioteca dell'Immagine, 1990.\n\n\n== External links ==\n Media related to History of cinema at Wikimedia Commons\nFilm history\nFilm history by decade\nCinema: From 1890 To Now\nThe History of the Discovery of Cinematography Archived 2020-10-05 at the Wayback Machine An Illustrated Chronology by Paul Burns\nWhat is a Camera Obscura?\nMuseum Of Motion Picture History, Inc.\nAn Introduction to Early cinema\nOrigins of Cinema Documentary\n\"Reality Film\". Archived from the original on 4 February 2008. Retrieved 24 January 2005.\nHistory of Film Formats\nFilm Sound History at FilmSound.org\nList of Early Sound Films 1894\u20131929 at Silent Era website\nEarly History of Wide Films \u2013 American Cinematographer, January 1930\nHollywood Movies History\nTechnicolor History\nA Brief, Early History of Computer Graphics in Film"}, {"id": 92, "title": "Electric car", "content": "An electric car or electric vehicle (EV) is a passenger automobile that is propelled by an electric traction motor, using only energy stored in on-board batteries. Compared to conventional internal combustion engine (ICE) vehicles, electric cars are quieter, more responsive, have superior energy conversion efficiency and no exhaust emissions and lower overall vehicle emissions (however the power plant supplying the electricity might generate its own emissions). The term \"electric car\" normally refers to battery electric vehicle (BEV), but broadly may also include plug-in hybrid electric vehicle (PHEV), range-extended electric vehicle (REEV) and fuel cell electric vehicle (FCEV).\nThe electric vehicle battery typically needs to be plugged into a mains electricity power supply for recharging in order to maximize the cruising range. Recharging an electric car can be done at a variety of charging stations; these charging stations can be installed in private homes, parking garages and public areas. There are also research and development in other technologies such as battery swapping and inductive charging. As the recharging infrastructures (especially those with fast chargers) are still in its relative infancy, range anxiety and time cost are frequent psychological obstacles against electric cars during consumer purchasing decisions.\nWorldwide, 10 million  plug-in electric cars were sold in 2022, a total of 14% of new car sales,  up from 9% in 2021.\nMany countries have established government incentives for plug-in electric vehicles, tax credits, subsidies, and other non-monetary incentives while several countries have legislated to phase-out sales of fossil fuel cars, to reduce air pollution and limit climate change.  EVs are expected to account for nearly one-fifth of global car sales in 2023, according to the International Energy Agency (IEA).China currently has the largest stock of electric vehicles in the world, with cumulative sales of 5.5 million units through December 2020, although these figures also include heavy-duty commercial vehicles such as buses, garbage trucks and sanitation vehicles, and only accounts for vehicles manufactured in China. In the United States and the European Union, as of 2020, the total cost of ownership of recent electric vehicles is cheaper than that of equivalent ICE cars, due to lower fueling and maintenance costs.In 2023 the Tesla Model Y became the world's best selling car.\nThe Tesla Model 3 became the world's all-time best-selling electric car in early 2020, and in June 2021 became the first electric car to pass 1 million global sales. Together with other emerging automotive technologies such as autonomous driving, connected vehicles and shared mobility, electric cars form a future mobility vision called Autonomous, Connected, Electric and Shared (ACES) Mobility.\n\n\n== Terminology ==\n\nThe term \"electric car\" typically refers specifically to battery electric vehicles (BEVs) or all-electric cars, a type of electric vehicle (EV) that has an onboard rechargeable battery pack that can be plugged in and charged from the electric grid, and the electricity stored on the vehicle is the only energy source that provide propulsion for the wheels. The term generally refers to highway-capable automobiles, but there are also low-speed electric vehicles with limitations in terms of weight, power and maximum speed that are allowed to travel on public roads. The latter are classified as Neighborhood Electric Vehicles (NEVs) in the United States, and as electric motorised quadricycles in Europe.\n\n\n== History ==\n\n\n=== Early developments ===\nRobert Anderson is often credited with inventing the first electric car some time between 1832 and 1839.The following experimental electric cars appeared during the 1880s:\n\nIn 1881, Gustave Trouv\u00e9 presented an electric car driven by an improved Siemens motor at the Exposition internationale d'\u00c9lectricit\u00e9 de Paris.\nIn 1882, Werner von Siemens presented the world's first trolleybus in Berlin\nIn 1884, over 20 years before the Ford Model T, Thomas Parker built an electric car in Wolverhampton using his own specially-designed high-capacity rechargeable batteries, although the only documentation is a photograph from 1895.\nIn 1888, the German Andreas Flocken designed the Flocken Elektrowagen, regarded by some as the first \"real\" electric car.\nIn 1890, Andrew Morrison introduced the first electric car to the United States. Electricity was among the preferred methods for automobile propulsion in the late-19th and early-20th centuries, providing a level of comfort and an ease of operation that could not be achieved by the gasoline-driven cars of the time. The electric vehicle fleet peaked at approximately 30,000 vehicles at the turn of the 20th century.In 1897, electric cars first found commercial use as taxis in Britain and in the United States. In London, Walter Bersey's electric cabs were the first self-propelled vehicles for hire at a time when cabs were horse-drawn. In New York City, a fleet of twelve hansom cabs and one brougham, based on the design of the Electrobat II, formed part of a project funded in part by the Electric Storage Battery Company of Philadelphia. During the 20th century, the main manufacturers of electric vehicles in the United States included Anthony Electric, Baker, Columbia, Anderson, Edison, Riker, Milburn, Bailey Electric, and Detroit Electric. Their electric vehicles were quieter than gasoline-powered ones, and did not require gear changes.Six electric cars held the land speed record in the 19th century. The last of them was the rocket-shaped La Jamais Contente, driven by Camille Jenatzy, which broke the 100 km/h (62 mph) speed barrier by reaching a top speed of 105.88 km/h (65.79 mph) in 1899.\nElectric cars remained popular until advances in internal-combustion engine (ICE) cars and mass production of cheaper gasoline- and diesel-powered vehicles, especially the Ford Model T, led to a decline. ICE cars' much quicker refueling times and cheaper production-costs made them more popular. However, a decisive moment came with the introduction in 1912 of the electric starter motor that replaced other, often laborious, methods of starting the ICE, such as hand-cranking.\n\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Modern electric cars ===\nIn the early 1990s the California Air Resources Board (CARB) began a push for more fuel-efficient, lower-emissions vehicles, with the ultimate goal of a move to zero-emissions vehicles such as electric vehicles. In response, automakers developed electric models. These early cars were eventually withdrawn from the U.S. market, because of a massive campaign by the US automakers to discredit the idea of electric cars.California electric-auto maker Tesla Motors began development in 2004 of what would become the Tesla Roadster, first delivered to customers in 2008. The Roadster was the first highway-legal all-electric car to use lithium-ion battery cells, and the first production all-electric car to travel more than 320 km (200 miles) per charge.Better Place, a venture-backed company based in Palo Alto, California, but steered from Israel, developed and sold battery charging and battery swapping services for electric cars. The company was publicly launched on 29 October 2007 and announced deployment of electric vehicle networks in Israel, Denmark and Hawaii in 2008 and 2009. The company planned to deploy the infrastructure on a country-by-country basis. In January 2008, Better Place announced a memorandum of understanding with Renault-Nissan to build the world's first Electric Recharge Grid Operator (ERGO) model for Israel. Under the agreement, Better Place would build the electric recharge grid and Renault-Nissan would provide the electric vehicles. Better Place filed for bankruptcy in Israel in May 2013. The company's financial difficulties were caused by mismanagement, wasteful efforts to establish toeholds and run pilots in too many countries, the high investment required to develop the charging and swapping infrastructure, and a market penetration far lower than originally predicted.The Mitsubishi i-MiEV, launched in 2009 in Japan, was the first highway-legal series production electric car, and also the first all-electric car to sell more than 10,000 units. Several months later, the Nissan Leaf, launched in 2010, surpassed the i MiEV as the best selling all-electric car at that time.Starting in 2008, a renaissance in electric vehicle manufacturing occurred due to advances in batteries, and the desire to reduce greenhouse-gas emissions and to improve urban air quality. During the 2010s, the electric vehicle industry in China expanded greatly with government support. The subsidies introduced by the Chinese government will however be cut by 20 to 30% and phased out completely before 2023. Several automakers marked up the prices of their electric vehicles in anticipation of the subsidy adjustment, including Tesla, Volkswagen and Guangzhou-based GAC Group, which counts Fiat, Honda, Isuzu, Mitsubishi, and Toyota as foreign partners.In July 2019 US-based Motor Trend magazine awarded the fully-electric Tesla Model S the title \"ultimate car of the year\". In March 2020 the Tesla Model 3 passed the Nissan Leaf to become the world's all-time best-selling electric car, with more than 500,000 units delivered; it reached the milestone of 1 million global sales in June 2021.\n\nIn the third quarter of 2021, the Alliance for Automotive Innovation reported that sales of electric vehicles had reached six percent of all US light-duty automotive sales, the highest volume of EV sales ever recorded at 187,000 vehicles. This was an 11% sales increase, as opposed to a 1.3% increase in gasoline and diesel-powered units. The report indicated that California was the US leader in EV with nearly 40% of US purchases, followed by Florida \u2013 6%, Texas \u2013 5% and New York 4.4%.Electric companies from the Middle East have been designing electric cars. Oman's Mays Motors have developed the Mays i E1 which is expected to begin production in 2023. Built from carbon fibre, it has a range of about 560 km (350 miles) and can accelerate from 0\u2013130 km/h (0\u201380 mph) in about 4 secs. In Turkey, the EV company Togg is starting production of its electric vehicles. Batteries will be created in a joint venture with the Chinese company Farasis Energy.\n\n\n== Economics ==\n\n\n=== Manufacturing cost ===\nThe most expensive part of an electric car is its battery. The price decreased from \u20ac605 per kWh in 2010, to \u20ac170 in 2017, to \u20ac100 in 2019. When designing an electric vehicle, manufacturers may find that for low production, converting existing platforms may be cheaper, as development cost is lower; however, for higher production, a dedicated platform may be preferred to optimize design, and cost.\n\n\n=== Total cost of ownership ===\nIn the EU and US, but not yet China, the total cost of ownership of recent electric cars is cheaper than that of equivalent gasoline cars, due to lower fueling and maintenance costs.The greater the distance driven per year, the more likely the total cost of ownership for an electric car will be less than for an equivalent ICE car. The break even distance varies by country depending on the taxes, subsidies, and different costs of energy.  In some countries the comparison may vary by city, as a type of car may have different charges to enter different cities; for example, in England, London charges ICE cars more than Birmingham does.\n\n\n==== Purchase cost ====\nSeveral national and local governments have established EV incentives to reduce the purchase price of electric cars and other plug-ins.As of 2020, the electric vehicle battery is more than a quarter of the total cost of the car. Purchase prices are expected to drop below those of new ICE cars when battery costs fall below US$100 per kWh, which is forecast to be in the mid-2020s.Leasing or subscriptions are popular in some countries, depending somewhat on national taxes and subsidies, and end of lease cars are expanding the second hand market.In a June 2022 report by AlixPartners,  the cost for raw materials on an average EV rose from $3,381 in March 2020 to $8,255 in May 2022. The cost increase voice is attributed mainly to lithium, nickel, and cobalt.\n\n\n==== Running costs ====\nElectricity almost always costs less than gasoline per kilometer travelled, but the price of electricity often varies depending on where and what time of day the car is charged. Cost savings are also affected by the price of gasoline which can vary by location.\n\n\n== Environmental aspects ==\n\nElectric cars have several benefits when replacing ICE cars, including a significant reduction of local air pollution, as they do not emit exhaust pollutants such as volatile organic compounds, hydrocarbons, carbon monoxide, ozone, lead, and various oxides of nitrogen. Similar to ICE vehicles, electric cars emit particulates from tyre and brake wear which may damage health, although regenerative braking in electric cars means less brake dust. More research is needed on non-exhaust particulates. The sourcing of fossil fuels (oil well to gasoline tank) causes further damage as well as use of resources during the extraction and refinement processes.\nDepending on the production process and the source of the electricity to charge the vehicle, emissions may be partly shifted from cities to the plants that generate electricity and produce the car as well as to the transportation of material. The amount of carbon dioxide emitted depends on the emissions of the electricity source and the efficiency of the vehicle. For electricity from the grid, the life-cycle emissions vary depending on the proportion of coal-fired power, but are always less than ICE cars.The cost of installing charging infrastructure has been estimated to be repaid by health cost savings in less than three years. According to a 2020 study, balancing lithium supply and demand for the rest of the century will require good recycling systems, vehicle-to-grid integration, and lower lithium intensity of transportation.Some activists and journalists have raised concerns over the perceived lack of impact of electric cars in solving the climate change crisis compared to other, less popularized methods. These concerns have largely centered around the existence of less carbon-intensive and more efficient forms of transportation such as active mobility, mass transit and e-scooters and the continuation of a system designed for cars first.\n\n\n=== Public opinion ===\nA 2022 survey found that 33% of car buyers in Europe will opt for a petrol or diesel car when purchasing a new vehicle. 67% of the respondents mentioned opting for the hybrid or electric version. More specifically, it found that electric cars are only preferred by 28% of Europeans, making them the least preferred type of vehicle. 39% of Europeans tend to prefer hybrid vehicles, while 33% prefer petrol or diesel vehicles.44% Chinese car buyers, on the other hand, are the most likely to buy an electric car, while 38% of Americans would opt for a hybrid car, 33% would prefer petrol or diesel, while only 29% would go for an electric car.Specifically for the EU, 47% of car buyers over 65 years old are likely to purchase a hybrid vehicle, while 31% of younger respondents do not consider hybrid vehicles a good option. 35% would rather opt for a petrol or diesel vehicle, and 24% for an electric car instead of a hybrid.In the EU, only 13% of the total population do not plan on owning a vehicle at all.\n\n\n== Performance ==\n\n\n=== Acceleration and drivetrain design ===\nElectric motors can provide high power-to-weight ratios. Batteries can be designed to supply the electrical current needed to support these motors. Electric motors have a flat torque curve down to zero speed. For simplicity and reliability, most electric cars use fixed-ratio gearboxes and have no clutch.\nMany electric cars have faster acceleration than average ICE cars, largely due to reduced drivetrain frictional losses and the more quickly-available torque of an electric motor. However, NEVs may have a low acceleration due to their relatively weak motors.\nElectric vehicles can also use a motor in each wheel hub or next to the wheels; this is rare but claimed to be safer. Electric vehicles that lack an axle, differential, or transmission can have less drivetrain inertia. Some direct current motor-equipped drag racer EVs have simple two-speed manual transmissions to improve top speed. The concept electric supercar Rimac Concept One claims it can go from 0\u201397 km/h (0\u201360 mph) in 2.5 seconds. Tesla claims the upcoming Tesla Roadster will go 0\u201360 mph (0\u201397 km/h) in 1.9 seconds.\n\n\n== Energy efficiency ==\n\nInternal combustion engines have thermodynamic limits on efficiency, expressed as a fraction of energy used to propel the vehicle compared to energy produced by burning fuel. Gasoline engines effectively use only 15% of the fuel energy content to move the vehicle or to power accessories; diesel engines can reach on-board efficiency of 20%; electric vehicles convert over 77% of the electrical energy from the grid to power at the wheels.Electric motors are more efficient than internal combustion engines in converting stored energy into driving a vehicle. However, they are not equally efficient at all speeds. To allow for this, some cars with dual electric motors have one electric motor with a gear optimised for city speeds and the second electric motor with a gear optimised for highway speeds. The electronics select the motor that has the best efficiency for the current speed and acceleration. Regenerative braking, which is most common in electric vehicles, can recover as much as one fifth of the energy normally lost during braking.\n\n\n=== Cabin heating and cooling ===\nCombustion powered cars harness waste heat from the engine to provide cabin heating, but this option is not available in an electric vehicle. While heating can be provided with an electric resistance heater, higher efficiency and integral cooling can be obtained with a reversible heat pump, such as on the Nissan Leaf. PTC junction cooling is also attractive for its simplicity\u2014this kind of system is used, for example, in the 2008 Tesla Roadster.\nTo avoid using part of the battery's energy for heating and thus reducing the range, some models allow the cabin to be heated while the car is plugged in. For example, the Nissan Leaf, the Mitsubishi i-MiEV, Renault Zoe and Tesla cars can be pre-heated while the vehicle is plugged in.Some electric cars (for example, the Citro\u00ebn Berlingo Electrique) use an auxiliary heating system (for example gasoline-fueled units manufactured by Webasto or Ebersp\u00e4cher) but sacrifice \"green\" and \"Zero emissions\" credentials. Cabin cooling can be augmented with solar power external batteries and USB fans or coolers, or by automatically allowing outside air to flow through the car when parked; two models of the 2010 Toyota Prius include this feature as an option.\n\n\n== Safety ==\nThe safety issues of BEVs are largely dealt with by the international standard ISO 6469. This document is divided into three parts dealing with specific issues:\n\nOn-board electrical energy storage, i.e. the battery\nFunctional safety means and protection against failures\nProtection of persons against electrical hazards\n\n\n=== Weight ===\nThe weight of the batteries themselves usually makes an EV heavier than a comparable gasoline vehicle. In a collision, the occupants of a heavy vehicle will, on average, suffer fewer and less serious injuries than the occupants of a lighter vehicle; therefore, the additional weight brings safety benefits to the occupant, while increasing harm to others. On average, an accident will cause about 50% more injuries to the occupants of a 2,000 lb (900 kg) vehicle than those in a 3,000 lb (1,400 kg) vehicle. Heavier cars are more dangerous to people outside the car if they hit a pedestrian or another vehicle.\n\n\n=== Stability ===\nThe battery in skateboard configuration lowers the center of gravity, increasing driving stability, lowering the risk of an accident through loss of control. If there is a separate motor near or in each wheel, this is claimed to be safer due to better handling.\n\n\n=== Risk of fire ===\n\nLike their ICE counterparts, electric vehicle batteries can catch fire after a crash or mechanical failure. Plug-in electric vehicle fire incidents have occurred, albeit fewer per distance traveled than ICE vehicles. Some cars' high-voltage systems are designed to shut down automatically in the event of an airbag deployment,  and in case of failure firefighters may be trained for manual high-voltage system shutdown. Much more water may be required than for ICE car fires and a thermal imaging camera is recommended to warn of possible re-ignition of battery fires.\n\n\n== Controls ==\nAs of 2018, most electric cars have similar driving controls to that of a car with a conventional automatic transmission. Even though the motor may be permanently connected to the wheels through a fixed-ratio gear, and no parking pawl may be present, the modes \"P\" and \"N\" are often still provided on the selector. In this case, the motor is disabled in \"N\" and an electrically actuated hand brake provides the \"P\" mode.\nIn some cars, the motor will spin slowly to provide a small amount of creep in \"D\", similar to a traditional automatic transmission car.When an internal combustion vehicle's accelerator is released, it may slow by engine braking, depending on the type of transmission and mode. EVs are usually equipped with regenerative braking that slows the vehicle and recharges the battery somewhat. Regenerative braking systems also decrease the use of the conventional brakes (similar to engine braking in an ICE vehicle), reducing brake wear and maintenance costs.\n\n\n== Batteries ==\n\nLithium-ion-based batteries are often used for their high power and energy density. Batteries with different chemical compositions are becoming more widely used, such as lithium iron phosphate which is not dependent on nickel and cobalt so can be used to make cheaper batteries and thus cheaper cars.\n\n\n=== Range ===\n\nThe range of an electric car depends on the number and type of batteries used, and (as with all vehicles), the aerodynamics, weight and type of vehicle, performance requirements, and the weather. Cars marketed for mainly city use are often manufactured with a short range battery to keep them small and light.Most electric cars are fitted with a display of the expected range. This may take into account how the vehicle is being used and what the battery is powering. However, since factors can vary over the route, the estimate can vary from the actual range. The display allows the driver to make informed choices about driving speed and whether to stop at a charging point en route. Some roadside assistance organizations offer charge trucks to recharge electric cars in case of emergency.\n\n\n=== Charging ===\n\n\n==== Connectors ====\n\nMost electric cars use a wired connection to supply electricity for recharging. Electric vehicle charging plugs are not universal throughout the world. However vehicles using one type of plug are generally able to charge at other types of charging stations through the use of plug adapters.The Type 2 connector is the most common type of plug, but different versions are used in China and Europe.The Type 1 (also called SAE J1772) connector is common in North America but rare elsewhere, as it does not support three-phase charging.Wireless charging, either for stationary cars or as an electric road, is less common as of 2021, but is used in some cities for taxis.\n\n\n==== Home charging ====\nElectric cars are usually charged overnight from a home charging station;  sometimes known as a charging point, wallbox charger, or simply a charger; in a garage or on the outside of a house. As of 2021 typical home chargers are 7 kW, but not all include smart charging. Compared to fossil fuel vehicles, the need for charging using public infrastructure is diminished because of the opportunities for home charging; vehicles can be plugged in and begin each day with a full charge. Charging from a standard outlet is also possible but very slow.\n\n\n==== Public charging ====\n\nPublic charging stations are almost always faster than home chargers, with many supplying direct current to avoid the bottleneck of going through the car's AC to DC converter, as of 2021 the fastest being 350 kW.Combined Charging System (CCS) is the most widespread charging standard, whereas the GB/T 27930 standard is used in China, and CHAdeMO in Japan. The United States has no de facto standard, with a mix of CCS, Tesla Superchargers, and CHAdeMO charging stations.\nCharging an electric vehicle using public charging stations takes longer than refueling a fossil fuel vehicle. The speed at which a vehicle can recharge depends on the charging station's charging speed and the vehicle's own capacity to receive a charge.  As of 2021 some cars are 400 volt and some 800 volt. Connecting a vehicle that can accommodate very fast charging to a charging station with a very high rate of charge can refill the vehicle's battery to 80% in 15 minutes. Vehicles and charging stations with slower charging speeds may take as long as two hours to refill a battery to 80%. As with a mobile phone, the final 20% takes longer because the systems slow down to fill the battery safely and avoid damaging it.\nSome companies are building battery swapping stations, to substantially reduce the effective time to recharge. Some electric cars (for example, the BMW i3) have an optional gasoline range extender. The system is intended as an emergency backup to extend range to the next recharging location, and not for long-distance travel.\n\n\n===== Electric roads =====\n\nElectric road technologies which power and charge electric vehicles while driving were assessed in Sweden from 2013.:\u200a12\u200a The assessment was scheduled to conclude in 2022. The first standard for electrical equipment on board a vehicle powered by a rail electric road system (ERS), CENELEC Technical Standard 50717, has been approved in late 2022. Following standards, encompassing \"full interoperability\" and a \"unified and interoperable solution\" for ground-level power supply, are scheduled to be published by the end 2024, detailing complete \"specifications for communication and power supply through conductive rails embedded in the road\". The first permanent electric road in Sweden is planned to be completed by 2026 on a section of the E20 route between Hallsberg and \u00d6rebro, followed by an expansion of further 3000 kilometers of electric roads by 2045. A working group of the French Ministry of Ecology considers ground-level power supply technologies the most likely candidate for electric roads, and recommended adopting a European electric road standard formulated with Sweden, Germany, Italy, the Netherlands, Spain, Poland, and others. France plans to invest 30 to 40 billion euro by 2035 in an electric road system spanning 8,800 kilometers that recharges electric cars, buses and trucks while driving. Two tenders for assessment of electric road technologies are expected to be announced by 2023.\n\n\n==== Vehicle-to-grid: uploading and grid buffering ====\n\nDuring peak load periods, when the cost of generation can be very high, electric vehicles with vehicle-to-grid capabilities could contribute energy to the grid. These vehicles can then be recharged during off-peak hours at cheaper rates while helping to absorb excess night time generation. The batteries in the vehicles serve as a distributed storage system to buffer power.\n\n\n=== Lifespan ===\n\nAs with all lithium-ion batteries, electric vehicle batteries may degrade over long periods of time, especially if they are frequently charged to 100%; however, this may take at least several years before being noticeable. A typical warranty is 8 years or 100,000 mi (160,000 km), but they usually last much longer, perhaps 15 to 20 years in the car and then more years in another use.\n\n\n== Currently available electric cars ==\n\n\n=== Sales of electric cars ===\n\nTesla became the world's leading electric vehicle manufacturer in December 2019. Its Model S was the world's top selling plug-in electric car in 2015 and 2016, its Model 3 has been the world's best selling plug-in electric car for four consecutive years, from 2018 to 2021, and the Model Y was the top selling plug-in car in 2022. The Tesla Model 3 surpassed the Leaf in early 2020 to become the world's cumulative best selling electric car. Tesla produced its 1 millionth electric car in March 2020, becoming the first auto manufacturer to do so, and in June 2021, the Model 3 became the first electric car to pass 1 million sales. Tesla has been listed as the world's top selling plug-in electric car manufacturer, both as a brand and by automotive group for four years running, from 2018 to 2021. At the end of 2021, Tesla's global cumulative sales since 2012 totaled 2.3 million units, with 936,222 of those delivered in 2021.As of December 2021, the Renault\u2013Nissan\u2013Mitsubishi Alliance listed as one of the world's leading all-electric vehicle manufacturers, with global all-electric vehicle sales totaling over 1 million light-duty electric vehicles, including those manufactured by Mitsubishi Motors since 2009.  Nissan leads global sales within the Alliance, with 1 million cars and vans sold by July 2023, followed by the Groupe Renault with more than 397,000 electric vehicles sold worldwide through December 2020, including its Twizy heavy quadricycle. Mitsubishi's only all-electric vehicle is the i-MiEV, with global sales of over 50,000 units by March 2015, accounting for all variants of the i-MiEV, including the two minicab versions sold in Japan. The Alliance's best-selling Nissan Leaf was the world's top-selling plug-in electric car in 2013 and 2014. Until early 2020, the Nissan Leaf was the world's all-time top-selling highway-legal electric car, and, as of July 2023, global sales totaled over 650,000 units since inception.Other leading electric vehicles manufacturers are BAIC Motor, with 480,000 units sold, SAIC Motor with 314,000 units, and Geely with 228,700, all cumulative sales in China as of December 2019, and Volkswagen.The following table lists the all-time best-selling highway-capable all-electric cars with cumulative global sales of over 250,000 units:\n\n\n=== Electric cars by country ===\n\nIn the year of 2021, the total number of electric cars on the world's roads went to about 16.5 million. The sales of electric cars in the first quarter of 2022 went up to 2 million. China has the largest all-electric car fleet in use, with 2.58 million at the end of 2019, more than half (53.9%) of the world's electric car stock.\nAll-electric cars have oversold plug-in hybrids since 2012.\n\n\n== Government policies and incentives ==\n\nSeveral national, provincial, and local governments around the world have introduced policies to support the mass-market adoption of plug-in electric vehicles. A variety of policies have been established to provide: financial support to consumers and manufacturers; non-monetary incentives; subsidies for the deployment of charging infrastructure; electric vehicle charging stations in buildings; and long-term regulations with specific targets.\nFinancial incentives for consumers are aiming to make electric car purchase price competitive with conventional cars due to the higher upfront cost of electric vehicles. Depending on battery size, there are one-time purchase incentives such as grants and tax credits; exemptions from import duties; exemptions from road tolls and congestion charges; and exemption of registration and annual fees.\nAmong the non-monetary incentives, there are several perks such allowing plug-in vehicles access to bus lanes and high-occupancy vehicle lanes, free parking and free charging. Some countries or cities that restrict private car ownership (for example, a purchase quota system for new vehicles), or have implemented permanent driving restrictions (for example, no-drive days), have these schemes exclude electric vehicles to promote their adoption. Several countries, including England and India, are introducing regulations that require electric vehicle charging stations in certain buildings.Some government have also established long term regulatory signals with specific targets such as zero-emissions vehicle (ZEV) mandates, national or regional CO2 emission regulations, stringent fuel economy standards, and the phase out of internal combustion engine vehicle sales. For example, Norway set a national goal that by 2025 all new car sales should be ZEVs (battery electric or hydrogen). While these incentives aim to facilitate a quicker transition from internal combustion cars, they have been criticized by some economists for creating excess deadweight loss in the electric car market, which may partially counteract environmental gains.\n\n\n== EV plans from major manufacturers ==\nElectric vehicles (EVs) have gained significant traction as an integral component of the global automotive landscape in recent years. Major automakers from around the world have adopted EVs as a critical component of their strategic plans, indicating a paradigm shift toward sustainable transportation.\n\n\n== Forecasts ==\nTotal global EV sales in 2030 were predicted to reach 31.1 million by Deloitte. The International Energy Agency predicted that the total global stock of EVs would reach almost 145 million by 2030 under current policies, or 230 million if Sustainable Development policies were adopted.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nHow an electric car works\nWikiversity:Can electric cars significantly help humanity get off fossil fuels?\nElectric cars range in 2022: table with 100 different models"}, {"id": 93, "title": "Streaming television", "content": "Streaming television is the digital distribution of television content, such as television shows and films, as streaming media delivered over the Internet.  Streaming television stands in contrast to dedicated terrestrial television delivered by over-the-air aerial systems, cable television, and/or satellite television systems.\n\n\n== History ==\nUp until the 1990s, it was not thought possible that a television show could be squeezed into the limited telecommunication bandwidth of a copper telephone cable to provide a streaming service of acceptable quality, as the required bandwidth of a digital television signal was around 200 Mbit/s, which was 2,000 times greater than the bandwidth of a speech signal over a copper telephone wire.Streaming services started as a result of two major technological developments: MPEG (motion-compensated DCT) video compression and asymmetric digital subscriber line (ADSL) data communication.The first worldwide live-streaming event was a radio live broadcast of a baseball game between the Seattle Mariners and the New York Yankees streamed by ESPN SportsZone on September 5, 1995. During the mid-2000s, the streaming media was based on UDP, whereas the basis of the majority of the Internet was HTTP and content delivery networks (CDNs). In 2007, HTTP-based adaptive streaming was introduced by Move Networks. This new technology would be a significant change for the industry. One year later the introduction of HTTP-based adaptive streaming, many companies such as Microsoft and Netflix developed their streaming technology. In 2009, Apple launched HTTP Live Streaming (HLS), and Adobe, in 2010, HTTP Dynamic Streaming (HDS). In addition, HTTP-based adaptive streaming was chosen for important streaming events such as Roland Garros, Wimbledon, Vancouver and London Olympic Games, and many others and on premium on-demand services (Netflix, Amazon Instant Video, etc.). The increase in streaming services required a new standardization, therefore in 2012, with the contributions of Apple, Netflix, Microsoft, and other companies, Dynamic Adaptive Streaming, known as MPEG-DASH. substituted HTTP.The mid-2000s were the beginning of television programs becoming available via the Internet. The Online video platform site YouTube was launched in early 2005, allowing users to share illegally posted television programs. YouTube co-founder Jawed Karim said the inspiration for YouTube first came from Janet Jackson's role in the 2004 Super Bowl incident, when her breast was exposed during her performance, and later from the 2004 Indian Ocean tsunami. Karim could not easily find video clips of either event online, which led to the idea of a video sharing site.Apple's iTunes service also began offering select television programs and series in 2005, available for download after direct payment. A few years later, television networks and other independent services began creating sites where shows and programs could be streamed online. Amazon Prime Video began in the United States as Amazon Unbox in 2006, but did not launch worldwide until 2016. Netflix, a website originally created for DVD rentals and sales, began providing streaming content in 2007. In 2008 Hulu, owned by NBC and Fox, was launched, followed by tv.com in 2009, owned by CBS. The first generation Apple TV was released in 2007 and in 2008 the first generation Roku streaming device was announced. Digital media players also began to become available to the public during this time. These digital media players have continued to be updated and new generations released.Smart TVs took over the television market after 2010 and continue to partner with new providers to bring streaming video to even more users. As of 2015, smart TVs are the only type of middle to high-end television being produced. Amazon's version of a digital media player, Amazon Fire TV, was not offered to the public until 2014.Access to television programming has evolved from computer and television access to include mobile devices such as smartphones and tablet computers. Corresponding apps for mobile devices started to become available via app stores in 2008, but they grew in popularity in the 2010s with the rapid deployment of LTE cellular network. These mobile apps allow users to view provided streaming media on mobile devices which support them.\nIn 2008, the International Academy of Web Television, headquartered in Los Angeles, formed in order to organize and support television actors, authors, executives, and producers in web series and streaming television. The organization also administers the selection of winners for the Streamy Awards. In 2009, the Los Angeles Web Series Festival was founded. Several other festivals and award shows have been dedicated solely to web content, including the Indie Series Awards and the Vancouver Web Series Festival. In 2013, in response to the shifting of the soap opera All My Children from broadcast to streaming television, a new category for \"Fantastic web-only series\" in the Daytime Emmy Awards was created. Later that year, Netflix made history by earning the first Primetime Emmy Award nominations for a streaming television series, for Arrested Development, Hemlock Grove, and House of Cards, at the 65th Primetime Emmy Awards. Hulu earned the first Emmy win for Outstanding Drama Series, for The Handmaid's Tale at the 69th Primetime Emmy Awards.\nTraditional cable and satellite television providers began to offer services such as Sling TV, owned by Dish Network, which was unveiled in January 2015. DirecTV, another satellite television provider launched their own streaming service, DirecTV Stream, in 2016. Sky launched a similar streaming service in the UK called Now.\nIn 2013, Video on demand website Netflix earned the first Primetime Emmy Award nominations for original streaming television at the 65th Primetime Emmy Awards. Three of its series, House of Cards, Arrested Development, and Hemlock Grove, earned nominations that year. On July 13, 2015, cable company Comcast announced an HBO plus broadcast TV package at a price discounted from basic broadband plus basic cable.In 2017, YouTube launched YouTube TV, a streaming service that allows users to watch live television programs from popular cable or network channels, and record shows to stream anywhere, anytime. As of 2017, 28% of US adults cite streaming services as their main means for watching television, and 61% of those ages 18 to 29 cite it as their main method. As of 2018, Netflix is the world's largest streaming TV network and also the world's largest Internet media and entertainment company with 117 million paid subscribers, and by revenue and market cap. In 2020, the COVID-19 pandemic had a strong impact in the television streaming business with the lifestyle changes such as staying at home and lockdowns.\n\n\n== Technology ==\nThe Hybrid Broadcast Broadband TV (HbbTV) consortium of industry companies (such as SES, Humax, Philips, and ANT Software) is currently promoting and establishing an open European standard for hybrid set-top boxes for the reception of broadcast and broadband digital television and multimedia applications with a single-user interface.BBC iPlayer originally incorporated peer-to-peer streaming, moved towards centralized distribution for their video streaming services.  BBC executive Anthony Rose cited network performance as an important factor in the decision, as well as consumers being unhappy with their own network bandwidth being used for transmitting content to other viewers. Samsung TV has also announced their plans to provide streaming options including 3D Video on Demand through their Explore 3D service.\n\n\n=== Access control ===\nSome streaming services incorporate digital rights management.  The W3C made the controversial decision to adopt Encrypted Media Extensions due in large part to motivations to provide copy protection for streaming content. Sky Go has software that is provided by Microsoft to prevent content being copied.Additionally, BBC iPlayer makes use of a parental control system giving users the option to \"lock\" content, requiring a password to access it.  The goal of these systems is to enable parents to keep children from viewing sexually themed, violent, or otherwise age-inappropriate material. Flagging systems can be used to warn a user that content may be certified or that it is intended for viewing post-watershed. Honour systems are also used where users are asked for their dates of birth or age to verify if they are able to view certain content.\n\n\n=== IPTV ===\n\nIPTV delivers television content using signals based on the Internet Protocol (IP), through the open, unmanaged Internet with the \"last-mile\" telecom company acting only as the Internet service provider (ISP). As described above, \"Internet television\" is \"over-the-top technology\" (OTT). Both IPTV and OTT use the Internet protocol over a packet-switched network to transmit data, but IPTV operates in a closed system\u2014a dedicated, managed network controlled by the local cable, satellite, telephone, or fiber-optic company. In its simplest form, IPTV simply replaces traditional circuit switched analog or digital television channels with digital channels which happen to use packet-switched transmission. In both the old and new systems, subscribers have set-top boxes or other customer-premises equipment that communicates directly over company-owned or dedicated leased lines with central-office servers. Packets never travel over the public Internet, so the television provider can guarantee enough local bandwidth for each customer's needs.\nThe Internet protocol is a cheap, standardized way to enable two-way communication and simultaneously provide different data (e.g., TV-show files, email, Web browsing) to different customers. This supports DVR-like features for time shifting television: for example, to catch up on a TV show that was broadcast hours or days ago, or to replay the current TV show from its beginning. It also supports video on demand\u2014browsing a catalog of videos (such as movies or television shows) which might be unrelated to the company's scheduled broadcasts.\nIPTV has an ongoing standardization process (for example, at the European Telecommunications Standards Institute).\n\n\n== Streaming quality ==\nStreaming quality is the quality of image and audio transmission from the servers of the distributor to the user's screen. Also, Streaming resolution helps to measure the size of the streaming quality of video pixels. High-definition video (720p+) and later standards require higher bandwidth and faster connection speeds than previous standards, because they carry higher spatial resolution image content. In addition, transmission packet loss and latency caused by network impairments and insufficient bandwidth degrade replay quality. Decoding errors may manifest themselves with video breakup and macro blocks. The generally accepted download rate for streaming high-definition (1080p) video encoded in AVC is 6000 kbit/s, whereas UHD requires upwards of 16,000 kbit/s.For users who do not have the bandwidth to stream HD/4K video or even SD video, most streaming platforms make use of an adaptive bitrate stream so that if the user's bandwidth suddenly drops, the platform will lower its streaming bitrate to compensate. Most modern television streaming platforms offer a wide range of both manual and automatic bitrate settings which are based on initial connection tests during the first few seconds of a video loading, and can be changed on the fly. This is valid for both Live and Catch-up content. Additionally, platforms can also offer content in standards such as HDR or Dolby Vision or at higher framerates which can require additional costs or subscription tiers to access.\n\n\n== Usage ==\nInternet television is common in most US households as of the mid-2010s. In a 2013 study by eMarketer, about one in four new televisions being sold is a smart TV. Within the same decade, rapid deployment of LTE cellular network and general availability of smartphones have increased popularity of the streaming services, and the corresponding apps on mobile devices. On August 18, 2022, Nielsen reported that for the first time, streaming viewership has surpassed cable.\nConsidering the popularity of smart TVs, smartphones, and devices such as the Roku and Chromecast, much of the US public can watch television via the Internet. Internet-only channels are now established enough to feature some Emmy-nominated shows, such as Netflix's House of Cards. Many networks also distribute their shows the next day to streaming providers such as Hulu Some networks may use a proprietary system, such as the BBC utilizes their BBC iPlayer format. This has resulted in bandwidth demands increasing to the point of causing issues for some networks. It was reported in February 2014 that Verizon Fios is having issues coping with the demand placed on their network infrastructure. Until long-term bandwidth issues are worked out and regulation such at net neutrality Internet Televisions push to HDTV may start to hinder growth.Aereo was launched in March 2012 in New York City (and subsequently stopped from broadcasting in June 2014). It streamed network TV only to New York customers over the Internet. Broadcasters filed lawsuits against Aereo, because Aereo captured broadcast signals and streamed the content to Aereo's customers without paying broadcasters. In mid-July 2012, a federal judge sided with the Aereo start-up. Aereo planned to expand to every major metropolitan area by the end of 2013. The Supreme Court ruled against Aereo June 24, 2014.Some have noted that as opposed to broadcast television, with demographics of mostly \"unspokenly straight\" white viewers, cable, and with streaming services, dollars from subscription can \"level the playing field,\" giving viewers from marginalized communities, and representation of their communities, \"equal power.\"\n\n\n== Market competitors ==\nMany providers of Internet television services exist\u2014including conventional television stations that have taken advantage of the Internet as a way to continue showing television shows after they have been broadcast, often advertised as \"on-demand\" and \"catch-up\" services. Today, almost every major broadcaster around the world is operating an Internet television platform. Examples include the BBC, which introduced the BBC iPlayer on 25 June 2008 as an extension to its \"RadioPlayer\" and already existing streamed video-clip content, and Channel 4 that launched 4oD (\"4 on Demand\") (now All 4) in November 2006 allowing users to watch recently shown content. Most Internet television services allow users to view content free of charge; however, some content is for a fee.\nSince 2012, around 200 over-the-top (OTT) platforms providing streamed and downloadable content have emerged. Investment by Netflix in new original content for its OTT platform reached $13bn in 2018.\n\n\n== Streaming platforms ==\n\n\n=== Netflix ===\nNetflix, founded by Reed Hastings and Marc Randolph, is a media streaming and video rental in 1997. Two years later, Netflix was offering the audience the possibility of an online subscription service. Subscribers could select movies and TV shows on Netflix's website and receive the chosen titles via DVDs in prepaid return envelopes. In 2007, Netflix's subscribers could watch some movies and TV shows online, directly from their homes. In 2010, Netflix launched an only-streaming plan with unlimited streaming services without DVDs. Starting from the United States, the only-streaming plan reached several countries; by 2016 more than 190 countries could use this service. In 2011, Netflix began to negotiate the production of original programming, starting with the series House of Cards.\n\n\n=== Amazon Prime Video ===\nAmazon Prime Video was originally launched in the year 2006. Upon its initial release, the popular streaming service was referred to as Amazon Unbox. Amazon Prime Video was created due to the development of Amazon Prime, which is a paid service that includes free shipping of different types of goods. Amazon Prime Video is available in approximately 200 countries around the world. Each year, Amazon invests in the production of films and TV series that are streamed as Amazon originals. \n\n\n=== Hulu ===\nHulu was created in 2007 and opened to the audience one year after its launch. Unlike other streaming platforms, it is only accessible in the United States due to international licensing restrictions. Hulu is one of the only streaming services that provides streaming for current on air television shows a few days after their original broadcast on cable tv. The streaming of these specific programs are accessible for a limited time. Upon launching, Hulu originally had both a free and paid plan for users. The free plan was accessible only via computer and there was a limited amount of content for users, whereas the paid plan could be accessible via computers, mobile devices, and connected television, and the number of content was larger than the free plan. In 2019, Walt Disney became the major owner of Hulu. The platform now has bundle deals where you can subscribe to both Hulu and Disney + for a certain amount of money.\n\n\n=== YouTube ===\nThe domain name of YouTube was bought and activated by Chad Hurley, Steve Chen, and Jawed Karim in the beginning of 2005. YouTube launched later that year as an online video sharing and social media platform. The video platform became popular among the audience thanks to a short video, called Lazy Sunday, uploaded by Saturday Night Live in December 2005. The SNL's video was not broadcast on TV, therefore people looked for it on Google by typing \"SNL rap video,\" \"Lazy Sunday SNL,\" or \"Chronicles of Narnia SNL.\" The first result of searches was a link video on YouTube, which was the beginning of sharing videos on YouTube. Because of its popularity, YouTube had some issues caused by its bandwidth expenses. In 2006, Google bought Youtube, and after some months the video platform was the second-largest engine search in the world.  \n\n\n=== Disney+ ===\nDisney+ is an American subscription streaming service owned and operated by the Disney Entertainment division of The Walt Disney Company. Released on November 12, 2019, the service primarily distributes films and television series produced by The Walt Disney Studios and Walt Disney Television, with dedicated content hubs for the brands The Walt Disney Company, Pixar, Marvel, Star Wars, and National Geographic, as well as Star in some regions. Original films and television series are also distributed on Disney+.\n\n\n=== Max ===\nMax is a streaming service released by Warner Bros. Discovery. The platform was released on May 27, 2020 in the United States, and within the first five months of launching, had amassed 8 million subscribers across the country. It offers classic Warner Bros. films and self-produced programs, and has won the right to exclusively air Ghibli Studios films in the United States. It is not until 45 days after the theatrical release from 2022 that the release is taking place on the platform and reached 70 million subscribers in December 2021. In September 2022, 92 million households were counted as subscribers, but since this was announced, including subscribers to the HBO channel, it is expected that the actual population of Max alone will be much smaller.\n\n\n=== Paramount + ===\nParamount+ is a streaming service that is owned by the Paramount Global Media Company. The streaming service was launched on October 28, 2014, and was known as CBS All Access originally. At the time of the release, the platform focused primarily on streaming programs from local CBS stations as well as complete access to all CBS network content. In 2016 the streaming service created original content that could only be found by using the platform. As the network continued to expand with its content, the service decided to rebrand themselves and took the name Paramount+, taking its name from Paramount Pictures film studio. The network has gone on to expand their services globally to places such as Latin America, Europe and Australia.\n\n\n=== Apple TV+ ===\nApple TV+ is a streaming service owned by Apple Inc. Apple TV+ is a streaming subscription platform that launched November 1, 2019. The service offers original content exclusively made by Apple, being seen as Apple Originals. This streaming platform solely releases content that can only be found on Apple TV+, there is no third party content found on the platform whereas several other streaming services have third party content. The AppleTV+ name derives from the Apple TV media player that was released in 2007.\n\n\n=== Peacock ===\nPeacock is a streaming service owned and operated by Peacock TV, which is a subsidiary of NBC Universal Television and Streaming. The streaming service gets its name from the NBC logo based on its colors. The platform had launched on July 15, 2020. The streaming service primarily features content that can be found on NBC networking channels as well as other third party sources. Additionally, Peacock now offers original content that cannot be found on any other streaming platform. In December 2022, Peacock reached 20 million paid subscribers. The following year in March 2023, the platform peaked at 22 million paid subscribers.\n\n\n== Binge-watching ==\nIn the 90s, the conception and the practice of watching entire seasons in a short amount of time emerged with the introduction of the DVD box. Media marathoning consists in watching at least one season of a TV show in a week or less, watching three or more films from the same series in a week or less, or reading three or more books from the same series in a month or less. The term \"binge-watching\" arrived with streaming TV, more precisely, when Netflix launched its first original production, House of Cards, and started marketing this process of watching TV series episode after episode in 2013. COVID-19 gave another connotation to binge-watching, which was considered a negative activity. Watching TV shows during the lockdown became something normal and a sort of healing for people.\n\n\n== Broadcasting rights ==\nBroadcasting rights (also called Streaming rights in this case) vary from country to country and even within provinces of countries. These rights govern the distribution of copyrighted content and media and allow the sole distribution of that content at any one time. An example of content only being aired in certain countries is BBC iPlayer. The BBC checks a user's IP address to make sure that only users located in the UK can stream content from the BBC. The BBC only allows free use of their product for users within the UK as those users have paid for a television license that funds part of the BBC. This IP address check is not foolproof as the user may be accessing the BBC website through a VPN or proxy server. Broadcasting rights can also be restricted to allowing a broadcaster rights to distribute that content for a limited time. Channel 4's online service All 4 can only stream shows created in the US by companies such as HBO for thirty days after they are aired on one of the Channel 4 group channels. This is to boost DVD sales for the companies who produce that media.\nSome companies pay very large amounts for broadcasting rights with sports and US sitcoms usually fetching the highest price from UK-based broadcasters. A trend among major content producers in North America is the use of the \"TV Everywhere\" system. Especially for live content, the TV Everywhere system restricts viewership of a video feed to select Internet service providers, usually cable television companies that pay a retransmission consent or subscription fee to the content producer. This often has the negative effect of making the availability of content dependent upon the provider, with the consumer having little or no choice on whether they receive the product.\n\n\n== Profits and costs ==\nWith the advent of broadband Internet connections, multiple streaming providers have come onto the market in the last couple of years. The main providers are Netflix, Hulu and Amazon. Some of these providers such as Hulu advertise and charge a monthly fee. Other such as Netflix and Amazon charge users a monthly fee and have no commercials. Netflix is the largest provider; it currently has over 217 million members. The rise of internet TV has resulted in cable companies losing customers to a new kind of customer called \"cord cutters\". Cord cutters are consumers who are cancelling their cable TV or satellite TV subscriptions and choosing instead to stream TV shows, movies and other content via the Internet. Cord cutters are forming communities. With the increasing availability of Online video platform (e.g., YouTube) and streaming services, there is an alternative to cable and satellite television subscriptions. Cord cutters tend to be younger people.\n\n\n== Overview of platforms and availability ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nIPTV future The Register 2006-05-05\nAs Internet TV Aims at Niche Audiences, the Slivercast Is Born New York Times 2006-03-12\nTV's future stars will come from the webThe Guardian 2008-09-11"}, {"id": 94, "title": "Data Encryption Standard", "content": "The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.\nDeveloped in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.The publication of an NSA-approved encryption standard led to its quick international adoption and widespread academic scrutiny. Controversies arose from classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, raising suspicions about a backdoor.  The S-boxes that had prompted those suspicions were designed by the NSA to remove a backdoor they secretly knew (differential cryptanalysis). However, the NSA also ensured that the key size was drastically reduced so that they could break the cipher by brute force attack. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.\nDES is insecure due to the relatively short 56-bit key size. In January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see \u00a7 Chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. This cipher has been superseded by the Advanced Encryption Standard (AES). DES has been withdrawn as a standard by the National Institute of Standards and Technology.Some documents distinguish between the DES standard and its algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm).\n\n\n== History ==\nThe origins of DES date to 1972, when a National Bureau of Standards study of US government computer security identified a need for a government-wide standard for encrypting unclassified, sensitive information.Around the same time, engineer Mohamed Atalla in 1972 founded Atalla Corporation and developed the first hardware security module (HSM), the so-called \"Atalla Box\" which was commercialized in 1973. It protected offline devices with a secure PIN generating key, and was a commercial success. Banks and credit card companies were fearful that Atalla would dominate the market, which spurred the development of an international encryption standard. Atalla was an early competitor to IBM in the banking market, and was cited as an influence by IBM employees who worked on the DES standard. The IBM 3624 later adopted a similar PIN verification system to the earlier Atalla system.On 15 May 1973, after consulting with the NSA, NBS solicited proposals for a cipher that would meet rigorous design criteria. None of the submissions was suitable. A second request was issued on 27 August 1974. This time, IBM submitted a candidate which was deemed acceptable\u2014a cipher developed during the period 1973\u20131974 based on an earlier algorithm, Horst Feistel's Lucifer cipher. The team at IBM involved in cipher design and analysis included Feistel, Walter Tuchman, Don Coppersmith, Alan Konheim, Carl Meyer, Mike Matyas, Roy Adler, Edna Grossman, Bill Notz, Lynn Smith, and Bryant Tuckerman.\n\n\n=== NSA's involvement in the design ===\nOn 17 March 1975, the proposed DES was published in the Federal Register. Public comments were requested, and in the following year two open workshops were held to discuss the proposed standard. There was criticism received from public-key cryptography pioneers Martin Hellman and Whitfield Diffie, citing a shortened key length and the mysterious \"S-boxes\" as evidence of improper interference from the NSA. The suspicion was that the algorithm had been covertly weakened by the intelligence agency so that they\u2014but no one else\u2014could easily read encrypted messages. Alan Konheim (one of the designers of DES) commented, \"We sent the S-boxes off to Washington. They came back and were all different.\" The United States Senate Select Committee on Intelligence reviewed the NSA's actions to determine whether there had been any improper involvement. In the unclassified summary of their findings, published in 1978, the Committee wrote:\n\nIn the development of DES, NSA convinced IBM that a reduced key size was sufficient; indirectly assisted in the development of the S-box structures; and certified that the final DES algorithm was, to the best of their knowledge, free from any statistical or mathematical weakness.\nHowever, it also found that\n\nNSA did not tamper with the design of the algorithm in any way. IBM invented and designed the algorithm, made all pertinent decisions regarding it, and concurred that the agreed upon key size was more than adequate for all commercial applications for which the DES was intended.\nAnother member of the DES team, Walter Tuchman, stated \"We developed the DES algorithm entirely within IBM using IBMers. The NSA did not dictate a single wire!\"\nIn contrast, a declassified NSA book on cryptologic history states:\n\nIn 1973 NBS solicited private industry for a data encryption standard (DES). The first offerings were disappointing, so NSA began working on its own algorithm. Then Howard Rosenblum, deputy director for research and engineering, discovered that Walter Tuchman of IBM was working on a modification to Lucifer for general use. NSA gave Tuchman a clearance and brought him in to work jointly with the Agency on his Lucifer modification.\"\nand\n\nNSA worked closely with IBM to strengthen the algorithm against all except brute-force attacks and to strengthen substitution tables, called S-boxes. Conversely, NSA tried to convince IBM to reduce the length of the key from 64 to 48 bits. Ultimately they compromised on a 56-bit key.\nSome of the suspicions about hidden weaknesses in the S-boxes were allayed in 1990, with the independent discovery and open publication by Eli Biham and Adi Shamir of differential cryptanalysis, a general method for breaking block ciphers. The S-boxes of DES were much more resistant to the attack than if they had been chosen at random, strongly suggesting that IBM knew about the technique in the 1970s. This was indeed the case; in 1994, Don Coppersmith published some of the original design criteria for the S-boxes. According to Steven Levy, IBM Watson researchers discovered differential cryptanalytic attacks in 1974 and were asked by the NSA to keep the technique secret. Coppersmith explains IBM's secrecy decision by saying, \"that was because [differential cryptanalysis] can be a very powerful tool, used against many schemes, and there was concern that such information in the public domain could adversely affect national security.\" Levy quotes Walter Tuchman: \"[t]hey asked us to stamp all our documents confidential... We actually put a number on each one and locked them up in safes, because they were considered U.S. government classified. They said do it. So I did it\". Bruce Schneier observed that \"It took the academic community two decades to figure out that the NSA 'tweaks' actually improved the security of DES.\"\n\n\n=== The algorithm as a standard ===\nDespite the criticisms, DES was approved as a federal standard in November 1976, and published on 15 January 1977 as FIPS PUB 46, authorized for use on all unclassified data. It was subsequently reaffirmed as the standard in 1983, 1988 (revised as FIPS-46-1), 1993 (FIPS-46-2), and again in 1999 (FIPS-46-3), the latter prescribing \"Triple DES\" (see below). On 26 May 2002, DES was finally superseded by the Advanced Encryption Standard (AES), following a public competition. On 19 May 2005, FIPS 46-3 was officially withdrawn, but NIST has approved Triple DES through the year 2030 for sensitive government information.The algorithm is also specified in ANSI X3.92 (Today X3 is known as INCITS and ANSI X3.92 as ANSI INCITS 92), NIST SP 800-67 and ISO/IEC 18033-3 (as a component of TDEA).\nAnother theoretical attack, linear cryptanalysis, was published in 1994, but it was the Electronic Frontier Foundation's DES cracker in 1998 that demonstrated that DES could be attacked very practically, and highlighted the need for a replacement algorithm. These and other methods of cryptanalysis are discussed in more detail later in this article.\nThe introduction of DES is considered to have been a catalyst for the academic study of cryptography, particularly of methods to crack block ciphers. According to a NIST retrospective about DES,\n\nThe DES can be said to have \"jump-started\" the nonmilitary study and development of encryption algorithms. In the 1970s there were very few cryptographers, except for those in military or intelligence organizations, and little academic study of cryptography. There are now many active academic cryptologists, mathematics departments with strong programs in cryptography, and commercial information security companies and consultants. A generation of cryptanalysts has cut its teeth analyzing (that is, trying to \"crack\") the DES algorithm. In the words of cryptographer Bruce Schneier, \"DES did more to galvanize the field of cryptanalysis than anything else. Now there was an algorithm to study.\" An astonishing share of the open literature in cryptography in the 1970s and 1980s dealt with the DES, and the DES is the standard against which every symmetric key algorithm since has been compared.\n\n\n=== Chronology ===\n\n\n== Description ==\n\nDES is the archetypal block cipher\u2014an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bitstring of the same length. In the case of DES, the block size is 64 bits. DES also uses a key to customize the transformation, so that decryption can supposedly only be performed by those who know the particular key used to encrypt. The key ostensibly consists of 64 bits; however, only 56 of these are actually used by the algorithm. Eight bits are used solely for checking parity, and are thereafter discarded. Hence the effective key length is 56 bits.\nThe key is nominally stored or transmitted as 8 bytes, each with odd parity. According to ANSI X3.92-1981 (Now, known as ANSI INCITS 92\u20131981), section 3.5:\n\nOne bit in each 8-bit byte of the KEY may be utilized for error detection in key generation, distribution, and storage. Bits 8, 16,..., 64 are for use in ensuring that each byte is of odd parity.\nLike other block ciphers, DES by itself is not a secure means of encryption, but must instead be used in a mode of operation. FIPS-81 specifies several modes for use with DES. Further comments on the usage of DES are contained in FIPS-74.Decryption uses the same structure as encryption, but with the keys used in reverse order. (This has the advantage that the same hardware or software can be used in both directions.)\n\n\n=== Overall structure ===\nThe algorithm's overall structure is shown in Figure 1: there are 16 identical stages of processing, termed rounds. There is also an initial and final permutation, termed IP and FP, which are inverses (IP \"undoes\" the action of FP, and vice versa). IP and FP have no cryptographic significance, but were included in order to facilitate loading blocks in and out of mid-1970s 8-bit based hardware.Before the main rounds, the block is divided into two 32-bit halves and processed alternately; this criss-crossing is known as the Feistel scheme. The Feistel structure ensures that decryption and encryption are very similar processes\u2014the only difference is that the subkeys are applied in the reverse order when decrypting. The rest of the algorithm is identical. This greatly simplifies implementation, particularly in hardware, as there is no need for separate encryption and decryption algorithms.\nThe \u2295 symbol denotes the \nexclusive-OR (XOR) operation. The F-function scrambles half a block together with some of the key. The output from the F-function is then combined with the other half of the block, and the halves are swapped before the next round. After the final round, the halves are swapped; this is a feature of the Feistel structure which makes encryption and decryption similar processes.\n\n\n=== The Feistel (F) function ===\nThe F-function, depicted in Figure 2, operates on half a block (32 bits) at a time and consists of four stages:\n\nExpansion: the 32-bit half-block is expanded to 48 bits using the expansion permutation, denoted E in the diagram, by duplicating half of the bits. The output consists of eight 6-bit (8 \u00d7 6 = 48 bits) pieces, each containing a copy of 4 corresponding input bits, plus a copy of the immediately adjacent bit from each of the input pieces to either side.\nKey mixing: the result is combined with a subkey using an XOR operation. Sixteen 48-bit subkeys\u2014one for each round\u2014are derived from the main key using the key schedule (described below).\nSubstitution: after mixing in the subkey, the block is divided into eight 6-bit pieces before processing by the S-boxes, or substitution boxes. Each of the eight S-boxes replaces its six input bits with four output bits according to a non-linear transformation, provided in the form of a lookup table. The S-boxes provide the core of the security of DES\u2014without them, the cipher would be linear, and trivially breakable.\nPermutation: finally, the 32 outputs from the S-boxes are rearranged according to a fixed permutation, the P-box. This is designed so that, after permutation, the bits from the output of each S-box in this round are spread across four different S-boxes in the next round.The alternation of substitution from the S-boxes, and permutation of bits from the P-box and E-expansion provides so-called \"confusion and diffusion\" respectively, a concept identified by Claude Shannon in the 1940s as a necessary condition for a secure yet practical cipher.\n\n\n=== Key schedule ===\nFigure 3 illustrates the key schedule for encryption\u2014the algorithm which generates the subkeys. Initially, 56 bits of the key are selected from the initial 64 by Permuted Choice 1 (PC-1)\u2014the remaining eight bits are either discarded or used as parity check bits. The 56 bits are then divided into two 28-bit halves; each half is thereafter treated separately. In successive rounds, both halves are rotated left by one or two bits (specified for each round), and then 48 subkey bits are selected by Permuted Choice 2 (PC-2)\u201424 bits from the left half, and 24 from the right. The rotations (denoted by \"<<<\" in the diagram) mean that a different set of bits is used in each subkey; each bit is used in approximately 14 out of the 16 subkeys.\nThe key schedule for decryption is similar\u2014the subkeys are in reverse order compared to encryption. Apart from that change, the process is the same as for encryption. The same 28 bits are passed to all rotation boxes.\n\n\n=== Pseudocode ===\nPseudocode for the DES algorithm follows.\n\n\n== Security and cryptanalysis ==\nAlthough more information has been published on the cryptanalysis of DES than any other block cipher, the most practical attack to date is still a brute-force approach. Various minor cryptanalytic properties are known, and three theoretical attacks are possible which, while having a theoretical complexity less than a brute-force attack, require an unrealistic number of known or chosen plaintexts to carry out, and are not a concern in practice.\n\n\n=== Brute-force attack ===\nFor any cipher, the most basic method of attack is brute force\u2014trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 256 bits to 56 bits to fit on a single chip.\nIn academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented\u2014or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: \"There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES.\" The machine brute-forced a key in a little more than 2 days' worth of searching.\nThe next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well.  One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware\u2014see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. SciEngines RIVYERA held the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has further lowered this time.\nIn 2012, David Hulton and Moxie Marlinspike announced a system with 48 Xilinx Virtex-6 LX240T FPGAs, each FPGA containing 40 fully pipelined DES cores running at 400 MHz, for a total capacity of  768 gigakeys/sec. The system can exhaustively search the entire 56-bit DES key space in about 26 hours and this service is offered for a fee online.\n\n\n=== Attacks faster than brute force ===\nThere are three attacks known that can break the full 16 rounds of DES with less complexity than a brute-force search: differential cryptanalysis (DC), linear cryptanalysis (LC), and Davies' attack. However, the attacks are theoretical and are generally considered infeasible to mount in practice; these types of attack are sometimes termed certificational weaknesses.\n\nDifferential cryptanalysis was rediscovered in the late 1980s by Eli Biham and Adi Shamir; it was known earlier to both IBM and the NSA and kept secret. To break the full 16 rounds, differential cryptanalysis requires 247 chosen plaintexts. DES was designed to be resistant to DC.\nLinear cryptanalysis was discovered by Mitsuru Matsui, and needs 243 known plaintexts (Matsui, 1993); the method was implemented (Matsui, 1994), and was the first experimental cryptanalysis of DES to be reported. There is no evidence that DES was tailored to be resistant to this type of attack. A generalization of LC\u2014multiple linear cryptanalysis\u2014was suggested in 1994 (Kaliski and Robshaw), and was further refined by Biryukov and others. (2004); their analysis suggests that multiple linear approximations could be used to reduce the data requirements of the attack by at least a factor of 4 (that is, 241 instead of 243). A similar reduction in data complexity can be obtained in a chosen-plaintext variant of linear cryptanalysis (Knudsen and Mathiassen, 2000). Junod (2001) performed several experiments to determine the actual time complexity of linear cryptanalysis, and reported that it was somewhat faster than predicted, requiring time equivalent to 239\u2013241 DES evaluations.\nImproved Davies' attack: while linear and differential cryptanalysis are general techniques and can be applied to a number of schemes, Davies' attack is a specialized technique for DES, first suggested by Donald Davies in the eighties, and improved by Biham and Biryukov (1997). The most powerful form of the attack requires 250 known plaintexts, has a computational complexity of 250, and has a 51% success rate.There have also been attacks proposed against reduced-round versions of the cipher, that is, versions of DES with fewer than 16 rounds. Such analysis gives an insight into how many rounds are needed for safety, and how much of a \"security margin\" the full version retains.\nDifferential-linear cryptanalysis was proposed by Langford and Hellman in 1994, and combines differential and linear cryptanalysis into a single attack. An enhanced version of the attack can break 9-round DES with 215.8 chosen plaintexts and has a 229.2 time complexity (Biham and others, 2002).\n\n\n=== Minor cryptanalytic properties ===\nDES exhibits the complementation property, namely that\n\n  \n    \n      \n        \n          E\n          \n            K\n          \n        \n        (\n        P\n        )\n        =\n        C\n        \n        \u27fa\n        \n        \n          E\n          \n            \n              K\n              \u00af\n            \n          \n        \n        (\n        \n          \n            P\n            \u00af\n          \n        \n        )\n        =\n        \n          \n            C\n            \u00af\n          \n        \n      \n    \n    {\\displaystyle E_{K}(P)=C\\iff E_{\\overline {K}}({\\overline {P}})={\\overline {C}}}\n  where \n  \n    \n      \n        \n          \n            x\n            \u00af\n          \n        \n      \n    \n    {\\displaystyle {\\overline {x}}}\n   is the bitwise complement of \n  \n    \n      \n        x\n        .\n      \n    \n    {\\displaystyle x.}\n   \n  \n    \n      \n        \n          E\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle E_{K}}\n   denotes encryption with key \n  \n    \n      \n        K\n        .\n      \n    \n    {\\displaystyle K.}\n   \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   denote plaintext and ciphertext blocks respectively. The complementation property means that the work for a brute-force attack could be reduced by a factor of 2 (or a single bit) under a chosen-plaintext assumption. By definition, this property also applies to TDES cipher.DES also has four so-called weak keys. Encryption (E) and decryption (D) under a weak key have the same effect (see involution):\n\n  \n    \n      \n        \n          E\n          \n            K\n          \n        \n        (\n        \n          E\n          \n            K\n          \n        \n        (\n        P\n        )\n        )\n        =\n        P\n      \n    \n    {\\displaystyle E_{K}(E_{K}(P))=P}\n   or equivalently, \n  \n    \n      \n        \n          E\n          \n            K\n          \n        \n        =\n        \n          D\n          \n            K\n          \n        \n        .\n      \n    \n    {\\displaystyle E_{K}=D_{K}.}\n  There are also six pairs of semi-weak keys. Encryption with one of the pair of semiweak keys, \n  \n    \n      \n        \n          K\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle K_{1}}\n  , operates identically to decryption with the other, \n  \n    \n      \n        \n          K\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle K_{2}}\n  :\n\n  \n    \n      \n        \n          E\n          \n            \n              K\n              \n                1\n              \n            \n          \n        \n        (\n        \n          E\n          \n            \n              K\n              \n                2\n              \n            \n          \n        \n        (\n        P\n        )\n        )\n        =\n        P\n      \n    \n    {\\displaystyle E_{K_{1}}(E_{K_{2}}(P))=P}\n   or equivalently, \n  \n    \n      \n        \n          E\n          \n            \n              K\n              \n                2\n              \n            \n          \n        \n        =\n        \n          D\n          \n            \n              K\n              \n                1\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E_{K_{2}}=D_{K_{1}}.}\n  It is easy enough to avoid the weak and semiweak keys in an implementation, either by testing for them explicitly, or simply by choosing keys randomly; the odds of picking a weak or semiweak key by chance are negligible. The keys are not really any weaker than any other keys anyway, as they do not give an attack any advantage.\nDES has also been proved not to be a group, or more precisely, the set \n  \n    \n      \n        {\n        \n          E\n          \n            K\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{E_{K}\\}}\n   (for all possible keys \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  ) under functional composition is not a group, nor \"close\" to being a group. This was an open question for some time, and if it had been the case, it would have been possible to break DES, and multiple encryption modes such as Triple DES would not increase the security, because repeated encryption (and decryptions) under different keys would be equivalent to encryption under another, single key.\n\n\n== Simplified DES ==\nSimplified DES (SDES) was designed for educational purposes only, to help students learn about modern cryptanalytic techniques.\nSDES has similar structure and properties to DES, but has been simplified to make it much easier to perform encryption and decryption by hand with pencil and paper.\nSome people feel that learning SDES gives insight into DES and other block ciphers, and insight into various cryptanalytic attacks against them.\n\n\n== Replacement algorithms ==\nConcerns about security and the relatively slow operation of DES in software motivated researchers to propose a variety of alternative block cipher designs, which started to appear in the late 1980s and early 1990s: examples include RC5, Blowfish, IDEA, NewDES, SAFER, CAST5 and FEAL. Most of these designs kept the 64-bit block size of DES, and could act as a \"drop-in\" replacement, although they typically used a 64-bit or 128-bit key. In the Soviet Union the GOST 28147-89 algorithm was introduced, with a 64-bit block size and a 256-bit key, which was also used in Russia later.\nDES itself can be adapted and reused in a more secure scheme. Many former DES users now use Triple DES (TDES) which was described and analysed by one of DES's patentees (see FIPS Pub 46\u20133); it involves applying DES three times with two (2TDES) or three (3TDES) different keys. TDES is regarded as adequately secure, although it is quite slow. A less computationally expensive alternative is DES-X, which increases the key size by XORing extra key material before and after DES. GDES was a DES variant proposed as a way to speed up encryption, but it was shown to be susceptible to differential cryptanalysis.\nOn January 2, 1997, NIST announced that they wished to choose a successor to DES. In 2001, after an international competition, NIST selected a new cipher, the Advanced Encryption Standard (AES), as a replacement. The algorithm which was selected as the AES was submitted by its designers under the name Rijndael. Other finalists in the NIST AES competition included RC6, Serpent, MARS, and Twofish.\n\n\n== See also ==\nBrute Force: Cracking the Data Encryption Standard\nDES supplementary material\nSkipjack (cipher)\nTriple DES\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nFIPS 46-3: The official document describing the DES standard (PDF)\nCOPACOBANA, a $10,000 DES cracker based on FPGAs by the Universities of Bochum and Kiel\nDES step-by-step presentation and reliable message encoding application\nA Fast New DES Implementation in Software - Biham\nOn Multiple Linear Approximations\nRFC4772 : Security Implications of Using the Data Encryption Standard (DES)\nPython code of DES Cipher implemented using DES Chapter from NIST SP 958"}, {"id": 95, "title": "Quantum algorithm", "content": "In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer,:\u200a126\u200a  the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\nProblems which are undecidable using classical computers remain undecidable using quantum computers.:\u200a127\u200a What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms because the quantum superposition and quantum entanglement that quantum algorithms exploit probably cannot be efficiently simulated on classical computers (see Quantum supremacy).\nThe best-known algorithms are Shor's algorithm for factoring and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs much (almost exponentially) faster than the best-known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task, a linear search.\n\n\n== Overview ==\nQuantum algorithms are usually described, in the commonly used circuit model of quantum computation, by a quantum circuit which acts on some input qubits and terminates with a measurement. A quantum circuit consists of simple quantum gates which act on at most a fixed number of qubits. The number of qubits has to be fixed because a changing number of qubits implies non-unitary evolution. Quantum algorithms may also be stated in other models of quantum computation, such as the Hamiltonian oracle model.Quantum algorithms can be categorized by the main techniques used by the algorithm. Some commonly used techniques/ideas in quantum algorithms include phase kick-back, phase estimation, the quantum Fourier transform, quantum walks, amplitude amplification and topological quantum field theory. Quantum algorithms may also be grouped by the type of problem solved, for instance see the survey on quantum algorithms for algebraic problems.\n\n\n== Algorithms based on the quantum Fourier transform ==\nThe quantum Fourier transform is the quantum analogue of the discrete Fourier transform, and is used in several quantum algorithms. The Hadamard transform is also an example of a quantum Fourier transform over an n-dimensional vector space over the field F2. The quantum Fourier transform can be efficiently implemented on a quantum computer using only a polynomial number of quantum gates.\n\n\n=== Deutsch\u2013Jozsa algorithm ===\n\nThe Deutsch\u2013Jozsa algorithm solves a black-box problem which requires exponentially many queries to the black box for any deterministic classical computer, but can be done with one query by a quantum computer. However, when comparing bounded-error classical and quantum algorithms, there is no speedup since a classical probabilistic algorithm can solve the problem with a constant number of queries with small probability of error. The algorithm determines whether a function f is either constant (0 on all inputs or 1 on all inputs) or balanced (returns 1 for half of the input domain and 0 for the other half).\n\n\n=== Bernstein\u2013Vazirani algorithm ===\n\nThe Bernstein\u2013Vazirani algorithm is the first quantum algorithm that solves a problem more efficiently than the best known classical algorithm. It was designed to create an oracle separation between BQP and BPP.\n\n\n=== Simon's algorithm ===\n\nSimon's algorithm solves a black-box problem exponentially faster than any classical algorithm, including bounded-error probabilistic algorithms. This algorithm, which achieves an exponential speedup over all classical algorithms that we consider efficient, was the motivation for Shor's factoring algorithm.\n\n\n=== Quantum phase estimation algorithm ===\n\nThe quantum phase estimation algorithm is used to determine the eigenphase of an eigenvector of a unitary gate given a quantum state proportional to the eigenvector and access to the gate.   The algorithm is frequently used as a subroutine in other algorithms.\n\n\n=== Shor's algorithm ===\n\nShor's algorithm solves the discrete logarithm problem and the integer factorization problem in polynomial time, whereas the best known classical algorithms take super-polynomial time. These problems are not known to be in P or NP-complete. It is also one of the few quantum algorithms that solves a non\u2013black-box problem in polynomial time where the best known classical algorithms run in super-polynomial time.\n\n\n=== Hidden subgroup problem ===\nThe abelian hidden subgroup problem is a generalization of many problems that can be solved by a quantum computer, such as Simon's problem, solving Pell's equation, testing the principal ideal of a ring R and factoring. There are efficient quantum algorithms known for the Abelian hidden subgroup problem. The more general hidden subgroup problem, where the group isn't necessarily abelian, is a generalization of the previously mentioned problems and graph isomorphism and certain lattice problems. Efficient quantum algorithms are known for certain non-abelian groups. However, no efficient algorithms are known for the symmetric group, which would give an efficient algorithm for graph isomorphism and the dihedral group, which would solve certain lattice problems.\n\n\n=== Estimating Gauss sums ===\nA Gauss sum is a type of exponential sum. The best known classical algorithm for estimating these sums takes exponential time. Since the discrete logarithm problem reduces to Gauss sum estimation, an efficient classical algorithm for estimating Gauss sums would imply an efficient classical algorithm for computing discrete logarithms, which is considered unlikely. However, quantum computers can estimate Gauss sums to polynomial precision in polynomial time.\n\n\n=== Fourier fishing and Fourier checking ===\nWe have an oracle consisting of n random Boolean functions mapping n-bit strings to a Boolean value. We are required to find n n-bit strings z1,..., zn such that for the Hadamard-Fourier transform, at least 3/4 of the strings satisfy\n\n  \n    \n      \n        \n          |\n        \n        \n          \n            \n              f\n              ~\n            \n          \n        \n        (\n        \n          z\n          \n            i\n          \n        \n        )\n        \n          |\n        \n        \u2a7e\n        1\n      \n    \n    {\\displaystyle |{\\tilde {f}}(z_{i})|\\geqslant 1}\n  and at least 1/4 satisfies\n\n  \n    \n      \n        \n          |\n        \n        \n          \n            \n              f\n              ~\n            \n          \n        \n        (\n        \n          z\n          \n            i\n          \n        \n        )\n        \n          |\n        \n        \u2a7e\n        2.\n      \n    \n    {\\displaystyle |{\\tilde {f}}(z_{i})|\\geqslant 2.}\n  This can be done in bounded-error quantum polynomial time (BQP).\n\n\n== Algorithms based on amplitude amplification ==\nAmplitude amplification is a technique that allows the amplification of a chosen subspace of a quantum state. Applications of amplitude amplification usually lead to quadratic speedups over the corresponding classical algorithms. It can be considered to be a generalization of Grover's algorithm.\n\n\n=== Grover's algorithm ===\n\nGrover's algorithm searches an unstructured database (or an unordered list) with N entries, for a marked entry, using only \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   queries instead of the \n  \n    \n      \n        O\n        (\n        \n          N\n        \n        )\n      \n    \n    {\\displaystyle O({N})}\n   queries required classically. Classically, \n  \n    \n      \n        O\n        (\n        \n          N\n        \n        )\n      \n    \n    {\\displaystyle O({N})}\n   queries are required even allowing bounded-error probabilistic algorithms.\nTheorists have considered a hypothetical generalization of a standard quantum computer that could access the histories of the hidden variables in Bohmian mechanics. (Such a computer is completely hypothetical and would not be a standard quantum computer, or even possible under the standard theory of quantum mechanics.) Such a hypothetical computer could implement a search of an N-item database at most in \n  \n    \n      \n        O\n        (\n        \n          \n            N\n            \n              3\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt[{3}]{N}})}\n   steps. This is slightly faster than the \n  \n    \n      \n        O\n        (\n        \n          \n            N\n          \n        \n        )\n      \n    \n    {\\displaystyle O({\\sqrt {N}})}\n   steps taken by Grover's algorithm. Neither search method would allow either model of quantum computer to solve NP-complete problems in polynomial time.\n\n\n=== Quantum counting ===\nQuantum counting solves a generalization of the search problem. It solves the problem of counting the number of marked entries in an unordered list, instead of just detecting if one exists. Specifically, it counts the number of marked entries in an \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  -element list, with error \n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n   making only \n  \n    \n      \n        \u0398\n        \n          (\n          \n            \n              \n                1\n                \u03b5\n              \n            \n            \n              \n                \n                  N\n                  k\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\Theta \\left({\\frac {1}{\\varepsilon }}{\\sqrt {\\frac {N}{k}}}\\right)}\n   queries, where \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is the number of marked elements in the list. More precisely, the algorithm outputs an estimate \n  \n    \n      \n        \n          k\n          \u2032\n        \n      \n    \n    {\\displaystyle k'}\n   for \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  , the number of marked entries, with the following accuracy: \n  \n    \n      \n        \n          |\n        \n        k\n        \u2212\n        \n          k\n          \u2032\n        \n        \n          |\n        \n        \u2264\n        \u03b5\n        k\n      \n    \n    {\\displaystyle |k-k'|\\leq \\varepsilon k}\n  .\n\n\n== Algorithms based on quantum walks ==\n\nA quantum walk is the quantum analogue of a classical random walk, which can be described by a probability distribution over some states. A quantum walk can be described by a quantum superposition over states. Quantum walks are known to give exponential speedups for some black-box problems. They also provide polynomial speedups for many problems. A framework for the creation of quantum walk algorithms exists and is quite a versatile tool.\n\n\n=== Boson sampling problem ===\n\nThe Boson Sampling Problem in an experimental configuration assumes an input of bosons (ex. photons of light) of moderate number getting randomly scattered into a large number of output modes constrained by a defined unitarity. When individual photons of light are used the problem is isomorphic to a multi-photon quantum walk. The problem is then to produce a fair sample of the probability distribution of the output which is dependent on the input arrangement of bosons and the Unitarity. Solving this problem with a classical computer algorithm requires computing the permanent of the unitary transform matrix, which may be either impossible or take a prohibitively long time. In 2014, it was proposed that existing technology and standard probabilistic methods of generating single photon states could be used as input into a suitable quantum computable linear optical network and that sampling of the output probability distribution would be demonstrably superior using quantum algorithms. In 2015, investigation predicted the sampling problem had similar complexity for inputs other than Fock state photons and identified a transition in computational complexity from classically simulatable to just as hard as the Boson Sampling Problem, dependent on the size of coherent amplitude inputs\n\n\n=== Element distinctness problem ===\n\nThe element distinctness problem is the problem of determining whether all the elements of a list are distinct. Classically, \u03a9(N) queries are required for a list of size N. However, it can be solved in \n  \n    \n      \n        \u0398\n        (\n        \n          N\n          \n            2\n            \n              /\n            \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Theta (N^{2/3})}\n   queries on a quantum computer. The optimal algorithm is by Andris Ambainis. Yaoyun Shi first proved a tight lower bound when the size of the range is sufficiently large. Ambainis and Kutin independently (and via different proofs) extended his work to obtain the lower bound for all functions.\n\n\n=== Triangle-finding problem ===\n\nThe triangle-finding problem is the problem of determining whether a given graph contains a triangle (a clique of size 3). The best-known lower bound for quantum algorithms is \u03a9(N), but the best algorithm known requires O(N1.297) queries, an improvement over the previous best O(N1.3) queries.\n\n\n=== Formula evaluation ===\nA formula is a tree with a gate at each internal node and an input bit at each leaf node. The problem is to evaluate the formula, which is the output of the root node, given oracle access to the input.\nA well studied formula is the balanced binary tree with only NAND gates. This type of formula requires \u0398(Nc) queries using randomness, where \n  \n    \n      \n        c\n        =\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        (\n        1\n        +\n        \n          \n            33\n          \n        \n        )\n        \n          /\n        \n        4\n        \u2248\n        0.754\n      \n    \n    {\\displaystyle c=\\log _{2}(1+{\\sqrt {33}})/4\\approx 0.754}\n  . With a quantum algorithm however, it can be solved in \u0398(N0.5) queries. No better quantum algorithm for this case was known until one was found for the unconventional Hamiltonian oracle model. The same result for the standard setting soon followed.Fast quantum algorithms for more complicated formulas are also known.\n\n\n=== Group commutativity ===\nThe problem is to determine if a black box group, given by k generators, is commutative. A black box group is a group with an oracle function, which must be used to perform the group operations (multiplication, inversion, and comparison with identity). We are interested in the query complexity, which is the number of oracle calls needed to solve the problem. The deterministic and randomized query complexities are \n  \n    \n      \n        \u0398\n        (\n        \n          k\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Theta (k^{2})}\n   and \n  \n    \n      \n        \u0398\n        (\n        k\n        )\n      \n    \n    {\\displaystyle \\Theta (k)}\n   respectively. A quantum algorithm requires \n  \n    \n      \n        \u03a9\n        (\n        \n          k\n          \n            2\n            \n              /\n            \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle \\Omega (k^{2/3})}\n   queries but the best known algorithm uses \n  \n    \n      \n        O\n        (\n        \n          k\n          \n            2\n            \n              /\n            \n            3\n          \n        \n        log\n        \u2061\n        k\n        )\n      \n    \n    {\\displaystyle O(k^{2/3}\\log k)}\n   queries.\n\n\n== BQP-complete problems ==\nThe complexity class BQP (bounded-error quantum polynomial time) is the set of decision problems solvable by a quantum computer in polynomial time with error probability of at most 1/3 for all instances. It is the quantum analogue to the classical complexity class BPP.\nA problem is BQP-complete if it is in BQP and any problem in BQP can be reduced to it in polynomial time. Informally, the class of BQP-complete problems are those that are as hard as the hardest problems in BQP and are themselves efficiently solvable by a quantum computer (with bounded error).\n\n\n=== Computing knot invariants ===\nWitten had shown that the Chern-Simons topological quantum field theory (TQFT) can be solved in terms of Jones polynomials. A quantum computer can simulate a TQFT, and thereby approximate the Jones polynomial, which as far as we know, is hard to compute classically in the worst-case scenario.\n\n\n=== Quantum simulation ===\n\nThe idea that quantum computers might be more powerful than classical computers originated in Richard Feynman's observation that classical computers seem to require exponential time to simulate many-particle quantum systems. Since then, the idea that quantum computers can simulate quantum physical processes exponentially faster than classical computers has been greatly fleshed out and elaborated. Efficient (that is, polynomial-time) quantum algorithms have been developed for simulating both Bosonic and Fermionic systems and in particular, the simulation of chemical reactions beyond the capabilities of current classical supercomputers requires only a few hundred qubits. Quantum computers can also efficiently simulate topological quantum field theories. In addition to its intrinsic interest, this result has led to efficient quantum algorithms for estimating quantum topological invariants such as Jones and HOMFLY polynomials, and the Turaev-Viro invariant of three-dimensional manifolds.\n\n\n=== Solving a linear systems of equations ===\n\nIn 2009 Aram Harrow, Avinatan Hassidim, and Seth Lloyd, formulated a quantum algorithm for solving linear systems. The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.Provided the linear system is a sparse and has a low condition number \n  \n    \n      \n        \u03ba\n      \n    \n    {\\displaystyle \\kappa }\n  , and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        (\n        N\n        )\n        \n          \u03ba\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(\\log(N)\\kappa ^{2})}\n  , where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in \n  \n    \n      \n        O\n        (\n        N\n        \u03ba\n        )\n      \n    \n    {\\displaystyle O(N\\kappa )}\n   (or \n  \n    \n      \n        O\n        (\n        N\n        \n          \n            \u03ba\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N{\\sqrt {\\kappa }})}\n   for positive semidefinite matrices).\n\n\n== Hybrid quantum/classical algorithms ==\nHybrid Quantum/Classical Algorithms combine quantum state preparation and measurement with classical optimization. These algorithms generally aim to determine the ground state eigenvector and eigenvalue of a Hermitian Operator.\n\n\n=== QAOA ===\nThe quantum approximate optimization algorithm takes inspiration from quantum annealing, performing a discretized approximation of quantum annealing on a quantum circuit. It can be used to solve problems in graph theory. The algorithm makes use of classical optimization of quantum operations to maximize an objective function.\n\n\n=== Variational quantum eigensolver ===\nThe variational quantum eigensolver (VQE) algorithm applies classical optimization to minimize the energy expectation of an ansatz state to find the ground state of an operator, such as a molecule's Hamiltonian. This can also be extended to find excited energies of molecules.\n\n\n=== Contracted quantum eigensolver ===\nThe CQE algorithm minimizes the residual of a contraction (or projection) of the Schr\u00f6dinger equation onto the space of two (or more) electrons to find the ground- or excited-state energy and two-electron reduced density matrix of a molecule.  It is based on classical methods for solving energies and two-electron reduced density matrices directly from the anti-Hermitian contracted Schr\u00f6dinger equation.\n\n\n== See also ==\nQuantum machine learning\nQuantum optimization algorithms\nQuantum sort\nPrimality test\n\n\n== References ==\n\n\n== External links ==\nThe Quantum Algorithm Zoo: A comprehensive list of quantum algorithms that provide a speedup over the fastest known classical algorithms.\nAndrew Childs' lecture notes on quantum algorithms\nThe Quantum search algorithm - brute force.\n\n\n=== Surveys ===\nDalzell, Alexander M.; et al. (2023). \"Quantum algorithms: A survey of applications and end-to-end complexities\". arXiv:2310.03011 [quant-ph].\nSmith, J.; Mosca, M. (2012). \"Algorithms for Quantum Computers\". Handbook of Natural Computing. pp. 1451\u20131492. doi:10.1007/978-3-540-92910-9_43. ISBN 978-3-540-92909-3. S2CID 16565723.\nChilds, A. M.; Van Dam, W. (2010). \"Quantum algorithms for algebraic problems\". Reviews of Modern Physics. 82 (1): 1\u201352. arXiv:0812.0380. Bibcode:2010RvMP...82....1C. doi:10.1103/RevModPhys.82.1. S2CID 119261679."}, {"id": 96, "title": "Shockley Queisser Limit", "content": "In physics, the  radiative efficiency limit (also known as the detailed balance limit, Shockley\u2013Queisser limit, Shockley Queisser Efficiency Limit or SQ Limit) is the maximum theoretical efficiency of a solar cell using a single p-n junction to collect power from the cell where the only loss mechanism is radiative recombination in the solar cell. It was first calculated by William Shockley and Hans-Joachim Queisser at Shockley Semiconductor in 1961, giving a maximum efficiency of 30% at 1.1 eV.  The limit is one of the most fundamental to solar energy production with photovoltaic cells, and is one of the field's most important contributions.This first calculation used the 6000K black-body spectrum as an approximation to the solar spectrum. Subsequent calculations have used measured global solar spectra, AM 1.5, and included a back surface mirror which increases the maximum solar conversion efficiency to 33.16% for a single-junction solar cell with a bandgap of 1.34 eV. That is, of all the power contained in sunlight (about 1000 W/m2) falling on an ideal solar cell, only 33.7% of that could ever be turned into electricity (337 W/m2). The most popular solar cell material, silicon, has a less favorable band gap of 1.1 eV, resulting in a maximum efficiency of about 32%. Modern commercial mono-crystalline solar cells produce about 24% conversion efficiency, the losses due largely to practical concerns like reflection off the front of the cell and light blockage from the thin wires on the cell surface.\nThe Shockley\u2013Queisser limit only applies to conventional solar cells with a single p-n junction; solar cells with multiple layers can (and do) outperform this limit, and so can solar thermal and certain other solar energy systems. In the extreme limit, for a multi-junction solar cell with an infinite number of layers, the corresponding limit is 68.7% for normal sunlight, or 86.8% using concentrated sunlight (see solar cell efficiency).\n\n\n== Background ==\nIn a traditional solid-state semiconductor such as silicon, a solar cell is made from two doped crystals, one an n-type semiconductor, which has extra free electrons, and the other a p-type semiconductor, which is lacking free electrons, referred to as \"holes.\" When initially placed in contact with each other, some of the electrons in the n-type portion will flow into the p-type to \"fill in\" the missing electrons. Eventually enough will flow across the boundary to equalize the Fermi levels of the two materials. The result is a region at the interface, the p-n junction, where charge carriers are depleted on each side of the interface. In silicon, this transfer of electrons produces a potential barrier of about 0.6 V to 0.7 V.When the material is placed in the sun, photons from the sunlight can be absorbed in the p-type side of the semiconductor, causing electrons in the valence band to be promoted in energy to the conduction band. This process is known as photoexcitation. As the name implies, electrons in the conduction band are free to move about the semiconductor. When a load is placed across the cell as a whole, these electrons will flow from the p-type side into the n-type side, lose energy while moving through the external circuit, and then go back into the p-type material where they can re-combine with the valence-band holes they left behind. In this way, sunlight creates an electric current.\n\n\n== The limit ==\nThe Shockley\u2013Queisser limit is calculated by examining the amount of electrical energy that is extracted per photon of incoming sunlight. There are several considerations:\n\n\n=== Blackbody radiation ===\nAny material, that is not at absolute zero (0 Kelvin), emits electromagnetic radiation through the black-body radiation effect. In a cell at room temperature, this represents approximately 7% of all the energy falling on the cell.\nAny energy lost in a cell is turned into heat, so any inefficiency in the cell increases the cell temperature when it is placed in sunlight. As the temperature of the cell increases, the outgoing radiation and heat loss through conduction and convection also increase, until an equilibrium is reached. In practice, this equilibrium is normally reached at temperatures as high as 360 Kelvin, and consequently, cells normally operate at lower efficiencies than their room-temperature rating. Module datasheets normally list this temperature dependency as TNOCT (NOCT - Nominal Operating Cell Temperature).\nFor a \"blackbody\" at normal temperatures, a very small part of this radiation (the number per unit time and per unit area given by Qc, \"c\" for \"cell\") is photons having energy greater than the band gap (wavelength less than about 1.1 microns for silicon), and part of these photons (Shockley and Queisser use the factor tc) are generated by recombination of electrons and holes, which decreases the amount of current that could be generated otherwise. This is a very small effect, but Shockley and Queisser assume that the total rate of recombination (see below) when the voltage across the cell is zero (short circuit or no light) is proportional to the blackbody radiation Qc. This rate of recombination plays a negative role in the efficiency. Shockley and Queisser calculate Qc to be 1700 photons per second per square centimetre for silicon at 300K.\n\n\n=== Recombination ===\nAbsorption of a photon creates an electron-hole pair, which could potentially contribute to the current. However, the reverse process must also be possible, according to the principle of detailed balance: an electron and a hole can meet and recombine, emitting a photon. This process reduces the efficiency of the cell. Other recombination processes may also exist (see \"Other considerations\" below), but this one is absolutely required.\nIn the Shockley\u2013Queisser model, the recombination rate depends on the voltage across the cell but is the same whether or not there is light falling on the cell. A factor fc gives the ratio of recombination that produces radiation to total recombination, so the rate of recombination per unit area when V = 0 is 2tcQc/fc and thus depends on Qc, the flux of blackbody photons above the band-gap energy. The factor of 2 was included on the assumption that radiation emitted by the cell goes in both directions. (This is actually debatable if a reflective surface is used on the shady side.) When the voltage is non-zero, the concentrations of charge carriers (electrons and holes) change (see Shockley diode equation), and according to the authors the rate of recombination changes by a factor of exp(V/Vc), where Vc is the voltage equivalent of the temperature of the cell, or \"thermal voltage\", namely\n\n  \n    \n      \n        \n          V\n          \n            c\n          \n        \n        =\n        k\n        \n          T\n          \n            c\n          \n        \n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle V_{c}=kT_{c}/q}\n  (q being the charge of an electron). Thus the rate of recombination, in this model, is proportional to exp(V/Vc) times the blackbody radiation above the band-gap energy:\n\n  \n    \n      \n        \n          Q\n          \n            c\n          \n        \n        =\n        \n          \u222b\n          \n            \n              \u03bd\n              \n                g\n              \n            \n          \n          \n            \u221e\n          \n        \n        \n          \n            1\n            \n              exp\n              \u2061\n              \n                (\n                \n                  \n                    \n                      h\n                      \u03bd\n                    \n                    \n                      k\n                      \n                        T\n                        \n                          c\n                        \n                      \n                    \n                  \n                \n                )\n              \n              \u2212\n              1\n            \n          \n        \n        \n          e\n          \n            \n              \n                q\n                V\n              \n              \n                k\n                \n                  T\n                  \n                    c\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n              2\n              \u03c0\n              \n                \u03bd\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        d\n        \u03bd\n      \n    \n    {\\displaystyle Q_{c}=\\int _{\\nu _{g}}^{\\infty }{\\frac {1}{\\exp \\left({\\frac {h\\nu }{kT_{c}}}\\right)-1}}e^{\\frac {qV}{kT_{c}}}{\\frac {2\\pi \\nu ^{2}}{c^{2}}}d\\nu }\n  (This is actually an approximation, correct so long as the cell is thick enough to act as a black body, to the more accurate expression\n\n  \n    \n      \n        \n          Q\n          \n            c\n          \n        \n        =\n        \n          \u222b\n          \n            \n              \u03bd\n              \n                g\n              \n            \n          \n          \n            \u221e\n          \n        \n        \n          \n            1\n            \n              exp\n              \u2061\n              \n                (\n                \n                  \n                    \n                      h\n                      \u03bd\n                      \u2212\n                      q\n                      V\n                    \n                    \n                      k\n                      \n                        T\n                        \n                          c\n                        \n                      \n                    \n                  \n                \n                )\n              \n              \u2212\n              1\n            \n          \n        \n        \n          \n            \n              2\n              \u03c0\n              \n                \u03bd\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        d\n        \u03bd\n        ,\n      \n    \n    {\\displaystyle Q_{c}=\\int _{\\nu _{g}}^{\\infty }{\\frac {1}{\\exp \\left({\\frac {h\\nu -qV}{kT_{c}}}\\right)-1}}{\\frac {2\\pi \\nu ^{2}}{c^{2}}}d\\nu ,}\n  The difference in maximum theoretical efficiency however is negligibly small, except for tiny bandgaps below 200meV.)\nThe rate of generation of electron-hole pairs not due to incoming sunlight stays the same, so recombination minus spontaneous generation is\n\n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        [\n        exp\n        \u2061\n        (\n        V\n        \n          /\n        \n        \n          V\n          \n            c\n          \n        \n        )\n        \u2212\n        1\n        ]\n        .\n      \n    \n    {\\displaystyle I_{0}[\\exp(V/V_{c})-1].}\n  \nwhere \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        =\n        2\n        q\n        \n          t\n          \n            c\n          \n        \n        \n          Q\n          \n            c\n          \n        \n        \n          /\n        \n        \n          f\n          \n            c\n          \n        \n        .\n      \n    \n    {\\displaystyle I_{0}=2qt_{c}Q_{c}/f_{c}.}\n  \n(Shockley and Queisser take fc to be a constant, although they admit that it may itself depend on voltage.)\nThe rate of generation of electron-hole pairs due to sunlight is\n\n  \n    \n      \n        \n          I\n          \n            s\n            h\n          \n        \n        =\n        q\n        (\n        \n          t\n          \n            s\n          \n        \n        \n          f\n          \n            \u03c9\n          \n        \n        \n          Q\n          \n            s\n          \n        \n        \u2212\n        2\n        \n          t\n          \n            c\n          \n        \n        \n          Q\n          \n            c\n          \n        \n        )\n      \n    \n    {\\displaystyle I_{sh}=q(t_{s}f_{\\omega }Q_{s}-2t_{c}Q_{c})}\n  where \n  \n    \n      \n        \n          f\n          \n            \u03c9\n          \n        \n        \n          Q\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle f_{\\omega }Q_{s}}\n   is the number of photons above the band-gap energy falling on the cell per unit area, and ts is the fraction of these that generate an electron-hole pair. This rate of generation is called Ish because it is the \"short circuit\" current (per unit area). When there is a load, then V will not be zero and we have a current equal to the rate of generation of pairs due to the sunlight minus the difference between recombination and spontaneous generation:\n\n  \n    \n      \n        I\n        =\n        \n          I\n          \n            s\n            h\n          \n        \n        \u2212\n        \n          I\n          \n            0\n          \n        \n        [\n        exp\n        \u2061\n        (\n        V\n        \n          /\n        \n        \n          V\n          \n            c\n          \n        \n        )\n        \u2212\n        1\n        ]\n        .\n      \n    \n    {\\displaystyle I=I_{sh}-I_{0}[\\exp(V/V_{c})-1].}\n  The open-circuit voltage is therefore given (assuming fc does not depend on voltage) by\n\n  \n    \n      \n        \n          V\n          \n            o\n            c\n          \n        \n        =\n        \n          V\n          \n            c\n          \n        \n        ln\n        \u2061\n        \n          (\n          \n            \n              \n                \n                  I\n                  \n                    s\n                    h\n                  \n                \n                \n                  I\n                  \n                    0\n                  \n                \n              \n            \n            +\n            1\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle V_{oc}=V_{c}\\ln \\left({\\frac {I_{sh}}{I_{0}}}+1\\right).}\n  The product of the short-circuit current Ish and the open-circuit voltage Voc Shockley and Queisser call the \"nominal power\". It is not actually possible to get this amount of power out of the cell, but we can get close (see \"Impedance matching\" below).\nThe ratio of the open-circuit voltage to the band-gap voltage Shockley and Queisser call V. Under open-circuit conditions, we have\n\n  \n    \n      \n        ln\n        \u2061\n        \n          I\n          \n            s\n            h\n          \n        \n        =\n        ln\n        \u2061\n        \n          I\n          \n            0\n          \n        \n        +\n        ln\n        \u2061\n        [\n        exp\n        \u2061\n        (\n        V\n        \n          /\n        \n        \n          V\n          \n            c\n          \n        \n        )\n        \u2212\n        1\n        ]\n        .\n      \n    \n    {\\displaystyle \\ln I_{sh}=\\ln I_{0}+\\ln[\\exp(V/V_{c})-1].}\n  Asymptotically, this gives\n\n  \n    \n      \n        \u2212\n        \n          V\n          \n            g\n          \n        \n        \n          /\n        \n        \n          V\n          \n            s\n          \n        \n        \u223c\n        \u2212\n        \n          V\n          \n            g\n          \n        \n        \n          /\n        \n        \n          V\n          \n            c\n          \n        \n        +\n        V\n        \n          /\n        \n        \n          V\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle -V_{g}/V_{s}\\sim -V_{g}/V_{c}+V/V_{c}}\n  or\n\n  \n    \n      \n        V\n        \n          /\n        \n        \n          V\n          \n            g\n          \n        \n        \u223c\n        1\n        \u2212\n        \n          V\n          \n            c\n          \n        \n        \n          /\n        \n        \n          V\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle V/V_{g}\\sim 1-V_{c}/V_{s}}\n  where Vs is the voltage equivalent of the temperature of the sun. As the ratio Vc/Vs goes to zero, the open-circuit voltage goes to the band-gap voltage, and as it goes to one, the open-circuit voltage goes to zero. This is why the efficiency falls if the cell heats up. In fact this expression represents the thermodynamic upper limit of the amount of work that can be obtained from a heat source at the temperature of the sun and a heat sink at the temperature of the cell.\n\n\n=== Spectrum losses ===\nSince the act of moving an electron from the valence band to the conduction band requires energy, only photons with more than that amount of energy will produce an electron-hole pair. In silicon the conduction band is about 1.1 eV away from the valence band, this corresponds to infrared light with a wavelength of about 1.1 microns. In other words, photons of red, yellow and blue light and some near-infrared will contribute to power production, whereas radio waves, microwaves, and most infrared photons will not. This places an immediate limit on the amount of energy that can be extracted from the sun. Of the 1,000 W/m2 in AM1.5 sunlight, about 19% of that has less than 1.1 eV of energy, and will not produce power in a silicon cell.\nAnother important contributor to losses is that any energy above and beyond the bandgap energy is lost. While blue light has roughly twice the energy of red light, that energy is not captured by devices with a single p-n junction. The electron is ejected with higher energy when struck by a blue photon, but it loses this extra energy as it travels toward the p-n junction (the energy is converted into heat). This accounts for about 33% of the incident sunlight, meaning that, for silicon, from spectrum losses alone there is a theoretical conversion efficiency limit of about 48%, ignoring all other factors.\nThere is a trade-off in the selection of a bandgap. If the band gap is large, not as many photons create pairs, whereas if the band gap is small, the electron-hole pairs do not contain as much energy.\nShockley and Queisser call the efficiency factor associated with spectrum losses u, for \"ultimate efficiency function\". Shockley and Queisser calculated that the best band gap for sunlight happens to be 1.1 eV, the value for silicon, and gives a u of 44%. They used blackbody radiation of 6000K for sunlight, and found that the optimum band gap would then have an energy of 2.2 kTs. (At that value, 22% of the blackbody radiation energy would be below the band gap.) Using a more accurate spectrum may give a slightly different optimum. A blackbody at 6000 K puts out 7348 W per square centimetre, so a value for u of 44% and a value of 5.73\u00d71018 photons per joule (corresponding to a band gap of 1.09 V, the value used by Shockley and Queisser) gives Qs equal to 1.85\u00d71022 photons per second per square centimetre.\n\n\n=== Impedance matching ===\nIf the resistance of the load is too high, the current will be very low, while if the load resistance is too low, the voltage drop across it will be very low. There is an optimal load resistance that will draw the most power from the solar cell at a given illumination level. Shockley and Queisser call the ratio of power extracted to IshVoc the impedance matching factor, m. (It is also called the fill factor.) The optimum depends on the shape of the I versus V curve. For very low illumination, the curve is more or less a diagonal line, and m will be 1/4. But for high illumination, m approaches 1. Shockley and Queisser give a graph showing m as a function of the ratio zoc of the open-circuit voltage to the thermal voltage Vc. According to the authors, this ratio is well approximated by ln(fQs/Qc), where f is the combination of factors fsf\u03c9ts/(2tc), in which f\u03c9 is the solid angle of the sun divided by \u03c0. The maximum value of f without light concentration (with reflectors for example) is just f\u03c9/2, or 1.09\u00d710\u22125, according to the authors. Using the above-mentioned values of Qs and Qc, this gives a ratio of open-circuit voltage to thermal voltage of 32.4 (Voc equal to 77% of the band gap). The authors derive the equation\n\n  \n    \n      \n        \n          z\n          \n            o\n            c\n          \n        \n        =\n        \n          z\n          \n            m\n          \n        \n        +\n        ln\n        \u2061\n        (\n        1\n        +\n        \n          z\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle z_{oc}=z_{m}+\\ln(1+z_{m})}\n  which can be solved to find zm, the ratio of optimal voltage to thermal voltage. For a zoc of 32.4, we find zm equal to 29.0. One can then use the formula\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              \n                z\n                \n                  m\n                \n                \n                  2\n                \n              \n              \n                /\n              \n              \n                z\n                \n                  o\n                  c\n                \n              \n            \n            \n              1\n              +\n              \n                z\n                \n                  m\n                \n              \n              \u2212\n              exp\n              \u2061\n              (\n              \u2212\n              \n                z\n                \n                  m\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle m={\\frac {z_{m}^{2}/z_{oc}}{1+z_{m}-\\exp(-z_{m})}}}\n  to find the impedance matching factor. For a zoc of 32.4, this comes to 86.5%.\n\n\n=== All together ===\nConsidering the spectrum losses alone, a solar cell has a peak theoretical efficiency of 48% (or 44% according to Shockley and Queisser \u2013 their \"ultimate efficiency factor\"). Thus the spectrum losses represent the vast majority of lost power. Including the effects of recombination and the I versus V curve, the efficiency is described by the following equation:\n\n  \n    \n      \n        \u03b7\n        =\n        \n          t\n          \n            s\n          \n        \n        u\n        (\n        \n          x\n          \n            g\n          \n        \n        )\n        v\n        (\n        f\n        ,\n        \n          x\n          \n            c\n          \n        \n        ,\n        \n          x\n          \n            g\n          \n        \n        )\n        m\n        (\n        v\n        \n          x\n          \n            g\n          \n        \n        \n          /\n        \n        \n          x\n          \n            c\n          \n        \n        )\n      \n    \n    {\\displaystyle \\eta =t_{s}u(x_{g})v(f,x_{c},x_{g})m(vx_{g}/x_{c})}\n  with\n\n  \n    \n      \n        \n          x\n          \n            g\n          \n        \n        =\n        \n          V\n          \n            g\n          \n        \n        \n          /\n        \n        \n          V\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle x_{g}=V_{g}/V_{s}}\n  \n\n  \n    \n      \n        \n          x\n          \n            c\n          \n        \n        =\n        \n          V\n          \n            c\n          \n        \n        \n          /\n        \n        \n          V\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle x_{c}=V_{c}/V_{s}}\n  where u, v, and m are respectively the ultimate efficiency factor, the ratio of open-circuit voltage Vop to band-gap voltage Vg, and the impedance matching factor (all discussed above), and Vc is the thermal voltage, and Vs is the voltage equivalent of the temperature of the Sun. Letting ts be 1, and using the values mentioned above of 44%, 77%, and 86.5% for the three factors gives about 29% overall efficiency. Shockley and Queisser say 30% in their abstract, but do not give a detailed calculation. A more recent reference gives, for a single-junction cell, a theoretical peak performance of about 33.7%, or about 337 W/m2 in AM1.5.When the amount of sunlight is increased using reflectors or lenses, the factor f\u03c9 (and therefore f) will be higher. This raises both v and m. Shockley and Queisser include a graph showing the overall efficiency as a function of band gap for various values of f. For a value of 1, the graph shows a maximum efficiency of just over 40%, getting close to the ultimate efficiency (by their calculation) of 44%.\n\n\n=== Other considerations ===\nShockley and Queisser's work considered the most basic physics only; there are a number of other factors that further reduce the theoretical power.\n\n\n==== Limited mobility ====\nWhen an electron is ejected through photoexcitation, the atom it was formerly bound to is left with a net positive charge. Under normal conditions, the atom will pull off an electron from a surrounding atom in order to neutralize itself. That atom will then attempt to remove an electron from another atom, and so forth, producing an ionization chain reaction that moves through the cell. Since these can be viewed as the motion of a positive charge, it is useful to refer to them as \"holes\", a sort of virtual positive electron.\nLike electrons, holes move around the material, and will be attracted towards a source of electrons. Normally these are provided through an electrode on the back surface of the cell. Meanwhile, the conduction-band electrons are moving forward towards the electrodes on the front surface. For a variety of reasons, holes in silicon move much more slowly than electrons. This means that during the finite time while the electron is moving forward towards the p-n junction, it may meet a slowly moving hole left behind by a previous photoexcitation. When this occurs, the electron recombines at that atom, and the energy is lost (normally through the emission of a photon of that energy, but there are a variety of possible processes).\nRecombination places an upper limit on the rate of production; past a certain rate there are so many holes in motion that new electrons will never make it to the p-n junction. In silicon this reduces the theoretical performance under normal operating conditions by another 10% over and above the thermal losses noted above. Materials with higher electron (or hole) mobility can improve on silicon's performance; gallium arsenide (GaAs) cells gain about 5% in real-world examples due to this effect alone. In brighter light, when it is concentrated by mirrors or lenses for example, this effect is magnified. Normal silicon cells quickly saturate, while GaAs continue to improve at concentrations as high as 1500 times.\n\n\n==== Non-radiative recombination ====\nRecombination between electrons and holes is detrimental in a solar cell, so designers try to minimize it. However, radiative recombination\u2014when an electron and hole recombine to create a photon that exits the cell into the air\u2014is inevitable, because it is the time-reversed process of light absorption. Therefore, the Shockley\u2013Queisser calculation takes radiative recombination into account; but it assumes (optimistically) that there is no other source of recombination. More realistic limits, which are lower than the Shockley\u2013Queisser limit, can be calculated by taking into account other causes of recombination. These include recombination at defects and grain boundaries.\nIn crystalline silicon, even if there are no crystalline defects, there is still Auger recombination, which occurs much more often than radiative recombination. By taking this into account, the theoretical efficiency of crystalline silicon solar cells was calculated to be 29.4%.\n\n\n==== Frequency-dependent absorption ====\nThe frequency dependence of the absorption and effectively the reflectance of materials can be taken into account when calculating the solar cell efficiency. According to Shockley-Quiesser limit, solar cell efficiency of semiconductors depend on the band gap of the material. Here, it is assumed that optical absorption starts above the band gap of the material. However, due to finite temperature, optical excitations are possible below the optical gap. We can clearly see this from the tail of the imaginary dielectric function below the optical gap depending on temperature. Since imaginary dielectric functions is, even though low, non-zero below the optical gap, there is absorption of light below the optical gap. For thick enough materials this can cause significant absorption. In the Shockley-Quiesser limit, 100% light absorption is assumed above the band gap of the material in order to find highest possible efficiency of a solar cell in case reflectance is reduced to zero for example by using an anti-reflecting coating. However, the problem with this assumption is that absorbance below the band gap of the material at finite temperatures is neglected which can effect the efficiency. With the inclusion of the absorption below the band gap, the lower limit of the short-circuit current integral is changed from band gap to zero and therefore the equation is defined as;\n\n  \n    \n      \n        \n          J\n          \n            s\n            c\n          \n        \n        =\n        \n          \u222b\n          \n            0\n          \n          \n            \u221e\n          \n        \n        A\n        \n          \n            \n              J\n              \n                p\n                h\n                (\n                \u210f\n                \u03c9\n                )\n              \n            \n            \n              \u210f\n              \u03c9\n            \n          \n        \n        d\n        (\n        \u210f\n        \u03c9\n        )\n      \n    \n    {\\displaystyle J_{sc}=\\int _{0}^{\\infty }A{\\frac {J_{ph(\\hbar \\omega )}}{\\hbar \\omega }}d(\\hbar \\omega )}\n  where Jsc is the short-circuit current, A is the thickness dependent absorbance of the material, Jph is the AM1.5 solar energy flux, and \u03c9 is the frequency of light.\n\n\n== Exceeding the limit ==\nIt is important to note that the analysis of Shockley and Queisser was based on the following assumptions:\n\nOne electron\u2013hole pair excited per incoming photon\nThermal relaxation of the electron\u2013hole pair energy in excess of the band gap\nIllumination with non-concentrated sunlightNone of these assumptions is necessarily true, and a number of different approaches have been used to significantly surpass the basic limit.\n\n\n=== Multijunction cells ===\n\nThe most widely explored path to higher efficiency solar cells has been multijunction photovoltaic cells, also known as \"tandem cells\". These cells use multiple p-n junctions, each one tuned to a particular frequency of the spectrum. This reduces the problem discussed above, that a material with a single given bandgap cannot absorb sunlight below the bandgap, and cannot take full advantage of sunlight far above the bandgap. In the most common design, a high-bandgap solar cell sits on top, absorbing high-energy, shorter-wavelength light, and transmitting the rest. Beneath it is a lower-bandgap solar cell which absorbs some of the lower-energy, longer-wavelength light. There may be yet another cell beneath that one, with as many as four layers in total.\nThe calculation of the fundamental efficiency limits of these multijunction cells works in a fashion similar to those for single-junction cells, with the caveat that some of the light will be converted to other frequencies and re-emitted within the structure. Using methods similar to the original Shockley\u2013Queisser analysis with these considerations in mind produces similar results; a two-layer cell can reach 42% efficiency, three-layer cells 49%, and a theoretical infinity-layer cell 86% in non-concentrated sunlight.The majority of tandem cells that have been produced to date use three layers, tuned to blue (on top), yellow (middle) and red (bottom). These cells require the use of semiconductors that can be tuned to specific frequencies, which has led to most of them being made of gallium arsenide (GaAs) compounds, often germanium for red, GaAs for yellow, and GaInP2 for blue. They are expensive, requiring techniques similar to microprocessor construction, but with \"chip\" sizes on the scale of several centimeters. In cases where performance is the only consideration, these cells have become common; they are widely used in satellite applications for instance, where the power-to-weight ratio overwhelms other considerations. They also can be used in concentrated photovoltaic applications (see below), where a relatively small solar cell can serve a large area.\nTandem cells are not restricted to high-performance applications; they are also used to make moderate-efficiency photovoltaics out of cheap but low-efficiency materials. One example is amorphous silicon solar cells, where triple-junction tandem cells are commercially available from Uni-Solar and other companies.\nIn 2023 Chinese manufacturer LONGi Green Energy Technology Co. announced a tandem silicon/perovskite cell that achieved 33.9% efficiency, the first time a silicon-based cell has exceeded the S-Q limit.\n\n\n=== Light concentration ===\n\nSunlight can be concentrated with lenses or mirrors to much higher intensity. The sunlight intensity is a parameter in the Shockley\u2013Queisser calculation, and with more concentration, the theoretical efficiency limit increases somewhat. If, however, the intense light heats up the cell, which often occurs in practice, the theoretical efficiency limit may go down all things considered.\nIn practice, the choice of whether or not to use light concentration is based primarily on other factors besides the small change in solar cell efficiency. These factors include the relative cost per area of solar cells versus focusing optics like lenses or mirrors, the cost of sunlight-tracking systems, the proportion of light successfully focused onto the solar cell, and so on.\nA wide variety of optical systems can be used to concentrate sunlight, including ordinary lenses and curved mirrors, fresnel lenses, arrays of small flat mirrors, and luminescent solar concentrators. Another proposal suggests spreading out an array of microscopic solar cells on a surface, and focusing light onto them via microlens arrays, while yet another proposal suggests designing a semiconductor nanowire array in such a way that light is concentrated in the nanowires.\n\n\n=== Intermediate band photovoltaics ===\n\nThere has been some work on producing mid-energy states within single crystal structures. These cells would combine some of the advantages of the multi-junction cell with the simplicity of existing silicon designs. A detailed limit calculation for these cells with infinite bands suggests a maximum efficiency of 77.2% To date, no commercial cell using this technique has been produced.\n\n\n=== Photon upconversion ===\n\nAs discussed above, photons with energy below the bandgap are wasted in ordinary single-junction solar cells. One way to reduce this waste is to use photon upconversion, i.e. incorporating into the module a molecule or material that can absorb two or more below-bandgap photons and then emit one above-bandgap photon. Another possibility is to use two-photon absorption, but this can only work at extremely high light concentration.\n\n\n=== Thermal photon upconversion ===\nThermal upconversion is based on the absorption of photons with low energies in the upconverter, which heats up and re-emits photons with higher energies. The upconversion efficiency can be improved by controlling the optical density of states of the absorber and also by tuning the angularly-selective emission characteristics. For example, a planar thermal upconverting platform can have a front surface that absorbs low-energy photons incident within a narrow angular range, and a back surface that efficiently emits only high-energy photons. A hybrid thermophotovoltaic platform exploiting thermal upconversion was theoretically predicted to demonstrate maximum conversion efficiency of 73% under illumination by non-concentrated sunlight. A detailed analysis of non-ideal hybrid platforms that allows for up to 15% of absorption/re-emission losses yielded limiting efficiency value of 45% for Si PV cells.\n\n\n=== Hot electron capture ===\nOne of the main loss mechanisms is due to the loss of excess carrier energy above the bandgap. It should be no surprise that there has been a considerable amount of research into ways to capture the energy of the carriers before they can lose it in the crystal structure. One system under investigation for this is quantum dots.\n\n\n=== Multiple exciton generation ===\n\nA related concept is to use semiconductors that generate more than one excited electron per absorbed photon, instead of a single electron at the band edge. Quantum dots have been extensively investigated for this effect, and they have been shown to work for solar-relevant wavelengths in prototype solar cells.Another, more straightforward way to utilise multiple exciton generation is a process called singlet fission (or singlet exciton fission) by which a singlet exciton is converted into two triplet excitons of lower energy. This allows for higher theoretical efficiencies when coupled to a low bandgap semiconductor and quantum efficiencies exceeding 100% have been reported.Also in materials where the (excited) electrons interact strongly with the remaining electrons such as Mott insulators multiple excitons can be generated.\n\n\n=== Fluorescent downconversion/downshifting ===\nAnother possibility for increased efficiency is to convert the frequency of light down towards the bandgap energy with a fluorescent material. In particular, to exceed the Shockley\u2013Queisser limit, it is necessary for the fluorescent material to convert a single high-energy photon into several lower-energy ones (quantum efficiency > 1). For example, one photon with more than double the bandgap energy can become two photons above the bandgap energy. In practice, however, this conversion process tends to be relatively inefficient. If a very efficient system were found, such a material could be painted on the front surface of an otherwise standard cell, boosting its efficiency for little cost. In contrast, considerable progress has been made in the exploration of fluorescent downshifting, which converts high-energy light (e. g., UV light) to low-energy light (e. g., red light) with a quantum efficiency smaller than 1. The cell may be more sensitive to these lower-energy photons. Dyes, rare-earth phosphors and quantum dots are actively investigated for fluorescent downshifting. For example, silicon quantum dots enabled downshifting has led to the efficiency enhancement of the state-of-the-art silicon solar cells.\n\n\n=== Thermophotovoltaic downconversion ===\n\nThermophotovoltaic cells are similar to phosphorescent systems, but use a plate to act as the downconvertor. Solar energy falling on the plate, typically black-painted metal, is re-emitted as lower-energy IR, which can then be captured in an IR cell. This relies on a practical IR cell being available, but the theoretical conversion efficiency can be calculated. For a converter with a bandgap of 0.92 eV, efficiency is limited to 54% with a single-junction cell, and 85% for concentrated light shining on ideal components with no optical losses and only radiative recombination.\n\n\n== See also ==\nThermodynamic efficiency limit\n\n\n== References ==\n\n\n== External links ==\nReproduction of the Shockley\u2013Queisser calculation (PDF), using the Mathematica software program. This code was used to calculate all the graphs in this article.\nLuque, Antonio, and Antonio Mart\u00ed. \"Chapter 4: Theoretical Limits of Photovoltaic Conversion and New-generation Solar Cells.\" Ed. Antonio Luque and Steven Hegedus. Handbook of Photovoltaic Science and Engineering. Second ed. N.p.: John Wiley & Sons, 2011. 130\u201368. Print."}, {"id": 97, "title": "Power (Physics)", "content": "In physics, power is the amount of energy transferred or converted per unit time. In the International System of Units, the unit of power is the watt, equal to one joule per second. In older works, power is sometimes called activity. Power is a scalar quantity.\nSpecifying power in particular systems may require attention to other quantities; for example, the power involved in moving a ground vehicle is the product of the aerodynamic drag plus traction force on the wheels, and the velocity of the vehicle. The output power of a motor is the product of the torque that the motor generates and the angular velocity of its output shaft.  Likewise, the power dissipated in an electrical element of a circuit is the product of the current flowing through the element and of the voltage across the element.\n\n\n== Definition ==\nPower is the rate with respect to time at which work is done; it is the time derivative of work:\n\nwhere P is power, W is work, and t is time.\nWe will now show that the mechanical power generated by a force F on a body moving at the velocity v can be expressed as the product: \n- If a constant force F is applied throughout a distance x, the work done is defined as \n  \n    \n      \n        W\n        =\n        \n          F\n        \n        \u22c5\n        \n          x\n        \n      \n    \n    {\\displaystyle W=\\mathbf {F} \\cdot \\mathbf {x} }\n  . In this case, power can be written as:\n\n- If instead the force is variable over a three-dimensional curve C, then the work is expressed in terms of the line integral:\n\nFrom the fundamental theorem of calculus, we know that  Hence the formula is valid for any general situation.\n\n\n== Units ==\nThe dimension of power is energy divided by time. In the International System of Units (SI), the unit of power is the watt (W), which is equal to one joule per second. Other common and traditional measures are horsepower (hp), comparing to the power of a horse; one mechanical horsepower equals about 745.7 watts. Other units of power include ergs per second (erg/s), foot-pounds per minute, dBm, a logarithmic measure relative to a reference of 1 milliwatt, calories per hour, BTU per hour (BTU/h), and tons of refrigeration.\n\n\n== Average power and instantaneous power ==\nAs a simple example, burning one kilogram of coal releases more energy than detonating a kilogram of TNT, but because the TNT reaction releases energy more quickly, it delivers more power than the coal.\nIf \u0394W  is the amount of work performed during a period of time of duration \u0394t, the average power Pavg over that period is given by the formula\n\nIt is the average amount of work done or energy converted per unit of time. Average power is often called \"power\" when the context makes it clear.\nInstantaneous power is the limiting value of the average power as the time interval \u0394t approaches zero.\n\nWhen power P is constant, the amount of work performed in time period t can be calculated as\n\nIn the context of energy conversion, it is more customary to use the symbol E rather than W.\n\n\n== Mechanical power ==\nPower in mechanical systems is the combination of forces and movement. In particular, power is the product of a force on an object and the object's velocity, or the product of a torque on a shaft and the shaft's angular velocity.\nMechanical power is also described as the time derivative of work.  In mechanics, the work done by a force F on an object that travels along a curve C is given by the line integral:\n\nwhere x defines the path C and v is the velocity along this path.\nIf the force F is derivable from a potential (conservative), then applying the gradient theorem (and remembering that force is the negative of the gradient of the potential energy) yields:\n\nwhere A and B are the beginning and end of the path along which the work was done.\nThe power at any point along the curve C is the time derivative:\n\nIn one dimension, this can be simplified to:\n\nIn rotational systems, power is the product of the torque \u03c4 and angular velocity \u03c9,\n\nwhere \u03c9 is angular frequency, measured in radians per second.  The \n  \n    \n      \n        \u22c5\n      \n    \n    {\\displaystyle \\cdot }\n   represents scalar product.\nIn fluid power systems such as hydraulic actuators, power is given by  where p is pressure in pascals or N/m2, and Q is volumetric flow rate in m3/s in SI units.\n\n\n=== Mechanical advantage ===\nIf a mechanical system has no losses, then the input power must equal the output power. This provides a simple formula for the mechanical advantage of the system.\nLet the input power to a device be a force FA acting on a point that moves with velocity vA and the output power be a force FB acts on a point that moves with velocity vB.  If there are no losses in the system, then\n\nand the mechanical advantage of the system (output force per input force) is given by\n\nThe similar relationship is obtained for rotating systems, where TA and \u03c9A are the torque and angular velocity of the input and TB and \u03c9B are the torque and angular velocity of the output.  If there are no losses in the system, then\n\nwhich yields the mechanical advantage\n\nThese relations are important because they define the maximum performance of a device in terms of velocity ratios determined by its physical dimensions.  See for example gear ratios.\n\n\n== Electrical power ==\n\nThe instantaneous electrical power P delivered to a component is given by\n\nwhere \n\n  \n    \n      \n        P\n        (\n        t\n        )\n      \n    \n    {\\displaystyle P(t)}\n   is the instantaneous power, measured in watts (joules per second),\n\n  \n    \n      \n        V\n        (\n        t\n        )\n      \n    \n    {\\displaystyle V(t)}\n   is the potential difference (or voltage drop) across the component, measured in volts, and\n\n  \n    \n      \n        I\n        (\n        t\n        )\n      \n    \n    {\\displaystyle I(t)}\n   is the current through it, measured in amperes.If the component is a resistor with time-invariant voltage to current ratio, then:\n\nwhere\n\nis the electrical resistance, measured in ohms.\n\n\n== Peak power and duty cycle ==\nIn the case of a periodic signal \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n   of period \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  , like a train of identical pulses, the instantaneous power \n  \n    \n      \n        p\n        (\n        t\n        )\n        =\n        \n          |\n        \n        s\n        (\n        t\n        )\n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\textstyle p(t)=|s(t)|^{2}}\n   is also a periodic function of period \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  .  The peak power is simply defined by:\n\nThe peak power is not always readily measurable, however, and the measurement of the average power \n  \n    \n      \n        \n          P\n          \n            \n              a\n              v\n              g\n            \n          \n        \n      \n    \n    {\\displaystyle P_{\\mathrm {avg} }}\n   is more commonly performed by an instrument.  If one defines the energy per pulse as\n\nthen the average power is\n\nOne may define the pulse length \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n   such that \n  \n    \n      \n        \n          P\n          \n            0\n          \n        \n        \u03c4\n        =\n        \n          \u03b5\n          \n            \n              p\n              u\n              l\n              s\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle P_{0}\\tau =\\varepsilon _{\\mathrm {pulse} }}\n   so that the ratios\n\nare equal. These ratios are called the duty cycle of the pulse train.\n\n\n== Radiant power ==\nPower is related to intensity at a radius \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  ; the power emitted by a source can be written as:\n\n\n== See also ==\nSimple machines\nOrders of magnitude (power)\nPulsed power\nIntensity \u2013 in the radiative sense, power per area\nPower gain \u2013 for linear, two-port networks\nPower density\nSignal strength\nSound power\n\n\n== References =="}, {"id": 98, "title": "Content creation", "content": "Content creation is the act of producing and sharing information or media content for specific audiences, particularly in digital contexts. According to Dictionary.com, content refers to \"something that is to be expressed through some medium, as speech, writing or any of various arts\" for self-expression, distribution, marketing and/or publication. Content creation encompasses various activities including maintaining and updating web sites, blogging, article writing, photography, videography, online commentary, social media accounts, and editing and distribution of digital media. In a survey conducted by Pew, content creation was defined as \"the material people contribute to the online world.\"\n\n\n== Content creators ==\n\n\n=== News organizations ===\nNews organizations, especially those with a large and global reach like The New York Times, NPR, and CNN, consistently create some of the most shared content on the Web, especially in relation to current events. In the words of a 2011 report from the Oxford School for the Study of Journalism and the Reuters Institute for the Study of Journalism, \"Mainstream media is the lifeblood of topical social media conversations in the UK.\" While the rise of digital media has disrupted traditional news outlets, many have adapted and have begun to produce content that is designed to function on the web and be shared on social media. The social media site Twitter is a major distributor and aggregator of breaking news from various sources , and the function and value of Twitter in the distribution of news is a frequent topic of discussion and research in journalism. User-generated content, social media blogging and citizen journalism have changed the nature of news content in recent years. The company Narrative Science is now using artificial intelligence to produce news articles and interpret data.\n\n\n=== Colleges, universities, and think tanks ===\nAcademic institutions, such as colleges and universities, create content in the form of books, journal articles, white papers, and some forms of digital scholarship, such as blogs that are group edited by academics, class wikis, or video lectures that support a massive open online course (MOOC). Through an open data initiative, institutions may make raw data supporting their experiments or conclusions available on the Web. Academic content may be gathered and made accessible to other academics or the public through publications, databases, libraries, and digital libraries. Academic content may be closed source or open access (OA). Closed-source content is only available to authorized users or subscribers. For example, an important journal or a scholarly database may be a closed source, available only to students and faculty through the institution's library. Open-access articles are open to the public, with the publication and distribution costs shouldered by the institution publishing the content.\n\n\n=== Companies ===\nCorporate content includes advertising and public relations content, as well as other types of content produced for profit, including white papers and sponsored research. Advertising can also include auto-generated content, with blocks of content generated by programs or bots for search engine optimization. Companies also create annual reports which are part of their company's workings and a detailed review of their financial year. This gives the stakeholders of the company insight into the company's current and future prospects and direction.\n\n\n=== Artists and writers ===\nCultural works, like music, movies, literature, and art, are also major forms of content. Examples include traditionally published books and e-books as well as self-published books, digital art, fanfiction, and fan art. Independent artists, including authors and musicians, have found commercial success by making their work available on the Internet.\n\n\n=== Government ===\nThrough digitization, sunshine laws, open records laws and data collection, governments may make statistical, legal or regulatory information available on the Internet. National libraries and state archives turn historical documents, public records, and unique relics into online databases and exhibits. This has raised significant privacy issues. In 2012, The Journal News, a New York state paper, sparked outcry when it published an interactive map of the state's gun owner locations using legally obtained public records. Governments also create online or digital propaganda or misinformation to support domestic and international goals. This can include astroturfing, or using media to create a false impression of mainstream belief or opinion.Governments can also use open content, such as public records and open data, in service of public health, educational and scientific goals, such as crowdsourcing solutions to complex policy problems. In 2013, National Aeronautics and Space Administration (NASA) joined asteroid mining company Planetary Resources to crowdsource the hunt for near-Earth objects. Describing NASA's crowdsourcing work in an interview, technology transfer executive David Locke spoke of the \"untapped cognitive surplus that exists in the world\" which could be used to help develop NASA technology. In addition to making governments more participatory, open records and open data have the potential to make governments more transparent and less corrupt.\n\n\n=== Users ===\nThe introduction of Web 2.0 made it possible for content consumers to be more involved in the generation and sharing of content. With the advent of digital media, the amount of user generated content, as well as the age and class range of users, has increased. 8% of Internet users are very active in content creation and consumption. Worldwide, about one in four Internet users are significant content creators, and users in emerging markets lead the world in engagement. Research has also found that young adults of a higher socioeconomic background tend to create more content than those from lower socioeconomic backgrounds. 69% of American and European internet users are \"spectators,\" who consume\u2014but don't create\u2014online and digital media. The ratio of content creators to the amount of content they generate is sometimes referred to as the 1% rule, a rule of thumb that suggests that only 1% of a forum's users create nearly all of its content. Motivations for creating new content may include the desire to gain new knowledge, the possibility of publicity, or simple altruism. Users may also create new content in order to bring about social reforms. However, researchers caution that in order to be effective, context must be considered, a diverse array of people must be included, and all users must participate throughout the process.According to a 2011 study, minorities create content in order to connect with their communities online. African-American users have been found to create content as a means of self-expression that was not previously available. Media portrayals of minorities are sometimes inaccurate and stereotypical which affects the general perception of these minorities. African-Americans respond to their portrayals digitally through the use of social media such as Twitter and Tumblr. The creation of Black Twitter has allowed a community to share their problems and ideas.\n\n\n==== Teens ====\nYounger users now have greater access to content, content creating applications, and the ability to publish to different types of media, such as Facebook, Blogger, Instagram, DeviantArt, or Tumblr. As of 2005, around 21 million teens used the internet and 57%, or 12 million teens, consider themselves content creators. This proportion of media creation and sharing is higher than that of adults. With the advent of the Internet, teens have had more access to tools for sharing and creating content. Increase in accessibility to technology, especially due to lower prices, has led to an increase in accessibility of content creation tools as well for teens. Some teens use this to become content creators through online platforms like YouTube, while others use it to connect to friends through social networking sites.\n\n\n== Issues ==\n\n\n=== Quality ===\nThe rise of anonymous and user-generated content presents both opportunities and challenges to Web users. Blogging, self-publishing and other forms of content creation give more people access to larger audiences. However, this can also perpetuate rumors and lead to misinformation. It can make it more difficult to find quality content that meets users' information needs.\nThe feature of user-generated content and personalized recommendation algorithms of digital media also gives a rise to confirmation bias. Users may tend to seek out information that confirms their existing beliefs and ignore information that contradicts them. This can lead to one-sided, unbalanced content that does not present a complete picture of an issue.\nThe quality of digital contents varies from traditional academic or published writing. Digital media writing is often more engaging and accessible to a broader audience than academic writing, which is usually intended for a specialized audience. Digital media writers often use a conversational tone, personal anecdotes, and multimedia elements like images and videos to enhance the reader's experience. For example, the veteran populist anti-EU campaigner Farage's tweets in 2017\u20132018 used a lot of colloquial expressions and catchphrases to resonate the \u201ccommon sense\u201d with audiences.At the same time, digital media is also necessary for professional (academic) communicators to reach an audience, as well as with connecting to scholars in their areas of expertise.The quality of digital contents is also influenced by capitalism and market-driven consumerism. Writers may have commercial interests that influence the content they produce. For example, a writer who is paid to promote a particular product or service may write articles that are biased in favor of that product or service, even if it is not the best option for the reader.  \n\n\n=== Metadata ===\nDigital content is difficult to organize and categorize. Websites, forums, and publishers all have different standards for metadata, or information about the content, such as its author and date of creation. The perpetuation of different standards of metadata can create problems of accessibility and discoverability.\n\n\n=== Ethics ===\nDigital writing and content creation has evolved significantly. This has led to various ethical issues, including privacy, individual rights, and representation. A focus on cultural identity has helped increase accessibility, empowerment, and social justice in digital media, but might also prevent users from freely communicating and expressing.\n\n\n=== Intellectual property ===\n\nThe ownership, origin, and right to share digital content can be difficult to establish. User-generated content presents challenges to traditional content creators (professional writers, artists, filmmakers, musicians, choreographers, etc.) with regard to the expansion of unlicensed and unauthorized derivative works, piracy and plagiarism. Also, the enforcement of copyright laws, such as the Digital Millennium Copyright Act in the U.S., makes it less likely that works will fall into the public domain.\n\n\n== Social movements ==\n\n\n=== 2011 Egyptian revolution ===\n\nContent creation serves as a useful form of protest on social media platforms. The 2011 Egyptian revolution was one example of content creation being used to network protestors globally for the common cause of protesting the \"authoritarian regimes in the Middle East and North Africa throughout 2011\". The protests took place in multiple cities in Egypt, and quickly evolved from peaceful protest into open conflict. Social media outlets allowed protestors from different regions to network with each other and raise awareness of the widespread corruption in Egypt's government, as well as helping coordinate their response. Youth activists promoting the rebellion were able to formulate a Facebook group, \"Progressive Youth of Tunisia\".\n\n\n=== Other ===\n\nExamples of recent social media protest through online content include the global widespread use of the hashtags #MeToo, used to raise awareness against sexual abuse, and #BlackLivesMatter, which focused on police brutality against black people.\n\n\n== See also ==\nContent intelligence\nContent marketing\nCopyright\nCreative commons\nCreativity\nCreator economy\nCultural technology\nGary Vaynerchuk\n\n\n== References =="}, {"id": 99, "title": "Digital marketing ", "content": "Digital marketing is the component of marketing that uses the Internet and online-based digital technologies such as desktop computers, mobile phones, and other digital media and platforms to promote products and services. Its development during the 1990s and 2000s changed the way brands and businesses use technology for marketing. As digital platforms became increasingly incorporated into marketing plans and everyday life, and as people increasingly used digital devices instead of visiting physical shops, digital marketing campaigns have become prevalent, employing combinations of search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e-books, and optical disks and games have become commonplace. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callbacks, and on-hold mobile ringtones. The extension to non-Internet channels differentiates digital marketing from online marketing.\n\n\n== History ==\nDigital marketing effectively began in 1990 when the Archie search engine was created as an index for FTP sites. In the 1980s, the storage capacity of computers was already large enough to store huge volumes of customer information. Companies started choosing online techniques, such as database marketing, rather than limited list brokers. Databases allowed companies to track customers' information more effectively, transforming the relationship between buyer and seller.\nIn the 1990s, the term digital marketing was coined. With the development of server/client architecture and the popularity of personal computers, Customer Relationship Management (CRM) applications became a significant factor in marketing technology. Fierce competition forced vendors to include more services in their software, for example, marketing, sales, and service applications. Marketers were also able to own online customer data through eCRM software after the Internet was born. This led to the first clickable banner ad going live in 1994, which was the \"You Will\" campaign by AT&T, and over the first four months of it going live, 44% of all people who saw it clicked on the ad.In the 2000s, with increasing numbers of Internet users and the birth of the iPhone, customers began searching for products and making decisions about their needs online first, instead of consulting a salesperson, which created a new problem for the marketing department of a company. In addition, a survey in 2000 in the United Kingdom found that most retailers still needed to register their own domain address. These problems encouraged marketers to find new ways to integrate digital technology into market development.\nIn 2007, marketing automation was developed as a response to the ever-evolving marketing climate. Marketing automation is the process by which software is used to automate conventional marketing processes. Marketing automation helped companies segment customers, launch multichannel marketing campaigns, and provide personalized information for customers., based on their specific activities. In this way, users' activity (or lack thereof) triggers a personal message that is customized to the user in their preferred platform. However, despite the benefits of marketing automation many companies are struggling to adopt it to their everyday uses correctly.Digital marketing became more sophisticated in the 2000s and the 2010s, \nwhen the proliferation of devices' capable of accessing digital media led to sudden growth. Statistics produced in 2012 and 2013 showed that digital marketing was still growing.\nWith the development of social media in the 2000s, such as LinkedIn, Facebook, YouTube and Twitter, consumers became highly dependent on digital electronics in daily lives. Therefore, they expected a seamless user experience across different channels for searching product's information. The change of customer behavior improved the diversification of marketing technology.The term \"Digital Marketing\" was coined in the 1990s. Digital marketing was formally known as and referred to as 'online marketing', 'internet marketing' or 'web marketing'.  Worldwide digital marketing has become the most common used term and took off in the business industry, especially after the year 2013. But in other countries like Italy, digital marketing is still known as web marketing.Digital media growth was estimated at 4.5 trillion online ads served annually with digital media spend at 48% growth in 2010. An increasing portion of advertising stems from businesses employing Online Behavioural Advertising (OBA) to tailor advertising for internet users, but OBA raises concern of consumer privacy and data protection.\n\n\n== New non-linear marketing approach ==\nNonlinear marketing, a type of interactive marketing, is a long-term marketing approach which builds on businesses collecting information about an Internet user's online activities and trying to be visible in multiple areas.Unlike traditional marketing techniques, which involve direct, one-way messaging to consumers (via print, television, and radio advertising), nonlinear digital marketing strategies are centered on reaching prospective customers across multiple online channels.Combined with higher consumer knowledge and the demand for more sophisticated consumer offerings, this change has forced many businesses to rethink their outreach strategy and adopt or incorporate omnichannel, nonlinear marketing techniques to maintain sufficient brand exposure, engagement, and reach.Nonlinear marketing strategies involve efforts to adapt the advertising to different platforms, and to tailor the advertising to different individual buyers rather than a large coherent audience.Tactics may include:\n\nSearch engine optimization (SEO)\nSocial media marketing\nVideo marketing\nEmail marketing\nBlogging & affiliate marketing\nWebsite marketing\nPay-per-click\nContent marketing\nSearch engine marketingSome studies indicate that consumer responses to traditional marketing approaches are becoming less predictable for businesses.  According to a 2018 study, nearly 90% of online consumers in the United States researched products and brands online before visiting the store or making a purchase. The Global Web Index estimated that in 2018, a little more than 50% of consumers researched products on social media. Businesses often rely on individuals portraying their products in a positive light on social media, and may adapt their marketing strategy to target people with large social media followings in order to generate such comments. In this manner, businesses can use consumers to advertise their products or services, decreasing the cost for the company.\n\n\n== Brand awareness ==\n\nOne of the key objectives of modern digital marketing is to raise brand awareness, the extent to which customers and the general public are familiar with and recognize a particular brand.\nEnhancing brand awareness is important in digital marketing, and marketing in general, because of its impact on brand perception and consumer decision-making. According to the 2015 essay, \"Impact of Brand on Consumer Behavior\":\n\"Brand awareness, as one of the fundamental dimensions of brand equity, is often considered to be a prerequisite of consumers\u2019 buying decision, as it represents the main factor for including a brand in the consideration set. Brand awareness can also influence consumers\u2019 perceived risk assessment and their confidence in the purchase decision, due to familiarity with the brand and its characteristics.\"Recent trends show that businesses and digital marketers are prioritizing brand awareness, focusing more on their digital marketing efforts on cultivating brand recognition and recall than in previous years. This is evidenced by a 2019 Content Marketing Institute study, which found that 81% of digital marketers have worked on enhancing brand recognition over the past year.Another Content Marketing Institute survey revealed 89% of B2B marketers now believe improving brand awareness to be more important than efforts directed at increasing sales.Increasing brand awareness is a focus of digital marketing strategy for a number of reasons:\n\nThe growth of online shopping. A survey by Statista projects 230.5 million people in the United States will use the internet to shop, compare, and buy products by 2021, up from 209.6 million in 2016. Research from business software firm Salesforce found 87% of people began searches for products and brands on digital channels in 2018.\nThe role of digital interaction in customer behavior. It\u2019s estimated that 70% of all retail purchases made in the U.S. are influenced to some degree by an interaction with a brand online.\nThe growing influence and role of brand awareness in online consumer decision-making: 82% of online shoppers searching for services give preference to brands they know of.\nThe use, convenience, and influence of social media. A recent report by Hootsuite estimated there were more than 3.4 billion active users on social media platforms, a 9% increase from 2018. A 2019 survey by The Manifest states that 74% of social media users follow brands on social sites, and 96% of people who follow businesses also engage with those brands on social platforms. According to Deloitte, one in three U.S. consumers are influenced by social media when buying a product, while 47% of millennials factor their interaction with a brand on social when making a purchase.\n\n\n== Online methods used to build brand awareness ==\nDigital marketing strategies may include the use of one or more online channels and techniques (omnichannel) to increase brand awareness among consumers.\nBuilding brand awareness may involve such methods/tools as:\n\n\n=== Search engine optimization (SEO) ===\nSearch engine optimization techniques may be used to improve the visibility of business websites and brand-related content for common industry-related search queries.\nThe importance of SEO to increase brand awareness is said to correlate with the growing influence of search results and search features like featured snippets, knowledge panels, and local SEO on customer behavior.\n\n\n=== Search engine marketing (SEM) ===\nSEM, also known as PPC advertising, involves the purchase of ad space in prominent, visible positions atop search results pages and websites. Search ads have been shown to have a positive impact on brand recognition, awareness and conversions.\n\n33% of searchers who click on paid ads do so because they directly respond to their particular search query.\n\n\n=== Social media marketing ===\nSocial media marketing has the characteristics of being in the marketing state and interacting with consumers all the time, emphasizing content and interaction skills. The marketing process needs to be monitored, analyzed, summarized and managed in real-time, and the marketing target needs to be adjusted according to the real-time feedback from the market and consumers. 70% of marketers list increasing brand awareness as their number one goal for marketing on social media platforms. Facebook, Instagram, Twitter, and YouTube are listed as the top platforms currently used by social media marketing teams. As of 2021, LinkedIn has been added as one of the most-used social media platforms by business leaders for its professional networking capabilities.\n\n\n=== Content marketing ===\n56% of marketers believe personalization content \u2013 brand-centered blogs, articles, social updates, videos, landing pages \u2013 improves brand recall and engagement.\n\n\n== Developments and strategies ==\nOne of the major changes that occurred in traditional marketing was the \"emergence of digital marketing\", this led to the reinvention of marketing strategies in order to adapt to this major change in traditional marketing.\nAs digital marketing is dependent on technology which is ever-evolving and fast-changing, the same features should be expected from digital marketing developments and strategies. This portion is an attempt to qualify or segregate the notable highlights existing and being used as of press time.\nSegmentation: More focus has been placed on segmentation within digital marketing, in order to target specific markets in both business-to-business and business-to-consumer sectors.\nInfluencer marketing: Important nodes are identified within related communities, known as influencers. This is becoming an important concept in digital targeting. Influencers allow brands to take advantage of social media and the large audiences available on many of these platforms. It is possible to reach influencers via paid advertising, such as Facebook Advertising or Google Ads campaigns, or through sophisticated sCRM (social customer relationship management) software, such as SAP C4C, Microsoft Dynamics, Sage CRM and Salesforce CRM. Many universities now focus, at Masters level, on engagement strategies for influencers.To summarize, Pull digital marketing is characterized by consumers actively seeking marketing content while Push digital marketing occurs when marketers send messages without that content being actively sought by the recipients.\n\nOnline behavioral advertising is the practice of collecting information about a user's online activity over time, \"on a particular device and across different, unrelated websites, in order to deliver advertisements tailored to that user's interests and preferences.\" Such Advertisements are based on site retargeting are customized based on each user behavior and pattern.\nCollaborative Environment: A collaborative environment can be set up between the organization, the technology service provider, and the digital agencies to optimize effort, resource sharing, reusability and communications.  Additionally, organizations are inviting their customers to help them better understand how to service them. This source of data is called user-generated content. Much of this is acquired via company websites where the organization invites people to share ideas that are then evaluated by other users of the site. The most popular ideas are evaluated and implemented in some form. Using this method of acquiring data and developing new products can foster the organization's relationship with its customer as well as spawn ideas that would otherwise be overlooked. UGC is low-cost advertising as it is directly from the consumers and can save advertising costs for the organization.\nData-driven advertising: Users generate a lot of data in every step they take on the path of customer journey and brands can now use that data to activate their known audience with data-driven programmatic media buying. Without exposing customers' privacy, users' data can be collected from digital channels (e.g.: when the customer visits a website, reads an e-mail, or launches and interact with a brand's mobile app), brands can also collect data from real-world customer interactions, such as brick and mortar stores visits and from CRM and sales engines datasets. Also known as people-based marketing or addressable media, data-driven advertising is empowering brands to find their loyal customers in their audience and deliver in real time a much more personal communication, highly relevant to each customers' moment and actions.An important consideration today while deciding on a strategy is that the digital tools have democratized the promotional landscape.\n\nRemarketing: Remarketing plays a major role in digital marketing. This tactic allows marketers to publish targeted ads in front of an interest category or a defined audience, generally called searchers in web speak, they have either searched for particular products or services or visited a website for some purpose.\nGame advertising: Game ads are advertisements that exist within computer or video games. One of the most common examples of in-game advertising is billboards appearing in sports games. In-game ads also might appear as brand-name products like guns, cars, or clothing that exist as gaming status symbols.Six principles for building online brand content:\nDo not consider individuals as consumers;\nHave an editorial position;\nDefine an identity for the brand;\nMaintain a continuity of contents;\nEnsure a regular interaction with audience;\nHave a channel for events.The new digital era has enabled brands to selectively target their customers that may potentially be interested in their brand or based on previous browsing interests. Businesses can now use social media to select the age range, location, gender, and interests of whom they would like their targeted post to be seen. Furthermore, based on a customer's recent search history they can be \u2018followed\u2019 on the internet so they see advertisements from similar brands, products, and services, This allows businesses to target the specific customers that they know and feel will most benefit from their product or service, something that had limited capabilities up until the digital era.\n\nTourism marketing: Advanced tourism, responsible and sustainable tourism, social media and online tourism marketing, and geographic information systems. As a broader research field matures and attracts more diverse and in-depth academic research\n\n\n== Ineffective forms of digital marketing ==\nDigital marketing activity is still growing across the world according to the headline global marketing index. A study published in September 2018, found that global outlays on digital marketing tactics are approaching $100 billion. Digital media continues to rapidly grow. While the marketing budgets are expanding, traditional media is declining. Digital media helps brands reach consumers to engage with their product or service in a personalized way. Five areas, which are outlined as current industry practices that are often ineffective are prioritizing clicks, balancing search and display, understanding mobiles, targeting, viewability, brand safety and invalid traffic, and cross-platform measurement. Why these practices are ineffective and some ways around making these aspects effective are discussed surrounding the following points.\n\n\n=== Prioritizing clicks ===\nPrioritizing clicks refers to display click ads, although advantageous by being \u2018simple, fast and inexpensive\u2019 rates for display ads in 2016 is only 0.10 percent in the United States. This means one in a thousand click ads is relevant therefore having little effect. This displays that marketing companies should not just use click ads to evaluate the effectiveness of display advertisements.\n\n\n=== Balancing search and display ===\nBalancing search and display for digital display ads is important. marketers tend to look at the last search and attribute all of the effectiveness of this. This, in turn, disregards other marketing efforts, which establish brand value within the consumer's mind. ComScore determined through drawing on data online, produced by over one hundred multichannel retailers that digital display marketing poses strengths when compared with or positioned alongside, paid search. This is why it is advised that when someone clicks on a display ad the company opens a landing page, not its home page. A landing page typically has something to draw the customer in to search beyond this page. Commonly marketers see increased sales among people exposed to a search ad. But the fact of how many people you can reach with a display campaign compared to a search campaign should be considered.  Multichannel retailers have an increased reach if the display is considered in synergy with search campaigns. Overall, both search and display aspects are valued as display campaigns build awareness for the brand so that more people are likely to click on these digital ads when running a search campaign.\n\n\n=== Understanding Mobiles ===\nUnderstanding mobile devices is a significant aspect of digital marketing because smartphones and tablets are now responsible for 64% of the time US consumers are online. Apps provide a big opportunity as well as challenge for the marketers because firstly the app needs to be downloaded and secondly the person needs to actually use it. This may be difficult as \u2018half the time spent on smartphone apps occurs on the individuals single most used app, and almost 85% of their time on the top four rated apps\u2019. Mobile advertising can assist in achieving a variety of commercial objectives and it is effective due to taking over the entire screen, and voice or status is likely to be considered highly. However, the message must not be seen or thought of as intrusive.  Disadvantages of digital media used on mobile devices also include limited creative capabilities, and reach.  Although there are many positive aspects including the user's entitlement to select product information, digital media creating a flexible message platform and there is potential for direct selling.\n\n\n=== Cross-platform measurement ===\nThe number of marketing channels continues to expand, as measurement practices are growing in complexity. A cross-platform view must be used to unify audience measurement and media planning. Market researchers need to understand how the Omni-channel affects consumer's behavior, although when advertisements are on a consumer's device this does not get measured. Significant aspects to cross-platform measurement involve deduplication and understanding that you have reached an incremental level with another platform, rather than delivering more impressions against people that have previously been reached. An example is \u2018ESPN and comScore partnered on Project Blueprint discovering the sports broadcaster achieved a 21% increase in unduplicated daily reach thanks to digital advertising\u2019. Television and radio industries are the electronic media, which competes with digital and other technological advertising. Yet television advertising is not directly competing with online digital advertising due to being able to cross platform with digital technology. Radio also gains power through cross platforms, in online streaming content. Television and radio continue to persuade and affect the audience, across multiple platforms.\n\n\n=== Targeting, viewability, brand safety, and invalid traffic ===\nTargeting, viewability, brand safety, and invalid traffic all are aspects used by marketers to help advocate digital advertising. Cookies are a form of digital advertising, which are tracking tools within desktop devices, causing difficulty, with shortcomings including deletion by web browsers, the inability to sort between multiple users of a device, inaccurate estimates for unique visitors, overstating reach, understanding frequency, problems with ad servers, which cannot distinguish between when cookies have been deleted and when consumers have not previously been exposed to an ad. Due to the inaccuracies influenced by cookies, demographics in the target market are low and vary. Another element, which is affected by digital marketing, is \u2018viewability\u2019 or whether the ad was actually seen by the consumer. Many ads are not seen by a consumer and may never reach the right demographic segment. Brand safety is another issue of whether or not the ad was produced in the context of being unethical or having offensive content. Recognizing fraud when an ad is exposed is another challenge marketers face. This relates to invalid traffic as premium sites are more effective at detecting fraudulent traffic, although non-premium sites are more so the problem.\n\n\n== Channels ==\nDigital Marketing Channels are systems based on the Internet that can create, accelerate, and transmit product value from producer to a consumer terminal, through digital networks. Digital marketing is facilitated by multiple Digital Marketing channels, as an advertiser one's core objective is to find channels which result in maximum two-way communication and a better overall ROI for the brand. There are multiple digital marketing channels available namely:\nAffiliate marketing - Affiliate marketing is perceived to not be considered a safe, reliable, and easy means of marketing through online platforms. This is due to a lack of reliability in terms of affiliates that can produce the demanded number of new customers. As a result of this risk and bad affiliates, it leaves the brand prone to exploitation in terms of claiming commission that isn't honestly acquired. Legal means may offer some protection against this, yet there are limitations in recovering any losses or investment. Despite this, affiliate marketing allows the brand to market towards smaller publishers and websites with smaller traffic. Brands that choose to use this marketing often should beware of such risks involved and look to associate with affiliates in which rules are laid down between the parties involved to assure and minimize the risk involved.\nDisplay advertising - As the term implies, online display advertising deals with showcasing promotional messages or ideas to the consumer on the internet. This includes a wide range of advertisements like advertising blogs, networks, interstitial ads, contextual data, ads on search engines, classified or dynamic advertisements, etc. The method can target specific audience tuning in from different types of locals to view a particular advertisement, the variations can be found as the most productive element of this method.\nEmail marketing - Email marketing in comparison to other forms of digital marketing is considered cheap. It is also a way to rapidly communicate a message such as their value proposition to existing or potential customers. Yet this channel of communication may be perceived by recipients to be bothersome and irritating especially to new or potential customers, therefore the success of email marketing is reliant on the language and visual appeal applied. In terms of visual appeal, there are indications that using graphics/visuals that are relevant to the message which is attempting to be sent, yet less visual graphics to be applied with initial emails are more effective in-turn creating a relatively personal feel to the email. In terms of language, the style is the main factor in determining how captivating the email is. Using a casual tone invokes a warmer, gentler and more inviting feel to the email, compared to a more formal tone.\nSearch engine marketing - Search engine marketing (SEM) is a form of Internet marketing that involves the promotion of websites by increasing their visibility in search engine results pages (SERPs) primarily through paid advertising. SEM may incorporate Search engine optimization, which adjusts or rewrites website content and site architecture to achieve a higher ranking in search engine results pages to enhance pay per click (PPC) listings.\nSocial Media Marketing - The term 'Digital Marketing' has a number of marketing facets as it supports different channels used in and among these, comes the Social Media. When we use social media channels (Facebook, Twitter, Pinterest, Instagram, Google+, etc.) to market a product or service, the strategy is called Social Media Marketing. It is a procedure wherein strategies are made and executed to draw in traffic for a website or to gain the attention of buyers over the web using different social media platforms.\nSocial networking service - A social networking service is an online platform which people use to build social networks or social relations with other people who share similar personal or career interests, activities, backgrounds or real-life connections\nIn-game advertising - In-Game advertising is defined as the \"inclusion of products or brands within a digital game.\" The game allows brands or products to place ads within their game, either in a subtle manner or in the form of an advertisement banner. There are many factors that exist in whether brands are successful in the advertising of their brand/product, these being: Type of game, technical platform, 3-D and 4-D technology, game genre, congruity of brand and game, prominence of advertising within the game. Individual factors consist of attitudes towards placement advertisements, game involvement, product involvement, flow, or entertainment. The attitude towards the advertising also takes into account not only the message shown but also the attitude towards the game. Dependent on how enjoyable the game is will determine how the brand is perceived, meaning if the game isn't very enjoyable the consumer may subconsciously have a negative attitude towards the brand/product being advertised. In terms of Integrated Marketing Communication \"integration of advertising in digital games into the general advertising, communication, and marketing strategy of the firm\" is important as it results in a more clarity about the brand/product and creates a larger overall effect.\nOnline public relations - The use of the internet to communicate with both potential and current customers in the public realm.\nVideo advertising - This type of advertising in terms of digital/online means are advertisements that play on online videos e.g., YouTube videos.  This type of marketing has seen an increase in popularity over time. Online Video Advertising usually consists of three types: Pre-Roll advertisements which play before the video is watched, Mid-Roll advertisements which play during the video, or Post-Roll advertisements which play after the video is watched. Post-roll advertisements were shown to have better brand recognition in relation to the other types, where-as \"ad-context congruity/incongruity plays an important role in reinforcing ad memorability\". Due to selective attention from viewers, there is the likelihood that the message may not be received. The main advantage of video advertising is that it disrupts the viewing experience of the video and therefore there is a difficulty in attempting to avoid them. How a consumer interacts with online video advertising can come down to three stages: Pre attention, attention, and behavioral decision. These online advertisements give the brand/business options and choices. These consist of length, position, adjacent video content which all directly affect the effectiveness of the produced advertisement time, therefore manipulating these variables will yield different results. The length of the advertisement has shown to affect memorability where-as a longer duration resulted in increased brand recognition. This type of advertising, due to its nature of interruption of the viewer, it is likely that the consumer may feel as if their experience is being interrupted or invaded, creating negative perception of the brand. These advertisements are also available to be shared by the viewers, adding to the attractiveness of this platform. Sharing these videos can be equated to the online version of word by mouth marketing, extending number of people reached. Sharing videos creates six different outcomes: these being \"pleasure, affection, inclusion, escape, relaxation, and control\". As well, videos that have entertainment value are more likely to be shared, yet pleasure is the strongest motivator to pass videos on. Creating a \u2018viral\u2019 trend from a mass amount of a brand advertisement can maximize the outcome of an online video advert whether it be positive or a negative outcome.\nNative Advertising - This involves the placement of paid content that replicates the look, feel, and oftentimes, the voice of a platform's existing content. It is most effective when used on digital platforms like websites, newsletters, and social media. Can be somewhat controversial as some critics feel it intentionally deceives consumers.\nContent Marketing - This is an approach to marketing that focuses on gaining and retaining customers by offering helpful content to customers that improves the buying experience and creates brand awareness.  A brand may use this approach to hold a customer\u2019s attention with the goal of influencing potential purchase decisions.\nSponsored Content - This utilises content created and paid for by a brand to promote a specific product or service.\nInbound Marketing- a market strategy that involves using content as a means to attract customers to a brand or product. Requires extensive research into the behaviors, interests, and habits of the brand's target market.\nSMS Marketing: Although the popularity is decreasing day by day, still SMS marketing plays huge role to bring new user, provide direct updates, provide new offers etc.\nPush Notification: In this digital era, Push Notification responsible for bringing new and abandoned customer through smart segmentation. Many online brands are using this to provide personalised appeals depending on the scenario of customer acquisition.It is important for a firm to reach out to consumers and create a two-way communication model, as digital marketing allows consumers to give back feedback to the firm on a community-based site or straight directly to the firm via email. Firms should seek this long-term communication relationship by using multiple forms of channels and using promotional strategies related to their target consumer as well as word-of-mouth marketing.\n\n\n== Benefits of digital marketing ==\nPossible benefits of digital marketing include:\n\nany information that is needed is accessible at any time and/or place\nsurpasses internet marketing and also possesses alternatives choices without the internet needed\ntop in presenting beneficial ways and features that reach, inform, engage, offer, and sell services and products to consumers\nbusinesses can attain data that present target audiences based on their age, location, interests, and education\nlow investment, the cost per lead is 61% less expensive than traditional marketing\nable to reach every mobile user, there are over 14 billion worldwide mobile devices and with a projection to grow to almost 18 billion by the year 2024\n\n\n== Self-regulation ==\nThe ICC Code has integrated rules that apply to marketing communications using digital interactive media throughout the guidelines. There is also an entirely updated section dealing with issues specific to digital interactive media techniques and platforms. Code self-regulation on the use of digital interactive media includes:\n\nClear and transparent mechanisms to enable consumers to choose not to have their data collected for advertising or marketing purposes;\nClear indication that a social network site is commercial and is under the control or influence of a marketer;\nLimits are set so that marketers communicate directly only when there are reasonable grounds to believe that the consumer has an interest in what is being offered;\nRespect for the rules and standards of acceptable commercial behavior in social networks and the posting of marketing messages only when the forum or site has clearly indicated its willingness to receive them;\nSpecial attention and protection for children.\n\n\n== Strategy ==\n\n\n=== Planning ===\nDigital marketing planning is a term used in marketing management.  It describes the first stage of forming a digital marketing strategy for the wider digital marketing system. The difference between digital and traditional marketing planning is that it uses digitally based communication tools and technology such as Social, Web, Mobile, Scannable Surface. Nevertheless, both are aligned with the vision, the mission of the company and the overarching business strategy.\n\n\n=== Stages of planning ===\nUsing Dr. Dave Chaffey's approach, the digital marketing planning (DMP) has three main stages: Opportunity, Strategy, and Action. He suggests that any business looking to implement a successful digital marketing strategy must structure their plan by looking at opportunity, strategy and action. This generic strategic approach often has phases of situation review, goal setting, strategy formulation, resource allocation and monitoring.\n\n\n==== Opportunity ====\nTo create an effective DMP, a business first needs to review the marketplace and set 'SMART' (Specific, Measurable, Actionable, Relevant, and Time-Bound) objectives. They can set SMART objectives by reviewing the current benchmarks and key performance indicators (KPIs) of the company and competitors. It is pertinent that the analytics used for the KPIs be customized to the type, objectives, mission, and vision of the company.Companies can scan for marketing and sales opportunities by reviewing their own outreach as well as influencer outreach. This means they have competitive advantage because they are able to analyse their co-marketers influence and brand associations.To seize the opportunity, the firm should summarize its current customers' personas and purchase journey from this they are able to deduce their digital marketing capability. This means they need to form a clear picture of where they are currently and how many resources, they can allocate for their digital marketing strategy i.e., labor, time, etc. By summarizing the purchase journey, they can also recognize gaps and growth for future marketing opportunities that will either meet objectives or propose new objectives and increase profit.\n\n\n==== Strategy ====\nTo create a planned digital strategy, the company must review their digital proposition (what you are offering to consumers) and communicate it using digital customer targeting techniques. So, they must define online value proposition (OVP), this means the company must express clearly what they are offering customers online e.g., brand positioning.\nThe company should also (re)select target market segments and personas and define digital targeting approaches.\nAfter doing this effectively, it is important to review the marketing mix for online options. The marketing mix comprises the 4Ps \u2013 Product, Price, Promotion, and Place. Some academics have added three additional elements to the traditional 4Ps of marketing Process, Place, and Physical appearance making it 7Ps of marketing.\n\n\n==== Action ====\nThe third and final stage requires the firm to set a budget and management systems. These must be measurable touchpoints, such as the audience reached across all digital platforms. Furthermore, marketers must ensure the budget and management systems are integrating the paid, owned, and earned media of the company. The Action and final stage of planning also requires the company to set in place measurable content creation e.g. oral, visual or written online media.After confirming the digital marketing plan, a scheduled format of digital communications (e.g. Gantt Chart) should be encoded throughout the internal operations of the company. This ensures that all platforms used fall in line and complement each other for the succeeding stages of digital marketing strategy.\n\n\n=== Understanding the market ===\nOne way marketers can reach out to consumers and understand their thought process is through what is called an empathy map. An empathy map is a four-step process. The first step is through asking questions that the consumer would be thinking in their demographic. The second step is to describe the feelings that the consumer may be having. The third step is to think about what the consumer would say in their situation. The final step is to imagine what the consumer will try to do based on the other three steps. This map is so marketing teams can put themselves in their target demographics shoes. Web Analytics are also a very important way to understand consumers. They show the habits that people have online for each website. One particular form of these analytics is predictive analytics which helps marketers figure out what route consumers are on. This uses the information gathered from other analytics and then creates different predictions of what people will do so that companies can strategize on what to do next, according to the people's trends.\nConsumer behavior: the habits or attitudes of a consumer that influences the buying process of a product or service. Consumer behavior impacts virtually every stage of the buying process specifically in relation to digital environments and devices.\nPredictive analytics: a form of data mining that involves using existing data to predict potential future trends or behaviors.  Can assist companies in predicting future behavior of customers.\nBuyer persona: employing research of consumer behavior regarding habits like brand awareness and buying behavior to profile prospective customers. Establishing a buyer persona helps a company better understand their audience and their specific wants/needs.\nMarketing Strategy: strategic planning employed by a brand to determine potential positioning within a market as well as the prospective target audience. It involves two key elements: segmentation and positioning. By developing a marketing strategy, a company is able to better anticipate and plan for each step in the marketing and buying process.\n\n\n== Sharing economy ==\nThe \"sharing economy\" refers to an economic pattern that aims to obtain a resource that is not fully used. Nowadays, the sharing economy has had an unimagined effect on many traditional elements including labor, industry, and distribution system. This effect is not negligible that some industries are obviously under threat. The sharing economy is influencing the traditional marketing channels by changing the nature of some specific concept including ownership, assets, and recruitment.Digital marketing channels and traditional marketing channels are similar in function that the value of the product or service is passed from the original producer to the end user by a kind of supply chain. Digital Marketing channels, however, consist of internet systems that create, promote, and deliver products or services from producer to consumer through digital networks. Increasing changes to marketing channels has been a significant contributor to the expansion and growth of the sharing economy. Such changes to marketing channels has prompted unprecedented and historic growth. In addition to this typical approach, the built-in control, efficiency and low cost of digital marketing channels is an essential features in the application of sharing economy.Digital marketing channels within the sharing economy are typically divided into three domains including, e-mail, social media, and search engine marketing or SEM.\nE-mail- a form of direct marketing characterized as being informative, promotional, and often a means of customer relationship management. Organization can update the activity or promotion information to the user by subscribing the newsletter mail that happened in consuming. Success is reliant upon a company\u2019s ability to access contact information from its past, present, and future clientele.\nSocial Media- Social media has the capability to reach a larger audience in a shorter time frame than traditional marketing channels. This makes social media a powerful tool for consumer engagement and the dissemination of information.\nSearch Engine Marketing or SEM- Requires more specialized knowledge of the technology embedded in online platforms. This marketing strategy requires long-term commitment and dedication to the ongoing improvement of a company\u2019s digital presence.Other emerging digital marketing channels, particularly branded mobile apps, have excelled in the sharing economy. Branded mobile apps are created specifically to initiate engagement between customers and the company. This engagement is typically facilitated through entertainment, information, or market transaction.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading =="}, {"id": 100, "title": "List of states and territories of the United States", "content": "The United States of America is a federal republic consisting of 50 states, a federal district (Washington, D.C., the capital city of the United States), five major territories, and various minor islands. Both the states and the United States as a whole are each sovereign jurisdictions. The Tenth Amendment to the United States Constitution allows states to exercise all powers of government not delegated to the federal government. Each state has its own constitution and government, and all states and their residents are represented in the federal Congress, a bicameral legislature consisting of the Senate and the House of Representatives. Each state is represented by two senators, while representatives are distributed among the states in proportion to the most recent constitutionally mandated decennial census. Additionally, each state is entitled to select a number of electors to vote in the Electoral College, the body that elects the president of the United States, equal to the total of representatives and senators in Congress from that state. The federal district does not have representatives in the Senate, but has a non-voting delegate in the House, and it is also entitled to electors in the Electoral College. Congress can admit more states, but it cannot create a new state from territory of an existing state or merge two or more states into one without the consent of all states involved, and each new state is admitted on an equal footing with the existing states.The United States has control over fourteen territories. Five of them (American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands) have a permanent, nonmilitary population, while nine of them (the United States Minor Outlying Islands) do not. With the exception of Navassa Island, Puerto Rico, and the U.S. Virgin Islands, which are located in the Caribbean, all territories are located in the Pacific Ocean. One territory, Palmyra Atoll, is considered to be incorporated, meaning the full body of the Constitution has been applied to it; the other territories are unincorporated, meaning the Constitution does not fully apply to them. Ten territories (the Minor Outlying Islands and American Samoa) are considered to be unorganized, meaning they have not had an organic act enacted by Congress; the four other territories are organized, meaning an organic act has been enacted by Congress. The five inhabited territories each have limited autonomy in addition to having territorial legislatures and governors, but residents cannot vote in federal elections, although all are represented by non-voting delegates in the House.\nThe largest state by population is California, with a population of 39,538,223 people, while the smallest is Wyoming, with a population of 576,851 people; the federal district has a larger population (689,545) than both Wyoming and Vermont. The largest state by area is Alaska, encompassing 665,384 square miles (1,723,337 square kilometers), while the smallest is Rhode Island, encompassing 1,545 square miles (4,001 square kilometers). The most recent states to be admitted, Alaska and Hawaii, were admitted in 1959. The largest territory by population is Puerto Rico, with a population of 3,285,874 people (larger than 21 states), while the smallest is the Northern Mariana Islands, with a population of 47,329 people. Puerto Rico is the largest territory by area, encompassing 5,325 square miles (13,791 square kilometers); the smallest territory, Kingman Reef, encompasses only 0.005 square miles (0.01 square kilometers).\n\n\n== States ==\n\n\n== Federal district ==\n\n\n== Territories ==\n\n\n=== Inhabited territories ===\n\n\n=== Uninhabited territories ===\n\n\n=== Disputed territories ===\n\n\n== See also ==\nAboriginal title in the United States\nHistoric regions of the United States\nList of Indian reservations in the United States\nList of regions of the United States\nLists of U.S. state topics\nLocal government in the United States\nOrganized incorporated territories of the United States\nProposals for a 51st state\nTerritorial evolution of the United States\nU.S. territorial sovereignty\nCompact of Free Association\n\n\n== Explanatory notes ==\n\n\n== References ==\n\nRadan, Peter (2007). Creating New States: Theory and Practice of Secession. Ashgate Publishing, Ltd. ISBN 9780754671633.\n\n\n== External links ==\n\nState Resource Guides, from the Library of Congress\nState and Territorial Governments on USA.gov"}]